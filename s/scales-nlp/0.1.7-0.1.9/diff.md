# Comparing `tmp/scales_nlp-0.1.7-py2.py3-none-any.whl.zip` & `tmp/scales_nlp-0.1.9-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,23 @@
-Zip file size: 14736 bytes, number of entries: 18
--rw-rw-r--  2.0 unx      289 b- defN 22-Sep-27 22:48 scales_nlp/__init__.py
--rw-rw-r--  2.0 unx     3372 b- defN 22-Sep-20 12:26 scales_nlp/analyzer.py
--rw-rw-r--  2.0 unx     2185 b- defN 22-Sep-09 02:33 scales_nlp/data.py
--rw-rw-r--  2.0 unx     2180 b- defN 22-Sep-09 04:37 scales_nlp/losses.py
--rw-rw-r--  2.0 unx     1398 b- defN 22-Jul-23 15:52 scales_nlp/metrics.py
--rw-rw-r--  2.0 unx     8413 b- defN 22-Sep-09 15:39 scales_nlp/pipelines.py
--rw-rw-r--  2.0 unx     9165 b- defN 22-Sep-19 02:32 scales_nlp/routines.py
--rw-rw-r--  2.0 unx     5510 b- defN 22-Jul-23 17:10 scales_nlp/train.py
--rw-rw-r--  2.0 unx     1477 b- defN 22-Sep-08 22:45 scales_nlp/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 22-Jul-22 16:38 scales_nlp/cli/__init__.py
--rw-rw-r--  2.0 unx     1404 b- defN 22-Jul-22 16:40 scales_nlp/cli/configure.py
--rw-rw-r--  2.0 unx      225 b- defN 22-Jul-22 17:27 scales_nlp/cli/main.py
--rw-rw-r--  2.0 unx     3277 b- defN 22-Sep-09 05:01 scales_nlp/cli/train.py
--rw-rw-r--  2.0 unx      427 b- defN 22-Sep-27 22:50 scales_nlp-0.1.7.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 22-Sep-27 22:50 scales_nlp-0.1.7.dist-info/WHEEL
--rw-rw-r--  2.0 unx       47 b- defN 22-Sep-27 22:50 scales_nlp-0.1.7.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       11 b- defN 22-Sep-27 22:50 scales_nlp-0.1.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1428 b- defN 22-Sep-27 22:50 scales_nlp-0.1.7.dist-info/RECORD
-18 files, 40918 bytes uncompressed, 12412 bytes compressed:  69.7%
+Zip file size: 222172 bytes, number of entries: 21
+-rw-rw-r--  2.0 unx      477 b- defN 22-Dec-05 10:46 scales_nlp/__init__.py
+-rw-rw-r--  2.0 unx     4935 b- defN 22-Nov-02 15:48 scales_nlp/cli.py
+-rw-rw-r--  2.0 unx     2923 b- defN 22-Dec-05 11:15 scales_nlp/config.py
+-rw-rw-r--  2.0 unx     5322 b- defN 22-Dec-05 11:07 scales_nlp/datasets.py
+-rw-rw-r--  2.0 unx    12798 b- defN 22-Dec-16 15:48 scales_nlp/docket.py
+-rw-rw-r--  2.0 unx      152 b- defN 22-Nov-16 20:32 scales_nlp/labels.py
+-rw-rw-r--  2.0 unx     2191 b- defN 22-Nov-01 04:23 scales_nlp/losses.py
+-rw-rw-r--  2.0 unx    11585 b- defN 22-Dec-16 11:17 scales_nlp/pipelines.py
+-rw-rw-r--  2.0 unx    10410 b- defN 22-Dec-05 11:08 scales_nlp/routines.py
+-rw-rw-r--  2.0 unx     4144 b- defN 22-Nov-23 13:37 scales_nlp/search.py
+-rw-rw-r--  2.0 unx     3413 b- defN 22-Dec-15 22:43 scales_nlp/utils.py
+-rw-rw-r--  2.0 unx       22 b- defN 22-Dec-20 02:07 scales_nlp/version.py
+-rw-rw-r--  2.0 unx    14654 b- defN 22-Nov-16 18:45 scales_nlp/data/courts.csv
+-rw-rw-r--  2.0 unx   286909 b- defN 22-Nov-23 13:44 scales_nlp/data/docket_examples.csv
+-rw-rw-r--  2.0 unx    14654 b- defN 22-Nov-16 18:45 scales_nlp-0.1.9.data/data/scales_nlp/courts.csv
+-rw-rw-r--  2.0 unx   286909 b- defN 22-Nov-23 13:44 scales_nlp-0.1.9.data/data/scales_nlp/docket_examples.csv
+-rw-rw-r--  2.0 unx      444 b- defN 22-Dec-20 02:07 scales_nlp-0.1.9.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 22-Dec-20 02:07 scales_nlp-0.1.9.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       46 b- defN 22-Dec-20 02:07 scales_nlp-0.1.9.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       11 b- defN 22-Dec-20 02:07 scales_nlp-0.1.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     1737 b- defN 22-Dec-20 02:07 scales_nlp-0.1.9.dist-info/RECORD
+21 files, 663846 bytes uncompressed, 219364 bytes compressed:  67.0%
```

## zipnote {}

```diff
@@ -1,55 +1,64 @@
 Filename: scales_nlp/__init__.py
 Comment: 
 
-Filename: scales_nlp/analyzer.py
+Filename: scales_nlp/cli.py
 Comment: 
 
-Filename: scales_nlp/data.py
+Filename: scales_nlp/config.py
 Comment: 
 
-Filename: scales_nlp/losses.py
+Filename: scales_nlp/datasets.py
+Comment: 
+
+Filename: scales_nlp/docket.py
+Comment: 
+
+Filename: scales_nlp/labels.py
 Comment: 
 
-Filename: scales_nlp/metrics.py
+Filename: scales_nlp/losses.py
 Comment: 
 
 Filename: scales_nlp/pipelines.py
 Comment: 
 
 Filename: scales_nlp/routines.py
 Comment: 
 
-Filename: scales_nlp/train.py
+Filename: scales_nlp/search.py
 Comment: 
 
 Filename: scales_nlp/utils.py
 Comment: 
 
-Filename: scales_nlp/cli/__init__.py
+Filename: scales_nlp/version.py
+Comment: 
+
+Filename: scales_nlp/data/courts.csv
 Comment: 
 
-Filename: scales_nlp/cli/configure.py
+Filename: scales_nlp/data/docket_examples.csv
 Comment: 
 
-Filename: scales_nlp/cli/main.py
+Filename: scales_nlp-0.1.9.data/data/scales_nlp/courts.csv
 Comment: 
 
-Filename: scales_nlp/cli/train.py
+Filename: scales_nlp-0.1.9.data/data/scales_nlp/docket_examples.csv
 Comment: 
 
-Filename: scales_nlp-0.1.7.dist-info/METADATA
+Filename: scales_nlp-0.1.9.dist-info/METADATA
 Comment: 
 
-Filename: scales_nlp-0.1.7.dist-info/WHEEL
+Filename: scales_nlp-0.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: scales_nlp-0.1.7.dist-info/entry_points.txt
+Filename: scales_nlp-0.1.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: scales_nlp-0.1.7.dist-info/top_level.txt
+Filename: scales_nlp-0.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: scales_nlp-0.1.7.dist-info/RECORD
+Filename: scales_nlp-0.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## scales_nlp/__init__.py

```diff
@@ -1,7 +1,22 @@
-from scales_nlp.analyzer import DocketAnalyzer
-import scales_nlp.data as data
-import scales_nlp.losses as losses
+from scales_nlp.version import __version__
+
+from scales_nlp.config import config
+from scales_nlp.cli import main as cli
+from scales_nlp.utils import *
+
+from scales_nlp.search import (
+    SearchIndex,
+    QuantizedSearchIndex,
+)
+
+from scales_nlp.docket import Docket
+
 from scales_nlp.pipelines import pipeline
+import scales_nlp.losses as losses
+import scales_nlp.datasets as datasets
 from scales_nlp.routines import training_routine
-from scales_nlp.utils import load_config
-from scales_nlp.cli.main import main as cli
+
+if 0:
+    import scales_nlp.data as data
+
+
```

## scales_nlp/losses.py

```diff
@@ -1,11 +1,12 @@
 from sentence_transformers.losses import BatchHardTripletLossDistanceFunction
 import torch
 
-class MultiLabelTripletLoss():
+
+class MultiDimensionalTripletLoss():
     def __init__(self, triplet_margin=1, dynamic_margin=True, discrete_labels=True):
         self.triplet_margin = triplet_margin
         self.dynamic_margin = True
         self.discrete_labels = discrete_labels
     
     def __call__(self, embeddings, labels):
         distance_metric = BatchHardTripletLossDistanceFunction.eucledian_distance
@@ -18,22 +19,23 @@
 
         if self.dynamic_margin:
             triplet_margin = -1 * label_dist_diff * self.triplet_margin
         else:
             triplet_margin = self.triplet_margin
 
         triplet_loss = emb_dist_diff + triplet_margin
-
         mask = self.get_triplet_mask(label_dist_diff)
         triplet_loss = mask.float() * triplet_loss
 
         triplet_loss = triplet_loss.reshape(triplet_loss.shape[0], -1)
         triplet_loss = triplet_loss.max(1).values
         triplet_loss = triplet_loss.mean()
         return triplet_loss
+
+
     
     @staticmethod
     def pairwise_dist_diff(pairwise_dist):
         positive_dist = pairwise_dist.unsqueeze(2)
         negative_dist = pairwise_dist.unsqueeze(1)
         return positive_dist - negative_dist
     
@@ -46,8 +48,10 @@
         indices_equal = torch.eye(label_dist_diff.size(0), device=label_dist_diff.device).bool()
         indices_not_equal = ~indices_equal
         i_not_equal_j = indices_not_equal.unsqueeze(2)
         i_not_equal_k = indices_not_equal.unsqueeze(1)
         j_not_equal_k = indices_not_equal.unsqueeze(0)
         distinct_indices = (i_not_equal_j & i_not_equal_k) & j_not_equal_k
         valid_labels = label_dist_diff < 0
-        return valid_labels & distinct_indices
+        return valid_labels & distinct_indices
+
+
```

## scales_nlp/pipelines.py

```diff
@@ -1,98 +1,130 @@
+from typing import Union, Optional, List, Dict, Any
 from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForTokenClassification
 import torch
 import numpy as np
 from toolz import partition_all
 from tqdm import tqdm
-from scales_nlp.utils import convert_default_binary_outputs, load_config
-
-
-def pipeline(pipeline_name, task=None, **kwargs):
-    task_pipelines = {
-        'classification': ClassificationPipeline,
-        'multi-label-classification': MultiLabelClassificationPipeline,
-        'ner': TokenClassificationPipeline,
-        'token-classification': TokenClassificationPipeline,
-        'sentence-encoding': SentenceEncodingPipeline,
-    }
-
-    pipelines = {
-        'ontology': OntologySingleLabelPipeline,
-        'docket-triplet-encoder': DocketTripletPipeline,
-    }
-
-    if task is not None:
-        if task in task_pipelines:
-            return task_pipelines[task](pipeline_name, **kwargs)
-        else:
-            raise Exception("'%s' is not a valid task name.  This should be one of 'classification', 'multi-label-classification'" % task)
-
-    if pipeline_name in pipelines:
-            return pipelines[pipeline_name](pipeline_name, **kwargs)
-    else:
-        raise Exception("'%s' is not a valid pipeline name.  This should be one of 'ontology'" % pipeline_name)
+from scales_nlp.utils import convert_default_binary_outputs
+from scales_nlp import config
 
 
 class BasePipeline(object):
-    def __init__(self, model_name, max_length=512, device=None, use_auth_token=False, **kwargs):
-        config = load_config()
+    """Base class for all pipelines."""
+    def __init__(self, model_name: str, max_length: int = 512, device: int = None, use_auth_token: bool = False, **kwargs):
+        """Initialize the pipeline.
+
+        :param model_name: Name of the Hugging Face model to use
+        :param max_length: Maximum length of input tokens
+        :param device: Device to use for inference. Use -1 for CPU, or the index of the GPU, if None, then will use GPU if available.
+        :param use_auth_token: Whether to use the Hugging Face auth token to download the model
+        """
 
         use_auth_token = use_auth_token if not self.require_auth_token else True
         if use_auth_token == True and config['HUGGING_FACE_TOKEN'] is not None:
             use_auth_token = config['HUGGING_FACE_TOKEN']
 
         self.device = device if device is not None else -1 if not torch.cuda.is_available() else torch.cuda.current_device()
         self.max_length = max_length
         self.use_auth_token = use_auth_token
         self.model_name = self.get_model_name(model_name, **kwargs)
         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_auth_token=self.use_auth_token)
         self.model = self.load_model()
-    
-    def place_on_device(self, obj):
-        if self.device >= 0:
+
+    def place_on_device(self, obj: Union[torch.Tensor, torch.nn.Module]) -> Union[torch.Tensor, torch.nn.Module]:
+        """Method for placing models and tensors on the correct device.
+
+        :param obj: Object to place on device
+        :return: Object on device
+        """
+        if self.device != -1:
             return obj.to(self.device)
         else:
             return obj.to('cpu')
-    
-    def get_model_name(self, model_name, **kwargs):
+
+    def get_model_name(self, model_name: str, **kwargs) -> str:
+        """Trivial method for getting the model name. Pipelines that rely on a specific model should override this method.
+        
+        :param model_name: Name of the model
+        :return: Updated of the model
+        """
         return model_name
 
-    def process_inputs(self, examples):
+    def process_inputs(self, examples: List[str]) -> List[str]:
+        """Hook for processing inputs before tokenization.
+        
+        :param examples: Input examples
+        :return: Processed inputs
+        """
         return examples
     
-    def process_predictions(self, predictions):
+    def process_predictions(self, examples, predictions):
+        """Hook for processing predictions before returning them.
+        
+        :param examples: Input examples
+        :param predictions: Predictions from model
+        :return: Processed predictions
+        """
         return predictions
     
-    def tokenize(self, texts):
+    def tokenize(self, texts: List[str]):
+        """Tokenize method.
+        
+        :param texts: List of texts to tokenize
+        :return: Tokenized inputs
+        """
         return self.tokenizer(texts, padding='max_length', max_length=self.max_length, truncation=True, return_tensors='pt')
     
-    def generate_batches(self, examples, batch_size):
+    def generate_batches(self, examples: List, batch_size: int) -> List[List]:
+        """Generate batches of examples.
+        
+        :param examples: List of examples
+        :param batch_size: Batch size
+        :return: List of batched examples
+        """
         return [list(batch) for batch in partition_all(batch_size, examples)]
         
     def load_model(self):
+        """Load the model. Must be implemented by all pipelines.
+        
+        :return: Loaded Hugging Face model
+        """
         raise NotImplementedError('load_model not implemented')
     
     def batch_predict(self, inputs, **kwargs):
+        """Predict on a batch of inputs. Must be implemented by all pipelines.
+        
+        :param inputs: Batch of inputs
+        :return: Predictions
+        """
         raise NotImplementedError('batch_predict not implemented')
 
-    def __call__(self, examples, batch_size=4, verbose=True, **kwargs):
+    def __call__(self, examples: Any, batch_size: int = 4, verbose: bool = True, **kwargs):
+        """Main inference method, which returns predictions for a list of example texts.
+        
+        :param examples: List of example texts
+        :param batch_size: Batch size
+        :param verbose: Whether to show a progress bar
+        :return: Predictions
+        """
         examples = self.process_inputs(examples)
         batches = self.generate_batches(examples, batch_size)
         predictions = []
         for batch in tqdm(batches, disable=not verbose):
             inputs = self.place_on_device(self.tokenize(batch))
             predictions += self.batch_predict(inputs, **kwargs)
-        return self.process_predictions(predictions)
+        return self.process_predictions(examples, predictions)
 
     @property
     def require_auth_token(self):
+        """Override this property to force pipeline to require an auth token."""
         return False
 
 
-# Task Pipelines
+# TASK PIPELINES
 class ClassificationPipeline(BasePipeline):
     def load_model(self):
         model = AutoModelForSequenceClassification.from_pretrained(self.model_name, use_auth_token=self.use_auth_token)
         model = self.place_on_device(model)
         return model
     
     def batch_predict(self, inputs, return_scores=False, **kwargs):
@@ -136,39 +168,46 @@
         model = AutoModelForTokenClassification.from_pretrained(self.model_name, use_auth_token=self.use_auth_token)
         model = self.place_on_device(model)
         return model
     
     def batch_predict(self, inputs, **kwargs):
         offsets = inputs.pop('offset_mapping')
         outputs = self.model(**inputs)
+        attention_mask = inputs['attention_mask'].detach().cpu()
+        input_ids = inputs['input_ids'].detach().cpu()
         logits = outputs.logits.detach().cpu()
         scores = torch.softmax(logits, dim=-1)
+        labels = torch.argmax(scores, dim=-1)
+
         predictions = []
-        for i in range(len(scores)):
+        for i, example_scores in enumerate(scores):
+            example_offsets = offsets[i][attention_mask[i] == 1].tolist()
+            if len(example_offsets) > 1:
+                example_offsets[-1] = [example_offsets[-2][1], example_offsets[-2][1]]
+            example_labels = labels[i][attention_mask[i] == 1].tolist()
             prediction = []
-            example_offsets = offsets[i]
-            input_ids = inputs['input_ids'][i]
-            example_scores = scores[i][inputs['attention_mask'][i] == 1]
-            example_scores, labels = torch.max(example_scores, dim=-1)
-            labels = [self.model.config.id2label[x.item()] for x in labels]
-            for token_idx in range(len(labels)):
-                label = labels[token_idx].split('-')
-                pos, entity = label[0], label[-1]
-                if pos == 'B' or (pos == 'I' and (len(prediction) == 0 or prediction[-1]['entity'] != entity)):
+            for token_idx, label in enumerate(example_labels):
+                label_name = self.model.config.id2label[label]
+                entity = label_name.split('-')[-1]
+                position = label_name.split('-')[0]
+
+                if position == 'B' or \
+                        (position == 'I' and (not prediction or prediction[-1]['entity'] != entity)):
                     prediction.append({
                         'entity': entity,
-                        'start': example_offsets[token_idx][0].item(),
-                        'end': example_offsets[token_idx][1].item(),
-                        'score': [example_scores[token_idx].item()],
-                        'text': [input_ids[token_idx]]
+                        'start': example_offsets[token_idx][0],
+                        'end': example_offsets[token_idx][1],
+                        'score': [example_scores[token_idx][label].item()],
+                        'text': [input_ids[i][token_idx].item()],
                     })
-                if pos == 'I':
-                    prediction[-1]['end'] = example_offsets[token_idx][1].item()
-                    prediction[-1]['score'].append(example_scores[token_idx].item())
-                    prediction[-1]['text'].append(input_ids[token_idx])
+                elif position == 'I':
+                    prediction[-1]['end'] = example_offsets[token_idx][1]
+                    prediction[-1]['score'].append(example_scores[token_idx][label].item())
+                    prediction[-1]['text'].append(input_ids[i][token_idx].item())
+
             for span in prediction:
                 span['score'] = np.mean(span['score'])
                 span['text'] = self.tokenizer.decode(span['text'])
             predictions.append(prediction)
         return predictions
 
 
@@ -178,33 +217,74 @@
         model = self.place_on_device(model)
         return model
     
     def batch_predict(self, inputs, **kwargs):
         outputs = self.model(**inputs)
         return [outputs[0].detach().cpu().numpy()[:,0,:]]
     
-    def process_predictions(self, predictions):
+    def process_predictions(self, examples, predictions):
         return np.concatenate(predictions, axis=0)
 
 
-# Custom Pipelines
+# CUSTOM PIPELINES
 class OntologySingleLabelPipeline(ClassificationPipeline):
     def get_model_name(self, model_name, **kwargs):
         return 'scales-okn/ontology-' + kwargs['label_name'].replace(' ', '-')
 
-    def process_predictions(self, predictions):
+    def process_predictions(self, examples, predictions):
         return convert_default_binary_outputs(predictions)
     
     @property
     def require_auth_token(self):
         return True
 
 
-class DocketTripletPipeline(SentenceEncodingPipeline):
+class DocketClassificationPipeline(MultiLabelClassificationPipeline):
+    def get_model_name(self, model_name, **kwargs):
+        return 'scales-okn/docket-classifier'
+
+    def process_predictions(self, examples, predictions):
+
+        updated_predictions = []
+        for example, pred in zip(examples, predictions):
+            
+            updated_predictions.append(pred)
+
+        return updated_predictions
+
+
+class DocketEncoderPipeline(SentenceEncodingPipeline):
     def get_model_name(self, model_name, **kwargs):
         return 'scales-okn/docket-triplet-vectors'
     
     @property
     def require_auth_token(self):
         return True
 
 
+def pipeline(pipeline_name: str, **kwargs) -> BasePipeline:
+    """Load a pipeline for a given task.
+
+    :param pipeline_name: Name of the pipeline to load
+    :param model_name: (str, optional) If using a generic task pipeline, must specify a Hugging Face model name
+    """
+
+    pipelines = {
+        # Generic pipelines
+        'classification': ClassificationPipeline,
+        'multi-label-classification': MultiLabelClassificationPipeline,
+        'ner': TokenClassificationPipeline,
+        'token-classification': TokenClassificationPipeline,
+        'sentence-encoding': SentenceEncodingPipeline,
+
+        # SCALES-NLP pipelines
+        'ontology': OntologySingleLabelPipeline,
+        'docket-classifier': DocketClassificationPipeline,
+        'docket-encoder': DocketEncoderPipeline,
+    }
+
+    model_name = kwargs.pop('model_name', None)
+
+    if pipeline_name in pipelines:
+        return pipelines[pipeline_name](model_name, **kwargs)
+    else:
+        raise Exception("'%s' is not a valid pipeline name." % pipeline_name)
```

## scales_nlp/routines.py

```diff
@@ -1,59 +1,63 @@
-from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForTokenClassification
+from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
+from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification
 from transformers import TrainingArguments, Trainer
-from datasets import Dataset, load_metric
+from transformers.integrations import TensorBoardCallback
+from datasets import Dataset
+import evaluate
+from sklearn.feature_extraction.text import CountVectorizer
 from sklearn.metrics import f1_score as sk_f1_score
 import torch
+import pandas as pd
 import numpy as np
 from pathlib import Path
 import shutil
-from scales_nlp.utils import DEFAULT_TRAINING_ARGS, load_config
+from scales_nlp import config
 import scales_nlp
 
 
-acc_metric = load_metric("accuracy")
-f1_metric = load_metric("f1")
-precision_metric = load_metric("precision")
-recall_metric = load_metric("recall")
+acc_metric = evaluate.load("accuracy")
+f1_metric = evaluate.load("f1")
+precision_metric = evaluate.load("precision")
+recall_metric = evaluate.load("recall")
 
 
 class BaseRoutine():
     def __init__(
-        self, model_name=DEFAULT_TRAINING_ARGS['model_name'], max_length=DEFAULT_TRAINING_ARGS['max_length'],
-        eval_split=DEFAULT_TRAINING_ARGS['eval_split'], epochs=DEFAULT_TRAINING_ARGS['epochs'], shuffle=DEFAULT_TRAINING_ARGS['epochs'],
-        train_batch_size=DEFAULT_TRAINING_ARGS['train_batch_size'], eval_batch_size=DEFAULT_TRAINING_ARGS['eval_batch_size'],
-        gradient_accumulation_steps=DEFAULT_TRAINING_ARGS['gradient_accumulation_steps'],
-        learning_rate=DEFAULT_TRAINING_ARGS['learning_rate'], warmup_ratio=DEFAULT_TRAINING_ARGS['warmup_ratio'],
-        weight_decay=DEFAULT_TRAINING_ARGS['weight_decay'], save_steps=DEFAULT_TRAINING_ARGS['save_steps'],
-        callbacks=[], additional_args={}, tokenizer_name=None,
+        self, model_name=config['MODEL_NAME'], max_length=config['MAX_LENGTH'],
+        eval_split=config['EVAL_SPLIT'], epochs=config['EPOCHS'], shuffle=config['SHUFFLE'],
+        train_batch_size=config['TRAIN_BATCH_SIZE'], eval_batch_size=config['EVAL_BATCH_SIZE'],
+        gradient_accumulation_steps=config['GRADIENT_ACCUMULATION_STEPS'],
+        learning_rate=config['LEARNING_RATE'], warmup_ratio=config['WARMUP_RATIO'],
+        weight_decay=config['WEIGHT_DECAY'], save_steps=config['SAVE_STEPS'],
+        callbacks=None, **kwargs
     ):
         self.model_name = model_name
-        self.tokenizer_name = model_name if tokenizer_name is None else tokenizer_name
         self.max_length = max_length
         self.eval_split = eval_split
         self.epochs = epochs
         self.shuffle = shuffle
         self.train_batch_size = train_batch_size
         self.eval_batch_size = eval_batch_size
         self.gradient_accumulation_steps = gradient_accumulation_steps
         self.learning_rate = learning_rate
         self.warmup_ratio = warmup_ratio
         self.weight_decay = weight_decay
         self.save_steps = save_steps
         self.callbacks = callbacks
-        self.additional_args = additional_args
+        self.kwargs = kwargs
 
     def process_labels(self, labels):
         raise NotImplementedError('process_labels not implemented')
 
     def load_model(self, model_name):
         raise NotImplementedError('load_model not implemented')
 
-    def create_dataset(self, texts, labels, tokenizer):
-        inputs = tokenizer(texts, padding="max_length", max_length=self.max_length, truncation=True)
+    def create_dataset(self, texts, labels, name):
+        inputs = self.tokenizer(texts, padding="max_length", max_length=self.max_length, truncation=True)
         inputs['labels'] = torch.Tensor(labels).reshape(len(texts), -1)
         return Dataset.from_dict(inputs)
     
     def load_trainer_class(self):
         if hasattr(self, 'compute_loss'):
             compute_loss_fn = self.compute_loss
             class CustomTrainer(Trainer):
@@ -63,32 +67,30 @@
         else:
             return Trainer
 
     def train(self, output_dir, texts, labels, push=None, overwrite=False):
         labels, label_names = self.process_labels(labels)
         self.label_names = label_names
 
-        tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)
+        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
         model = self.load_model(self.model_name)
 
         if self.shuffle:
             data = list(zip(texts, labels))
             np.random.shuffle(data)
             texts, labels = zip(*data)
             texts, labels = list(texts), list(labels)
         
         split = len(texts) - int(self.eval_split * len(texts))
         
-        train_dataset = self.create_dataset(texts[:split], labels[:split], tokenizer)
-        eval_dataset = self.create_dataset(texts[split:], labels[split:], tokenizer)
+        train_dataset = self.create_dataset(texts[:split], labels[:split], 'train')
+        eval_dataset = self.create_dataset(texts[split:], labels[split:], 'eval')
 
         trainer_class = self.load_trainer_class()
 
-        config = load_config()
-
         output_dir = Path(output_dir)
         args = TrainingArguments(
             output_dir,
             num_train_epochs=self.epochs,
             per_device_train_batch_size=self.train_batch_size,
             per_device_eval_batch_size=self.eval_batch_size,
             gradient_accumulation_steps=self.gradient_accumulation_steps,
@@ -96,40 +98,48 @@
             warmup_ratio=self.warmup_ratio,
             weight_decay=self.weight_decay,
             remove_unused_columns=False,
             eval_steps=self.save_steps,
             save_steps=self.save_steps,
             evaluation_strategy='steps',
             load_best_model_at_end=True,
-            save_total_limit=1,
+            save_total_limit=5,
             logging_steps=5,
+            logging_dir=output_dir / 'runs',
             hub_strategy='end',
             hub_model_id=push,
             hub_token=config['HUGGING_FACE_TOKEN'],
             push_to_hub=push is not None,
-            **self.additional_args
         )
 
         if output_dir.exists():
             if overwrite:
                 shutil.rmtree(output_dir)
             else:
                 raise Exception("Output directory already exists, please use the overwrite argument to overwrite it")
 
         trainer_class_args = dict(
             model=model,
-            tokenizer=tokenizer,
+            tokenizer=self.tokenizer,
             args=args,
             train_dataset=train_dataset,
             eval_dataset=eval_dataset,
         )
+
         if hasattr(self, 'compute_metrics'):
             trainer_class_args['compute_metrics'] = self.compute_metrics
+        
+        if hasattr(self, 'data_collator'):
+            trainer_class_args['data_collator'] = self.data_collator
+
         self.trainer = trainer_class(**trainer_class_args)
 
+        if self.callbacks is None:
+            self.callbacks = [TensorBoardCallback()]
+
         for callback in self.callbacks:
             self.trainer.add_callback(callback)
 
         self.trainer.train()
         results = self.trainer.evaluate()
         print(results)
 
@@ -183,38 +193,69 @@
         predictions = (logits > 0.5).astype(int)
         scores = sk_f1_score(predictions, labels, average=None)
         scores = {'labels': {self.label_names[i]: scores[i] for i in range(len(scores))}}
         scores['f1_macro'] = sk_f1_score(predictions, labels, average='macro')
         return scores
 
 
+class TokenClassificationRoutine(BaseRoutine):    
+    def process_labels(self, labels):
+        label_names = []
+        for spans in labels:
+            for span in spans:
+                label_names.append(span['label'])
+        label_names = list(sorted(list(set(label_names))))
+        return labels, label_names
+
+    def load_model(self, model_name):
+        id2label = {0: 'O'}
+        for label_name in sorted(self.label_names):
+            id2label[len(id2label)] = 'B-' + label_name
+            id2label[len(id2label)] = 'I-' + label_name
+
+        model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(id2label))
+        model.config.id2label = id2label
+        model.config.label2id = {v:k for k,v in id2label.items()}
+        return model
+
+    def create_dataset(self, texts, labels, name):
+        return scales_nlp.datasets.TokenClassificationDataset(self.tokenizer, texts, labels, self.label_names, max_length=self.max_length)
+
+    @property
+    def data_collator(self):
+        return DataCollatorForTokenClassification(self.tokenizer)
+
+    
+
+
+
 class TripletVectorRoutine(BaseRoutine):    
     def process_labels(self, labels):
         labels = [labels[i] for i in range(len(labels))]
         return labels, []
 
     def load_model(self, model_name):
         model = AutoModel.from_pretrained(model_name)
         return model
     
-    def create_dataset(self, texts, labels, tokenizer):
+    def create_dataset(self, texts, labels, name):
         labels = np.vstack(labels)
-        return scales_nlp.data.PairedSimilarExamplesDataset(texts, labels, tokenizer, self.max_length, shuffle=self.shuffle)
+        return scales_nlp.data.PairedSimilarExamplesDataset(texts, labels, self.tokenizer, self.max_length, shuffle=self.shuffle)
     
     def compute_loss(self, model, inputs, return_outputs=False):
         labels = inputs.pop('labels')
         outputs = model(**inputs)
         embeddings = outputs[0][:, 0, :]
-        loss_fn = scales_nlp.losses.MultiLabelTripletLoss(discrete_labels=False)
+        loss_fn = scales_nlp.losses.MultiDimensionalTripletLoss(discrete_labels=False)
         loss = loss_fn(embeddings, labels)
         return (loss, outputs) if return_outputs else loss
-    
 
 
 def training_routine(task, **kwargs):
     task2routine = {
         'classification': ClassificationRoutine,
         'multi-label-classification': MultiLabelClassificationRoutine,
+        'token-classification': TokenClassificationRoutine,
+        'ner': TokenClassificationRoutine,
         'triplet-vector-encoding': TripletVectorRoutine,
     }
     return task2routine[task](**kwargs)
-
```

## scales_nlp/utils.py

```diff
@@ -1,62 +1,109 @@
-import os
-from pathlib import Path
 import json
+from pathlib import Path
+from typing import Union, List, Tuple
+import pandas as pd
+import requests
 import scales_nlp
+from scales_nlp import config
 
-CACHE_PATH = Path.home() / '.cache' / 'scales_nlp'
-CACHE_PATH.mkdir(parents=True, exist_ok=True)
+PACKAGE_DIR = Path(__file__).parent
+PACKAGE_DATA_DIR = PACKAGE_DIR / 'data'
 
-CONFIG_PATH = CACHE_PATH / 'config.json'
+LABEL_DATA_DIR = config['LABEL_DATA_DIR'] if config['LABEL_DATA_DIR'] is not None else config['PACER_DIR']
 
-CONFIG_KEYS = [
-    'HUGGING_FACE_TOKEN',
-    'LABEL_STUDIO_HOST',
-    'LABEL_STUDIO_TOKEN',
-]
-
-DEVELOPER_CONFIG_KEYS = [
-    'DOCKET_VIEWER_HOST',
-    'DOCKET_VIEWER_API_KEY',
-]
-
-DEFAULT_TRAINING_ARGS = {
-    'model_name': 'scales-okn/docket-language-model',
-    'max_length': 256,
-    'eval_split': 0.2,
-    'epochs': 5,
-    'train_batch_size': 4,
-    'eval_batch_size': 8,
-    'gradient_accumulation_steps': 2,
-    'learning_rate': 3e-5,
-    'warmup_ratio': 0.06,
-    'weight_decay': 0.01,
-    'save_steps': 100,
-}
+COURTS = pd.read_csv(PACKAGE_DATA_DIR / 'courts.csv')
+STATES = COURTS['state'].dropna().unique()
+DIVISIONS = COURTS['cardinal'].dropna().unique()
 
 
-def load_json(path):
-    with open(str(path), 'r') as f:
-        return json.loads(f.read())
+def courts() -> pd.DataFrame:
+    return COURTS.copy()
+
 
+def states() -> List[str]:
+    return STATES
 
-def load_config():
-    config = {}
-    if CONFIG_PATH.exists():
-        config = load_json(CONFIG_PATH)
-    
-    for key in CONFIG_KEYS + DEVELOPER_CONFIG_KEYS:
-        value = os.environ.get(key, None)
-        if value is not None:
-            config[key] = value
-        if key not in config:
-            config[key] = None
 
-    return config
+def divisions() -> List[str]:
+    return DIVISIONS
+
+
+def load_json(path: Union[str, Path]) -> dict:
+    with open(str(path), 'r') as f:
+        return json.loads(f.read())
 
 
-def convert_default_binary_outputs(predictions):
+def get_ucid_components(ucid: str) -> Tuple[str, str, str]:
+    court = ucid.split(';;')[0]
+    docket_number = ucid.split(';;')[1]
+    office_number = docket_number.split(':')[0]
+    year = docket_number.split(':')[1].split('-')[0]
+    return court, docket_number, office_number, year
+
+
+def case_path(ucid: str) -> Path:
+    court, docket_number, _, year = get_ucid_components(ucid)
+    filename = docket_number.replace(':', '-') + '.json'
+    return config['PACER_DIR'] / court / 'json' / year / filename
+
+
+def load_case(ucid: str) -> dict:
+    path = case_path(ucid)
+    return load_json(path)
+
+
+def load_case_classifier_labels(ucid: str) -> List:
+    court, docket_number, _, year = get_ucid_components(ucid)
+    filename = docket_number.replace(':', '-') + '.json'
+    path = LABEL_DATA_DIR / court / 'labels' / year / filename
+    if path.exists():
+        return load_json(path)
+    else:
+        print('labels not computed for {}'.format(ucid))
+        return []
+
+
+def load_court(court: str) -> dict:
+    courts = COURTS[COURTS['abbreviation'] == court]
+    if len(courts) == 0:
+        raise ValueError(f'Court {court} not found')
+    return courts.iloc[0].to_dict()
+
+
+def convert_default_binary_outputs(
+    predictions: List[Union[str, dict[str, float], bool]]
+) -> List[Union[float, bool]]:
     if isinstance(predictions[0], str):
-        predictions = [prediction == 'LABEL_1' for prediction in predictions]
+        converted_predictions = [prediction == 'LABEL_1' for prediction in predictions]
     elif isinstance(predictions[0], dict):
-        predictions = [prediction['LABEL_1'] for prediction in predictions]
-    return predictions
+        converted_predictions = [prediction['LABEL_1'] for prediction in predictions]
+    else:
+        converted_predictions = predictions
+    return converted_predictions
+
+
+def get_label_studio_data(project_id:int) -> List[dict]:
+    url = f"{scales_nlp.config['LABEL_STUDIO_URL']}/api/projects/{project_id}/export?exportType=JSON"
+    headers = {'Authorization': f"Token {scales_nlp.config['LABEL_STUDIO_TOKEN']}"}
+
+    r = requests.get(url, headers=headers)
+    return r.json()
+
+
+def get_label_studio_ner_data(project_id:int) -> Tuple[List[str], List[List[dict]]]:
+    label_studio_data = get_label_studio_data(project_id)
+    data = []
+    for example in label_studio_data:
+        spans = []
+        for anno in example['annotations']:
+            for result in anno['result']:
+                value = result['value']
+                spans.append({
+                    'start': value['start'],
+                    'end': value['end'],
+                    'label': value['labels'][0],
+                })
+
+        data.append({'text': example['data']['text'], 'spans': spans})
+    data = pd.DataFrame(data)
+    return data['text'].tolist(), data['spans'].tolist()
```

