# Comparing `tmp/tablite-2022.9.3-py3-none-any.whl.zip` & `tmp/tablite-2023.6.dev1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,33 @@
-Zip file size: 58991 bytes, number of entries: 18
--rw-rw-rw-  2.0 fat      181 b- defN 22-Jul-08 17:21 tablite/__init__.py
--rw-rw-rw-  2.0 fat      800 b- defN 22-Jul-26 10:22 tablite/config.py
--rw-rw-rw-  2.0 fat   109851 b- defN 22-Aug-19 10:04 tablite/core.py
--rw-rw-rw-  2.0 fat    26932 b- defN 22-Aug-18 11:40 tablite/datatypes.py
--rw-rw-rw-  2.0 fat     8113 b- defN 22-Aug-19 10:04 tablite/file_reader_utils.py
--rw-rw-rw-  2.0 fat     4944 b- defN 22-Jul-08 17:21 tablite/groupby_utils.py
--rw-rw-rw-  2.0 fat    50898 b- defN 22-Aug-16 11:28 tablite/memory_manager.py
--rw-rw-rw-  2.0 fat     6057 b- defN 22-Jul-08 17:21 tablite/sortation.py
--rw-rw-rw-  2.0 fat     8035 b- defN 22-Jul-08 17:21 tablite/utils.py
--rw-rw-rw-  2.0 fat      134 b- defN 22-Aug-19 10:04 tablite/version.py
--rw-rw-rw-  2.0 fat     1069 b- defN 22-Jul-08 17:21 tablite-2022.9.3.data/data/LICENSE
--rw-rw-rw-  2.0 fat     6073 b- defN 22-Aug-09 21:37 tablite-2022.9.3.data/data/README.md
--rw-rw-rw-  2.0 fat      203 b- defN 22-Aug-06 15:08 tablite-2022.9.3.data/data/requirements.txt
--rw-rw-rw-  2.0 fat     1069 b- defN 22-Aug-19 10:14 tablite-2022.9.3.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     7591 b- defN 22-Aug-19 10:14 tablite-2022.9.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 22-Aug-19 10:14 tablite-2022.9.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 22-Aug-19 10:14 tablite-2022.9.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     1456 b- defN 22-Aug-19 10:14 tablite-2022.9.3.dist-info/RECORD
-18 files, 233506 bytes uncompressed, 56623 bytes compressed:  75.8%
+Zip file size: 77208 bytes, number of entries: 31
+-rw-r--r--  2.0 unx      244 b- defN 23-Jun-06 17:23 tablite/__init__.py
+-rw-r--r--  2.0 unx    55667 b- defN 23-Jun-06 17:23 tablite/base.py
+-rw-r--r--  2.0 unx     2262 b- defN 23-Jun-06 17:23 tablite/config.py
+-rw-r--r--  2.0 unx    27578 b- defN 23-Jun-06 17:23 tablite/core.py
+-rw-r--r--  2.0 unx     4504 b- defN 23-Jun-06 17:23 tablite/datasets.py
+-rw-r--r--  2.0 unx    26976 b- defN 23-Jun-06 17:23 tablite/datatypes.py
+-rw-r--r--  2.0 unx     2872 b- defN 23-Jun-06 17:23 tablite/diff.py
+-rw-r--r--  2.0 unx     5340 b- defN 23-Jun-06 17:23 tablite/export_utils.py
+-rw-r--r--  2.0 unx    10141 b- defN 23-Jun-06 17:23 tablite/file_reader_utils.py
+-rw-r--r--  2.0 unx     4889 b- defN 23-Jun-06 17:23 tablite/groupby_utils.py
+-rw-r--r--  2.0 unx     5652 b- defN 23-Jun-06 17:23 tablite/groupbys.py
+-rw-r--r--  2.0 unx    17867 b- defN 23-Jun-06 17:23 tablite/import_utils.py
+-rw-r--r--  2.0 unx     7589 b- defN 23-Jun-06 17:23 tablite/imputation.py
+-rw-r--r--  2.0 unx    12182 b- defN 23-Jun-06 17:23 tablite/joins.py
+-rw-r--r--  2.0 unx     6737 b- defN 23-Jun-06 17:23 tablite/lookup.py
+-rw-r--r--  2.0 unx     3199 b- defN 23-Jun-06 17:23 tablite/mp_utils.py
+-rw-r--r--  2.0 unx     8863 b- defN 23-Jun-06 17:23 tablite/pivots.py
+-rw-r--r--  2.0 unx     9171 b- defN 23-Jun-06 17:23 tablite/redux.py
+-rw-r--r--  2.0 unx     6098 b- defN 23-Jun-06 17:23 tablite/sort_utils.py
+-rw-r--r--  2.0 unx     5299 b- defN 23-Jun-06 17:23 tablite/sortation.py
+-rw-r--r--  2.0 unx     1125 b- defN 23-Jun-06 17:23 tablite/tools.py
+-rw-r--r--  2.0 unx    11195 b- defN 23-Jun-06 17:23 tablite/utils.py
+-rw-r--r--  2.0 unx      139 b- defN 23-Jun-06 17:23 tablite/version.py
+-rw-r--r--  2.0 unx     1069 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.data/data/LICENSE
+-rw-r--r--  2.0 unx     6960 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.data/data/README.md
+-rw-r--r--  2.0 unx      246 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.data/data/requirements.txt
+-rw-r--r--  2.0 unx     1069 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     8677 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2457 b- defN 23-Jun-06 17:23 tablite-2023.6.dev1.dist-info/RECORD
+31 files, 256167 bytes uncompressed, 73340 bytes compressed:  71.4%
```

## zipnote {}

```diff
@@ -1,55 +1,94 @@
 Filename: tablite/__init__.py
 Comment: 
 
+Filename: tablite/base.py
+Comment: 
+
 Filename: tablite/config.py
 Comment: 
 
 Filename: tablite/core.py
 Comment: 
 
+Filename: tablite/datasets.py
+Comment: 
+
 Filename: tablite/datatypes.py
 Comment: 
 
+Filename: tablite/diff.py
+Comment: 
+
+Filename: tablite/export_utils.py
+Comment: 
+
 Filename: tablite/file_reader_utils.py
 Comment: 
 
 Filename: tablite/groupby_utils.py
 Comment: 
 
-Filename: tablite/memory_manager.py
+Filename: tablite/groupbys.py
+Comment: 
+
+Filename: tablite/import_utils.py
+Comment: 
+
+Filename: tablite/imputation.py
+Comment: 
+
+Filename: tablite/joins.py
+Comment: 
+
+Filename: tablite/lookup.py
+Comment: 
+
+Filename: tablite/mp_utils.py
+Comment: 
+
+Filename: tablite/pivots.py
+Comment: 
+
+Filename: tablite/redux.py
+Comment: 
+
+Filename: tablite/sort_utils.py
 Comment: 
 
 Filename: tablite/sortation.py
 Comment: 
 
+Filename: tablite/tools.py
+Comment: 
+
 Filename: tablite/utils.py
 Comment: 
 
 Filename: tablite/version.py
 Comment: 
 
-Filename: tablite-2022.9.3.data/data/LICENSE
+Filename: tablite-2023.6.dev1.data/data/LICENSE
 Comment: 
 
-Filename: tablite-2022.9.3.data/data/README.md
+Filename: tablite-2023.6.dev1.data/data/README.md
 Comment: 
 
-Filename: tablite-2022.9.3.data/data/requirements.txt
+Filename: tablite-2023.6.dev1.data/data/requirements.txt
 Comment: 
 
-Filename: tablite-2022.9.3.dist-info/LICENSE
+Filename: tablite-2023.6.dev1.dist-info/LICENSE
 Comment: 
 
-Filename: tablite-2022.9.3.dist-info/METADATA
+Filename: tablite-2023.6.dev1.dist-info/METADATA
 Comment: 
 
-Filename: tablite-2022.9.3.dist-info/WHEEL
+Filename: tablite-2023.6.dev1.dist-info/WHEEL
 Comment: 
 
-Filename: tablite-2022.9.3.dist-info/top_level.txt
+Filename: tablite-2023.6.dev1.dist-info/top_level.txt
 Comment: 
 
-Filename: tablite-2022.9.3.dist-info/RECORD
+Filename: tablite-2023.6.dev1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tablite/__init__.py

```diff
@@ -1,9 +1,5 @@
-from .core import Table, Column 
-from .datatypes import DataTypes
-from .groupby_utils import GroupBy
-from .file_reader_utils import get_headers
-from .version import __version__
-
-
-
-
+from .core import Table, Column  # noqa
+from tablite.datatypes import DataTypes  # noqa
+from tablite.groupby_utils import GroupBy  # noqa
+from tablite.file_reader_utils import get_headers  # noqa
+from tablite.version import __version__  # noqa
```

## tablite/config.py

```diff
@@ -1,29 +1,67 @@
-import pathlib, tempfile
-
-
-# The default location for the storage
-H5_STORAGE = pathlib.Path(tempfile.gettempdir()) / "tablite.hdf5"
-# to overwrite first import the config class:
-# >>> from tablite.config import Config
-# >>> Config.H5_STORAGE = /this/new/location
-# then import the Table class 
-# >>> from tablite import Table
-# for every new table or record this path will be used.
-
-H5_PAGE_SIZE = 1_000_000  # sets the page size limit.
-
-H5_ENCODING = 'UTF-8'  # sets the page encoding when using bytes
-
-SINGLE_PROCESSING_LIMIT = 1_000_000  
-# when the number of fields (rows x columns) 
-# exceed this value, multiprocessing is used.
-
-TEMPDIR = pathlib.Path(tempfile.gettempdir()) / 'tablite-tmp'
-if not TEMPDIR.exists():
-    TEMPDIR.mkdir()
-# tempdir for file_reader and other temporary files.
-
-
-
+import os
+import pathlib
+import tempfile
+
+
+class Config(object):
+    """Config class for Tablite Tables.
+
+    The default location for the storage is loaded as
+
+    Config.workdir = pathlib.Path(os.environ.get("TABLITE_TMPDIR", f"{tempfile.gettempdir()}/tablite-tmp"))
+
+    to overwrite, first import the config class, then set the new workdir.
+    >>> from tablite import config
+    >>> from pathlib import Path
+    >>> config.workdir = Path("/this/new/location")
+    for every new table or record this path will be used.
+
+    PAGE_SIZE = 1_000_000 sets the page size limit.
+
+    Multiprocessing is enabled in one of three modes:
+    AUTO = "auto"
+    FALSE = "sp"
+    FORCE = "mp"
+
+    MULTIPROCESSING_MODE = AUTO  is default.
+
+    SINGLE_PROCESSING_LIMIT = 1_000_000
+    when the number of fields (rows x columns) exceed this value,
+    multiprocessing is used.
+    """
+
+    workdir = pathlib.Path(os.environ.get("TABLITE_TMPDIR", f"{tempfile.gettempdir()}/tablite-tmp"))
+    workdir.mkdir(parents=True, exist_ok=True)
+
+    PAGE_SIZE = 1_000_000  # sets the page size limit.
+    ENCODING = "UTF-8"  # sets the page encoding when using bytes
+    DISK_LIMIT = 10e9  # 10e9 (10Gb) on 100 Gb disk means raise at
+    # 90 Gb disk usage.
+
+    SINGLE_PROCESSING_LIMIT = 1_000_000
+    # when the number of fields (rows x columns)
+    # exceed this value, multiprocessing is used.
+
+    AUTO = "auto"
+    FALSE = "sp"
+    FORCE = "mp"
+    MULTIPROCESSING_MODE = AUTO
+    # Usage example (from import_utils in text_reader)
+    # if cpu_count < 2 or Config.MULTIPROCESSING_MODE == Config.FALSE:
+    #         for task in tasks:
+    #             task.execute()
+    #             pbar.update(dump_size)
+    #     else:
+    #         with TaskManager(cpu_count - 1) as tm:
+    #             errors = tm.execute(tasks, pbar=PatchTqdm())  # I expects a list of None's if everything is ok.
+    #             if any(errors):
+    #                 raise Exception("\n".join(e for e in errors if e))
+
+    @classmethod
+    def reset(cls):
+        """Resets the config class to original values."""
+        for k, v in _default_values.items():
+            setattr(Config, k, v)
 
 
+_default_values = {k: v for k, v in Config.__dict__.items() if not k.startswith("__") or callable(v)}
```

## tablite/core.py

```diff
@@ -1,2739 +1,805 @@
-import os
-import math
-import pathlib
-import json
 import sys
-import itertools
-import operator
-import warnings
 import logging
+from pathlib import Path
+
+from tqdm import tqdm as _tqdm
+
+from tablite.base import Table as BaseTable
+from tablite.base import Column  # noqa
+from tablite.utils import type_check
+from tablite import import_utils
+from tablite import export_utils
+from tablite import redux
+from tablite import joins
+from tablite import lookup
+from tablite import sortation
+from tablite import groupbys
+from tablite import pivots
+from tablite import imputation
+from tablite import diff
+
+
+TIMEOUT_MS = 60 * 1000  # maximum msec tolerance waiting for OS to release hdf5 write lock
 
-from collections import defaultdict
-from multiprocessing import shared_memory
 
-logging.getLogger('lml').propagate = False
-logging.getLogger('pyexcel_io').propagate = False
-logging.getLogger('pyexcel').propagate = False
+logging.getLogger("lml").propagate = False
+logging.getLogger("pyexcel_io").propagate = False
+logging.getLogger("pyexcel").propagate = False
 
 log = logging.getLogger(__name__)
 
 
-import chardet
-import pyexcel
-import pyperclip
-from tqdm import tqdm as _tqdm
-import numpy as np
-import h5py
-import psutil
-from mplite import TaskManager, Task
+class Table(BaseTable):
+    def __init__(self, columns=None, headers=None, rows=None, _path=None) -> None:
+        """creates Table
+
+        Args:
+            EITHER:
+                columns (dict, optional): dict with column names as keys, values as lists.
+                Example: t = Table(columns={"a": [1, 2], "b": [3, 4]})
+            OR
+                headers (list of strings, optional): list of column names.
+                rows (list of tuples or lists, optional): values for columns
+                Example: t = Table(headers=["a", "b"], rows=[[1,3], [2,4]])
+        """
+        super().__init__(columns, headers, rows, _path)
 
+    @classmethod
+    def from_file(
+        cls,
+        path,
+        columns=None,
+        first_row_has_headers=True,
+        encoding=None,
+        start=0,
+        limit=sys.maxsize,
+        sheet=None,
+        guess_datatypes=True,
+        newline="\n",
+        text_qualifier=None,
+        delimiter=None,
+        strip_leading_and_tailing_whitespace=True,
+        text_escape_openings="",
+        text_escape_closures="",
+        tqdm=_tqdm,
+    ):
+        """
+        reads path and imports 1 or more tables
 
-PYTHON_EXIT = False  # exit handler so that Table.__del__ doesn't run into import error during exit.
+        REQUIRED
+        --------
+        path: pathlib.Path or str
+            selection of filereader uses path.suffix.
+            See `filereaders`.
 
-def exiting():
-    global PYTHON_EXIT
-    PYTHON_EXIT = True
+        OPTIONAL
+        --------
+        columns:
+            None: (default) All columns will be imported.
+            List: only column names from list will be imported (if present in file)
+                  e.g. ['A', 'B', 'C', 'D']
+
+                  datatype is detected using Datatypes.guess(...)
+                  You can try it out with:
+                  >> from tablite.datatypes import DataTypes
+                  >> DataTypes.guess(['001','100'])
+                  [1,100]
 
-import atexit
-atexit.register(exiting)
+                  if the format cannot be achieved the read type is kept.
+            Excess column names are ignored.
 
+            HINT: To get the head of file use:
+            >>> from tablite.tools import head
+            >>> head = head(path)
+
+        first_row_has_headers: boolean
+            True: (default) first row is used as column names.
+            False: integers are used as column names.
 
-from tablite.memory_manager import MemoryManager, Page, Pages
-from tablite.file_reader_utils import TextEscape, get_headers
-from tablite.utils import summary_statistics, unique_name
-from tablite import sortation
-from tablite.groupby_utils import GroupBy, GroupbyFunction
-from tablite.config import SINGLE_PROCESSING_LIMIT, TEMPDIR, H5_ENCODING
-from tablite.datatypes import DataTypes
-
-
-mem = MemoryManager()
-
-class Table(object):
-    
-    def __init__(self,key=None, save=False, _create=True, config=None) -> None:
-        if key is None:
-            key = mem.new_id('/table')
-        elif not isinstance(key, str):
-            raise TypeError
-        self.key = key
-
-        self.group = f"/table/{self.key}"
-        self._columns = {}  # references for virtual datasets that behave like lists.
-        if _create:
-            if config is not None:
-                if not isinstance(config, str):
-                    raise TypeError("expected config as utf-8 encoded json")
-            mem.create_table(key=key, save=save, config=config)  # attrs. 'columns'
-        self._saved = save
-    
-    @property
-    def save(self):
-        return self._saved
-
-    @save.setter
-    def save(self, value):
-        if not isinstance(value, bool):
-            raise TypeError(f'expected bool, got: {type(value)}')
-        if self._saved != value:
-            self._saved = value
-            mem.set_saved_flag(self.group, value)
-
-    def __del__(self):
-        if PYTHON_EXIT:
-            return
-
-        try:
-            for key in list(self._columns):
-                del self[key]
-            mem.delete_table(self.group)
-        except KeyError:
-            log.info("Table.__del__ suppressed.")
-        
-    def __str__(self):
-        return f"Table({len(self._columns):,} columns, {len(self):,} rows)"
-
-    def __repr__(self):
-        return self.__str__()
-
-    @property
-    def columns(self):
-        return list(self._columns.keys())
-
-    @property
-    def rows(self):
-        """
-        enables iteration
-
-        for row in Table.rows:
-            print(row)
-        """
-        
-        n_max = len(self)
-        generators = []
-        for name, mc in self._columns.items():
-            if len(mc) < n_max:
-                warnings.warn(f"Column {name} has length {len(mc)} / {n_max}. None will appear as fill value.")
-            generators.append(itertools.chain(iter(mc), itertools.repeat(None, times=n_max-len(mc))))
-        
-        for _ in range(len(self)):
-            yield [next(i) for i in generators]
-    
-    def __iter__(self):
-        """
-        Disabled. Users should use Table.rows or Table.columns
-        """
-        raise AttributeError("use Table.rows or Table.columns")
-
-    def __len__(self):
-        for v in self._columns.values():  
-            return len(v)  # return on first key.
-        return 0  # if there are no columns.
-
-    def __setitem__(self, keys, values):
-        if isinstance(keys, str): 
-            if isinstance(values, (tuple,list,np.ndarray)):
-                if len(values) == 0:
-                    raise ValueError(f"Column has zero values?, {values}")
-                self._columns[keys] = column = Column(values)  # overwrite if exists.
-                mem.create_column_reference(self.key, column_name=keys, column_key=column.key)
-            elif isinstance(values, Column):
-                col = self._columns.get(keys,None)
-                if col is None:  # it's a column from another table.
-                    self._columns[keys] = column = values.copy()
-                    mem.create_column_reference(self.key, column_name=keys, column_key=column.key)
-                elif values.key == col.key:  # it's update from += or similar
-                    self._columns[keys] = values
-                else:                    
-                    raise NotImplemented()
-            elif values is None:
-                self._columns[keys] = Column(values)
-            else:
-                raise NotImplemented()
-        elif isinstance(keys, tuple) and len(keys) == len(values):
-            for key, value in zip(keys,values):
-                self.__setitem__(key,value)
-        else:
-            raise NotImplementedError()
-    
-    def __getitem__(self, *keys):
-        """
-        Enables selection of columns and rows
-        Examples: 
-
-            table['a']   # selects column 'a'
-            table[:10]   # selects first 10 rows from all columns
-            table['a','b', slice(3,20,2)]  # selects a slice from columns 'a' and 'b'
-            table['b', 'a', 'a', 'c', 2:20:3]  # selects column 'b' and 'c' and 'a' twice for a slice.
-
-        returns values in same order as selection.
-        """
-        if not isinstance(keys, tuple):
-            keys = (keys, )
-        if len(keys)==1 and all(isinstance(i,tuple) for i in keys):
-            keys = keys[0]           
-        
-        cols = [c for c in keys if isinstance(c,str) and c in self._columns]
-        cols = self.columns if not cols else cols
-        slices = [i for i in keys if isinstance(i, slice)]
-        if len(cols)==1:
-            col = self._columns[cols[0]]
-            if slices:
-                return col[slices[0]]
-            else:
-                return col
-        elif slices:
-            slc = slices[0]
-            t = Table()
-            for name in cols:
-                t[name] = self._columns[name][slc]
-            return t
-        else:
-            t = Table()
-            for name in cols:
-                t[name] = self._columns[name]
-            return t
-
-    def __delitem__(self, key):
-        """
-        del table['a']  removes column 'a'
-        del table[-3:] removes last 3 rows from all columns.
-        """
-        if isinstance(key, str) and key in self._columns:
-            col = self._columns[key]
-            mem.delete_column_reference(self.group, key, col.key)
-            del self._columns[key]  # dereference the Column
-        elif isinstance(key, slice):
-            for col in self._columns.values():
-                del col[key]
-        else:
-            raise NotImplemented()
+        encoding: str. Defaults to None (autodetect)
 
-    def copy(self):
-        t = Table()
-        for name, col in self._columns.items():
-            t[name] = col
-        return t
-
-    def clear(self):
-        for name in self.columns:
-            self.__delitem__(name)
-
-    def __eq__(self, __o: object) -> bool:
-        if not isinstance(__o, Table):
-            return False
-        if id(self) == id(__o):
-            return True
-        if len(self) != len(__o):
-            return False
-        if self.columns != __o.columns:
-            return False
-        for name, col in self._columns.items():
-            if col != __o._columns[name]:
-                return False
-        return True
-
-    def __add__(self,other):
-        """
-        enables concatenation for tables with the same column names.
-        """
-        c = self.copy()
-        c += other
-        return c
- 
-    def __iadd__(self,other):
-        """
-        enables extension with other tables with the same column names.
-        """
-        if not isinstance(other, Table):
-            raise TypeError(f"no method for {type(other)}")
-        if set(self.columns) != set(other.columns) or len(self.columns) != len(other.columns):
-            raise ValueError("Columns names are not the same. Use table.stack instead.")
-        for name, col in self._columns.items():
-            col += other[name]
-        return self
-
-    def __mul__(self,other):
-        """
-        enables repetition of a table
-        Example: Table_x_10 = table * 10
-        """
-        if not isinstance(other, int):
-            raise TypeError(f"can't multiply Table with {type(other)}")
-        t = self.copy()
-        for col in t._columns.values():
-            col *= other
-        return t
-
-    def __imul__(self,other):
-        """
-        extends a table N times onto using itself as source.
-        """
-        if not isinstance(other, int):
-            raise TypeError(f"can't multiply Table with {type(other)}")
-
-        for col in self._columns.values():
-            col *= other
-        return self
+        start: the first line to be read (default: 0)
+
+        limit: the number of lines to be read from start (default sys.maxint ~ 2**63)
+
+        OPTIONAL FOR EXCEL AND ODS READERS
+        ----------------------------------
+
+        sheet: sheet name to import  (applicable to excel- and ods-reader only)
+            e.g. 'sheet_1'
+            sheets not found excess names are ignored.
+
+        OPTIONAL FOR TEXT READERS
+        -------------------------
+        guess_datatype: bool
+            True: (default) datatypes are guessed using DataTypes.guess(...)
+            False: all data is imported as strings.
+
+        newline: newline character (applicable to text_reader only)
+            str: '\n' (default) or '\r\n'
+
+        text_qualifier: character (applicable to text_reader only)
+            None: No text qualifier is used.
+            str: " or '
+
+        delimiter: character (applicable to text_reader only)
+            None: file suffix is used to determine field delimiter:
+                .txt: "|"
+                .csv: ",",
+                .ssv: ";"
+                .tsv: "\t" (tab)
+
+        strip_leading_and_tailing_whitespace: bool:
+            True: default
+
+        text_escape_openings: (applicable to text_reader only)
+            None: default
+            str: list of characters such as ([{
+
+        text_escape_closures: (applicable to text_reader only)
+            None: default
+            str: list of characters such as }])
 
-    @classmethod
-    def reload_saved_tables(cls,path=None):
         """
-        Loads saved tables from a hdf5 storage.
-        
-        The default storage locations is:
-        >>> from tablite.config import HDF5_Config
-        >>> print(Config.H5_STORAGE)
-
-        To import without changing the default location use:
-        tables = reload_saved_tables("c:/another/location.hdf5)
-        """
-        tables = []
-        if path is None:
-            path = mem.path
-        unsaved = 0
-        with h5py.File(path, 'r+') as h5:
-            if "/table" not in h5.keys():
-                return []
-
-            for table_key in h5["/table"].keys():
-                dset = h5[f"/table/{table_key}"]
-                if dset.attrs['saved'] is False:
-                    unsaved += 1
-                else:
-                    t = Table.load(path, key=table_key)
-                    tables.append(t)
-        if unsaved:
-            warnings.warn(f"Dropping {unsaved} tables from cache where save==False.")
-        return tables
+        if isinstance(path, str):
+            path = Path(path)
+        type_check(path, Path)
 
-    @classmethod
-    def load(cls, path, key):
-        with h5py.File(path, 'r+') as h5:
-            group = f"/table/{key}"
-            dset = h5[group]
-            saved = dset.attrs['saved']
-            t = Table(key=key, save=saved, _create=False)
-            columns = json.loads(dset.attrs['columns'])
-            for col_name, column_key in columns.items():
-                c = Column.load(key=column_key)
-                col_dset = h5[f"/column/{column_key}"]
-                c._len = col_dset.attrs['length'] 
-                t[col_name] = c
-            return t
+        if not path.exists():
+            raise FileNotFoundError(f"file not found: {path}")
 
-    @classmethod
-    def reset_storage(cls):
-        """ Resets all stored tables. """
-        mem.reset_storage()
-
-    def add_rows(self, *args, **kwargs):
-        """ its more efficient to add many rows at once. 
-        
-        supported cases:
+        if not isinstance(start, int) or not 0 <= start <= sys.maxsize:
+            raise ValueError(f"start {start} not in range(0,{sys.maxsize})")
 
-        t = Table()
-        t.add_columns('row','A','B','C')
+        if not isinstance(limit, int) or not 0 < limit <= sys.maxsize:
+            raise ValueError(f"limit {limit} not in range(0,{sys.maxsize})")
 
-        (1) t.add_rows(1, 1, 2, 3)  # individual values as args
-        (2) t.add_rows([2, 1, 2, 3])  # list of values as args
-        (3) t.add_rows((3, 1, 2, 3))  # tuple of values as args
-        (4) t.add_rows(*(4, 1, 2, 3))  # unpacked tuple becomes arg like (1)
-        (5) t.add_rows(row=5, A=1, B=2, C=3)   # kwargs
-        (6) t.add_rows(**{'row': 6, 'A': 1, 'B': 2, 'C': 3})  # dict / json interpreted a kwargs
-        (7) t.add_rows((7, 1, 2, 3), (8, 4, 5, 6))  # two (or more) tuples as args
-        (8) t.add_rows([9, 1, 2, 3], [10, 4, 5, 6])  # two or more lists as rgs
-        (9) t.add_rows({'row': 11, 'A': 1, 'B': 2, 'C': 3},
-                       {'row': 12, 'A': 4, 'B': 5, 'C': 6})  # two (or more) dicts as args - roughly comma sep'd json.
-        (10) t.add_rows( *[ {'row': 13, 'A': 1, 'B': 2, 'C': 3},
-                            {'row': 14, 'A': 1, 'B': 2, 'C': 3} ])  # list of dicts as args
-        (11) t.add_rows(row=[15,16], A=[1,1], B=[2,2], C=[3,3])  # kwargs with lists as values
-        
-        if both args and kwargs, then args are added first, followed by kwargs.
-        """
-        if args:
-            if all(isinstance(i, (list, tuple, dict)) for i in args):
-                if all(len(i) == len(self._columns) for i in args):
-                    for arg in args:
-                        if isinstance(arg, (list,tuple)):  # 2,3,5,6
-                            for col,value in zip(self._columns.values(), arg):
-                                col.append(value)
-                        elif isinstance(arg, dict):  # 7,8
-                            for k,v in arg.items():
-                                col = self._columns[k]
-                                col.append(v)
-                        else:
-                            raise TypeError(f"{arg}?")
-            elif len(args) == len(self._columns):  # 1,4
-                for col, value in zip(self._columns.values(), args):
-                    col.append(value)
-            else:
-                raise ValueError(f"format not recognised: {args}")
-
-        if kwargs:
-            if isinstance(kwargs, dict):
-                if all(isinstance(v, (list, tuple)) for v in kwargs.values()):
-                    for k,v in kwargs.items():
-                        col = self._columns[k]
-                        col.extend(v)
-                else:
-                    for k,v in kwargs.items():
-                        col = self._columns[k]
-                        col.append(v)
-            else:
-                raise ValueError(f"format not recognised: {kwargs}")
-        
-        return
-
-    def add_columns(self, *names):
-        for name in names:
-            self.__setitem__(name,None)
-
-    def add_column(self,name, data=None):
-        if not isinstance(name, str):
-            raise TypeError()
-        if name in self.columns:
-            raise ValueError(f"{name} already in {self.columns}")
-        if not data:
-            pass
-        self.__setitem__(name,data)
+        if not isinstance(first_row_has_headers, bool):
+            raise TypeError("first_row_has_headers is not bool")
 
-    def stack(self, other):
-        """
-        returns the joint stack of tables
-        Example:
+        import_as = path.suffix
+        if import_as.startswith("."):
+            import_as = import_as[1:]
 
-        | Table A|  +  | Table B| = |  Table AB |
-        | A| B| C|     | A| B| D|   | A| B| C| -|
-                                    | A| B| -| D|
-        """
-        if not isinstance(other, Table):
-            raise TypeError(f"stack only works for Table, not {type(other)}")
-        
-        t = self.copy()
-        for name , col2 in other._columns.items():
-            if name in t.columns:
-                t[name].extend(col2)
-            elif len(self) > 0:
-                t[name] = [None] * len(self)
-            else:
-                t[name] = col2
-
-        for name, col in t._columns.items():
-            if name not in other.columns:
-                if len(other) > 0:
-                    if len(self) > 0:
-                        col.extend([None]*len(other))
-                    else:
-                        t[name] = [None]*len(other)
-        return t
-
-    def types(self):
-        d = {}
-        for name,col in self._columns.items():
-            assert isinstance(col, Column)
-            d[name] = col.types()
-        return d
-
-    def to_ascii(self, blanks=None, row_counts=None,split_after=None):
-        """
-        enables viewing in terminals
-        returns the table as ascii string
-
-        blanks: any stringable item.
-        row_counts: declares the column with row counts, so it is presented as the first column.
-        split_after: integer: inserts "..." to highlight split of rows
-        """
-        widths = {}
-        column_types = {}
-        names = list(self.columns)
-        if not names:
-            return "Empty table"
-        column_lengths = set()
-        for name,col in self._columns.items():
-            types = col.types()
-            if name == row_counts:
-                column_types[name] = 'row'
-            elif len(types) == 1:
-                dt, _ = types.popitem()
-                column_types[name] = dt.__name__
-            else:
-                column_types[name] = 'mixed'
-            dots = len("...") if split_after is not None else 0
-            widths[name] = max(
-                [len(column_types[name]), len(name), dots] +\
-                [len(str(v)) if not isinstance(v,str) else len(str(v)) for v in col] +\
-                [len(str(None)) if len(col)!= len(self) else 0]
-            )
-            column_lengths.add(len(col))
+        reader = import_utils.file_readers.get(import_as, None)
+        if reader is None:
+            raise ValueError(f"{import_as} is not in supported format: {import_utils.valid_readers}")
 
-        def adjust(v, length):
-            if v is None:
-                return str(blanks).ljust(length)
-            elif isinstance(v, str):
-                return v.ljust(length)
-            else:
-                return str(v).rjust(length)
-
-        s = []
-        s.append("+" + "+".join(["=" * widths[n] for n in names]) + "+")
-        s.append("|" + "|".join([n.center(widths[n], " ") for n in names]) + "|")
-        s.append("|" + "|".join([column_types[n].center(widths[n], " ") for n in names]) + "|")
-        s.append("+" + "+".join(["-" * widths[n] for n in names]) + "+")
-        for ix, row in enumerate(self.rows):
-            s.append("|" + "|".join([adjust(v, widths[n]) for v, n in zip(row, names)]) + "|")
-            if ix == split_after:
-                s.append("|" + "|".join([adjust("...", widths[n]) for _, n in zip(row, names)]) + "|")
-                
-        s.append("+" + "+".join(["=" * widths[h] for h in names]) + "+")
-
-        if len(column_lengths)!=1:
-            s.append("Warning: Columns have different lengths. None is used as fill value.")
-        
-        return "\n".join(s)
-
-    def show(self, *args, blanks=None):
-        """
-        accepted args:
-          - slice
-        """ 
-        if not self.columns:
-            print("Empty Table")
-            return
-
-        row_count_tags = ['#', '~', '*'] 
-        cols = set(self.columns)
-        for n,tag in itertools.product(range(1,6), row_count_tags):
-            if n*tag not in cols:
-                tag = n*tag
-                break
+        additional_configs = {"tqdm": tqdm}
+        if reader == import_utils.text_reader:
+            # here we inject tqdm, if tqdm is not provided, use generic iterator
+            # fmt:off
+            config = (path, columns, first_row_has_headers, encoding, start, limit, newline,
+                      guess_datatypes, text_qualifier, strip_leading_and_tailing_whitespace,
+                      delimiter, text_escape_openings, text_escape_closures)
+            # fmt:on
+
+        elif reader == import_utils.excel_reader:
+            # config = path, first_row_has_headers, sheet, columns, start, limit
+            config = (
+                str(path),
+                first_row_has_headers,
+                sheet,
+                columns,
+                start,
+                limit,
+            )  # if file length changes - re-import.
+
+        if reader == import_utils.ods_reader:
+            # path, first_row_has_headers=True, sheet=None, columns=None, start=0, limit=sys.maxsize,
+            config = (
+                str(path),
+                first_row_has_headers,
+                sheet,
+                columns,
+                start,
+                limit,
+            )  # if file length changes - re-import.
 
-        t = Table()
-        split_after = None
-        if args:
-            for arg in args:
-                if isinstance(arg, slice):
-                    ro = range(*arg.indices(len(self)))
-                    if len(ro)!=0:
-                        t[tag] = [f"{i:,}" for i in ro]  # add rowcounts as first column.
-                        for name,col in self._columns.items():
-                            t[name] = col[arg]  # copy to match slices
-                    else:
-                        t.add_columns(*[tag] + self.columns)
-
-        elif len(self) < 20:
-            t[tag] = [f"{i:,}".rjust(2) for i in range(len(self))]  # add rowcounts to copy 
-            for name,col in self._columns.items():
-                t[name] = col
-
-        else:  # take first and last 7 rows.
-            n = len(self)
-            j = int(math.ceil(math.log10(n))/3) + len(str(n))
-            split_after = 6
-            t[tag] = [f"{i:,}".rjust(j) for i in range(7)] + [f"{i:,}".rjust(j) for i in range(n-7, n)]
-            for name, col in self._columns.items():
-                t[name] = [i for i in col[:7]] + [i for i in col[-7:]] 
-
-        print(t.to_ascii(blanks=blanks,row_counts=tag, split_after=split_after))
-
-    def _repr_html_(self):
-        """ Ipython display compatible format
-        https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display
-        """
-        start, end = "<div><table border=1>", "</table></div>"
-
-        if not self.columns:
-            return f"{start}<tr>Empty Table</tr>{end}"
-
-        row_count_tags = ['#', '~', '*'] 
-        cols = set(self.columns)
-        for n,tag in itertools.product(range(1,6), row_count_tags):
-            if n*tag not in cols:
-                tag = n*tag
-                break
-
-        html = ["<tr>" + f"<th>{tag}</th>" +"".join( f"<th>{cn}</th>" for cn in self.columns) + "</tr>"]
-        
-        column_types = {}
-        column_lengths = set()
-        for name,col in self._columns.items():
-            types = col.types()
-            if len(types) == 1:
-                dt, _ = types.popitem()
-                column_types[name] = dt.__name__
-            else:
-                column_types[name] = 'mixed'
-            column_lengths.add(len(col))
-
-        html.append("<tr>" + f"<th>row</th>" +"".join( f"<th>{column_types[name]}</th>" for name in self.columns) + "</tr>")
-
-        if len(self)<20:
-            for ix, row in enumerate(self.rows):
-                html.append( "<tr>" + f"<td>{ix}</td>" + "".join(f"<td>{v}</td>" for v in row) + "</tr>")
-        else:
-            t = Table()
-            for name,col in self._columns.items():
-                t[name] = [i for i in col[:7]] + [i for i in col[-7:]] 
-            
-            c = len(self)-7
-            for ix, row in enumerate(t.rows):
-                if ix < 7:
-                    html.append( "<tr>" + f"<td>{ix}</td>" + "".join(f"<td>{v}</td>" for v in row) + "</tr>")
-                if ix == 7: 
-                    html.append( "<tr>" + f"<td>...</td>" + "".join(f"<td>...</td>" for _ in self._columns) + "</tr>")
-                if ix >= 7:
-                    html.append( "<tr>" + f"<td>{c}</td>" + "".join(f"<td>{v}</td>" for v in row) + "</tr>")
-                    c += 1
-
-        warning = "Warning: Columns have different lengths. None is used as fill value." if len(column_lengths)!=1 else ""
-
-        return start + ''.join(html) + end + warning  
-
-    def index(self, *args):
-        cols = []
-        for arg in args:
-            col = self._columns.get(arg, None)
-            if col is not None:
-                cols.append(col)
-        if not cols:
-            raise ValueError("no columns?")
-
-        c = np.column_stack(cols)
-        idx = defaultdict(set)
-        for ix, key in enumerate(c):
-            idx[tuple(key)].add(ix)
-        return idx
-    
-    def copy_to_clipboard(self):
-        """ copy data from a Table into clipboard. """
-        try:
-            s = ["\t".join([f"{name}" for name in self.columns])]
-            for row in self.rows:
-                s.append("\t".join((str(i) for i in row)))
-            s = "\n".join(s)
-            pyperclip.copy(s)
-        except MemoryError:
-            raise MemoryError("Cannot copy to clipboard. Select slice instead.")
-
-    @staticmethod
-    def copy_from_clipboard():
-        """ copy data from clipboard into Table. """
-        t = Table()
-        txt = pyperclip.paste().split('\n')
-        t.add_columns(*txt[0].split('\t'))
+        # At this point the import config seems valid.
+        # Now we check if the file already has been imported.
 
-        for row in txt[1:]:
-            data = row.split('\t')
-            t.add_rows(data)    
-        return t
-
-    def to_dict(self, row_count="row id", columns=None, slice_=None, start_on=1):
-        """
-        row_count: name of row counts. Default "row id". Use None to leave it out.
-        columns: list of column names. Default is None == all columns.
-        slice_: slice. Default is None == all rows.
-        start_on: integer: first row (typically 0 or 1)
-        """
-        if slice_ is None:
-            slice_  = slice(0, len(self))      
-        assert isinstance(slice_, slice)
-        
-        if columns is None:
-            columns = self.columns 
-        if not isinstance(columns, list):
-            raise TypeError("expected columns as list of strings")
-        
-        column_selection, own_cols = [], set(self.columns)
-        for name in columns:
-            if name in own_cols:
-                column_selection.append(name)
-            else:
-                raise ValueError(f"column({name}) not found")
-        
-        cols = {}
-
-        if row_count is not None:
-            cols[row_count] = [i + start_on for i in range(*slice_.indices(len(self)))]
-
-        for name in column_selection:
-            new_name = unique_name(name, list_of_names=list(cols.keys()))
-            col = self._columns[name]
-            cols[new_name] = col[slice_].tolist()  # pure python objects. No numpy.
-        d = {"columns": cols, "total_rows": len(self)}
-        return d
-
-    def as_json_serializable(self, row_count="row id", columns=None, slice_=None):
-        args = row_count, columns, slice_
-        d = self.to_dict(*args)
-        for k,data in d['columns'].items():
-            d['columns'][k] = [DataTypes.to_json(v) for v in data]  # deal with non-json datatypes.
-        return d
+        # publish the settings
+        return reader(cls, *config, **additional_configs)
+
+    @classmethod
+    def from_pandas(cls, df):
+        """
+        Creates Table using pd.to_dict('list')
 
-    def to_json(self, *args, **kwargs):
-        return json.dumps(self.as_json_serializable(*args, **kwargs))
+        similar to:
+        >>> import pandas as pd
+        >>> df = pd.DataFrame({'a':[1,2,3], 'b':[4,5,6]})
+        >>> df
+            a  b
+            0  1  4
+            1  2  5
+            2  3  6
+        >>> df.to_dict('list')
+        {'a': [1, 2, 3], 'b': [4, 5, 6]}
+
+        >>> t = Table.from_dict(df.to_dict('list))
+        >>> t.show()
+            +===+===+===+
+            | # | a | b |
+            |row|int|int|
+            +---+---+---+
+            | 0 |  1|  4|
+            | 1 |  2|  5|
+            | 2 |  3|  6|
+            +===+===+===+
+        """
+        return import_utils.from_pandas(cls, df)
+
+    @classmethod
+    def from_hdf5(cls, path):
+        """
+        imports an exported hdf5 table.
+        """
+        return import_utils.from_hdf5(cls, path)
 
     @classmethod
     def from_json(cls, jsn):
-        d = json.loads(jsn)
-        t = Table()
-        for name, data in d['columns'].items():
-            if not isinstance(name, str):
-                raise TypeError(f"expect {name} as a string")
-            if not isinstance(data, list):
-                raise TypeError(f"expected {data} as list")
-            t[name] = data
-        return t
+        """
+        Imports tables exported using .to_json
+        """
+        return import_utils.from_json(cls, jsn)
 
     def to_hdf5(self, path):
         """
         creates a copy of the table as hdf5
         """
-        if isinstance(path, str):
-            path = pathlib.Path(path)
-        
-        total = ":,".format(len(self.columns) * len(self))
-        print(f"writing {total} records to {path}")
-
-        with h5py.File(path, 'a') as f:
-            with _tqdm(total=len(self.columns), unit='columns') as pbar:
-                n = 0
-                for name, mc in self.columns.values():
-                    f.create_dataset(name, data=mc[:])  # stored in hdf5 as '/name'
-                    n += 1
-                    pbar.update(n)
-        print(f"writing {path} to HDF5 done")
+        export_utils.to_hdf5(self, path)
 
-    def from_hdf5(self, path):
+    def to_pandas(self):
         """
-        imports an exported hdf5 table.
+        returns pandas.DataFrame
         """
-        if isinstance(path, str):
-            path = pathlib.Path(path)
-        
-        t = Table()
-        with h5py.File(path, 'r') as h5:
-            for col_name in h5.keys():
-                dset = h5[col_name]
-                t[col_name] = dset[:]
-        return t
+        return export_utils.to_pandas(self)
 
-    def to_sql(self):
+    def to_sql(self, name):
         """
         generates ANSI-92 compliant SQL.
         """
-        prefix = "Table"
-        create_table = """CREATE TABLE {}{} ({})"""
-        columns = []
-        for name,col in self._columns.items():
-            dtype = col.types()
-            if len(dtype) == 1:
-                dtype,_ = dtype.popitem()
-                if dtype is int:
-                    dtype = "INTEGER"
-                elif dtype is float:
-                    dtype = "REAL"
-                else:
-                    dtype = "TEXT"
-            else:
-                dtype = "TEXT"
-            definition = f"{name} {dtype}"
-            columns.append(definition)
-
-        create_table = create_table.format(prefix, self.key, ", ".join(columns))
-        
-        # return create_table
-        row_inserts = []
-        for row in self.rows:
-            row_inserts.append(str(tuple([i if i is not None else 'NULL' for i in row])))
-        row_inserts = f"INSERT INTO {prefix}{self.key} VALUES " + ",".join(row_inserts) 
-        return "begin; {}; {}; commit;".format(create_table, row_inserts)
-
-    def export(self, path):
-        if isinstance(path,str):
-            path = pathlib.Path(path)
-        if not isinstance(path, pathlib.Path):
-            raise TypeError(f"expected pathlib.Path, not {type(path)}")
-        
-        ext = path.suffix[1:]  # .xlsx --> xlsx
-
-        if ext not in exporters:
-            raise TypeError(f"{ext} not in list of supported formats\n{list(file_readers.keys())}")
+        return export_utils.to_sql(self, name)  # remove after update to test suite.
 
-        handler = exporters.get(ext)
-        handler(table=self, path=path)
+    def to_json(self):
+        """
+        returns JSON
+        """
+        return export_utils.to_json(self)
 
-        log.info(f"exported {self.key} to {path}")
+    def to_xls(self, path):
+        """
+        exports table to path
+        """
+        export_utils.excel_writer(self, path)
 
-    @classmethod
-    def import_file(cls, path,  import_as, 
-        newline='\n', text_qualifier=None,  delimiter=',', first_row_has_headers=True, columns=None, sheet=None, 
-        start=0, limit=sys.maxsize, strip_leading_and_tailing_whitespace=True, encoding=None, tqdm=_tqdm):
+    def to_ods(self, path):
+        """
+        exports table to path
         """
-        reads path and imports 1 or more tables as hdf5
+        export_utils.excel_writer(self, path)
 
-        path: pathlib.Path or str
-        import_as: 'csv','xlsx','txt'                               *123
-        newline: newline character '\n', '\r\n' or b'\n', b'\r\n'   *13
-        text_qualifier: character: " or '                           +13
-        delimiter: character: typically ",", ";" or "|"             *1+3
-        first_row_has_headers: boolean                              *123
-        columns: dict with column names or indices and datatypes    *123
-            {'A': int, 'B': str, 'C': float, D: datetime}
-            Excess column names are ignored.
+    def to_csv(self, path):
+        """
+        exports table to path
+        """
+        export_utils.text_writer(self, path)
 
-        sheet: sheet name to import (e.g. 'sheet_1')                 *2
-            sheets not found excess names are ignored.
-            filenames will be {path}+{sheet}.h5
-        
-        start: the first line to be read.
-        limit: the number of lines to be read from start
-        strip_leading_and_tailing_whitespace: bool: default True. 
-        encoding: str. Defaults to None (autodetect)
+    def to_tsv(self, path):
+        """
+        exports table to path
+        """
+        export_utils.text_writer(self, path)
 
-        (*) required, (+) optional, (1) csv, (2) xlsx, (3) txt, (4) h5
+    def to_text(self, path):
+        """
+        exports table to path
+        """
+        export_utils.text_writer(self, path)
 
-        TABLES FROM IMPORTED FILES ARE IMMUTABLE.
-        OTHER TABLES EXIST IN MEMORY MANAGERs CACHE IF USE DISK == True
+    def to_html(self, path):
         """
-        if isinstance(path, str):
-            path = pathlib.Path(path)
-        if not isinstance(path, pathlib.Path):
-            raise TypeError(f"expected pathlib.Path, got {type(path)}")
-        if not path.exists():
-            raise FileNotFoundError(f"file not found: {path}")
-        if not isinstance(import_as, str):
-            raise TypeError(f"import_as is expected to be str, not {type(import_as)}: {import_as}")
-        if import_as.startswith("."):
-            import_as = import_as[1:]
-        reader = file_readers.get(import_as,None)
-        if reader is None:
-            raise ValueError(f"{import_as} is not in list of supported reader:\n{list(file_readers.keys())}")
+        exports table to path
+        """
+        export_utils.to_html(self, path)
 
-        additional_configs = {}
+    def expression(self, expression):
+        """
+        filters based on an expression, such as:
 
-        if reader is text_reader:
-            # here we inject tqdm, if tqdm is not provided, use generic iterator
-            additional_configs["tqdm"] = tqdm if tqdm is not None else iter
+            "all((A==B, C!=4, 200<D))"
 
-        if not isinstance(strip_leading_and_tailing_whitespace, bool):
-            raise TypeError()
-        
-        if columns is None:
-            sample = get_headers(path)
-            
-            if "is_empty" in sample:
-                return Table()
-
-            if import_as in {'csv', 'txt'}:
-                columns = {k:'f' for k in sample[path.name][0]}
-            elif sheet is not None:
-                columns = sample[sheet][0]
-            else:
-                pass  # let it fail later.
-        if not first_row_has_headers:
-            columns = {str(i):'f' for i in range(len(columns))}
+        which is interpreted using python's compiler to:
 
-        # At this point the import seems valid.
-        # Now we check if the file already has been imported.
-        config = {
-            'import_as': import_as,
-            'path': str(path),
-            'filesize': path.stat().st_size,  # if file length changes - re-import.
-            'delimiter': delimiter,
-            'columns': columns, 
-            'newline': newline,
-            'first_row_has_headers': first_row_has_headers,
-            'text_qualifier': text_qualifier,
-            'sheet': sheet,
-            'start': start,
-            'limit': limit,
-            'strip_leading_and_tailing_whitespace': strip_leading_and_tailing_whitespace,
-            'encoding': encoding
-        }
-        jsn_str = json.dumps(config)
-        for table_key, jsnb in mem.get_imported_tables().items():
-            if jsn_str == jsnb:
-                return Table.load(mem.path, table_key)  # table already imported.
-        # not returned yet? Then it's an import job:
-        t = reader(**config, **additional_configs)
-        mem.set_config(t.group, jsn_str)
-        if t.save is False:
-            raise AttributeError("filereader should set table.save = True to avoid repeated imports")
-        return t
-    
-    def index(self, *keys):
-        """ 
-        Returns index on *keys columns as d[(key tuple, )] = {index1, index2, ...} 
-        """
-        idx = defaultdict(set)
-        tbl = self.__getitem__(*keys)
-        g = tbl.rows if isinstance(tbl, Table) else iter(tbl)
-        for ix, key in enumerate(g):
-            if isinstance(key, list):
-                key = tuple(key)
-            else:
-                key = (key,)
-            idx[key].add(ix)
-        return idx
+            def _f(A,B,C,D):
+                return all((A==B, C!=4, 200<D))
+        """
+        return redux._filter_using_expression(self, expression)
 
-    def filter(self, expressions, filter_type='all'):
+    def filter(self, expressions, filter_type="all", tqdm=_tqdm):
         """
         enables filtering across columns for multiple criteria.
-        
-        expressions: 
-        list of dicts:
-        L = [
-            {'column1':'A', 'criteria': "==", 'column2': 'B'}, 
-            {'column1':'C', 'criteria': "!=", "value2": '4'},
-            {'value1': 200, 'criteria': "<", column2: 'D' }
-        ]
 
-        accepted dictionary keys: 'column1', 'column2', 'criteria', 'value1', 'value2'
+        expressions:
+
+            str: Expression that can be compiled and executed row by row.
+                exampLe: "all((A==B and C!=4 and 200<D))"
+
+            list of dicts: (example):
+
+                L = [
+                    {'column1':'A', 'criteria': "==", 'column2': 'B'},
+                    {'column1':'C', 'criteria': "!=", "value2": '4'},
+                    {'value1': 200, 'criteria': "<", column2: 'D' }
+                ]
+
+            accepted dictionary keys: 'column1', 'column2', 'criteria', 'value1', 'value2'
 
         filter_type: 'all' or 'any'
         """
-        if not isinstance(expressions, list):
-            raise TypeError
+        return redux.filter(self, expressions, filter_type, tqdm)
 
-        for expression in expressions:
-            if not isinstance(expression, dict):
-                raise TypeError(f"invalid expression: {expression}")
-            if not len(expression)==3:
-                raise ValueError(f"expected 3 items, got {expression}")
-            x = {'column1', 'column2', 'criteria', 'value1', 'value2'}
-            if not set(expression.keys()).issubset(x):
-                raise ValueError(f"got unknown key: {set(expression.keys()).difference(x)}")
-            if expression['criteria'] not in filter_ops:
-                raise ValueError(f"criteria missing from {expression}")
-
-            c1 = expression.get('column1',None) 
-            if c1 is not None and c1 not in self.columns: 
-                raise ValueError(f"no such column: {c1}")
-            v1 = expression.get('value1', None)
-            if v1 is not None and c1 is not None:
-                raise ValueError("filter can only take 1 left expr element. Got 2.")
-
-            c2 = expression.get('column2',None) 
-            if c2 is not None and c2 not in self.columns: 
-                raise ValueError(f"no such column: {c2}")
-            v2 = expression.get('value2', None)
-            if v2 is not None and c2 is not None:
-                raise ValueError("filter can only take 1 right expression element. Got 2.")
-               
-        if not isinstance(filter_type, str):
-            raise TypeError()
-        if not filter_type in {'all', 'any'}:
-            raise ValueError(f"filter_type: {filter_type} not in ['all', 'any']")
-
-        # the results are to be gathered here:
-        arr = np.zeros(shape=(len(expressions), len(self)), dtype=bool)
-        shm = shared_memory.SharedMemory(create=True, size=arr.nbytes)
-        _ = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)
-        
-        # the task manager enables evaluation of a column per core,
-        # which is assembled in the shared array.
-        max_task_size = math.floor(SINGLE_PROCESSING_LIMIT / len(self.columns))  # 1 million fields per core (best guess!)
-        
-        filter_tasks = []
-        for ix, expression in enumerate(expressions):
-            for step in range(0, len(self), max_task_size):
-                config = {'table_key':self.key, 'expression':expression, 
-                          'shm_name':shm.name, 'shm_index':ix, 'shm_shape': arr.shape, 
-                          'slice_':slice(step, min(step+max_task_size, len(self)))}
-                task = Task(f=filter_evaluation_task, **config)
-                filter_tasks.append(task)
-
-        merge_tasks = []
-        for step in range(0, len(self), max_task_size):
-            config = {
-                'table_key': self.key,
-                'true_key': mem.new_id('/table'),
-                'false_key': mem.new_id('/table'),
-                'shm_name': shm.name, 'shm_shape': arr.shape,
-                'slice_': slice(step, min(step+max_task_size,len(self)),1),
-                'filter_type': filter_type
-            }
-            task = Task(f=filter_merge_task, **config)
-            merge_tasks.append(task)
-
-        n_cpus = min(max(len(filter_tasks),len(merge_tasks)), psutil.cpu_count())
-        with TaskManager(n_cpus) as tm: 
-            # EVALUATE 
-            errs = tm.execute(filter_tasks)  # tm.execute returns the tasks with results, but we don't really care as the result is in the result array.
-            if any(errs):
-                raise Exception(errs)
-            # MERGE RESULTS
-            errs = tm.execute(merge_tasks)  # tm.execute returns the tasks with results, but we don't really care as the result is in the result array.
-            if any(errs):
-                raise Exception(errs)
-
-        table_true, table_false = None, None
-        for task in merge_tasks:
-            tmp_true = Table.load(mem.path, key=task.kwargs['true_key'])
-            if table_true is None:
-                table_true = tmp_true
-            elif len(tmp_true):
-                table_true += tmp_true
-            else:
-                pass
-                
-            tmp_false = Table.load(mem.path, key=task.kwargs['false_key'])
-            if table_false is None:
-                table_false = tmp_false
-            elif len(tmp_false):
-                table_false += tmp_false
-            else:
-                pass
-        return table_true, table_false
-    
-    def sort_index(self, sort_mode='excel', **kwargs):  
-        """ 
-        helper for methods `sort` and `is_sorted` 
-        sort_mode: str: "alphanumeric", "unix", or, "excel"
-        kwargs: sort criteria. See Table.sort()
+    def sort_index(self, sort_mode="excel", tqdm=_tqdm, pbar=None, **kwargs):
+        """
+        helper for methods `sort` and `is_sorted`
+
+        param: sort_mode: str: "alphanumeric", "unix", or, "excel" (default)
+        param: **kwargs: sort criteria. See Table.sort()
+        """
+        return sortation.sort_index(self, sort_mode, tqdm=_tqdm, pbar=None, **kwargs)
+
+    def reindex(self, index):
         """
-        logging.info(f"Table.sort_index running 1 core")  # TODO: This is single core code.
+        index: list of integers that declare sort order.
+
+        Examples:
 
-        if not isinstance(kwargs, dict):
-            raise ValueError("Expected keyword arguments, did you forget the ** in front of your dict?")
-        if not kwargs:
-            kwargs = {c: False for c in self.columns}
-        
-        for k, v in kwargs.items():
-            if k not in self.columns:
-                raise ValueError(f"no column {k}")
-            if not isinstance(v, bool):
-                raise ValueError(f"{k} was mapped to {v} - a non-boolean")
-        
-        if sort_mode not in sortation.modes:
-            raise ValueError(f"{sort_mode} not in list of sort_modes: {list(sortation.Sortable.modes.modes)}")
-
-        rank = {i: tuple() for i in range(len(self))}  # create index and empty tuple for sortation.
-        for key, reverse in _tqdm(kwargs.items(), desc='creating sort index'):
-            col = self._columns[key]
-            assert isinstance(col, Column)
-            ranks = sortation.rank(values=set(col[:].tolist()), reverse=reverse, mode=sort_mode)
-            assert isinstance(ranks, dict)
-            for ix, v in enumerate(col):
-                rank[ix] += (ranks[v],)  # add tuple
-
-        new_order = [(r, i) for i, r in rank.items()]  # tuples are listed and sort...
-        rank.clear()  # free memory.
-        new_order.sort()
-        sorted_index = [i for _, i in new_order]  # new index is extracted.
-        new_order.clear()
-        return sorted_index
+            Table:  ['a','b','c','d','e','f','g','h']
+            index:  [0,2,4,6]
+            result: ['b','d','f','h']
+
+            Table:  ['a','b','c','d','e','f','g','h']
+            index:  [0,2,4,6,1,3,5,7]
+            result: ['a','c','e','g','b','d','f','h']
+
+        """
+        return sortation.reindex(self, index)
 
-    def sort(self, sort_mode='excel', **kwargs):  
-        """ Perform multi-pass sorting with precedence given order of column names.
+    def drop_duplicates(self, *args):
+        """
+        removes duplicate rows based on column names
+
+        args: (optional) column_names
+        if no args, all columns are used.
+        """
+        if not args:
+            args = self.columns
+        index = [min(v) for v in self.index(*args).values()]
+        return self.reindex(index)
+
+    def sort(self, sort_mode="excel", **kwargs):
+        """Perform multi-pass sorting with precedence given order of column names.
         sort_mode: str: "alphanumeric", "unix", or, "excel"
-        kwargs: 
-            keys: columns, 
+        kwargs:
+            keys: columns,
             values: 'reverse' as boolean.
-            
-        examples: 
-        Table.sort('A'=False)  means sort by 'A' in ascending order.
-        Table.sort('A'=True, 'B'=False) means sort 'A' in descending order, then (2nd priority) sort B in ascending order.
-        """
-        if len(self) * len(self.columns) < SINGLE_PROCESSING_LIMIT :  # the task is so small that multiprocessing doesn't make sense.
-            sorted_index = self.sort_index(sort_mode=sort_mode, **kwargs)
-            t = Table()
-            for col_name, col in self._columns.items():  # this LOOP can be done with TaskManager
-                data = list(col[:])
-                t.add_column(col_name, data=[data[ix] for ix in sorted_index])
-            return t
-        else:
-            arr = np.zeros(shape=(len(self), ), dtype=np.int64)
-            shm = shared_memory.SharedMemory(create=True, size=arr.nbytes)  # the co_processors will read this.
-            sort_index = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)
-            sort_index[:] = self.sort_index(sort_mode=sort_mode, **kwargs)
-
-            tasks = []
-            columns_refs = {}
-            for name in self.columns:
-                col = self[name]
-                columns_refs[name] = d_key = mem.new_id('/column')
-                tasks.append(Task(indexing_task, source_key=col.key, destination_key=d_key, shm_name_for_sort_index=shm.name, shape=arr.shape))
-
-            with TaskManager(cpu_count=min(psutil.cpu_count(), len(tasks))) as tm:
-                errs = tm.execute(tasks)
-                if any(errs):
-                    msg = '\n'.join(errs)
-                    raise Exception(f"multiprocessing error:{msg}")
-
-            table_key = mem.new_id('/table')
-            mem.create_table(key=table_key, columns=columns_refs)
-            
-            shm.close()
-            shm.unlink()
-            t = Table.load(path=mem.path, key=table_key)
-            return t            
-
-    def is_sorted(self, **kwargs):  
-        """ Performs multi-pass sorting check with precedence given order of column names.
-        nan_value: value used to represent non-sortable values such as None and np.nan during sort.
-        **kwargs: sort criteria. See Table.sort()
+
+        examples:
+        Table.sort('A'=False) means sort by 'A' in ascending order.
+        Table.sort('A'=True, 'B'=False) means sort 'A' in descending order, then (2nd priority)
+        sort B in ascending order.
+        """
+        return sortation.sort(self, sort_mode, **kwargs)
+
+    def is_sorted(self, **kwargs):
+        """Performs multi-pass sorting check with precedence given order of column names.
+        **kwargs: optional: sort criteria. See Table.sort()
         :return bool
         """
-        logging.info(f"Table.is_sorted running 1 core")  # TODO: This is single core code.
-        sorted_index = self.sort_index(**kwargs)
-        if any(ix != i for ix, i in enumerate(sorted_index)):
-            return False
-        return True
-
-    def _mp_compress(self, mask):
-        """
-        helper for `any` and `all` that performs compression of the table self according to mask
-        using multiprocessing.
-        """
-        arr = np.zeros(shape=(len(self), ), dtype=bool)
-        shm = shared_memory.SharedMemory(create=True, size=arr.nbytes)  # the co_processors will read this.
-        compresssion_mask = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)
-        compresssion_mask[:] = mask
+        return sortation.is_sorted(self, **kwargs)
 
-        t = Table()
-        tasks = []
-        columns_refs = {}
-        for name in self.columns:
-            col = self[name]
-            d_key = mem.new_id('/column')
-            columns_refs[name] = d_key
-            t = Task(compress_task, source_key=col.key, destination_key=d_key, shm_index_name=shm.name, shape=arr.shape)
-            tasks.append(t)
-
-        with TaskManager(cpu_count=min(psutil.cpu_count(), len(tasks))) as tm:
-            results = tm.execute(tasks)
-            if any(r is not None for r in results):
-                for r in results:
-                    print(r)
-                raise Exception("!")
-        
-        with h5py.File(mem.path, 'r+') as h5:
-            table_key = mem.new_id('/table')
-            dset = h5.create_dataset(name=f"/table/{table_key}", dtype=h5py.Empty('f'))
-            dset.attrs['columns'] = json.dumps(columns_refs)  
-            dset.attrs['saved'] = False
-        
-        shm.close()
-        shm.unlink()
-        t = Table.load(path=mem.path, key=table_key)
-        return t
+    def any(self, **kwargs):
+        """
+        returns Table for rows where ANY kwargs match
+        :param kwargs: dictionary with headers and values / boolean callable
+        """
+        return redux.filter_any(self, **kwargs)
 
-    def all(self, **kwargs):  
+    def all(self, **kwargs):
         """
         returns Table for rows where ALL kwargs match
         :param kwargs: dictionary with headers and values / boolean callable
-        """
-        if not isinstance(kwargs, dict):
-            raise TypeError("did you forget to add the ** in front of your dict?")
-        if not all(k in self.columns for k in kwargs):
-            raise ValueError(f"Unknown column(s): {[k for k in kwargs if k not in self.columns]}")
-
-        ixs = None
-        for k, v in kwargs.items():
-            col = self._columns[k]
-            if ixs is None:  # first header generates base set.
-                if callable(v):
-                    ix2 = {ix for ix, i in enumerate(col) if v(i)}
-                else:
-                    ix2 = {ix for ix, i in enumerate(col) if v == i}
-
-            else:  # remaining headers reduce the base set.
-                if callable(v):
-                    ix2 = {ix for ix in ixs if v(col[ix])}
-                else:
-                    ix2 = {ix for ix in ixs if v == col[ix]}
-
-            if not isinstance(ixs, set):
-                ixs = ix2
-            else:
-                ixs = ixs.intersection(ix2)
 
-            if not ixs:  # There are no matches.
-                break
+        Examples:
 
-        if len(self)*len(self.columns) < SINGLE_PROCESSING_LIMIT:
             t = Table()
-            for col_name in self.columns:
-                data = self[col_name]
-                t[col_name] = [data[i] for i in ixs]
-            return t
-        else:
-            mask =  np.array([True if i in ixs else False for i in range(len(self))],dtype=bool)
-            return self._mp_compress(mask)
+            t['a'] = [1,2,3,4]
+            t['b'] = [10,20,30,40]
+
+            def f(x):
+                return x == 4
+            def g(x):
+                return x < 20
+
+            t2 = t.any( **{"a":f, "b":g})
+            assert [r for r in t2.rows] == [[1, 10], [4, 40]]
+
+            t2 = t.any(a=f,b=g)
+            assert [r for r in t2.rows] == [[1, 10], [4, 40]]
+
+            def h(x):
+                return x>=2
+
+            def i(x):
+                return x<=30
+
+            t2 = t.all(a=h,b=i)
+            assert [r for r in t2.rows] == [[2,20], [3, 30]]
+
 
-    def any(self, **kwargs):  
         """
-        returns Table for rows where ANY kwargs match
-        :param kwargs: dictionary with headers and values / boolean callable
+        return redux.filter_all(self, **kwargs)
+
+    def drop(self, *args):
         """
-        if not isinstance(kwargs, dict):
-            raise TypeError("did you forget to add the ** in front of your dict?")
+        removes all rows where args are present.
 
-        ixs = set()
-        for k, v in kwargs.items():
-            col = self._columns[k]
-            if callable(v):
-                ix2 = {ix for ix, r in enumerate(col) if v(r)}
-            else:
-                ix2 = {ix for ix, r in enumerate(col) if v == r}
-            ixs.update(ix2)
+        Exmaple:
+        >>> t = Table()
+        >>> t['A'] = [1,2,3,None]
+        >>> t['B'] = [None,2,3,4]
+        >>> t2 = t.drop(None)
+        >>> t2['A'][:], t2['B'][:]
+        ([2,3], [2,3])
 
-        if len(self) * len(self.columns) < SINGLE_PROCESSING_LIMIT:
-            t = Table()
-            for col_name in self.columns:
-                data = self[col_name]
-                t[col_name] = [data[i] for i in ixs]
-            return t
-        else:
-            mask =  np.array([i in ixs for i in range(len(self))],dtype=bool)
-            return self._mp_compress(mask)
-
-    def groupby(self, keys, functions):  # TODO: This is slow single core code.
-        """
-        rows: column names for grouping as rows.
-        columns: column names for grouping as columns.
-        functions: list of column names and group functions (See GroupyBy)
-        sum_on_rows: outputs group functions as extra rows if True, else as columns
-        returns: table
+        """
+        if not args:
+            raise ValueError("What to drop? None? np.nan? ")
+        d = {n: lambda x: x not in set(args) for n in self.columns}
+        return self.all(**d)
 
-        * NB: Column names can only be used once in rows & columns
+    def replace(self, mapping, columns=None):
+        """replaces all mapped keys with values from named columns
 
-        Example usage:
-            from tablite import Table, GroupBy
-            t = Table()
-            t.add_column('date', data=[1,1,1,2,2,2])
-            t.add_column('sku',  data=[1,2,3,1,2,3])
-            t.add_column('qty',  data=[4,5,4,5,3,7])
-            grp = t.groupby(rows=['sku'], functions=[('qty', GroupBy.Sum)])
-            grp.show()
-        """
-        if len(set(keys)) != len(keys):
-            duplicates = [k for k in keys if keys.count(k) > 1]
-            s = "" if len(duplicates) > 1 else "s"
-            raise ValueError(f"duplicate key{s} found across rows and columns: {duplicates}")
-
-        if not isinstance(functions, list):
-            raise TypeError(f"Expected functions to be a list of tuples. Got {type(functions)}")
-
-        if not all(len(i) == 2 for i in functions):
-            raise ValueError(f"Expected each tuple in functions to be of length 2. \nGot {functions}")
-
-        if not all(isinstance(a, str) for a, _ in functions):
-            L = [(a, type(a)) for a, _ in functions if not isinstance(a, str)]
-            raise ValueError(f"Expected column names in functions to be strings. Found: {L}")
-
-        if not all(issubclass(b, GroupbyFunction) and b in GroupBy.functions for _, b in functions):
-            L = [b for _, b in functions if b not in GroupBy._functions]
-            if len(L) == 1:
-                singular = f"function {L[0]} is not in GroupBy.functions"
-                raise ValueError(singular)
-            else:
-                plural = f"the functions {L} are not in GroupBy.functions"
-                raise ValueError(plural)
-        
-        # 1. Aggregate data.
-        aggregation_functions = defaultdict(dict)
-        cols = keys + [col_name for col_name,_ in functions]
-        seen,L = set(),[]
-        for c in cols:
-            if c not in seen:
-                seen.add(c)
-                L.append(c)
-        for row in self.__getitem__(*L).rows:
-            d = {col_name: value for col_name,value in zip(L, row)}
-            key = tuple([d[k] for k in keys])
-            agg_functions = aggregation_functions.get(key)
-            if not agg_functions:
-                aggregation_functions[key] = agg_functions =[(col_name, f()) for col_name, f in functions]
-            for col_name, f in agg_functions:
-                f.update(d[col_name])
-        
-        # 2. make dense table.
-        cols = [[] for _ in cols]
-        for key_tuple, funcs in aggregation_functions.items():
-            for ix, key_value in enumerate(key_tuple):
-                cols[ix].append(key_value)
-            for ix, (_, f) in enumerate(funcs,start=len(keys)):
-                cols[ix].append(f.value)
-        
-        new_names = keys + [f"{f.__name__}({col_name})" for col_name,f in functions]
-        result = Table()
-        for ix, (col_name, data) in enumerate(zip(new_names, cols)):
-            revised_name = unique_name(col_name, result.columns)
-            result[revised_name] = data            
-        return result
-
-    def pivot(self, rows, columns, functions, values_as_rows=True):
-        if isinstance(rows, str):
-            rows = [rows]
-        if not all(isinstance(i,str) for i in rows)            :
-            raise TypeError(f"Expected rows as a list of column names, not {[i for i in rows if not isinstance(i,str)]}")
-        
-        if isinstance(columns, str):
+        Args:
+            mapping (dict): keys are targets for replacement,
+                            values are replacements.
+            columns (list or str, optional): target columns.
+                Defaults to None (all columns)
+
+        Raises:
+            ValueError: _description_
+        """
+        if columns is None:
+            columns = list(self.columns)
+        if not isinstance(columns, list) and columns in self.columns:
             columns = [columns]
-        if not all(isinstance(i,str) for i in columns):
-            raise TypeError(f"Expected columns as a list of column names, not {[i for i in columns if not isinstance(i, str)]}")
+        type_check(columns, list)
+        for n in columns:
+            if n not in self.columns:
+                raise ValueError(f"column not found: {n}")
+
+        for name in columns:
+            col = self.columns[name]
+            col.replace(mapping)
 
-        if not isinstance(values_as_rows, bool):
-            raise TypeError(f"expected sum_on_rows as boolean, not {type(values_as_rows)}")
-        
-        keys = rows + columns 
-        assert isinstance(keys, list)
-
-        grpby = self.groupby(keys, functions)
-
-        if len(grpby) == 0:  # return empty table. This must be a test?
-            return Table()
-        
-        # split keys to determine grid dimensions
-        row_key_index = {}  
-        col_key_index = {}
-
-        r = len(rows)
-        c = len(columns)
-        g = len(functions)
-        
-        records = defaultdict(dict)
-
-        for row in grpby.rows:
-            row_key = tuple(row[:r])
-            col_key = tuple(row[r:r+c])
-            func_key = tuple(row[r+c:])
-            
-            if row_key not in row_key_index:
-                row_key_index[row_key] = len(row_key_index)  # Y
-
-            if col_key not in col_key_index:
-                col_key_index[col_key] = len(col_key_index)  # X
-
-            rix = row_key_index[row_key]
-            cix = col_key_index[col_key]
-            if cix in records:
-                if rix in records[cix]:
-                    raise ValueError("this should be empty.")
-            records[cix][rix] = func_key
-        
-        result = Table()
-        
-        if values_as_rows:  # ---> leads to more rows.
-            # first create all columns left to right
-
-            n = r + 1  # rows keys + 1 col for function values.
-            cols = [[] for _ in range(n)]
-            for row, ix in row_key_index.items():
-                for (col_name, f)  in functions:
-                    cols[-1].append(f"{f.__name__}({col_name})")
-                    for col_ix, v in enumerate(row):
-                        cols[col_ix].append(v)
-
-            for col_name, values in zip(rows + ["function"], cols):
-                col_name = unique_name(col_name, result.columns)
-                result[col_name] = values
-            col_length = len(cols[0])
-            cols.clear()
-            
-            # then populate the sparse matrix.
-            for col_key, c in col_key_index.items():
-                col_name = "(" + ",".join([f"{col_name}={value}" for col_name, value in zip(columns, col_key)]) + ")"
-                col_name = unique_name(col_name, result.columns)
-                L = [None for _ in range(col_length)]
-                for r, funcs in records[c].items():
-                    for ix, f in enumerate(funcs):
-                        L[g*r+ix] = f
-                result[col_name] = L
-                
-        else:  # ---> leads to more columns.
-            n = r
-            cols = [[] for _ in range(n)]
-            for row in row_key_index:
-                for col_ix, v in enumerate(row):
-                    cols[col_ix].append(v)  # write key columns.
-            
-            for col_name, values in zip(rows, cols):
-                result[col_name] = values
-            
-            col_length = len(row_key_index)
-
-            # now populate the sparse matrix.
-            for col_key, c in col_key_index.items():  # select column.
-                cols, names = [],[]
-                
-                for f,v in zip(functions, func_key):
-                    agg_col, func = f
-                    col_name = f"{func.__name__}(" + ",".join([agg_col] + [f"{col_name}={value}" for col_name, value in zip(columns, col_key)]) + ")"
-                    col_name = unique_name(col_name, result.columns)
-                    names.append(col_name)
-                    cols.append( [None for _ in range(col_length)] )
-                for r, funcs in records[c].items():
-                    for ix, f in enumerate(funcs):
-                        cols[ix][r] = f
-                for name,col in zip(names,cols):
-                    result[name] = col
-
-        return result
-
-    def _join_type_check(self, other, left_keys, right_keys, left_columns, right_columns):
-        if not isinstance(other, Table):
-            raise TypeError(f"other expected other to be type Table, not {type(other)}")
-
-        if not isinstance(left_keys, list) and all(isinstance(k, str) for k in left_keys):
-            raise TypeError(f"Expected keys as list of strings, not {type(left_keys)}")
-        if not isinstance(right_keys, list) and all(isinstance(k, str) for k in right_keys):
-            raise TypeError(f"Expected keys as list of strings, not {type(right_keys)}")
-
-        if any(key not in self.columns for key in left_keys):
-            raise ValueError(f"left key(s) not found: {[k for k in left_keys if k not in self.columns]}")
-        if any(key not in other.columns for key in right_keys):
-            raise ValueError(f"right key(s) not found: {[k for k in right_keys if k not in other.columns]}")
-
-        if len(left_keys) != len(right_keys):
-            raise ValueError(f"Keys do not have same length: \n{left_keys}, \n{right_keys}")
-
-        for L, R in zip(left_keys, right_keys):
-            Lcol, Rcol = self[L], other[R]
-            if not set(Lcol.types()).intersection(set(Rcol.types())):
-                raise TypeError(f"{L} is {Lcol.types()}, but {R} is {Rcol.types()}")
-
-        if not isinstance(left_columns, list) or not left_columns:
-            raise TypeError("left_columns (list of strings) are required")
-        if any(column not in self.columns for column in left_columns):
-            raise ValueError(f"Column not found: {[c for c in left_columns if c not in self.columns]}")
-
-        if not isinstance(right_columns, list) or not right_columns:
-            raise TypeError("right_columns (list or strings) are required")
-        if any(column not in other.columns for column in right_columns):
-            raise ValueError(f"Column not found: {[c for c in right_columns if c not in other.columns]}")
-        # Input is now guaranteed to be valid.
+    def groupby(self, keys, functions, tqdm=_tqdm, pbar=None):
+        """
+        keys: column names for grouping.
+        functions: [optional] list of column names and group functions (See GroupyBy class)
+        returns: table
+
+        Example:
+
+        t = Table()
+        t.add_column('A', data=[1, 1, 2, 2, 3, 3] * 2)
+        t.add_column('B', data=[1, 2, 3, 4, 5, 6] * 2)
+        t.add_column('C', data=[6, 5, 4, 3, 2, 1] * 2)
+
+        t.show()
+        # +=====+=====+=====+
+        # |  A  |  B  |  C  |
+        # | int | int | int |
+        # +-----+-----+-----+
+        # |    1|    1|    6|
+        # |    1|    2|    5|
+        # |    2|    3|    4|
+        # |    2|    4|    3|
+        # |    3|    5|    2|
+        # |    3|    6|    1|
+        # |    1|    1|    6|
+        # |    1|    2|    5|
+        # |    2|    3|    4|
+        # |    2|    4|    3|
+        # |    3|    5|    2|
+        # |    3|    6|    1|
+        # +=====+=====+=====+
+
+        g = t.groupby(keys=['A', 'C'], functions=[('B', gb.sum)])
+        g.show()
+        # +===+===+===+======+
+        # | # | A | C |Sum(B)|
+        # |row|int|int| int  |
+        # +---+---+---+------+
+        # |0  |  1|  6|     2|
+        # |1  |  1|  5|     4|
+        # |2  |  2|  4|     6|
+        # |3  |  2|  3|     8|
+        # |4  |  3|  2|    10|
+        # |5  |  3|  1|    12|
+        # +===+===+===+======+
+
+        Cheat sheet:
+
+        # list of unique values
+        >>> g1 = t.groupby(keys=['A'], functions=[])
+        >>> g1['A'][:]
+        [1,2,3]
+
+        # alternatively:
+        >>> t['A'].unique()
+        [1,2,3]
+
+        # list of unique values, grouped by longest combination.
+        >>> g2 = t.groupby(keys=['A', 'B'], functions=[])
+        >>> g2['A'][:], g2['B'][:]
+        ([1,1,2,2,3,3], [1,2,3,4,5,6])
+
+        # alternatively:
+        >>> list(zip(*t.index('A', 'B').keys()))
+        [(1,1,2,2,3,3) (1,2,3,4,5,6)]
+
+        # A key (unique values) and count hereof.
+        >>> g3 = t.groupby(keys=['A'], functions=[('A', gb.count)])
+        >>> g3['A'][:], g3['Count(A)'][:]
+        ([1,2,3], [4,4,4])
+
+        # alternatively:
+        >>> t['A'].histogram()
+        ([1,2,3], [4,4,4])
+
+        for more exmaples see:
+            https://github.com/root-11/tablite/blob/master/tests/test_groupby.py
+
+        """
+        return groupbys.groupby(self, keys, functions, tqdm=_tqdm, pbar=None)
+
+    def pivot(self, rows, columns, functions, values_as_rows=True, tqdm=_tqdm, pbar=None):
+        """
+        param: rows: column names to keep as rows
+        param: columns: column names to keep as columns
+        param: functions: aggregation functions from the Groupby class as
+
+        example:
+
+        t.show()
+        # +=====+=====+=====+
+        # |  A  |  B  |  C  |
+        # | int | int | int |
+        # +-----+-----+-----+
+        # |    1|    1|    6|
+        # |    1|    2|    5|
+        # |    2|    3|    4|
+        # |    2|    4|    3|
+        # |    3|    5|    2|
+        # |    3|    6|    1|
+        # |    1|    1|    6|
+        # |    1|    2|    5|
+        # |    2|    3|    4|
+        # |    2|    4|    3|
+        # |    3|    5|    2|
+        # |    3|    6|    1|
+        # +=====+=====+=====+
+
+        t2 = t.pivot(rows=['C'], columns=['A'], functions=[('B', gb.sum)])
+        t2.show()
+        # +===+===+========+=====+=====+=====+
+        # | # | C |function|(A=1)|(A=2)|(A=3)|
+        # |row|int|  str   |mixed|mixed|mixed|
+        # +---+---+--------+-----+-----+-----+
+        # |0  |  6|Sum(B)  |    2|None |None |
+        # |1  |  5|Sum(B)  |    4|None |None |
+        # |2  |  4|Sum(B)  |None |    6|None |
+        # |3  |  3|Sum(B)  |None |    8|None |
+        # |4  |  2|Sum(B)  |None |None |   10|
+        # |5  |  1|Sum(B)  |None |None |   12|
+        # +===+===+========+=====+=====+=====+
 
-    def join(self, other, left_keys, right_keys, left_columns, right_columns, kind='inner'):
+        """
+        return pivots.pivot(self, rows, columns, functions, values_as_rows, tqdm=_tqdm, pbar=None)
+
+    def join(self, other, left_keys, right_keys, left_columns, right_columns, kind="inner", tqdm=_tqdm, pbar=None):
         """
         short-cut for all join functions.
         kind: 'inner', 'left', 'outer', 'cross'
         """
         kinds = {
-            'inner':self.inner_join,
-            'left':self.left_join,
-            'outer':self.outer_join,
-            'cross': self.cross_join,
+            "inner": self.inner_join,
+            "left": self.left_join,
+            "outer": self.outer_join,
+            "cross": self.cross_join,
         }
         if kind not in kinds:
             raise ValueError(f"join type unknown: {kind}")
-        f = kinds.get(kind,None)
-        return f(self,other,left_keys,right_keys,left_columns,right_columns)
-    
-    def _sp_join(self, other, LEFT,RIGHT, left_columns, right_columns):
-        """
-        helper for single processing join
-        """
-        result = Table()
-        for col_name in left_columns:
-            col_data = self[col_name][:]
-            result[col_name] = [col_data[k] if k is not None else None for k in LEFT]
-        for col_name in right_columns:
-            col_data = other[col_name][:]
-            revised_name = unique_name(col_name, result.columns)
-            result[revised_name] = [col_data[k] if k is not None else None for k in RIGHT]
-        return result
-
-    def _mp_join(self, other, LEFT,RIGHT, left_columns, right_columns):
-        """ 
-        helper for multiprocessing join
-        """
-        left_arr = np.zeros(shape=(len(LEFT)), dtype=np.int64)
-        left_shm = shared_memory.SharedMemory(create=True, size=left_arr.nbytes)  # the co_processors will read this.
-        left_index = np.ndarray(left_arr.shape, dtype=left_arr.dtype, buffer=left_shm.buf)
-        left_index[:] = LEFT
-
-        right_arr = np.zeros(shape=(len(RIGHT)), dtype=np.int64)
-        right_shm = shared_memory.SharedMemory(create=True, size=right_arr.nbytes)  # the co_processors will read this.
-        right_index = np.ndarray(right_arr.shape, dtype=right_arr.dtype, buffer=right_shm.buf)
-        right_index[:] = RIGHT
-
-        tasks = []
-        columns_refs = {}
-        for name in left_columns:
-            col = self[name]
-            columns_refs[name] = d_key = mem.new_id('/column')
-            tasks.append(Task(indexing_task, source_key=col.key, destination_key=d_key, shm_name_for_sort_index=left_shm.name, shape=left_arr.shape))
-
-        for name in right_columns:
-            col = other[name]
-            columns_refs[name] = d_key = mem.new_id('/column')
-            tasks.append(Task(indexing_task, source_key=col.key, destination_key=d_key, shm_name_for_sort_index=right_shm.name, shape=right_arr.shape))
-
-        with TaskManager(cpu_count=min(psutil.cpu_count(), len(tasks))) as tm:
-            results = tm.execute(tasks)
-            
-            if any(i is not None for i in results):
-                for err in results:
-                    if err is not None:
-                        print(err)
-                raise Exception("multiprocessing error.")
-            
-        with h5py.File(mem.path, 'r+') as h5:
-            table_key = mem.new_id('/table')
-            dset = h5.create_dataset(name=f"/table/{table_key}", dtype=h5py.Empty('f'))
-            dset.attrs['columns'] = json.dumps(columns_refs)  
-            dset.attrs['saved'] = False
-        
-        left_shm.close()
-        left_shm.unlink()
-        right_shm.close()
-        right_shm.unlink()
-
-        t = Table.load(path=mem.path, key=table_key)
-        return t            
+        f = kinds.get(kind, None)
+        return f(other, left_keys, right_keys, left_columns, right_columns, tqdm=tqdm, pbar=pbar)
 
-    def left_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None):  # TODO: This is slow single core code.
+    def left_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None, tqdm=_tqdm, pbar=None):
         """
         :param other: self, other = (left, right)
         :param left_keys: list of keys for the join
         :param right_keys: list of keys for the join
         :param left_columns: list of left columns to retain, if None, all are retained.
         :param right_columns: list of right columns to retain, if None, all are retained.
         :return: new Table
         Example:
         SQL:   SELECT number, letter FROM numbers LEFT JOIN letters ON numbers.colour == letters.color
-        Tablite: left_join = numbers.left_join(letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter'])
+        Tablite: left_join = numbers.left_join(
+            letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter']
+        )
         """
-        if left_columns is None:
-            left_columns = list(self.columns)
-        if right_columns is None:
-            right_columns = list(other.columns)
-
-        self._join_type_check(other, left_keys, right_keys, left_columns, right_columns)  # raises if error
-
-        left_index = self.index(*left_keys)
-        right_index = other.index(*right_keys)
-        LEFT,RIGHT = [],[]
-        for left_key, left_ixs in left_index.items():
-            right_ixs = right_index.get(left_key, (None,))
-            for left_ix in left_ixs:
-                for right_ix in right_ixs:
-                    LEFT.append(left_ix)
-                    RIGHT.append(right_ix)
-
-        if len(LEFT) * len(left_columns + right_columns) < SINGLE_PROCESSING_LIMIT:
-            return self._sp_join(other, LEFT, RIGHT, left_columns, right_columns)
-        else:  # use multi processing
-            return self._mp_join(other, LEFT, RIGHT, left_columns, right_columns)
-            
-    def inner_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None):  # TODO: This is slow single core code.
+        return joins.left_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=_tqdm, pbar=None)
+
+    def inner_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None, tqdm=_tqdm, pbar=None):
         """
         :param other: self, other = (left, right)
         :param left_keys: list of keys for the join
         :param right_keys: list of keys for the join
         :param left_columns: list of left columns to retain, if None, all are retained.
         :param right_columns: list of right columns to retain, if None, all are retained.
         :return: new Table
         Example:
         SQL:   SELECT number, letter FROM numbers JOIN letters ON numbers.colour == letters.color
-        Tablite: inner_join = numbers.inner_join(letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter'])
+        Tablite: inner_join = numbers.inner_join(
+            letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter']
+            )
         """
-        if left_columns is None:
-            left_columns = list(self.columns)
-        if right_columns is None:
-            right_columns = list(other.columns)
-
-        self._join_type_check(other, left_keys, right_keys, left_columns, right_columns)  # raises if error
-
-        left_index = self.index(*left_keys)
-        right_index = other.index(*right_keys)
-        LEFT,RIGHT = [],[]
-        for left_key, left_ixs in left_index.items():
-            right_ixs = right_index.get(left_key, None)
-            if right_ixs is None:
-                continue
-            for left_ix in left_ixs:
-                for right_ix in right_ixs:
-                    LEFT.append(left_ix)
-                    RIGHT.append(right_ix)
-
-        if len(LEFT) * len(left_columns + right_columns) < SINGLE_PROCESSING_LIMIT:
-            return self._sp_join(other, LEFT, RIGHT, left_columns, right_columns)       
-        else:  # use multi processing
-            return self._mp_join(other, LEFT, RIGHT, left_columns, right_columns)
+        return joins.inner_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=_tqdm, pbar=None)
 
-    def outer_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None):  # TODO: This is single core code.
+    def outer_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None, tqdm=_tqdm, pbar=None):
         """
         :param other: self, other = (left, right)
         :param left_keys: list of keys for the join
         :param right_keys: list of keys for the join
         :param left_columns: list of left columns to retain, if None, all are retained.
         :param right_columns: list of right columns to retain, if None, all are retained.
         :return: new Table
         Example:
         SQL:   SELECT number, letter FROM numbers OUTER JOIN letters ON numbers.colour == letters.color
-        Tablite: outer_join = numbers.outer_join(letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter'])
+        Tablite: outer_join = numbers.outer_join(
+            letters, left_keys=['colour'], right_keys=['color'], left_columns=['number'], right_columns=['letter']
+            )
         """
-        if left_columns is None:
-            left_columns = list(self.columns)
-        if right_columns is None:
-            right_columns = list(other.columns)
-
-        self._join_type_check(other, left_keys, right_keys, left_columns, right_columns)  # raises if error
-
-        left_index = self.index(*left_keys)
-        right_index = other.index(*right_keys)
-        LEFT,RIGHT,RIGHT_UNUSED = [],[], set(right_index.keys())
-        for left_key, left_ixs in left_index.items():
-            right_ixs = right_index.get(left_key, (None,))
-            for left_ix in left_ixs:
-                for right_ix in right_ixs:
-                    LEFT.append(left_ix)
-                    RIGHT.append(right_ix)
-                    RIGHT_UNUSED.discard(left_key)
-
-        for right_key in RIGHT_UNUSED:
-            for right_ix in right_index[right_key]:
-                LEFT.append(None)
-                RIGHT.append(right_ix)
-
-        if len(LEFT) * len(left_columns + right_columns) < SINGLE_PROCESSING_LIMIT:
-            return self._sp_join(other, LEFT, RIGHT, left_columns, right_columns)
-        else:  # use multi processing
-            return self._mp_join(other, LEFT, RIGHT, left_columns, right_columns)
+        return joins.outer_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=_tqdm, pbar=None)
 
-    def cross_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None):
+    def cross_join(self, other, left_keys, right_keys, left_columns=None, right_columns=None, tqdm=_tqdm, pbar=None):
         """
-        CROSS JOIN returns the Cartesian product of rows from tables in the join. 
-        In other words, it will produce rows which combine each row from the first table 
+        CROSS JOIN returns the Cartesian product of rows from tables in the join.
+        In other words, it will produce rows which combine each row from the first table
         with each row from the second table
         """
-        if left_columns is None:
-            left_columns = list(self.columns)
-        if right_columns is None:
-            right_columns = list(other.columns)
-
-        self._join_type_check(other, left_keys, right_keys, left_columns, right_columns)  # raises if error
-
-        LEFT, RIGHT = zip(*itertools.product(range(len(self)), range(len(other))))
-        if len(LEFT) < SINGLE_PROCESSING_LIMIT:
-            return self._sp_join(other, LEFT, RIGHT, left_columns, right_columns)
-        else:  # use multi processing
-            return self._mp_join(other, LEFT, RIGHT, left_columns, right_columns)
-        
-    def lookup(self, other, *criteria, all=True):  # TODO: This is slow single core code.
-        """ function for looking up values in `other` according to criteria in ascending order.
+        return joins.cross_join(self, other, left_keys, right_keys, left_columns, right_columns, tqdm=_tqdm, pbar=None)
+
+    def lookup(self, other, *criteria, all=True, tqdm=_tqdm):
+        """function for looking up values in `other` according to criteria in ascending order.
         :param: other: Table sorted in ascending search order.
         :param: criteria: Each criteria must be a tuple with value comparisons in the form:
             (LEFT, OPERATOR, RIGHT)
         :param: all: boolean: True=ALL, False=Any
+
         OPERATOR must be a callable that returns a boolean
         LEFT must be a value that the OPERATOR can compare.
         RIGHT must be a value that the OPERATOR can compare.
+
         Examples:
               ('column A', "==", 'column B')  # comparison of two columns
               ('Date', "<", DataTypes.date(24,12) )  # value from column 'Date' is before 24/12.
               f = lambda L,R: all( ord(L) < ord(R) )  # uses custom function.
               ('text 1', f, 'text 2')
               value from column 'text 1' is compared with value from column 'text 2'
         """
-        assert isinstance(self, Table)
-        assert isinstance(other, Table)
+        return lookup.lookup(self, other, *criteria, all=all, tqdm=tqdm)
 
-        all = all
-        any = not all
+    def replace_missing_values(self, *args, **kwargs):
+        raise AttributeError("See imputation")
 
-        def not_in(a, b):
-            return not operator.contains(a, b)
+    def imputation(self, targets, missing=None, method="carry forward", sources=None, tqdm=_tqdm):
+        """
+        In statistics, imputation is the process of replacing missing data with substituted values.
 
-        ops = {
-            "in": operator.contains,
-            "not in": not_in,
-            "<": operator.lt,
-            "<=": operator.le,
-            ">": operator.gt,
-            ">=": operator.ge,
-            "!=": operator.ne,
-            "==": operator.eq,
-        }
+        See more: https://en.wikipedia.org/wiki/Imputation_(statistics)
 
-        functions, left_criteria, right_criteria = [], set(), set()
+        Args:
+            table (Table): source table.
 
-        for left, op, right in criteria:
-            left_criteria.add(left)
-            right_criteria.add(right)
-            if callable(op):
-                pass  # it's a custom function.
-            else:
-                op = ops.get(op, None)
-                if not callable(op):
-                    raise ValueError(f"{op} not a recognised operator for comparison.")
-
-            functions.append((op, left, right))
-        left_columns = [n for n in left_criteria if n in self.columns]
-        right_columns = [n for n in right_criteria if n in other.columns]
-
-        results = []
-        lru_cache = {}
-        left = self.__getitem__(*left_columns)
-        if isinstance(left, Column):
-            tmp, left = left, Table()
-            left[left_columns[0]] = tmp
-        right = other.__getitem__(*right_columns)
-        if isinstance(right, Column):
-            tmp, right = right, Table()
-            right[right_columns[0]] = tmp
-        assert isinstance(left, Table)
-        assert isinstance(right, Table)
-
-        for row1 in _tqdm(left.rows, total=self.__len__()):
-            row1_tup = tuple(row1)
-            row1d = {name: value for name, value in zip(left_columns, row1)}
-            row1_hash = hash(row1_tup)
-
-            match_found = True if row1_hash in lru_cache else False
-
-            if not match_found:  # search.
-                for row2ix, row2 in enumerate(right.rows):
-                    row2d = {name: value for name, value in zip(right_columns, row2)}
-
-                    evaluations = {op(row1d.get(left, left), row2d.get(right, right)) for op, left, right in functions}
-                    # The evaluations above does a neat trick:
-                    # as L is a dict, L.get(left, L) will return a value 
-                    # from the columns IF left is a column name. If it isn't
-                    # the function will treat left as a value.
-                    # The same applies to right.
-                    A = all and (False not in evaluations)
-                    B = any and True in evaluations
-                    if A or B:
-                        match_found = True
-                        lru_cache[row1_hash] = row2ix
-                        break
-
-            if not match_found:  # no match found.
-                lru_cache[row1_hash] = None
-            
-            results.append(lru_cache[row1_hash])
-
-        result = self.copy()
-        if len(self) * len(other.columns) < SINGLE_PROCESSING_LIMIT:
-            for col_name in other.columns:
-                col_data = other[col_name][:]
-                revised_name = unique_name(col_name, result.columns)
-                result[revised_name] = [col_data[k] if k is not None else None for k in results]
-            return result
-        else:
-            # 1. create shared memory array.
-            right_arr = np.zeros(shape=(len(results)), dtype=np.int64)
-            right_shm = shared_memory.SharedMemory(create=True, size=right_arr.nbytes)  # the co_processors will read this.
-            right_index = np.ndarray(right_arr.shape, dtype=right_arr.dtype, buffer=right_shm.buf)
-            right_index[:] = results
-            # 2. create tasks
-            tasks = []
-            columns_refs = {}
-
-            for name in other.columns:
-                col = other[name]
-                columns_refs[name] = d_key = mem.new_id('/column')
-                tasks.append(Task(indexing_task, source_key=col.key, destination_key=d_key, shm_name_for_sort_index=right_shm.name, shape=right_arr.shape))
-
-            # 3. let task manager handle the tasks
-            with TaskManager(cpu_count=min(psutil.cpu_count(), len(tasks))) as tm:
-                errs = tm.execute(tasks)
-                if any(errs):
-                    raise Exception(f"multiprocessing error. {[e for e in errs if e]}")
-            
-            # 4. close the share memory and deallocate
-            right_shm.close()
-            right_shm.unlink()
-
-            # 5. update the result table.
-            with h5py.File(mem.path, 'r+') as h5:
-                dset = h5[f"/table/{result.key}"]
-                columns = dset.attrs['columns']
-                columns.update(columns_refs)
-                dset.attrs['columns'] = json.dumps(columns)  
-                dset.attrs['saved'] = False
-            
-            # 6. reload the result table
-            t = Table.load(path=mem.path, key=result.key)
-            return t
-
-
-class Column(object):
-    def __init__(self, data=None, key=None) -> None:
-
-        if key is None:
-            self.key = mem.new_id('/column')
-        else:
-            self.key = key            
-        self.group = f"/column/{self.key}"
-        if key is None:
-            self._len = 0
-            if data is not None:
-                self.extend(data)
-        else:
-            length, pages = mem.load_column_attrs(self.group)
-            self._len = length
-    
-    def __str__(self) -> str:
-        return f"<{self.__class__.__name__}>({self._len} values | key={self.key})"
+            targets (str or list of strings): column names to find and
+                replace missing values
 
-    def __repr__(self) -> str:
-        return self.__str__()
+            missing (any): value to be replaced
 
-    def types(self):
-        """
-        returns dict with datatype: frequency of occurrence
-        """
-        return mem.get_pages(self.group).get_types()
+            method (str): method to be used for replacement. Options:
 
-    @classmethod
-    def load(cls, key):
-        return Column(key=key)
-    
-    def __iter__(self):
-        return (v for v in self.__getitem__())
-
-    def __getitem__(self, item=None):
-        if isinstance(item, int):
-            slc = slice(item,item+1,1)
-        if item is None:
-            slc = slice(0,None,1)
-        if isinstance(item, slice):
-            slc = item
-        if not isinstance(slc, slice):
-            raise TypeError(f"expected slice or int, got {type(item)}")
-        
-        result = mem.get_data(self.group, slc)
-
-        if isinstance(item, int) and len(result)==1:
-            return result[0]
-        else:
-            return result
-
-    def clear(self):
-        old_pages = mem.get_pages(self.group)
-        self._len = mem.create_virtual_dataset(self.group, pages_before=old_pages, pages_after=[])
-
-    def append(self, value):
-        self.__setitem__(key=slice(self._len,None,None), value=[value])
-        
-    def insert(self, index, value):        
-        old_pages = mem.get_pages(self.group)
-        new_pages = old_pages[:]
-
-        ix, start, _, page = old_pages.get_page_by_index(index)
-
-        if mem.get_ref_count(page) == 1:
-            new_page = page  # ref count match. Now let the page class do the insert.
-            new_page.insert(index - start, value)
-        else:
-            data = page[:].tolist()
-            data.insert(index-start,value)
-            new_page = Page(data)  # copy the existing page so insert can be done below
-
-        new_pages[ix] = new_page  # insert the changed page.
-        self._len = mem.create_virtual_dataset(self.group, pages_before=old_pages, pages_after=new_pages)
-    
-    def extend(self, values):
-        self.__setitem__(slice(self._len,None,None), values)  #  self._extend_from_column(values)
-        
-    def remove(self, value):
-        """ see also remove_all """
-        pages = mem.get_pages(self.group)
-        for ix, page in enumerate(pages):
-            if value not in page[:]:
-                continue
-            if mem.get_ref_count(page) == 1:
-                page.remove(value)
-                new_pages = pages[:]
-            else:
-                data = page[:]  # copy the data.
-                data = data.tolist()  
-                data.remove(value)  # remove from the copy.
-                new_page = page(data)  # create new page from copy
-                new_pages = pages[:] 
-                new_pages[ix] = new_page  # register the newly copied page.
-            self._len = mem.create_virtual_dataset(self.group, pages_before=pages, pages_after=new_pages)
-            return
-        raise ValueError(f"value not found: {value}")
-
-    def remove_all(self, value):
-        """ see also remove """
-        pages = mem.get_pages(self.group)
-        new_pages = pages[:]
-        for ix, page in enumerate(pages):
-            if value not in page[:]:
-                continue
-            new_data = [v for v in page[:] if v != value]
-            new_page = Page(new_data)
-            new_pages[ix] = new_page
-        self._len = mem.create_virtual_dataset(self.group, pages_before=pages, pages_after=new_pages)
-        
-    def pop(self, index):
-        index = self._len + index if index < 0 else index
-        if index > self._len:
-            raise IndexError(f"can't reach index {index} when length is {self._len}")
-
-        pages = mem.get_pages(self.group)
-        ix,start,_, page = pages.get_page_by_index(index)
-        if mem.get_ref_count(page) == 1:
-            value = page.pop(index-start)
-        else:
-            data = page[:]
-            value = data.pop(index-start)
-            new_page = Page(data)
-            new_pages = pages[:]
-            new_pages[ix] = new_page
-        shape = mem.create_virtual_dataset(self.group, pages_before=pages, pages_after=new_pages)
-        self._len = shape
-        return value
-
-    def __setitem__(self, key, value):
-        """
-        Column.__setitem__(key,value) behaves just like a list
-        """
-        if isinstance(key, int):
-            if isinstance(value, (list,tuple)):
-                raise TypeError(f"your key is an integer, but your value is a {type(value)}. Did you mean to insert? F.x. [{key}:{key+1}] = {value} ?")
-            if -self._len-1 < key < self._len:
-                key = self._len + key if key < 0 else key
-                pages = mem.get_pages(self.group)
-                ix,start,_,page = pages.get_page_by_index(key)
-                if mem.get_ref_count(page) == 1:
-                    page[key-start] = value
-                else:
-                    data = page[:].tolist()
-                    data[key-start] = value
-                    new_page = Page(data)
-                    new_pages = pages[:]
-                    new_pages[ix] = new_page
-                    self._len = mem.create_virtual_dataset(self.group, pages_before=pages, pages_after=new_pages)
-            else:
-                raise IndexError("list assignment index out of range")
-
-        elif isinstance(key, slice):
-            start,stop,step = key.indices(self._len)
-            if key.start == key.stop == None and key.step in (None,1): 
-                # documentation: new = list(value)
-                # example: L[:] = [1,2,3]
-                before = mem.get_pages(self.group)
-                if isinstance(value, Column):
-                    after = mem.get_pages(value.group)
-                elif isinstance(value, (list,tuple,np.ndarray)):
-                    new_page = Page(value)
-                    after = Pages([new_page])
-                else:
-                    raise TypeError
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-
-            elif key.start != None and key.stop == key.step == None:   
-                # documentation: new = old[:key.start] + list(value)
-                # example: L[0:] = [1,2,3]
-                before = mem.get_pages(self.group) 
-                before_slice = before.getslice(0,start)
-
-                if isinstance(value, Column):
-                    after = before_slice + mem.get_pages(value.group)
-                elif isinstance(value, (list, tuple, np.ndarray)):
-                    if not before_slice:
-                        after = Pages((Page(value),))
-                    else:
-                        last_page = before_slice[-1] 
-                        if mem.get_ref_count(last_page) == 1:
-                            last_page.extend(value)
-                            after = before_slice
-                        else:  # ref count > 1
-                            new_page = Page(value)
-                            after = before_slice + Pages([new_page])
-                else:
-                    raise TypeError
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-
-            elif key.stop != None and key.start == key.step == None:  
-                # documentation: new = list(value) + old[key.stop:] 
-                # example: L[:3] = [1,2,3]
-                before = mem.get_pages(self.group)
-                before_slice = before.getslice(stop, self._len)
-                if isinstance(value, Column):
-                    after = mem.get_pages(value.group) + before_slice
-                elif isinstance(value, (list,tuple, np.ndarray)):
-                    new_page = Page(value)
-                    after = Pages([new_page]) + before_slice
-                else:
-                    raise TypeError
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-                
-            elif key.step == None and key.start != None and key.stop != None:  # L[3:5] = [1,2,3]
-                # documentation: new = old[:start] + list(values) + old[stop:] 
-                
-                stop = max(start,stop)  #  one of python's archaic rules.
-
-                before = mem.get_pages(self.group)
-                A, B = before.getslice(0,start), before.getslice(stop, self._len)
-                if isinstance(value, Column):
-                    after = A + mem.get_pages(value.group) + B
-                elif isinstance(value, (list, tuple, np.ndarray)):
-                    if value:
-                        new_page = Page(value)
-                        after = A + Pages([new_page]) + B  # new = old._getslice_(0,start) + list(value) + old._getslice_(stop,len(self.items))
-                    else:
-                        after = A + B
-                else:
-                    raise TypeError
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-
-            elif key.step != None:
-                seq = range(start,stop,step)
-                seq_size = len(seq)
-                if len(value) > seq_size:
-                    raise ValueError(f"attempt to assign sequence of size {len(value)} to extended slice of size {seq_size}")
-                
-                # documentation: See also test_slice_rules.py/MyList for details
-                before = mem.get_pages(self.group)
-                new = mem.get_data(self.group, slice(None)).tolist()  # new = old[:]  # cheap shallow pointer copy in case anything goes wrong.
-                for new_index, position in zip(range(len(value)), seq):
-                    new[position] = value[new_index]
-                # all went well. No exceptions. Now update self.
-                after = Pages([Page(new)])  # This may seem redundant, but is in fact is good as the user may 
-                # be cleaning up the dataset, so that we end up with a simple datatype instead of mixed.
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-            else:
-                raise KeyError(f"bad key: {key}")
-        else:
-            raise TypeError(f"bad key: {key}")
-
-    def __delitem__(self, key):
-        if isinstance(key, int):
-            if -self._len-1 < key < self._len:
-                before = mem.get_pages(self.group)
-                after = before[:]
-                ix,start,_,page = before.get_page_by_index(key)
-                if mem.get_ref_count(page) == 1:
-                    del page[key-start]
-                else:
-                    data = mem.get_data(page.group)
-                    mask = np.ones(shape=data.shape)
-                    new_data = np.compress(mask, data,axis=0)
-                    after[ix] = Page(new_data)
-            else:
-                raise IndexError("list assignment index out of range")
-
-            self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-
-        elif isinstance(key, slice):
-            start,stop,step = key.indices(self._len)
-            before = mem.get_pages(self.group)
-            if key.start == key.stop == None and key.step in (None,1):   # del L[:] == L.clear()
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=[])
-            elif key.start != None and key.stop == key.step == None:   # del L[0:] 
-                after = before.getslice(0, start)
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-            elif key.stop != None and key.start == key.step == None:  # del L[:3] 
-                after = before.getslice(stop, self._len)
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-            elif key.step == None and key.start != None and key.stop != None:  # del L[3:5]
-                after = before.getslice(0, start) + before.getslice(stop, self._len)
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-            elif key.step != None:
-                before = mem.get_pages(self.group)
-                data = mem.get_data(self.group, slice(None))
-                mask = np.ones(shape=data.shape)
-                for i in range(start,stop,step):
-                    mask[i] = 0
-                new = np.compress(mask, data, axis=0)
-                # all went well. No exceptions.
-                after = Pages([Page(new)])  # This may seem redundant, but is in fact is good as the user may 
-                # be cleaning up the dataset, so that we end up with a simple datatype instead of mixed.
-                self._len = mem.create_virtual_dataset(self.group, pages_before=before, pages_after=after)
-            else:
-                raise TypeError(f"bad key: {key}")
-        else:
-            raise TypeError(f"bad key: {key}")
-
-    def __len__(self):
-        return self._len
-
-    def __eq__(self, other):
-        if isinstance(other, (list,tuple)):
-            return all(a==b for a,b in zip(self[:],other))
-        
-        elif isinstance(other, Column):  
-            if mem.get_pages(self.group) == mem.get_pages(other.group):  # special case.
-                return True  
-            else:
-                return (self[:] == other[:]).all()
-        elif isinstance(other, np.ndarray): 
-            return (self[:]==other).all()
-        else:
-            raise TypeError
-        
-    def copy(self):
-        return Column(data=self)
-
-    def __copy__(self):
-        return self.copy()
-    
-    def index(self):
-        data = self.__getitem__()
-        d = {k:[] for k in np.unique(data)}  
-        for ix,k in enumerate(data):
-            d[k].append(ix)
-        return d
-
-    def unique(self):  
-        try:
-            return np.unique(self.__getitem__())
-        except TypeError:  # np arrays can't handle dtype='O':
-            return np.array({i for i in self.__getitem__()})
-
-    def histogram(self):
-        """ 
-        returns 2 arrays: unique elements and count of each element 
-        
-        example:
-        >>> for item, counts in zip(self.histogram()):
-        >>>     print(item,counts)
+                'carry forward':
+                    takes the previous value, and carries forward into fields
+                    where values are missing.
+                    +: quick. Realistic on time series.
+                    -: Can produce strange outliers.
+
+                'mean':
+                    calculates the column mean (exclude `missing`) and copies
+                    the mean in as replacement.
+                    +: quick
+                    -: doesn't work on text. Causes data set to drift towards the mean.
+
+                'mode':
+                    calculates the column mode (exclude `missing`) and copies
+                    the mean in as replacement.
+                    +: quick
+                    -: most frequent value becomes over-represented in the sample
+
+                'nearest neighbour':
+                    calculates normalised distance between items in source columns
+                    selects nearest neighbour and copies value as replacement.
+                    +: works for any datatype.
+                    -: computationally intensive (e.g. slow)
+
+            sources (list of strings): NEAREST NEIGHBOUR ONLY
+                column names to be used during imputation.
+                if None or empty, all columns will be used.
+
+        Returns:
+            table: table with replaced values.
         """
-        try:
-            uarray, carray = np.unique(self.__getitem__(), return_counts=True)
-        except TypeError:  # np arrays can't handle dtype='O':
-            d = defaultdict(int)
-            for i in self.__getitem__():
-                d[i]+=1
-            uarray, carray = [],[]
-            for k,v in d.items():
-                uarray.append(k), carray.append(v)
-        return uarray, carray
-
-    def statistics(self):
-        """
-        returns dict with:
-        - min (int/float, length of str, date)
-        - max (int/float, length of str, date)
-        - mean (int/float, length of str, date)
-        - median (int/float, length of str, date)
-        - stdev (int/float, length of str, date)
-        - mode (int/float, length of str, date)
-        - distinct (int/float, length of str, date)
-        - iqr (int/float, length of str, date)
-        - sum (int/float, length of str, date)
-        - histogram
-        """
-        return summary_statistics(self.histogram())
-
-    def __add__(self, other):
-        c = self.copy()
-        c.extend(other)
-        return c
-    
-    def __contains__(self, item):
-        return item in self.__getitem__()
-    
-    def __iadd__(self, other):
-        self.extend(other)
-        return self
-    
-    def __imul__(self, other):
-        if not isinstance(other, int):
-            raise TypeError(f"a column can be repeated an integer number of times, not {type(other)} number of times")
-        old_pages = mem.get_pages(self.group)
-        new_pages = old_pages * other
-        self._len = mem.create_virtual_dataset(self.group, old_pages, new_pages)
-        return self
-    
-    def __mul__(self, other):
-        if not isinstance(other, int):
-            raise TypeError(f"a column can be repeated an integer number of times, not {type(other)} number of times")
-        new = Column()
-        old_pages = mem.get_pages(self.group)
-        new_pages = old_pages * other
-        new._len = mem.create_virtual_dataset(new.group, old_pages, new_pages)
-        return new
-    
-    def __ne__(self, other):
-        if len(self) != len(other):  # quick cheap check.
-            return True
-        if not isinstance(other, np.ndarray):
-            other = np.array(other)
-        return (self.__getitem__()!=other).any()  # speedy np c level comparison.
-    
-    def __le__(self,other):
-        raise NotImplemented()
-    
-    def __lt__(self,other):
-        raise NotImplemented()
-    
-    def __ge__(self,other):
-        raise NotImplemented()
-
-    def __gt__(self,other):
-        raise NotImplemented()
-
-
-def _in(a,b):
-    """
-    enables filter function 'in'
-    """
-    return a.decode('utf-8') in b.decode('utf-8')  # TODO tested?
-
-
-filter_ops = {
-            ">": operator.gt,
-            ">=": operator.ge,
-            "==": operator.eq,
-            "<": operator.lt,
-            "<=": operator.le,
-            "!=": operator.ne,
-            "in": _in
-        }
+        return imputation.imputation(self, targets, missing, method, sources, tqdm=_tqdm)
 
-filter_ops_from_text = {
-    "gt": ">",
-    "gteq": ">=",
-    "eq": "==",
-    "lt": "<",
-    "lteq": "<=",
-    "neq": "!=",
-    "in": _in
-}
-
-def filter_evaluation_task(table_key, expression, shm_name, shm_index, shm_shape, slice_):
-    """
-    multiprocessing tasks for evaluating Table.filter
-    """
-    assert isinstance(table_key, str)  # 10 --> group = '/table/10'
-    assert isinstance(expression, dict)
-    assert len(expression)==3
-    assert isinstance(shm_name, str)
-    assert isinstance(shm_index, int)
-    assert isinstance(shm_shape, tuple)
-    assert isinstance(slice_,slice)
-    c1 = expression.get('column1',None)
-    c2 = expression.get('column2',None)
-    c = expression.get('criteria',None)
-    assert c in filter_ops
-    f = filter_ops.get(c)
-    assert callable(f)
-    v1 = expression.get('value1',None)
-    v2 = expression.get('value2',None)
-
-    columns = mem.mp_get_columns(table_key)
-    if c1 is not None:
-        column_key = columns[c1]
-        dset_A = mem.get_data(f'/column/{column_key}', slice_)
-    else:  # v1 is active:
-        dset_A = np.array([v1] * (slice_.stop-slice_.start))
-    
-    if c2 is not None:
-        column_key = columns[c2]
-        dset_B = mem.get_data(f'/column/{column_key}', slice_)
-    else:  # v2 is active:
-        dset_B = np.array([v2] * (slice_.stop-slice_.start))
-
-    existing_shm = shared_memory.SharedMemory(name=shm_name)  # connect
-    result_array = np.ndarray(shm_shape, dtype=np.bool, buffer=existing_shm.buf)
-    result_array[shm_index][slice_] = f(dset_A,dset_B)  # Evaluate
-    existing_shm.close()  # disconnect
-
-
-def filter_merge_task(table_key, true_key, false_key, shm_name, shm_shape, slice_, filter_type):
-    """ 
-    multiprocessing task for merging data after the filter task has been completed.
-    """
-    existing_shm = shared_memory.SharedMemory(name=shm_name)  # connect
-    result_array = np.ndarray(shm_shape, dtype=np.bool, buffer=existing_shm.buf)
-    mask_source = result_array
-
-    if filter_type == 'any':
-        true_mask = np.any(mask_source, axis=0)
-    else:
-        true_mask = np.all(mask_source, axis=0)
-    true_mask = true_mask[slice_]
-    false_mask = np.invert(true_mask)    
-    
-    # 2. load source
-    columns = mem.mp_get_columns(table_key)
-
-    true_columns, false_columns = {}, {}
-    for col_name, column_key in columns.items():
-        col = Column(key=column_key)
-        slize = col[slice_]  # maybe use .tolist() ?
-        true_values = slize[true_mask]
-        if np.any(true_mask):
-            true_columns[col_name] = mem.mp_write_column(true_values)
-        false_values = slize[false_mask]
-        if np.any(false_mask):
-            false_columns[col_name] = mem.mp_write_column(false_values) 
-        
-    mem.mp_write_table(true_key, true_columns)
-    mem.mp_write_table(false_key, false_columns)
-    
-    existing_shm.close()  # disconnect
-
-
-def excel_reader(path, first_row_has_headers=True, sheet=None, columns=None, start=0, limit=sys.maxsize, **kwargs):
-    """
-    returns Table(s) from excel path
-
-    **kwargs are excess arguments that are ignored.
-    """
-    book = pyexcel.get_book(file_name=str(path))
-
-    if sheet is None:  # help the user.
-        raise ValueError(f"No sheet_name declared: \navailable sheets:\n{[s.name for s in book]}")
-    elif sheet not in {ws.name for ws in book}:
-        raise ValueError(f"sheet not found: {sheet}")
-    
-    if not (isinstance(start, int) and start >= 0):
-        raise ValueError("expected start as an integer >=0")
-    if not (isinstance(limit, int) and limit > 0):
-        raise ValueError("expected limit as integer > 0")
-
-    # import all sheets or a subset
-    for ws in book:
-        if ws.name != sheet:
-            continue
-        else:
-            break
-    assert ws.name == sheet, "sheet not found."
-
-    if columns is None:
-        if first_row_has_headers:
-            raise ValueError(f"no columns declared: \navailable columns: {[i[0] for i in ws.columns()]}")
-        else:
-            raise ValueError(f"no columns declared: \navailable columns: {[str(i) for i in range(len(ws.columns()))]}")
-
-    config = {**kwargs, **{"first_row_has_headers":first_row_has_headers, "sheet":sheet, "columns":columns, 'start':start, 'limit':limit}}
-    t = Table(save=True, config=json.dumps(config))
-    for idx, column in enumerate(ws.columns()):
-        
-        if first_row_has_headers:
-            header, start_row_pos = str(column[0]), max(1, start)
-        else:
-            header, start_row_pos = str(idx), max(0,start)
-
-        if header not in columns:
-            continue
-
-        t[header] = [v for v in column[start_row_pos:start_row_pos+limit]]
-    return t
-
-
-def ods_reader(path, first_row_has_headers=True, sheet=None, columns=None, start=0, limit=sys.maxsize, **kwargs):
-    """
-    returns Table from .ODS
-
-    **kwargs are excess arguments that are ignored.
-    """
-    sheets = pyexcel.get_book_dict(file_name=str(path))
-
-    if sheet is None or sheet not in sheets:
-        raise ValueError(f"No sheet_name declared: \navailable sheets:\n{[s.name for s in sheets]}")
-            
-    data = sheets[sheet]
-    for _ in range(len(data)):  # remove empty lines at the end of the data.
-        if "" == "".join(str(i) for i in data[-1]):
-            data = data[:-1]
-        else:
-            break
-    
-    if not (isinstance(start, int) and start >= 0):
-        raise ValueError("expected start as an integer >=0")
-    if not (isinstance(limit, int) and limit > 0):
-        raise ValueError("expected limit as integer > 0")
-
-    config = {**kwargs, **{"first_row_has_headers":first_row_has_headers, "sheet":sheet, "columns":columns, 'start':start, 'limit':limit}}
-    t = Table(save=True, config=json.dumps(config))
-    for ix, value in enumerate(data[0]):
-        if first_row_has_headers:
-            header, start_row_pos = str(value), 1
-        else:
-            header, start_row_pos = f"_{ix + 1}", 0
-
-        if columns is not None:
-            if header not in columns:
-                continue    
-
-        t[header] = [row[ix] for row in data[start_row_pos:start_row_pos+limit] if len(row) > ix]
-    return t
-
-
-def text_reader(path, newline='\n', text_qualifier=None, delimiter=',', first_row_has_headers=True, 
-                columns=None, start=0, limit=sys.maxsize, 
-                strip_leading_and_tailing_whitespace=True, encoding=None, tqdm=_tqdm, **kwargs):
-    """
-    **kwargs are excess arguments that are ignored.
-    """
-    # define and specify tasks.
-    memory_usage_ceiling = 0.9
-    free_memory = psutil.virtual_memory().free * memory_usage_ceiling
-    free_memory_per_vcpu = free_memory / psutil.cpu_count()
-    
-    path = pathlib.Path(path)
-       
-    if encoding is None:
-        with path.open('rb') as fi:
-            rawdata = fi.read(10000)
-            encoding = chardet.detect(rawdata)['encoding']       
-
-    text_escape = TextEscape(delimiter=delimiter, qoute=text_qualifier, strip_leading_and_tailing_whitespace=strip_leading_and_tailing_whitespace)  # configure t.e.
-
-    config = {
-            "source":None,
-            "table_key":None, 
-            "columns":columns,
-            "newline":newline, 
-            "delimiter":delimiter, 
-            "qoute":text_qualifier,
-            "text_escape_openings":'', 
-            "text_escape_closures":'',
-            "encoding":encoding,
-            "strip_leading_and_tailing_whitespace":strip_leading_and_tailing_whitespace
-        }
+    def transpose(self, tqdm=_tqdm):
+        return pivots.transpose(self, tqdm)
 
-    tasks = []
-    with path.open('r', encoding=encoding) as fi:
-        # task: find chunk ...
-        # Here is the problem in a nutshell:
-        # --------------------------------------------------------
-        # bs = "this is my \n text".encode('utf-16')
-        # >>> bs
-        # b'\xff\xfet\x00h\x00i\x00s\x00 \x00i\x00s\x00 \x00m\x00y\x00 \x00\n\x00 \x00t\x00e\x00x\x00t\x00'
-        # >>> nl = "\n".encode('utf-16')
-        # >>> nl in bs
-        # False
-        # >>> nl.decode('utf-16') in bs.decode('utf-16')
-        # True
-        # --------------------------------------------------------
-        # This means we can't read the encoded stream to check if in contains a particular character.
-        # We will need to decode it.
-        # furthermore fi.tell() will not tell us which character we a looking at.
-        # so the only option left is to read the file and split it in workable chunks.
-        for line in fi:
-            header_line = line
-            line = line.rstrip('\n')
-            break  # break on first
-        fi.seek(0)
-        headers = text_escape(line) # use t.e.
-
-        if first_row_has_headers:
-            if not columns:
-                raise ValueError(f"No columns selected:\nAvailable columns: {headers}")
-            for name in columns:
-                if name not in headers:
-                    raise ValueError(f"column not found: {name}")
-        else: # no headers.
-            valid_indices = [str(i) for i in range(len(headers))]
-            if not columns:
-                raise ValueError(f"No column index selected:\nAvailable columns: {valid_indices}")
-            for index in columns:
-                if index not in valid_indices:
-                    raise IndexError(f"{index} not in valid range: {valid_indices}")
-            header_line = delimiter.join(valid_indices) + '\n'
-
-        if not (isinstance(start, int) and start >= 0):
-            raise ValueError("expected start as an integer >= 0")
-        if not (isinstance(limit, int) and limit > 0):
-            raise ValueError("expected limit as integer > 0")
-
-        try:
-            newlines = sum(1 for _ in fi)  # log.info(f"{newlines} lines found.")
-            fi.seek(0)
-        except Exception as e:
-            raise ValueError(f"file could not be read with encoding={encoding}\n{str(e)}")
-
-        file_length = path.stat().st_size  # 9,998,765,432 = 10Gb
-        bytes_per_line = math.ceil(file_length / newlines)
-        working_overhead = 40  # MemoryError will occur if you get this wrong.
-        
-        total_workload = working_overhead * file_length
-        cpu_count = max(psutil.cpu_count(logical=False) - 1, 1) # there's always at least one core!
-        memory_usage_ceiling = 0.9
-        
-        free_memory = int(psutil.virtual_memory().free * memory_usage_ceiling) - cpu_count * 20e6  # 20Mb per subproc.
-        free_memory_per_vcpu = int(free_memory / cpu_count)  # 8 gb/ 16vCPU = 500Mb/vCPU
-        
-        if total_workload < free_memory_per_vcpu and total_workload < 10_000_000:  # < 1Mb --> use 1 vCPU
-            lines_per_task = newlines
-        else:  # total_workload > free_memory or total_workload > 10_000_000
-            use_all_memory = free_memory_per_vcpu / (bytes_per_line * working_overhead)  # 500Mb/vCPU / (10 * 109 bytes / line ) = 458715 lines per task
-            use_all_cores = newlines / (cpu_count)  # 8,000,000 lines / 16 vCPU = 500,000 lines per task
-            lines_per_task = int(min(use_all_memory, use_all_cores))
-        
-        if not cpu_count * lines_per_task * bytes_per_line * working_overhead < free_memory:
-            raise ValueError(f"{[cpu_count, lines_per_task , bytes_per_line , working_overhead , free_memory]}")
-        assert newlines / lines_per_task >= 1
-        
-        if newlines <= start + (1 if first_row_has_headers else 0):  # Then start > end.
-            t = Table()
-            t.add_columns(*list(columns.keys()))
-            t.save = True
-            return t
-
-        parts = []
-        
-        assert header_line != ""
-        with tqdm(desc=f"splitting {path.name} for multiprocessing", total=newlines, unit="lines") as pbar:
-            for ix, line in enumerate(fi, start=(-1 if first_row_has_headers else 0) ):
-                if ix < start:
-                    # ix is -1 if the first row has headers, but header_line already has the first line.
-                    # ix is 0 if there are no headers, and if start is 0, the first row is added to parts.
-                    continue
-                if ix >= start + limit:
-                    break
-
-                parts.append(line)
-                if ix!=0 and ix % lines_per_task == 0:
-                    p = TEMPDIR / (path.stem + f'{ix}' + path.suffix)
-                    with p.open('w', encoding=H5_ENCODING) as fo:
-                        parts.insert(0, header_line)
-                        fo.write("".join(parts))
-                    pbar.update(len(parts))
-                    parts.clear()
-                    tasks.append( Task( text_reader_task, **{**config, **{"source":str(p), "table_key":mem.new_id('/table'), 'encoding':'utf-8'}} ) )
-                    
-            if parts:  # any remaining parts at the end of the loop.
-                p = TEMPDIR / (path.stem + f'{ix}' + path.suffix)
-                with p.open('w', encoding=H5_ENCODING) as fo:
-                    parts.insert(0, header_line)
-                    fo.write("".join(parts))
-                pbar.update(len(parts))
-                parts.clear()
-                config.update({"source":str(p), "table_key":mem.new_id('/table')})
-                tasks.append( Task( text_reader_task, **{**config, **{"source":str(p), "table_key":mem.new_id('/table'), 'encoding':'utf-8'}} ) )
-        
-        # execute the tasks
-    # with TaskManager(cpu_count=min(cpu_count, len(tasks))) as tm:
-    with TaskManager(cpu_count) as tm:
-        errors = tm.execute(tasks)   # I expects a list of None's if everything is ok.
-        
-        # clean up the tmp source files, before raising any exception.
-        for task in tasks:
-            tmp = pathlib.Path(task.kwargs['source'])
-            tmp.unlink()
-
-        if any(errors):
-            raise Exception("\n".join(e for e in errors if e))
-
-    # consolidate the task results
-    t = None
-    for task in tasks:
-        tmp = Table.load(path=mem.path, key=task.kwargs["table_key"])
-        if t is None:
-            t = tmp.copy()
-        else:
-            t += tmp
-        tmp.save = False  # allow deletion of subproc tables.
-    t.save = True
-    return t
-
-
-def text_reader_task(source, table_key, columns, 
-    newline, delimiter=',', qoute='"', text_escape_openings='', text_escape_closures='', 
-    strip_leading_and_tailing_whitespace=True, encoding='utf-8'):
-    """ PARALLEL TASK FUNCTION
-    reads columnsname + path[start:limit] into hdf5.
-
-    source: csv or txt file
-    destination: available filename
-    
-    columns: column names or indices to import
-
-    newline: '\r\n' or '\n'
-    delimiter: ',' ';' or '|'
-    text_escape_openings: str: default: "({[ 
-    text_escape_closures: str: default: ]})" 
-    strip_leading_and_tailing_whitespace: bool
-
-    encoding: chardet encoding ('utf-8, 'ascii', ..., 'ISO-22022-CN')
-    root: hdf5 root, cannot be the same as a column name.
-    """
-    if isinstance(source, str):
-        source = pathlib.Path(source)
-    if not isinstance(source, pathlib.Path):
-        raise TypeError()
-    if not source.exists():
-        raise FileNotFoundError(f"File not found: {source}")
-
-    if not isinstance(table_key, str):
-        raise TypeError()
-
-    if not isinstance(columns, dict):
-        raise TypeError
-    if not all(isinstance(name,str) for name in columns):
-        raise ValueError()
-
-    # declare CSV dialect.
-    text_escape = TextEscape(text_escape_openings, text_escape_closures, qoute=qoute, delimiter=delimiter, 
-                             strip_leading_and_tailing_whitespace=strip_leading_and_tailing_whitespace)
-
-    with source.open('r', encoding=encoding) as fi:  # --READ
-        for line in fi:
-            line = line.rstrip(newline)
-            break  # break on first
-        headers = text_escape(line)
-        indices = {name: headers.index(name) for name in columns}        
-        data = {h: [] for h in indices}
-        for line in fi:  # 1 IOP --> RAM.
-            fields = text_escape(line.rstrip('\n'))
-            if fields == [""] or fields == []:
-                break
-            for header,index in indices.items():
-                data[header].append(fields[index])
-    # -- WRITE
-    columns_refs = {}
-    for col_name, values in data.items():
-        values = DataTypes.guess(values)
-        columns_refs[col_name] = mem.mp_write_column(values)
-    mem.mp_write_table(table_key, columns=columns_refs)
-
-
-file_readers = {
-    'fods': excel_reader,
-    'json': excel_reader,
-    'html': excel_reader,
-    'simple': excel_reader,
-    'rst': excel_reader,
-    'mediawiki': excel_reader,
-    'xlsx': excel_reader,
-    'xls': excel_reader,
-    'xlsm': excel_reader,
-    'csv': text_reader,
-    'tsv': text_reader,
-    'txt': text_reader,
-    'ods': ods_reader
-}
-
-def _check_input(table, path):
-    if not isinstance(table, Table): 
-        raise TypeError
-    if not isinstance(path, pathlib.Path):
-        raise TypeError
-
-
-def excel_writer(table, path):
-    """ 
-    writer for excel files. 
-    
-    This can create xlsx files beyond Excels.
-    If you're using pyexcel to read the data, you'll see the data is there.
-    If you're using Excel, Excel will stop loading after 1,048,576 rows.
-    
-    See pyexcel for more details:
-    http://docs.pyexcel.org/
-    """
-    _check_input(table,path)
-        
-    def gen(table):  # local helper 
-        yield table.columns
-        for row in table.rows:
-            yield row
-    data = list(gen(table))
-    if path.suffix in ['.xls', '.ods']:
-        data = [[str(v) if (isinstance(v, (int,float)) and abs(v) > 2**32-1) else DataTypes.to_json(v) for v in row] for row in data]
-    
-    pyexcel.save_as(array=data, dest_file_name=str(path))
-
-
-def text_writer(table, path):
-    """ exports table to csv, tsv or txt dependening on path suffix.
-    follows the JSON norm. text escape is ON for all strings.
-    
-    """
-    _check_input(table,path)
-
-    def txt(value):  # helper for text writer
-        if isinstance(value, str):
-            if not (value.startswith('"') and value.endswith('"')):
-                return f'"{value}"'  # this must be escape: "the quick fox, jumped over the comma"
-            else:
-                return value  # this would for example be an empty string: ""
-        else:
-            return str(DataTypes.to_json(value))  # this handles datetimes, timedelta, etc.
-
-    delimiters = {
-        ".csv": ',',
-        ".tsv": '\t',
-        ".txt": '|'
-    }
-    delimiter = delimiters.get(path.suffix)
-
-    with path.open('w', encoding='utf-8') as fo:
-        fo.write(delimiter.join(c for c in table.columns)+'\n')
-        for row in _tqdm(table.rows, total=len(table)):
-            fo.write(delimiter.join(txt(c) for c in row)+'\n')
-
-def sql_writer(table, path):
-    _check_input(table,path)
-    with path.open('w', encoding='utf-8') as fo:
-        fo.write(table.to_sql())
-
-def json_writer(table, path):
-    _check_input(table,path)
-    with path.open('w') as fo:
-        fo.write(table.to_json())
-
-def h5_writer(table, path):
-    _check_input(table,path)
-    table.to_hdf5(path)
-
-def html_writer(table, path):
-    _check_input(table,path)
-    with path.open('w', encoding='utf-8') as fo:
-        fo.write(table._repr_html_())
-
-
-exporters = {  # the commented formats are not yet supported by the pyexcel plugins:
-    # 'fods': excel_writer,
-    'json': json_writer,
-    'html': html_writer,
-    # 'simple': excel_writer,
-    # 'rst': excel_writer,
-    # 'mediawiki': excel_writer,
-    'xlsx': excel_writer,
-    'xls': excel_writer,
-    # 'xlsm': excel_writer,
-    'csv': text_writer,
-    'tsv': text_writer,
-    'txt': text_writer,
-    'ods': excel_writer,
-    'sql': sql_writer,
-    # 'hdf5': h5_writer,
-    # 'h5': h5_writer
-}
-
-
-def indexing_task(source_key, destination_key, shm_name_for_sort_index, shape):
-    """
-    performs the creation of a column sorted by sort_index (shared memory object).
-    source_key: column to read
-    destination_key: column to write
-    shm_name_for_sort_index: sort index' shm.name created by main.
-    shape: shm array shape.
-
-    *used by sort and all join functions.
-    """
-    existing_shm = shared_memory.SharedMemory(name=shm_name_for_sort_index)  # connect
-    sort_index = np.ndarray(shape, dtype=np.int64, buffer=existing_shm.buf)
-
-    data = mem.get_data(f'/column/{source_key}', slice(None)) # --- READ!
-    values = [data[ix] for ix in sort_index]
-    
-    existing_shm.close()  # disconnect
-    mem.mp_write_column(values, column_key=destination_key)  # --- WRITE!
-
-
-def compress_task(source_key, destination_key, shm_index_name, shape):
-    """
-    compresses the source using boolean mask from shared memory
-
-    source_key: column to read
-    destination_key: column to write
-    shm_name_for_sort_index: sort index' shm.name created by main.
-    shape: shm array shape.
-    """
-    existing_shm = shared_memory.SharedMemory(name=shm_index_name)  # connect
-    mask = np.ndarray(shape, dtype=np.int64, buffer=existing_shm.buf)
-    
-    data = mem.get_data(f'/column/{source_key}', slice(None))  # --- READ!
-    values = np.compress(mask, data)
-    
-    existing_shm.close()  # disconnect
-    mem.mp_write_column(values, column_key=destination_key)  # --- WRITE!
+    def pivot_transpose(self, columns, keep=None, column_name="transpose", value_name="value", tqdm=_tqdm):
+        """Transpose a selection of columns to rows.
+
+        Args:
+            columns (list of column names): column names to transpose
+            keep (list of column names): column names to keep (repeat)
+
+        Returns:
+            Table: with columns transposed to rows
 
+        Example:
+            transpose columns 1,2 and 3 and transpose the remaining columns, except `sum`.
+
+        Input:
+
+        | col1 | col2 | col3 | sun | mon | tue | ... | sat | sum  |
+        |------|------|------|-----|-----|-----|-----|-----|------|
+        | 1234 | 2345 | 3456 | 456 | 567 |     | ... |     | 1023 |
+        | 1244 | 2445 | 4456 |     |   7 |     | ... |     |    7 |
+        | ...  |      |      |     |     |     |     |     |      |
+
+        t.transpose(keep=[col1, col2, col3], transpose=[sun,mon,tue,wed,thu,fri,sat])`
+
+        Output:
+
+        |col1| col2| col3| transpose| value|
+        |----|-----|-----|----------|------|
+        |1234| 2345| 3456| sun      |   456|
+        |1234| 2345| 3456| mon      |   567|
+        |1244| 2445| 4456| mon      |     7|
 
+        """
+        return pivots.pivot_transpose(self, columns, keep, column_name, value_name, tqdm=_tqdm)
+
+    def diff(self, other, columns=None):
+        """compares table self with table other
+
+        Args:
+            self (Table): Table
+            other (Table): Table
+            columns (List, optional): list of column names to include in comparison. Defaults to None.
+
+        Returns:
+            Table: diff of self and other with diff in columns 1st and 2nd.
+        """
+        return diff.diff(self, other, columns)
```

## tablite/datatypes.py

```diff
@@ -1,155 +1,180 @@
 from datetime import date, datetime, time, timedelta, timezone
 from collections import defaultdict
 import numpy as np
 import pickle
 
 
 class DataTypes(object):
+    """DataTypes is the conversion library for all datatypes.
+
+    It supports any / all python datatypes.
+    """
+
     # supported datatypes.
     int = int
     str = str
     float = float
     bool = bool
     date = date
     datetime = datetime
     time = time
-    timedelta = timedelta  
+    timedelta = timedelta
 
     numeric_types = {int, float, date, time, datetime}
-    epoch = datetime(2000,1,1,0,0,0,0,timezone.utc)
-    epoch_no_tz = datetime(2000,1,1,0,0,0,0)
-    digits = '1234567890'
-    decimals = set('1234567890-+eE.')
-    integers = set('1234567890-+')
-    nones = {'null', 'Null', 'NULL', '#N/A', '#n/a', "", 'None', None, np.nan}
+    epoch = datetime(2000, 1, 1, 0, 0, 0, 0, timezone.utc)
+    epoch_no_tz = datetime(2000, 1, 1, 0, 0, 0, 0)
+    digits = "1234567890"
+    decimals = set("1234567890-+eE.")
+    integers = set("1234567890-+")
+    nones = {"null", "Null", "NULL", "#N/A", "#n/a", "", "None", None, np.nan}
     none_type = type(None)
 
-    _type_codes ={
+    _type_codes = {
         type(None): 1,
         bool: 2,
         int: 3,
         float: 4,
         str: 5,
         bytes: 6,
         datetime: 7,
         date: 8,
         time: 9,
         timedelta: 10,
-        'pickle': 11
+        "pickle": 11,
     }
 
     @classmethod
-    def type_code(cls, value): 
+    def type_code(cls, value):
         if type(value) in cls._type_codes:
             return cls._type_codes[type(value)]
-        elif hasattr(value,'dtype'):
-            dtype = numpy_types.get(np.dtype(value).name)
+        elif hasattr(value, "dtype"):
+            dtype = pytype(value)
             return cls._type_codes[dtype]
         else:
-            return cls._type_codes['pickle']
-    
+            return cls._type_codes["pickle"]
+
     def b_none(v):
         return b"None"
+
     def b_bool(v):
-        return bytes(str(v), encoding='utf-8')
+        return bytes(str(v), encoding="utf-8")
+
     def b_int(v):
-        return bytes(str(v), encoding='utf-8')
+        return bytes(str(v), encoding="utf-8")
+
     def b_float(v):
-        return bytes(str(v), encoding='utf-8')
+        return bytes(str(v), encoding="utf-8")
+
     def b_str(v):
-        return v.encode('utf-8')
+        return v.encode("utf-8")
+
     def b_bytes(v):
         return v
+
     def b_datetime(v):
-        return bytes(v.isoformat(), encoding='utf-8')
+        return bytes(v.isoformat(), encoding="utf-8")
+
     def b_date(v):
-        return bytes(v.isoformat(), encoding='utf-8')
+        return bytes(v.isoformat(), encoding="utf-8")
+
     def b_time(v):
-        return bytes(v.isoformat(), encoding='utf-8')
+        return bytes(v.isoformat(), encoding="utf-8")
+
     def b_timedelta(v):
-        return bytes(str(float(v.days + (v.seconds / (24*60*60)))), 'utf-8')
+        return bytes(str(float(v.days + (v.seconds / (24 * 60 * 60)))), "utf-8")
+
     def b_pickle(v):
         return pickle.dumps(v, protocol=0)
-        
+
     bytes_functions = {
         type(None): b_none,
         bool: b_bool,
         int: b_int,
         float: b_float,
         str: b_str,
         bytes: b_bytes,
         datetime: b_datetime,
         date: b_date,
         time: b_time,
-        timedelta: b_timedelta
+        timedelta: b_timedelta,
     }
 
     @classmethod
     def to_bytes(cls, v):
         if type(v) in cls.bytes_functions:  # it's a python native type
             f = cls.bytes_functions[type(v)]
-        elif hasattr(v, 'dtype'):  # it's a numpy/c type.
-            dtype = numpy_types.get(np.dtype(v).name)
+        elif hasattr(v, "dtype"):  # it's a numpy/c type.
+            dtype = pytype(v)
             f = cls.bytes_functions[dtype]
         else:
             f = cls.b_pickle
         return f(v)
 
     def _none(v):
         return None
+
     def _bool(v):
-        return bool(v.decode('utf-8'))
+        return bool(v.decode("utf-8") == "True")
+
     def _int(v):
-        return int(v.decode('utf-8'))
+        return int(v.decode("utf-8"))
+
     def _float(v):
-        return float(v.decode('utf-8'))
+        return float(v.decode("utf-8"))
+
     def _str(v):
-        return v.decode('utf-8')
+        return v.decode("utf-8")
+
     def _bytes(v):
         return v
+
     def _datetime(v):
-        return datetime.fromisoformat(v.decode('utf-8'))
+        return datetime.fromisoformat(v.decode("utf-8"))
+
     def _date(v):
-        return date.fromisoformat(v.decode('utf-8'))
+        return date.fromisoformat(v.decode("utf-8"))
+
     def _time(v):
-        return time.fromisoformat(v.decode('utf-8'))
+        return time.fromisoformat(v.decode("utf-8"))
+
     def _timedelta(v):
         days = float(v)
-        seconds = 24 * 60 * 60 * ( float(v) - int( float(v) ) )
+        seconds = 24 * 60 * 60 * (float(v) - int(float(v)))
         return timedelta(int(days), seconds)
+
     def _unpickle(v):
         return pickle.loads(v)
-    
+
     type_code_functions = {
         1: _none,
         2: _bool,
         3: _int,
         4: _float,
         5: _str,
         6: _bytes,
         7: _datetime,
         8: _date,
         9: _time,
         10: _timedelta,
-        11: _unpickle
+        11: _unpickle,
     }
 
     pytype_from_type_code = {
         1: type(None),
         2: bool,
         3: int,
         4: float,
         5: str,
         6: bytes,
         7: datetime,
         8: date,
         9: time,
         10: timedelta,
-        11: 'pickled object'
+        11: "pickled object",
     }
 
     @classmethod
     def from_type_code(cls, value, code):
         f = cls.type_code_functions[code]
         return f(value)
 
@@ -187,161 +212,176 @@
         "NN N NNNN": lambda x: date(*[int(i) for i in x.split(" ")][::-1]),
         "N NN NNNN": lambda x: date(*[int(i) for i in x.split(" ")][::-1]),
         "NNNNNNNN": lambda x: date(*(int(x[:4]), int(x[4:6]), int(x[6:]))),
     }
 
     datetime_formats = {
         # Note: Only recognised ISO8601 formats are accepted.
-
         # year first
-        'NNNN-NN-NNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x),  # -T
-        'NNNN-NN-NNTNN:NN': lambda x: DataTypes.pattern_to_datetime(x),
-
-        'NNNN-NN-NN NN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, T=" "),  # - space
-        'NNNN-NN-NN NN:NN': lambda x: DataTypes.pattern_to_datetime(x, T=" "),
-
-        'NNNN/NN/NNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/'),  # / T
-        'NNNN/NN/NNTNN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/'),
-
-        'NNNN/NN/NN NN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', T=" "),  # / space
-        'NNNN/NN/NN NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', T=" "),
-
-        'NNNN NN NNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd=' '),  # space T
-        'NNNN NN NNTNN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd=' '),
-
-        'NNNN NN NN NN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd=' ', T=" "),  # space
-        'NNNN NN NN NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd=' ', T=" "),
-
-        'NNNN.NN.NNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='.'),  # dot T
-        'NNNN.NN.NNTNN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='.'),
-
-        'NNNN.NN.NN NN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='.', T=" "),  # dot
-        'NNNN.NN.NN NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='.', T=" "),
-
-
+        "NNNN-NN-NNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x),  # -T
+        "NNNN-NN-NNTNN:NN": lambda x: DataTypes.pattern_to_datetime(x),
+        "NNNN-NN-NN NN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, T=" "),  # - space
+        "NNNN-NN-NN NN:NN": lambda x: DataTypes.pattern_to_datetime(x, T=" "),
+        "NNNN/NN/NNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/"),  # / T
+        "NNNN/NN/NNTNN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/"),
+        "NNNN/NN/NN NN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", T=" "),  # / space
+        "NNNN/NN/NN NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", T=" "),
+        "NNNN NN NNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=" "),  # space T
+        "NNNN NN NNTNN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=" "),
+        "NNNN NN NN NN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=" ", T=" "),  # space
+        "NNNN NN NN NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=" ", T=" "),
+        "NNNN.NN.NNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="."),  # dot T
+        "NNNN.NN.NNTNN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="."),
+        "NNNN.NN.NN NN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=".", T=" "),  # dot
+        "NNNN.NN.NN NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=".", T=" "),
         # day first
-        'NN-NN-NNNNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='-', T=' ', day_first=True),  # - T
-        'NN-NN-NNNNTNN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='-', T=' ', day_first=True),
-
-        'NN-NN-NNNN NN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='-', T=' ', day_first=True),  # - space
-        'NN-NN-NNNN NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='-', T=' ', day_first=True),
-
-        'NN/NN/NNNNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', day_first=True),  # / T
-        'NN/NN/NNNNTNN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', day_first=True),
-
-        'NN/NN/NNNN NN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', T=' ', day_first=True),  # / space
-        'NN/NN/NNNN NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', T=' ', day_first=True),
-
-        'NN NN NNNNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', day_first=True),  # space T
-        'NN NN NNNNTNN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', day_first=True),
-
-        'NN NN NNNN NN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', day_first=True),  # space
-        'NN NN NNNN NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='/', day_first=True),
-
-        'NN.NN.NNNNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='.', day_first=True),  # space T
-        'NN.NN.NNNNTNN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='.', day_first=True),
-
-        'NN.NN.NNNN NN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='.', day_first=True),  # space
-        'NN.NN.NNNN NN:NN': lambda x: DataTypes.pattern_to_datetime(x, ymd='.', day_first=True),
-
+        "NN-NN-NNNNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="-", T=" ", day_first=True),  # - T
+        "NN-NN-NNNNTNN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="-", T=" ", day_first=True),
+        "NN-NN-NNNN NN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="-", T=" ", day_first=True),  # - space
+        "NN-NN-NNNN NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="-", T=" ", day_first=True),
+        "NN/NN/NNNNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", day_first=True),  # / T
+        "NN/NN/NNNNTNN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", day_first=True),
+        "NN/NN/NNNN NN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", T=" ", day_first=True),  # / space
+        "NN/NN/NNNN NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", T=" ", day_first=True),
+        "NN NN NNNNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", day_first=True),  # space T
+        "NN NN NNNNTNN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", day_first=True),
+        "NN NN NNNN NN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", day_first=True),  # space
+        "NN NN NNNN NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd="/", day_first=True),
+        "NN.NN.NNNNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=".", day_first=True),  # space T
+        "NN.NN.NNNNTNN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=".", day_first=True),
+        "NN.NN.NNNN NN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=".", day_first=True),  # space
+        "NN.NN.NNNN NN:NN": lambda x: DataTypes.pattern_to_datetime(x, ymd=".", day_first=True),
         # compact formats - type 1
-        'NNNNNNNNTNNNNNN': lambda x: DataTypes.pattern_to_datetime(x, compact=1),
-        'NNNNNNNNTNNNN': lambda x: DataTypes.pattern_to_datetime(x, compact=1),
-        'NNNNNNNNTNN': lambda x: DataTypes.pattern_to_datetime(x, compact=1),
+        "NNNNNNNNTNNNNNN": lambda x: DataTypes.pattern_to_datetime(x, compact=1),
+        "NNNNNNNNTNNNN": lambda x: DataTypes.pattern_to_datetime(x, compact=1),
+        "NNNNNNNNTNN": lambda x: DataTypes.pattern_to_datetime(x, compact=1),
         # compact formats - type 2
-        'NNNNNNNNNN': lambda x: DataTypes.pattern_to_datetime(x, compact=2),
-        'NNNNNNNNNNNN': lambda x: DataTypes.pattern_to_datetime(x, compact=2),
-        'NNNNNNNNNNNNNN': lambda x: DataTypes.pattern_to_datetime(x, compact=2),
+        "NNNNNNNNNN": lambda x: DataTypes.pattern_to_datetime(x, compact=2),
+        "NNNNNNNNNNNN": lambda x: DataTypes.pattern_to_datetime(x, compact=2),
+        "NNNNNNNNNNNNNN": lambda x: DataTypes.pattern_to_datetime(x, compact=2),
         # compact formats - type 3
-        'NNNNNNNNTNN:NN:NN': lambda x: DataTypes.pattern_to_datetime(x, compact=3),
+        "NNNNNNNNTNN:NN:NN": lambda x: DataTypes.pattern_to_datetime(x, compact=3),
     }
 
     @staticmethod
     def pattern_to_datetime(iso_string, ymd=None, T=None, compact=0, day_first=False):
         assert isinstance(iso_string, str)
         if compact:
             s = iso_string
             if compact == 1:  # has T
-                slices = [(0, 4, "-"), (4, 6, "-"), (6, 8, "T"), (9, 11, ":"), (11, 13, ":"), (13, len(s), "")]
+                slices = [
+                    (0, 4, "-"),
+                    (4, 6, "-"),
+                    (6, 8, "T"),
+                    (9, 11, ":"),
+                    (11, 13, ":"),
+                    (13, len(s), ""),
+                ]
             elif compact == 2:  # has no T.
-                slices = [(0, 4, "-"), (4, 6, "-"), (6, 8, "T"), (8, 10, ":"), (10, 12, ":"), (12, len(s), "")]
+                slices = [
+                    (0, 4, "-"),
+                    (4, 6, "-"),
+                    (6, 8, "T"),
+                    (8, 10, ":"),
+                    (10, 12, ":"),
+                    (12, len(s), ""),
+                ]
             elif compact == 3:  # has T and :
-                slices = [(0, 4, "-"), (4, 6, "-"), (6, 8, "T"), (9, 11, ":"), (12, 14, ":"), (15, len(s), "")]
+                slices = [
+                    (0, 4, "-"),
+                    (4, 6, "-"),
+                    (6, 8, "T"),
+                    (9, 11, ":"),
+                    (12, 14, ":"),
+                    (15, len(s), ""),
+                ]
             else:
                 raise TypeError
             iso_string = "".join([s[a:b] + c for a, b, c in slices if b <= len(s)])
             iso_string = iso_string.rstrip(":")
 
         if day_first:
             s = iso_string
             iso_string = "".join((s[6:10], "-", s[3:5], "-", s[0:2], s[10:]))
 
         if "," in iso_string:
             iso_string = iso_string.replace(",", ".")
 
-        dot = iso_string[::-1].find('.')
+        dot = iso_string[::-1].find(".")
         if 0 < dot < 10:
             ix = len(iso_string) - dot
-            microsecond = int(float(f"0{iso_string[ix - 1:]}") * 10 ** 6)
-            iso_string = iso_string[:len(iso_string) - dot] + str(microsecond).rjust(6, "0")
+            microsecond = int(float(f"0{iso_string[ix - 1:]}") * 10**6)
+            iso_string = iso_string[: len(iso_string) - dot] + str(microsecond).rjust(6, "0")
         if ymd:
-            iso_string = iso_string.replace(ymd, '-', 2)
+            iso_string = iso_string.replace(ymd, "-", 2)
         if T:
             iso_string = iso_string.replace(T, "T")
         return datetime.fromisoformat(iso_string)
 
-    @staticmethod
-    def round(value, multiple, up=None):
-        """ a nicer way to round numbers.
+    @classmethod
+    def round(cls, value, multiple, up=None):
+        """a nicer way to round numbers.
 
-        :param value: float/integer
-        :param multiple: base of the rounding.
+        :param value: float, integer or datetime to be rounded.
+        :param multiple: float, integer or timedelta to be used as the base of the rounding.
         :param up: None (default) or boolean rounds half, up or down.
             round(1.6, 1) rounds to 2.
             round(1.4, 1) rounds to 1.
             round(1.5, 1, up=True) rounds to 2.
             round(1.5, 1, up=False) rounds to 1.
-        :return: value
+        :return: rounded value
 
         Examples:
-            multiple = 1 is the same as rounding to whole integers.
-            multiple = 0.001 is the same as rounding to 3 digits precision.
-            mulitple = 3.1415 is rounding to nearest multiplier of 3.1415
-
-            dt = datetime(2022,8,18,11,14,53,440)
-            td = timedelta(hours=0.5)
-            round(dt,td) is datetime(2022,8,18,11,0)
+
+        [1] multiple = 1 is the same as rounding to whole integers.
+        [2] multiple = 0.001 is the same as rounding to 3 digits precision.
+        [3] mulitple = 3.1415 is rounding to nearest multiplier of 3.1415
+        [4] value = datetime(2022,8,18,11,14,53,440)
+        [5] multiple = timedelta(hours=0.5)
+        [6] xround(value,multiple) is datetime(2022,8,18,11,0)
         """
         epoch = 0
         if isinstance(value, (datetime)) and isinstance(multiple, timedelta):
             if value.tzinfo is None:
-                epoch = DataTypes.epoch_no_tz
+                epoch = cls.epoch_no_tz
             else:
-                epoch = DataTypes.epoch
-        
-        low = ((value - epoch) // multiple) * multiple
+                epoch = cls.epoch
+
+        value2 = value - epoch
+        if value2 == 0:
+            return value2
+
+        low = (value2 // multiple) * multiple
         high = low + multiple
         if up is True:
             return high + epoch
         elif up is False:
             return low + epoch
         else:
-            if abs((high + epoch) - value) < abs(value-(low + epoch)):
+            if abs((high + epoch) - value) < abs(value - (low + epoch)):
                 return high + epoch
             else:
                 return low + epoch
 
     @staticmethod
     def to_json(v):
-        if hasattr(v, 'dtype'):
-            pytype = numpy_types[v.dtype.name]
-            v = pytype(v)
+        """converts any python type to json.
+
+        Args:
+            v (any): value to convert to json
+
+        Returns:
+            json compatible value from v
+        """
+        if hasattr(v, "dtype"):
+            v = numpy_to_python(v)
         if v is None:
             return v
-        elif v is False:  # using isinstance(v, bool): won't work as False also is int of zero.
+        elif v is False:
+            # using isinstance(v, bool): won't work as False also is int of zero.
             return str(v)
         elif v is True:
             return str(v)
         elif isinstance(v, int):
             return v
         elif isinstance(v, str):
             return v
@@ -356,90 +396,102 @@
         elif isinstance(v, timedelta):
             return f"P{v.days}DT{v.seconds + (v.microseconds / 1e6)}S"
         else:
             raise TypeError(f"The datatype {type(v)} is not supported.")
 
     @staticmethod
     def from_json(v, dtype):
+        """converts json to python datatype
+
+        Args:
+            v (any): value
+            dtype (python type): any python type
+
+        Returns:
+            python type of value v
+        """
         if v in DataTypes.nones:
             if dtype is str and v == "":
                 return ""
             else:
                 return None
         if dtype is int:
             return int(v)
         elif dtype is str:
             return str(v)
         elif dtype is float:
             return float(v)
         elif dtype is bool:
-            if v == 'False':
+            if v == "False":
                 return False
-            elif v == 'True':
+            elif v == "True":
                 return True
             else:
                 raise ValueError(v)
         elif dtype is date:
             return date.fromisoformat(v)
         elif dtype is datetime:
             return datetime.fromisoformat(v)
         elif dtype is time:
             return time.fromisoformat(v)
         elif dtype is timedelta:
-            L = v.split('DT')
-            days = int(L[0].lstrip('P'))
+            L = v.split("DT")
+            days = int(L[0].lstrip("P"))
             seconds = float(L[1].rstrip("S"))
             return timedelta(days, seconds)
         else:
             raise TypeError(f"The datatype {str(dtype)} is not supported.")
 
     # Order is very important!
     types = [datetime, date, time, int, bool, float, str]
 
     @staticmethod
     def guess_types(*values):
-        """
-        Attempts to guess the datatype for *values
+        """Attempts to guess the datatype for *values
         returns dict with matching datatypes and probabilities
-        """
 
+        Returns:
+            dict: {key: type, value: probability}
+        """
         d = defaultdict(int)
         probability = Rank(DataTypes.types[:])
-        
+
         for value in values:
-            if hasattr(value, 'dtype'):
-                value = numpy_types[np.dtype(value).name](value)
+            if hasattr(value, "dtype"):
+                value = numpy_to_python(value)
 
             for dtype in probability:
                 try:
-                    _ = DataTypes.infer(value,dtype)
-                    d[dtype] += 1 
+                    _ = DataTypes.infer(value, dtype)
+                    d[dtype] += 1
                     probability.match(dtype)
                     break
                 except (ValueError, TypeError):
                     pass
         if not d:
-            d[str]=len(values)
-        return {k:round(v/len(values),3) for k,v in d.items()}
+            d[str] = len(values)
+        return {k: round(v / len(values), 3) for k, v in d.items()}
 
     @staticmethod
     def guess(*values):
-        """
-        Makes a best guess the datatype for *values
+        """Makes a best guess the datatype for *values
         returns list of native python values
+
+        Returns:
+            list: list of native python values
         """
         probability = Rank(*DataTypes.types[:])
         matches = [None for _ in values[0]]
-        
+
         for ix, value in enumerate(values[0]):
-            if hasattr(value, 'dtype'):
-                value = numpy_types[np.dtype(value).name](value)
+            if hasattr(value, "dtype"):
+                value = numpy_to_python(value)
             for dtype in probability:
                 try:
-                    matches[ix] = DataTypes.infer(value,dtype)
+                    matches[ix] = DataTypes.infer(value, dtype)
                     probability.match(dtype)
                     break
                 except (ValueError, TypeError):
                     pass
         return matches
 
     @classmethod
@@ -488,46 +540,46 @@
         if isinstance(value, int):
             return value
         elif isinstance(value, float):
             if int(value) == value:
                 return int(value)
             raise ValueError("it's a float")
         elif isinstance(value, str):
-            value = value.replace('"', '')  # "1,234" --> 1,234
+            value = value.replace('"', "")  # "1,234" --> 1,234
             value = value.replace(" ", "")  # 1 234 --> 1234
-            value = value.replace(',', '')  # 1,234 --> 1234
+            value = value.replace(",", "")  # 1,234 --> 1234
             value_set = set(value)
             if value_set - DataTypes.integers:  # set comparison.
                 raise ValueError
             try:
                 return int(value)
             except Exception:
                 raise ValueError(f"{value} is not an integer")
         else:
             raise ValueError()
 
     @classmethod
     def _infer_float(cls, value):
         if isinstance(value, int):
-            raise ValueError("it's an integer")
+            return float(value)
         if isinstance(value, float):
             return value
         elif isinstance(value, str):
-            value = value.replace('"', '')
-            dot_index, comma_index = value.find('.'), value.find(',')
+            value = value.replace('"', "")
+            dot_index, comma_index = value.find("."), value.find(",")
             if dot_index == comma_index == -1:
                 pass  # there are no dots or commas.
             elif 0 < dot_index < comma_index:  # 1.234,567
-                value = value.replace('.', '')  # --> 1234,567
-                value = value.replace(',', '.')  # --> 1234.567
+                value = value.replace(".", "")  # --> 1234,567
+                value = value.replace(",", ".")  # --> 1234.567
             elif dot_index > comma_index > 0:  # 1,234.678
-                value = value.replace(',', '')
+                value = value.replace(",", "")
 
             elif comma_index and dot_index == -1:
-                value = value.replace(',', '.')
+                value = value.replace(",", ".")
             else:
                 pass
 
             value_set = set(value)
 
             if not value_set.issubset(DataTypes.decimals):
                 raise TypeError()
@@ -536,48 +588,50 @@
             # check that reverse conversion is valid,
             # otherwise we have loss of precision. F.ex.:
             # int(0.532) --> 0
             try:
                 float_value = float(value)
             except Exception:
                 raise ValueError(f"{value} is not a float.")
-            if value_set.intersection('Ee'):  # it's scientific notation.
+            if value_set.intersection("Ee"):  # it's scientific notation.
                 v = value.lower()
-                if v.count('e') != 1:
+                if v.count("e") != 1:
                     raise ValueError("only 1 e in scientific notation")
 
-                e = v.find('e')
+                e = v.find("e")
                 v_float_part = float(v[:e])
-                v_exponent = int(v[e + 1:])
+                v_exponent = int(v[e + 1 :])
                 return float(f"{v_float_part}e{v_exponent}")
 
-            elif "." in str(float_value) and not "." in value_set:
+            elif "." in str(float_value) and "." not in value_set:
                 # when traversing through Datatype.types,
                 # integer is presumed to have failed for the column,
                 # so we ignore this and turn it into a float...
                 reconstructed_input = str(int(float_value))
 
             elif "." in value:
                 precision = len(value) - value.index(".") - 1
-                formatter = '{0:.' + str(precision) + 'f}'
+                formatter = "{0:." + str(precision) + "f}"
                 reconstructed_input = formatter.format(float_value)
 
             else:
                 reconstructed_input = str(float_value)
 
             if value.lower() != reconstructed_input:
                 raise ValueError()
 
             return float_value
         else:
             raise ValueError()
 
     @classmethod
     def _infer_date(cls, value):
-        if isinstance(value, date):
+        if isinstance(value, datetime):
+            return date(value.year, value.month, value.day)
+        elif isinstance(value, date):
             return value
         elif isinstance(value, str):
             try:
                 return date.fromisoformat(value)
             except ValueError:
                 pattern = "".join(["N" if n in DataTypes.digits else n for n in value])
                 f = DataTypes.date_formats.get(pattern, None)
@@ -588,22 +642,24 @@
         else:
             raise ValueError()
 
     @classmethod
     def _infer_datetime(cls, value):
         if isinstance(value, datetime):
             return value
+        elif isinstance(value, date):
+            return datetime(value.year, value.month, value.day)
         elif isinstance(value, str):
             try:
                 return datetime.fromisoformat(value)
             except ValueError:
-                if '.' in value:
-                    dot = value.find('.', 11)  # 11 = len("1999.12.12")
-                elif ',' in value:
-                    dot = value.find(',', 11)
+                if "." in value:
+                    dot = value.find(".", 11)  # 11 = len("1999.12.12")
+                elif "," in value:
+                    dot = value.find(",", 11)
                 else:
                     dot = len(value)
 
                 pattern = "".join(["N" if n in DataTypes.digits else n for n in value[:dot]])
                 f = DataTypes.datetime_formats.get(pattern, None)
                 if f:
                     return f(value)
@@ -629,94 +685,78 @@
         else:
             return str(value)
 
     @classmethod
     def _infer_none(cls, value):
         if value is None:
             return None
-        if isinstance(value,str) and value == str(None):
+        if isinstance(value, str) and value == str(None):
             return None
         raise ValueError()
 
 
-def _get_numpy_types():
-    # d = {}
-    # for name in dir(np):
-    #     obj = getattr(np,name)
-    #     if hasattr(obj, 'dtype'):
-    #         try:
-    #             if 'time' in name:
-    #                 npn = obj(0, 'D')
-    #             else:
-    #                 npn = obj(0)
-    #             nat = npn.item()
-    #             d[name] = type(nat)
-    #             d[npn.dtype.char] = type(nat)
-    #         except:
-    #             pass
-    # return d
-    return {
-        "bool": bool,
-        "bool_":bool,  #  ('?') -> <class 'bool'>
-        "byte": int,  # ('b') -> <class 'int'>
-        "bytes0": bytes,  #  ('S') -> <class 'bytes'>
-        "bytes_": bytes,  #  ('S') -> <class 'bytes'>
-        "cdouble": complex,   #('D') -> <class 'complex'>
-        "cfloat": complex,  # ('D') -> <class 'complex'>
-        "clongdouble": float,  # ('G') -> <class 'numpy.clongdouble'>
-        "clongfloat": float,  # ('G') -> <class 'numpy.clongdouble'>
-        "complex128": complex,  # ('D') -> <class 'complex'>
-        "complex64": complex,  # ('F') -> <class 'complex'>
-        "complex_": complex,  # ('D') -> <class 'complex'>
-        "csingle": complex,  # ('F') -> <class 'complex'>
-        "datetime64": date,  # ('M') -> <class 'datetime.date'>
-        "double": float,  # ('d') -> <class 'float'>
-        "float16": float,  # ('e') -> <class 'float'>
-        "float32": float,  # ('f') -> <class 'float'>
-        "float64": float,  # ('d') -> <class 'float'>
-        "float_": float,  # ('d') -> <class 'float'>
-        "half": float,  # ('e') -> <class 'float'>
-        "int0": int,  # ('q') -> <class 'int'>
-        "int16": int,  # ('h') -> <class 'int'>
-        "int32": int,  # ('l') -> <class 'int'>
-        "int64": int,  # ('q') -> <class 'int'>
-        "int8": int,  # ('b') -> <class 'int'>
-        "int_": int,  # ('l') -> <class 'int'>
-        "intc": int,  # ('i') -> <class 'int'>
-        "intp": int,  # ('q') -> <class 'int'>
-        "longcomplex":float,  # ('G') -> <class 'numpy.clongdouble'>
-        "longdouble": float,  # ('g') -> <class 'numpy.longdouble'>
-        "longfloat":  float,  # ('g') -> <class 'numpy.longdouble'>
-        "longlong": int,  # ('q') -> <class 'int'>
-        "matrix": int,  # ('l') -> <class 'int'>
-        "record": bytes,  # ('V') -> <class 'bytes'>
-        "short": int,  # ('h') -> <class 'int'>
-    }
+def numpy_to_python(obj):
+    """See https://numpy.org/doc/stable/reference/arrays.scalars.html"""
+    if isinstance(obj, np.generic):
+        return obj.item()
+    return obj
+
 
-numpy_types = _get_numpy_types()
+def pytype(obj):
+    """Returns the python type of any object"""
+    if isinstance(obj, np.generic):
+        return type(obj.item())
+    return type(obj)
 
 
 class Rank(object):
     def __init__(self, *items):
-        self.items = {i:ix for i,ix in zip(items,range(len(items)))}
+        self.items = {i: ix for i, ix in zip(items, range(len(items)))}
         self.ranks = [0 for _ in items]
         self.items_list = [i for i in items]
-    
+
     def match(self, k):  # k+=1
         ix = self.items[k]
         r = self.ranks
-        r[ix]+=1
+        r[ix] += 1
 
         if ix > 0:
             p = self.items_list
-            while r[ix] > r[ix-1] and ix >0:  # use a simple bubble sort to maintain rank
-                r[ix], r[ix-1] = r[ix-1], r[ix]
-                p[ix], p[ix-1] = p[ix-1], p[ix]
+            while r[ix] > r[ix - 1] and ix > 0:  # use a simple bubble sort to maintain rank
+                r[ix], r[ix - 1] = r[ix - 1], r[ix]
+                p[ix], p[ix - 1] = p[ix - 1], p[ix]
                 old = p[ix]
                 self.items[old] = ix
-                self.items[k] = ix-1
+                self.items[k] = ix - 1
                 ix -= 1
-    
+
     def __iter__(self):
         return iter(self.items_list)
 
 
+def list_to_np_array(iterable):
+    """helper to make correct np array from python types.
+    Example of problem where numpy turns mixed types into strings.
+    >>> np.array([4,'5'])
+    np.ndarray(['4', '5'])
+    """
+    dtypes = set()
+    for v in iterable:
+        dtypes.add(type(v))
+        if len(dtypes) > 1:  # no need to continue after we know it'll be dtype=object
+            break
+    if len(dtypes) > 1:
+        value = np.array(iterable, dtype=object)
+    else:
+        value = np.array(iterable)
+    return value
+
+
+def np_type_unify(arrays):
+    dtypes = {arr.dtype: len(arr) for arr in arrays}
+    if len(dtypes) == 1:
+        dtype, _ = dtypes.popitem()
+    else:
+        for ix, arr in enumerate(arrays):
+            arrays[ix] = np.array(arr, dtype=object)
+        dtype = object
+    return np.concatenate(arrays, dtype=dtype)
```

## tablite/file_reader_utils.py

```diff
@@ -1,21 +1,23 @@
 import re
-from pathlib import Path
-import pyexcel
 import chardet
+import openpyxl
+from pathlib import Path
+
+ENCODING_GUESS_BYTES = 10000
 
 
 def split_by_sequence(text, sequence):
-    """ helper to split text according to a split sequence. """
+    """helper to split text according to a split sequence."""
     chunks = tuple()
     for element in sequence:
         idx = text.find(element)
         if idx < 0:
             raise ValueError(f"'{element}' not in row")
-        chunk, text = text[:idx], text[len(element) + idx:]
+        chunk, text = text[:idx], text[len(element) + idx :]
         chunks += (chunk,)
     chunks += (text,)  # the remaining text.
     return chunks
 
 
 class TextEscape(object):
     """
@@ -23,110 +25,123 @@
 
     Example:
     text_escape = TextEscape()  # set up the instance.
     for line in somefile.readlines():
         list_of_words = text_escape(line)  # use the instance.
         ...
     """
-    def __init__(self, openings='({[', closures=']})', qoute='"', delimiter=',', strip_leading_and_tailing_whitespace=False):
+
+    def __init__(
+        self,
+        openings="({[",
+        closures="]})",
+        text_qualifier='"',
+        delimiter=",",
+        strip_leading_and_tailing_whitespace=False,
+    ):
         """
-        As an example, the Danes and Germans use " for inches and ' for feet, 
-        so we will see data that contains nail (75 x 4 mm, 3" x 3/12"), so 
+        As an example, the Danes and Germans use " for inches and ' for feet,
+        so we will see data that contains nail (75 x 4 mm, 3" x 3/12"), so
         for this case ( and ) are valid escapes, but " and ' aren't.
 
         """
         if openings is None:
             openings = [None]
         elif isinstance(openings, str):
             self.openings = {c for c in openings}
         else:
-            raise TypeError(f"expected str, got {type(openings)}")           
+            raise TypeError(f"expected str, got {type(openings)}")
 
         if closures is None:
             closures = [None]
         elif isinstance(closures, str):
             self.closures = {c for c in closures}
         else:
             raise TypeError(f"expected str, got {type(closures)}")
 
         if not isinstance(delimiter, str):
             raise TypeError(f"expected str, got {type(delimiter)}")
         self.delimiter = delimiter
         self._delimiter_length = len(delimiter)
-        self.strip_leading_and_tailing_whitespace= strip_leading_and_tailing_whitespace
-        
-        if qoute is None:
+        self.strip_leading_and_tailing_whitespace = strip_leading_and_tailing_whitespace
+
+        if text_qualifier is None:
             pass
-        elif qoute in openings + closures:
+        elif text_qualifier in openings + closures:
             raise ValueError("It's a bad idea to have qoute character appears in openings or closures.")
         else:
-            self.qoute = qoute
-        
-        if not qoute:
-            self.c = self._call1
+            self.qoute = text_qualifier
+
+        if not text_qualifier:
+            if not self.strip_leading_and_tailing_whitespace:
+                self.c = self._call_1
+            else:
+                self.c = self._call_2
         elif not any(openings + closures):
-            self.c = self._call2
+            self.c = self._call_3
         else:
             try:
                 # TODO: The regex below needs to be constructed dynamically depending on the inputs.
-                self.re = re.compile("([\d\w\s\u4e00-\u9fff]+)(?=,|$)|((?<=\A)|(?<=,))(?=,|$)|(\(.+\)|\".+\")", "gmu") # <-- Disclaimer: Audrius wrote this.
-                self.c = self._call3
+                # fmt: off
+                self.re = re.compile('([\d\w\s\u4e00-\u9fff]+)(?=,|$)|((?<=\A)|(?<=,))(?=,|$)|(\(.+\)|".+")', "gmu")  # noqa <-- Disclaimer: Audrius wrote this.
+                # fmt: on
+                self.c = self._call_4
             except TypeError:
-                self.c = self._call3_slow
-            
-    def __call__(self,s):
-        s2 = self.c(s)
-        if self.strip_leading_and_tailing_whitespace:
-            s2 = [w.rstrip(" ").lstrip(" ") for w in s2]
-        return s2
-       
-    def _call1(self,s):  # just looks for delimiter.
+                self.c = self._call_4_slow
+
+    def __call__(self, s):
+        return self.c(s)
+
+    def _call_1(self, s):  # just looks for delimiter.
         return s.split(self.delimiter)
 
-    def _call2(self,s): # looks for qoutes.
+    def _call_2(self, s):
+        return [w.rstrip(" ").lstrip(" ") for w in self._call_1(s)]
+
+    def _call_3(self, s):  # looks for qoutes.
         words = []
-        qoute= False
+        qoute = False
         ix = 0
-        while ix < len(s):  
+        while ix < len(s):
             c = s[ix]
             if c == self.qoute:
                 qoute = not qoute
             if qoute:
                 ix += 1
                 continue
             if c == self.delimiter:
-                word, s = s[:ix], s[ix+self._delimiter_length:]
+                word, s = s[:ix], s[ix + self._delimiter_length :]
                 word = word.lstrip(self.qoute).rstrip(self.qoute)
                 words.append(word)
                 ix = -1
-            ix+=1
+            ix += 1
         if s:
-            s=s.lstrip(self.qoute).rstrip(self.qoute)
+            s = s.lstrip(self.qoute).rstrip(self.qoute)
             words.append(s)
         return words
 
-    def _call3(self, s):  # looks for qoutes, openings and closures.
+    def _call_4(self, s):  # looks for qoutes, openings and closures.
         return self.re.match(s)  # TODO - TEST!
-    
-    def _call3_slow(self, s):
+
+    def _call_4_slow(self, s):
         words = []
         qoute = False
-        ix,depth = 0,0
-        while ix < len(s):  
+        ix, depth = 0, 0
+        while ix < len(s):
             c = s[ix]
 
             if c == self.qoute:
                 qoute = not qoute
 
             if qoute:
-                ix+=1
+                ix += 1
                 continue
 
             if depth == 0 and c == self.delimiter:
-                word, s = s[:ix], s[ix+self._delimiter_length:]
+                word, s = s[:ix], s[ix + self._delimiter_length :]
                 words.append(word.rstrip(self.qoute).lstrip(self.qoute))
                 ix = -1
             elif c in self.openings:
                 depth += 1
             elif c in self.closures:
                 depth -= 1
             else:
@@ -134,15 +149,14 @@
             ix += 1
 
         if s:
             words.append(s.rstrip(self.qoute).lstrip(self.qoute))
         return words
 
 
-
 def detect_seperator(text):
     """
     :param path: pathlib.Path objects
     :param encoding: file encoding.
     :return: 1 character.
     """
     # After reviewing the logic in the CSV sniffer, I concluded that all it
@@ -150,30 +164,33 @@
     # determined by the first line, which almost always is a line of headers,
     # the text characters will be utf-8,16 or ascii letters plus white space.
     # This leaves the characters ,;:| and \t as potential separators, with one
     # exception: files that use whitespace as separator. My logic is therefore
     # to (1) find the set of characters that intersect with ',;:|\t' which in
     # practice is a single character, unless (2) it is empty whereby it must
     # be whitespace.
-    if len(text) == 0: return None
-    seps = {',', '\t', ';', ':', '|'}.intersection(text)
+    if len(text) == 0:
+        return None
+    seps = {",", "\t", ";", ":", "|"}.intersection(text)
     if not seps:
         if " " in text:
             return " "
+        if "\n" in text:
+            return "\n"
         else:
             raise ValueError("separator not detected")
     if len(seps) == 1:
         return seps.pop()
     else:
         frq = [(text.count(i), i) for i in seps]
         frq.sort(reverse=True)  # most frequent first.
         return frq[0][-1]
 
 
-def get_headers(path, linecount=10): 
+def get_headers(path, linecount=10, delimiter=None):
     """
     file format	definition
     csv	    comma separated values
     tsv	    tab separated values
     csvz	a zip file that contains one or many csv files
     tsvz	a zip file that contains one or many tsv files
     xls	    a spreadsheet file format created by MS-Excel 97-2003
@@ -191,47 +208,94 @@
         path = Path(path)
     if not isinstance(path, Path):
         raise TypeError("expected pathlib path.")
     if not path.exists():
         raise FileNotFoundError(str(path))
 
     delimiters = {
-        '.csv': ',',
-        '.tsv': '\t',
-        '.txt': None,
+        ".csv": ",",
+        ".tsv": "\t",
+        ".txt": None,
     }
 
     d = {}
     if path.suffix not in delimiters:
         try:
-            book = pyexcel.get_book(file_name=str(path))
-            for sheet_name in book.sheet_names():
-                sheet = book[sheet_name]
-                stop = sheet.number_of_rows()
-                d[sheet_name] = [sheet.row[i] for i in range(0, min(linecount,stop))]
-                d['delimiter'] = None
+            book = openpyxl.open(str(path), read_only=True)
+
+            try:
+                all_sheets = book.sheetnames
+
+                for sheet_name, sheet in ((name, book[name]) for name in all_sheets):
+                    max_rows = min(sheet.max_row, linecount)
+                    container = [None] * max_rows
+                    padding_ends = 0
+                    max_column = sheet.max_column
+
+                    for i, row_data in enumerate(sheet.iter_rows(0, max_rows, values_only=True)):
+                        container[i] = list(row_data)
+
+                        for j, cell in enumerate(reversed(row_data)):
+                            if cell is None:
+                                continue
+
+                            padding_ends = max(padding_ends, max_column - j)
+
+                            break
+
+                    d[sheet_name] = [c[0:padding_ends] for c in container]
+                    d["delimiter"] = None
+            finally:
+                book.close()
+
             return d
         except Exception:
             pass  # it must be a raw text format.
 
     try:
-        with path.open('rb') as fi:
-            rawdata = fi.read(10000)
-            encoding = chardet.detect(rawdata)['encoding']
-        with path.open('r', encoding=encoding) as fi:
+        with path.open("rb") as fi:
+            rawdata = fi.read(ENCODING_GUESS_BYTES)
+            encoding = chardet.detect(rawdata)["encoding"]
+        with path.open("r", encoding=encoding, errors="ignore") as fi:
             lines = []
             for n, line in enumerate(fi):
-                line = line.rstrip('\n')
+                line = line.rstrip("\n")
                 lines.append(line)
                 if n > linecount:
                     break  # break on first
-            d['delimiter'] = delimiter = detect_seperator('\n'.join(lines))
+            if delimiter is None:
+                d["delimiter"] = delimiter = detect_seperator("\n".join(lines))
 
             if delimiter is None:
-                d['delimiter'] = delimiters[path.suffix] # pickup the default one
-                d['is_empty'] = True # mark as empty to return an empty table instead of throwing
+                d["delimiter"] = delimiters[path.suffix]  # pickup the default one
+                d["is_empty"] = True  # mark as empty to return an empty table instead of throwing
 
             d[path.name] = [line.split(delimiter) for line in lines]
         return d
     except Exception:
         raise ValueError(f"can't read {path.suffix}")
 
+
+def get_encoding(path, nbytes=ENCODING_GUESS_BYTES):
+    size = path.stat().st_size
+    if nbytes > size:
+        nbytes = size
+    with path.open("rb") as fi:
+        rawdata = fi.read(nbytes)
+        encoding = chardet.detect(rawdata)["encoding"]
+        if encoding == "ascii":  # utf-8 is backwards compatible with ascii
+            return "utf-8"  # --   so should the first 10k chars not be enough,
+        return encoding  # --      the utf-8 encoding will still get it right.
+
+
+def get_delimiter(path, encoding):
+    with path.open("r", encoding=encoding, errors="ignore") as fi:
+        lines = []
+        for n, line in enumerate(fi):
+            line = line.rstrip("\n")
+            lines.append(line)
+            if n > 10:
+                break  # break on first
+        delimiter = detect_seperator("\n".join(lines))
+        if delimiter is None:
+            raise ValueError("Delimiter could not be determined")
+        return delimiter
```

## tablite/groupby_utils.py

```diff
@@ -1,10 +1,9 @@
 from collections import defaultdict
-from datetime import date,time,datetime,timedelta
-from tablite.datatypes import DataTypes
+from datetime import date, time, datetime, timedelta  # noqa
 
 
 class GroupbyFunction(object):
     pass
 
 
 class Limit(GroupbyFunction):
@@ -32,29 +31,31 @@
         super().__init__()
         self.f = min
 
 
 class Sum(GroupbyFunction):
     def __init__(self):
         self.value = 0
-    def update(self,value):
+
+    def update(self, value):
         if isinstance(value, (type(None), date, time, datetime, str)):
             raise ValueError(f"Sum of {type(value)} doesn't make sense.")
         self.value += value
 
 
 class Product(GroupbyFunction):
     def __init__(self) -> None:
         self.value = 1
-    def update(self,value):
+
+    def update(self, value):
         self.value *= value
 
 
 class First(GroupbyFunction):
-    empty = (None, )
+    empty = (None,)
     # we will never receive a tuple, so using (None,) as the initial
     # value will assure that IF None is the first value, then it can
     # be captured correctly.
 
     def __init__(self):
         self.value = self.empty
 
@@ -92,34 +93,35 @@
 class Average(GroupbyFunction):
     def __init__(self):
         self.sum = 0
         self.count = 0
         self.value = 0
 
     def update(self, value):
-        if isinstance(value, (date,time,datetime,str)):
+        if isinstance(value, (date, time, datetime, str)):
             raise ValueError(f"Sum of {type(value)} doesn't make sense.")
         if value is not None:
             self.sum += value
             self.count += 1
             self.value = self.sum / self.count
 
 
 class StandardDeviation(GroupbyFunction):
     """
     Uses J.P. Welfords (1962) algorithm.
     For details see https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm
     """
+
     def __init__(self):
         self.count = 0
         self.mean = 0
         self.c = 0.0
 
     def update(self, value):
-        if isinstance(value, (date,time,datetime,str)):
+        if isinstance(value, (date, time, datetime, str)):
             raise ValueError(f"Std.dev. of {type(value)} doesn't make sense.")
         if value is not None:
             self.count += 1
             dt = value - self.mean
             self.mean += dt / self.count
             self.c += dt * (value - self.mean)
 
@@ -176,30 +178,24 @@
     def value(self):
         L = [(v, k) for k, v in self.hist.items()]
         L.sort(reverse=True)
         _, most_frequent = L[0]  # top of the list.
         return most_frequent
 
 
-
-class GroupBy(object):    
+class GroupBy(object):
     max = Max  # shortcuts to avoid having to type a long list of imports.
     min = Min
     sum = Sum
     product = Product
     first = First
     last = Last
     count = Count
     count_unique = CountUnique
     avg = Average
     stdev = StandardDeviation
     median = Median
     mode = Mode
 
-    functions = [
-        Max, Min, Sum, First, Last, Product,
-        Count, CountUnique,
-        Average, StandardDeviation, Median, Mode
-    ]
+    functions = [Max, Min, Sum, First, Last, Product, Count, CountUnique, Average, StandardDeviation, Median, Mode]
 
     function_names = {f.__name__: f for f in functions}
-
```

## tablite/sortation.py

```diff
@@ -1,213 +1,165 @@
-from datetime import datetime,date,time,timedelta
-from pyuca import Collator
-uca_collator = Collator()
-
-# EXCEL
-_excel_typecodes = {  # declares sortation rank: 0 < 1, etc.
-    time: 0,
-    int: 0,
-    float: 0,
-    date: 0,
-    datetime: 0,
-    timedelta: 0,
-    str: 1,
-    bool: 2,
-    type(None): 3
-}
-
-_excel_date_epoc = date(1900,1,1)
-_excel_datetime_epoc = datetime(1900,1,1)
-
-# excel helpers
-def _excel_none(value):
-    return float('inf')
-
-def _excel_float(value):
-    return value
-
-def _excel_int(value):
-    return value
-
-def _excel_time(value):
-    return (value.hour * 60 * 60 + value.minute * 60 + value.second + (value.microsecond / 1e6)) / (24 * 60 * 60)
-
-def _excel_date(value):
-    dt = value - _excel_date_epoc
-    return dt.days + (dt.seconds / (24*60*60))
-
-def _excel_datetime(value):
-    dt = value - _excel_datetime_epoc
-    return dt.days + (dt.seconds / (24*60*60))
-
-def _excel_timedelta(value):
-    return value.days + (value.seconds / (24*60*60))
-
-def _excel_bool(value):
-    return int(value)
-
-_excel_value_function = {
-    time: _excel_time,
-    date: _excel_date,
-    datetime: _excel_datetime,
-    timedelta: _excel_timedelta,
-    bool: _excel_bool,
-    float: _excel_float,
-    int: _excel_int,
-    type(None): _excel_none,
-    # str is handled by pyUCA.
-}
-
-# UNIX
-_unix_typecodes = {
-    type(None): 0,
-    bool: 1,
-    int: 2,
-    float: 2,
-    time: 3,
-    date: 4,
-    datetime: 5,
-    timedelta: 6,
-    str: 7,  # string is handled by pyUCA.
-}
-
-_unix_date_epoc = date(1970,1,1)
-_unix_datetime_epoc = datetime(1970,1,1)
-
-def _unix_none(value):
-    return -float('inf')
-
-def _unix_float(value):
-    return value
-
-def _unix_int(value):
-    return value
-
-def _unix_time(value):
-    return (value.hour * 60 * 60 + value.minute * 60 + value.second + (value.microsecond / 1e6)) / (24 * 60 * 60)
-
-def _unix_date(value):
-    dt = value - _unix_date_epoc
-    return dt.days + (dt.seconds / (24*60*60))
-
-def _unix_datetime(value):
-    dt = value - _unix_datetime_epoc
-    return dt.days + (dt.seconds / (24*60*60))
-
-def _unix_timedelta(value):
-    return value.days + (value.seconds / (24*60*60))
-
-def _unix_bool(value):
-    return int(value)
-
-_unix_value_function = {
-    time: _unix_time,
-    date: _unix_date,
-    datetime: _unix_datetime,
-    timedelta: _unix_timedelta,
-    bool: _unix_bool,
-    float: _unix_float,
-    int: _unix_int,
-    type(None): _unix_none,
-}
-
-def text_sort(values, reverse):
-    """
-    Sorts everything as text.
-    """
-    text = {str(i):i for i in values}
-    L = list(text.keys())
-    L.sort(key=uca_collator.sort_key, reverse=reverse)
-    d = {text[value]:ix for ix,value in enumerate(L)}
-    return d
-
-def unix_sort(values, reverse):
-    """
-    Unix sortation sorts by the following order:
-
-    | rank | type      | value                                      |
-    +------+-----------+--------------------------------------------+
-    |   0  | None      | floating point -infinite                   |
-    |   1  | bool      | 0 as False, 1 as True                      |
-    |   2  | int       | as numeric value                           |
-    |   2  | float     | as numeric value                           |
-    |   3  | time      |  * seconds into the day / (24 * 60 * 60)  |  
-    |   4  | date      | as integer days since 1970/1/1             | 
-    |   5  | datetime  | as float using date (int) + time (decimal) |
-    |   6  | timedelta | as float using date (int) + time (decimal) |
-    |   7  | str       | using unicode                              |        
-    +------+-----------+--------------------------------------------+
-    
-     = 2 * 
-
-    """
-    L = []
-    text = [i for i in values if isinstance(i, str)]
-    text.sort(key=uca_collator.sort_key, reverse=reverse)
-    text_code = _unix_typecodes[str]
-    L = [(text_code,ix,v) for ix,v in enumerate(text)]
-
-    for value in (i for i in values if not isinstance(i, str)):
-        t = type(value)
-        TC = _unix_typecodes[t]
-        tf = _unix_value_function[t]
-        VC = tf(value)
-        L.append((TC,VC,value))
-    L.sort(reverse=reverse)
-    d = {value:ix for ix,(_,_,value) in enumerate(L)}
-    return d
-
-
-def excel_sort(values, reverse):
-    """
-    Excel sortation sorts by the following order:
-
-    | rank | type      | value                                      |
-    +------+-----------+--------------------------------------------+
-    |   1  | int       | as numeric value                           |
-    |   1  | float     | as numeric value                           |
-    |   1  | time      | as seconds into the day / (24 * 60 * 60)   |
-    |   1  | date      | as integer days since 1900/1/1             | 
-    |   1  | datetime  | as float using date (int) + time (decimal) |
-    |  (1)*| timedelta | as float using date (int) + time (decimal) |
-    |   2  | str       | using unicode                              |
-    |   3  | bool      | 0 as False, 1 as True                      |
-    |   4  | None      | floating point infinite.                   |
-    +------+-----------+--------------------------------------------+
-    
-    * Excel doesn't have timedelta.
-    """
-    L = []
-    text = [i for i in values if isinstance(i, str)]
-    
-    text.sort(key=uca_collator.sort_key, reverse=reverse)
-    L = [(2,ix,v) for ix,v in enumerate(text)]
-    
-    for value in (i for i in values if not isinstance(i,str)):
-        t = type(value)
-        TC = _excel_typecodes[t]
-        tf = _excel_value_function[t]
-        VC = tf(value)
-        L.append((TC,VC,value))
-
-    L.sort(reverse=reverse)
-    d = {value:ix for ix,(_,_,value) in enumerate(L)}
-    return d
-
-modes = {
-    'alphanumeric': text_sort,
-    'unix': unix_sort,
-    'excel': excel_sort
-}
-
-
-def rank(values, reverse, mode):
-    """
-    values: list of values to sort.
-    reverse: bool
-    mode: as 'text', as 'numeric' or as 'excel'
-    return: dict: d[value] = rank
-    """
-    if mode not in modes:
-        raise ValueError(f"{mode} not in list of modes: {list(modes)}")
-    f = modes.get(mode)
-    return f(values,reverse)
+import os
+import numpy as np
+import psutil
+from mplite import Task, TaskManager
+from tablite.mp_utils import share_mem, reindex_task, select_processing_method
+from tablite.sort_utils import modes as sort_modes
+from tablite.sort_utils import rank as sort_rank
+from tablite.base import Table, Column, Page
+from tablite.utils import sub_cls_check, type_check
+
+from tqdm import tqdm as _tqdm
+
+
+def sort_index(T, sort_mode="excel", tqdm=_tqdm, pbar=None, **kwargs):
+    """
+    helper for methods `sort` and `is_sorted`
+
+    param: sort_mode: str: "alphanumeric", "unix", or, "excel" (default)
+    param: **kwargs: sort criteria. See Table.sort()
+    """
+
+    sub_cls_check(T, Table)
+
+    if not isinstance(kwargs, dict):
+        raise ValueError("Expected keyword arguments, did you forget the ** in front of your dict?")
+    if not kwargs:
+        kwargs = {c: False for c in T.columns}
+
+    for k, v in kwargs.items():
+        if k not in T.columns:
+            raise ValueError(f"no column {k}")
+        if not isinstance(v, bool):
+            raise ValueError(f"{k} was mapped to {v} - a non-boolean")
+
+    if sort_mode not in sort_modes:
+        raise ValueError(f"{sort_mode} not in list of sort_modes: {list(sort_modes)}")
+
+    rank = {i: tuple() for i in range(len(T))}  # create index and empty tuple for sortation.
+
+    _pbar = tqdm(total=len(kwargs.items()), desc="creating sort index") if pbar is None else pbar
+
+    for key, reverse in kwargs.items():
+        col = T[key][:]
+        col = col.tolist() if isinstance(col, np.ndarray) else col
+        ranks = sort_rank(values=set(col), reverse=reverse, mode=sort_mode)
+        assert isinstance(ranks, dict)
+        for ix, v in enumerate(col):
+            rank[ix] += (ranks[v],)  # add tuple
+
+        _pbar.update(1)
+
+    new_order = [(r, i) for i, r in rank.items()]  # tuples are listed and sort...
+    rank.clear()  # free memory.
+    new_order.sort()
+    sorted_index = [i for _, i in new_order]  # new index is extracted.
+    new_order.clear()
+    return np.array(sorted_index, dtype=np.int64)
+
+
+def reindex(T, index):
+    """
+    index: list of integers that declare sort order.
+
+    Examples:
+
+        Table:  ['a','b','c','d','e','f','g','h']
+        index:  [0,2,4,6]
+        result: ['b','d','f','h']
+
+        Table:  ['a','b','c','d','e','f','g','h']
+        index:  [0,2,4,6,1,3,5,7]
+        result: ['a','c','e','g','b','d','f','h']
+
+    """
+    sub_cls_check(T, Table)
+    if isinstance(index, list):
+        index = np.array(index, dtype=int)
+    type_check(index, np.ndarray)
+    if max(index) >= len(T):
+        raise IndexError("index out of range: max(index) > len(self)")
+    if min(index) < -len(T):
+        raise IndexError("index out of range: min(index) < -len(self)")
+
+    fields = len(T) * len(T.columns)
+    m = select_processing_method(fields, _sp_reindex, _mp_reindex)
+    return m(T, index)
+
+
+def _sp_reindex(T, index):
+    t = type(T)()
+    for name in T.columns:
+        t[name] = np.take(T[name][:], index)
+    return t
+
+
+def _mp_reindex(T, index):
+    assert isinstance(index, np.ndarray)
+    return _sp_reindex(T, index)
+
+    index, shm = share_mem(index, dtype=index.dtype)
+    # shm = shared_memory.SharedMemory(create=True, size=index.nbytes)  # the co_processors will read this.
+    # sort_index = np.ndarray(index.shape, dtype=index.dtype, buffer=shm.buf)
+    # sort_index[:] = index
+
+    new = {}
+    tasks = []
+    for name in T.columns:
+        col = T[name]
+        new[name] = []
+
+        start, end = 0, 0
+        for page in col.pages:
+            start, end = end, start + len(page)
+            src = page.path
+            dst = page.path.parent / f"{next(Page.ids)}.npy"
+            t = Task(reindex_task, src, dst, shm.name, index.shape, start, end)
+            new[name].append(dst)
+            tasks.append(t)
+
+    cpus = min(len(tasks), psutil.cpu_count(logical=False))
+    with TaskManager(cpu_count=cpus) as tm:
+        errs = tm.execute(tasks)
+        if any(errs):
+            raise Exception("\n".join(filter(lambda x: x is not None, errs)))
+
+    shm.close()
+    shm.unlink()
+
+    t = type(T)()
+    for name in T.columns:
+        t[name] = Column(t.path)
+        for dst in new[name]:
+            data = np.load(dst, allow_pickle=True, fix_imports=False)
+            t[name].extend(data)
+            os.remove(dst)
+    return t
+
+
+def sort(T, sort_mode="excel", **kwargs):
+    """Perform multi-pass sorting with precedence given order of column names.
+    sort_mode: str: "alphanumeric", "unix", or, "excel"
+    kwargs:
+        keys: columns,
+        values: 'reverse' as boolean.
+
+    examples:
+    Table.sort('A'=False) means sort by 'A' in ascending order.
+    Table.sort('A'=True, 'B'=False) means sort 'A' in descending order, then (2nd priority)
+    sort B in ascending order.
+    """
+    sub_cls_check(T, Table)
+
+    index = sort_index(T, sort_mode=sort_mode, **kwargs)
+    m = select_processing_method(len(T) * len(T.columns), _sp_reindex, _mp_reindex)
+    return m(T, index)
+
+
+def is_sorted(T, **kwargs):
+    """Performs multi-pass sorting check with precedence given order of column names.
+    **kwargs: optional: sort criteria. See Table.sort()
+    :return bool
+    """
+    index = sort_index(T, **kwargs)
+    match = np.arange(len(T))
+    return np.all(index == match)
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## tablite/utils.py

```diff
@@ -1,82 +1,172 @@
 from collections import defaultdict
 import math
-from datetime import datetime,date,time,timedelta
+import ast
+from datetime import datetime, date, time, timedelta, timezone  # noqa
 from itertools import compress
-from statistics import StatisticsError
 
 
-def unique_name(wanted_name, list_of_names):
+def type_check(var, kind):
+    if not isinstance(var, kind):
+        raise TypeError(f"Expected {kind}, not {type(var)}")
+
+
+def sub_cls_check(c, kind):
+    if not issubclass(type(c), kind):
+        raise TypeError(f"Expected {kind}, not {type(c)}")
+
+
+def unique_name(wanted_name, set_of_names):
     """
     returns a wanted_name as wanted_name_i given a list of names
     which guarantees unique naming.
     """
-    name,i = wanted_name,1
-    while name in list_of_names:
+    if not isinstance(set_of_names, set):
+        set_of_names = set(set_of_names)
+    name, i = wanted_name, 1
+    while name in set_of_names:
         name = f"{wanted_name}_{i}"
-        i+=1
+        i += 1
     return name
 
 
-def intercept(A,B):
+def expression_interpreter(expression, columns):
     """
-    enables calculation of the intercept of two range objects.
-    Used to determine if a datablock contains a slice.
-    
-    A: range
-    B: range
-    
-    returns: range as intercept of ranges A and B.
+    Interprets valid expressions such as:
+
+        "all((A==B, C!=4, 200<D))"
+
+    as:
+        def _f(A,B,C,D):
+            return all((A==B, C!=4, 200<D))
+
+    using python's compiler.
     """
-    if not isinstance(A, range):
+    if not isinstance(expression, str):
+        raise TypeError(f"`{expression}` is not a str")
+    if not isinstance(columns, list):
         raise TypeError
-    if A.step < 0: # turn the range around
-        A = range(A.stop, A.start, abs(A.step))
-
-    if not isinstance(B, range):
+    if not all(isinstance(i, str) for i in columns):
         raise TypeError
-    if B.step < 0:  # turn the range around
-        B = range(B.stop, B.start, abs(B.step))
-    
-    boundaries = [A.start, A.stop, B.start, B.stop]
-    boundaries.sort()
-    a,b,c,d = boundaries
-    if [A.start, A.stop] in [[a,b],[c,d]]:
-        return range(0) # then there is no intercept
-    # else: The inner range (subset) is b,c, limited by the first shared step.
-    A_start_steps = math.ceil((b - A.start) / A.step)
-    A_start = A_start_steps * A.step + A.start
 
-    B_start_steps = math.ceil((b - B.start) / B.step)
-    B_start = B_start_steps * B.step + B.start
+    req_columns = ", ".join(i for i in columns if i in expression)
+    script = f"def f({req_columns}):\n    return {expression}"
+    tree = ast.parse(script)
+    code = compile(tree, filename="blah", mode="exec")
+    namespace = {}
+    exec(code, namespace)
+    f = namespace["f"]
+    if not callable(f):
+        raise ValueError(f"The expression could not be parse: {expression}")
+    return f
 
-    if A.step == 1 or B.step == 1:
-        start = max(A_start,B_start)
-        step = B.step if A.step==1 else A.step
-        end = c
+
+def intercept(A, B):
+    """Enables calculation of the intercept of two range objects.
+    Used to determine if a datablock contains a slice.
+
+    Args:
+        A: range
+        B: range
+
+    Returns:
+        range: The intercept of ranges A and B.
+    """
+    type_check(A, range)
+    type_check(B, range)
+
+    if A.step < 1:
+        A = range(A.stop + 1, A.start + 1, 1)
+    if B.step < 1:
+        B = range(B.stop + 1, B.start + 1, 1)
+
+    if len(A) == 0:
+        return range(0)
+    if len(B) == 0:
+        return range(0)
+
+    if A.stop <= B.start:
+        return range(0)
+    if A.start >= B.stop:
+        return range(0)
+
+    if A.start <= B.start:
+        if A.stop <= B.stop:
+            start, end = B.start, A.stop
+        elif A.stop > B.stop:
+            start, end = B.start, B.stop
+        else:
+            raise ValueError("bad logic")
+    elif A.start < B.stop:
+        if A.stop <= B.stop:
+            start, end = A.start, A.stop
+        elif A.stop > B.stop:
+            start, end = A.start, B.stop
+        else:
+            raise ValueError("bad logic")
     else:
-        intersection = set(range(A_start, c, A.step)).intersection(set(range(B_start, c, B.step)))
-        if not intersection:
+        raise ValueError("bad logic")
+
+    a_steps = math.ceil((start - A.start) / A.step)
+    a_start = (a_steps * A.step) + A.start
+
+    b_steps = math.ceil((start - B.start) / B.step)
+    b_start = (b_steps * B.step) + B.start
+
+    if A.step == 1 or B.step == 1:
+        start = max(a_start, b_start)
+        step = max(A.step, B.step)
+        return range(start, end, step)
+    elif A.step == B.step:
+        a, b = min(A.start, B.start), max(A.start, B.start)
+        if (b - a) % A.step != 0:  # then the ranges are offset.
             return range(0)
-        start = min(intersection)
-        end = max(c, max(intersection))
-        intersection.remove(start)
-        step = min(intersection) - start
-    
-    return range(start, end, step)
+        else:
+            return range(b, end, step)
+    else:
+        # determine common step size:
+        step = max(A.step, B.step) if math.gcd(A.step, B.step) != 1 else A.step * B.step
+        # examples:
+        # 119 <-- 17 if 1 != 1 else 119 <-- max(7, 17) if math.gcd(7, 17) != 1 else 7 * 17
+        #  30 <-- 30 if 3 != 1 else 90 <-- max(3, 30) if math.gcd(3, 30) != 1 else 3*30
+        if A.step < B.step:
+            for n in range(a_start, end, A.step):  # increment in smallest step to identify the first common value.
+                if n < b_start:
+                    continue
+                elif (n - b_start) % B.step == 0:
+                    return range(n, end, step)  # common value found.
+        else:
+            for n in range(b_start, end, B.step):
+                if n < a_start:
+                    continue
+                elif (n - a_start) % A.step == 0:
+                    return range(n, end, step)
 
+        return range(0)
 
-# This list is the contact:
+
+# This list is the contract:
 required_keys = {
-    'min','max','mean','median','stdev','mode',
-    'distinct', 'iqr_low','iqr_high','iqr','sum',
-    'summary type', 'histogram'}
+    "min",
+    "max",
+    "mean",
+    "median",
+    "stdev",
+    "mode",
+    "distinct",
+    "iqr_low",
+    "iqr_high",
+    "iqr",
+    "sum",
+    "summary type",
+    "histogram",
+}
 
 
-def summary_statistics(values,counts):
+def summary_statistics(values, counts):
     """
     values: any type
     counts: integer
 
     returns dict with:
     - min (int/float, length of str, date)
     - max (int/float, length of str, date)
@@ -88,187 +178,218 @@
     - iqr (int/float, length of str, date)
     - sum (int/float, length of str, date)
     - histogram (2 arrays: values, count of each values)
     """
     # determine the dominant datatype:
     dtypes = defaultdict(int)
     most_frequent, most_frequent_dtype = 0, int
-    for v,c in zip(values, counts):
+    for v, c in zip(values, counts):
         dtype = type(v)
         total = dtypes[dtype] + c
         dtypes[dtype] = total
         if total > most_frequent:
             most_frequent_dtype = dtype
             most_frequent = total
-    
+
+    if most_frequent == 0:
+        return {}
+
     most_frequent_dtype = max(dtypes, key=dtypes.get)
-    mask = [type(v)==most_frequent_dtype for v in values]
+    mask = [type(v) == most_frequent_dtype for v in values]
     v = list(compress(values, mask))
     c = list(compress(counts, mask))
-    
-    f = summary_methods.get(most_frequent_dtype, int)        
-    result = f(v,c)
-    result['distinct'] = len(values)
-    result['summary type'] = most_frequent_dtype.__name__
-    result['histogram'] = [values, counts]
+
+    f = summary_methods.get(most_frequent_dtype, int)
+    result = f(v, c)
+    result["distinct"] = len(values)
+    result["summary type"] = most_frequent_dtype.__name__
+    result["histogram"] = [values, counts]
     assert set(result.keys()) == required_keys, "Key missing!"
     return result
 
 
-def _numeric_statistics_summary(v,c):
-    VC = [[v,c] for v,c in zip(v, c)]
+def _numeric_statistics_summary(v, c):
+    VC = [[v, c] for v, c in zip(v, c)]
     VC.sort()
-    
-    total_val, mode, median, total_cnt =  0, None, None, sum(c)
-    
+
+    total_val, mode, median, total_cnt = 0, None, None, sum(c)
+
     max_cnt, cnt_n = -1, 0
-    mn,cstd = 0, 0.0
-    iqr25 = total_cnt * 1/4
-    iqr50 = total_cnt * 1/2
-    iqr75 = total_cnt * 3/4
+    mn, cstd = 0, 0.0
+    iqr25 = total_cnt * 1 / 4
+    iqr50 = total_cnt * 1 / 2
+    iqr75 = total_cnt * 3 / 4
     iqr_low, iqr_high = 0, 0
     vx_0 = None
-    vmin,vmax = VC[0][0], VC[-1][0]
+    vmin, vmax = VC[0][0], VC[-1][0]
 
     for vx, cx in VC:
         cnt_0 = cnt_n
         cnt_n += cx
 
-        if cnt_0 < iqr25 < cnt_n:  # iqr 25% 
+        if cnt_0 < iqr25 < cnt_n:  # iqr 25%
             iqr_low = vx
         elif cnt_0 == iqr25:
-            _,delta = divmod(1*(total_cnt-1), 4)
-            iqr_low = (vx_0 * (4-delta) + vx * delta) / 4
+            _, delta = divmod(1 * (total_cnt - 1), 4)
+            iqr_low = (vx_0 * (4 - delta) + vx * delta) / 4
 
         # median calculations
-        if cnt_n-cx < iqr50 < cnt_n:
+        if cnt_n - cx < iqr50 < cnt_n:
             median = vx
         elif cnt_0 == iqr50:
-            _,delta = divmod(2*(total_cnt-1), 4)
-            median = (vx_0 * (4-delta) + vx * delta) / 4
+            _, delta = divmod(2 * (total_cnt - 1), 4)
+            median = (vx_0 * (4 - delta) + vx * delta) / 4
 
         if cnt_0 < iqr75 < cnt_n:  # iqr 75%
             iqr_high = vx
         elif cnt_0 == iqr75:
-            _,delta = divmod(3*(total_cnt-1), 4)
-            iqr_high = (vx_0 * (4-delta) + vx * delta) / 4
-        
+            _, delta = divmod(3 * (total_cnt - 1), 4)
+            iqr_high = (vx_0 * (4 - delta) + vx * delta) / 4
+
         # stdev calulations
-        # cnt = cnt_n  # self.count += 1
-        dt = cx * (vx-mn) # dt = value - self.mean
+        dt = cx * (vx - mn)  # dt = value - self.mean
         mn += dt / cnt_n  # self.mean += dt / self.count
-        cstd += dt * (vx-mn)  #self.c += dt * (value - self.mean)
+        cstd += dt * (vx - mn)  # self.c += dt * (value - self.mean)
 
         # mode calculations
         if cx > max_cnt:
-            mode,max_cnt = vx,cx
+            mode, max_cnt = vx, cx
 
-        total_val += vx*cx
+        total_val += vx * cx
         vx_0 = vx
-    
-    var = cstd / (cnt_n-1) if cnt_n > 1 else 0
-    stdev = var**(1/2) if cnt_n > 1 else 0
+
+    var = cstd / (cnt_n - 1) if cnt_n > 1 else 0
+    stdev = var ** (1 / 2) if cnt_n > 1 else 0
 
     d = {
-        'min': vmin,
-        'max': vmax,
-        'mean': total_val / (total_cnt if total_cnt >= 1 else None),
-        'median': median,
-        'stdev': stdev,
-        'mode': mode,
-        'iqr_low': iqr_low,
-        'iqr_high': iqr_high,
-        'iqr': iqr_high - iqr_low,
-        'sum': total_val,
+        "min": vmin,
+        "max": vmax,
+        "mean": total_val / (total_cnt if total_cnt >= 1 else None),
+        "median": median,
+        "stdev": stdev,
+        "mode": mode,
+        "iqr_low": iqr_low,
+        "iqr_high": iqr_high,
+        "iqr": iqr_high - iqr_low,
+        "sum": total_val,
     }
     return d
 
 
-def _none_type_summary(v,c):
-    return {k:'n/a' for k in required_keys}
+def _none_type_summary(v, c):
+    return {k: "n/a" for k in required_keys}
 
 
-def _boolean_statistics_summary(v,c):
+def _boolean_statistics_summary(v, c):
     v = [int(vx) for vx in v]
-    d = _numeric_statistics_summary(v,c)
-    for k,v in d.items():
-        if k in {'mean','stdev','sum','iqr_low','iqr_high','iqr'}:
+    d = _numeric_statistics_summary(v, c)
+    for k, v in d.items():
+        if k in {"mean", "stdev", "sum", "iqr_low", "iqr_high", "iqr"}:
             continue
         elif v == 1:
             d[k] = True
         elif v == 0:
             d[k] = False
         else:
             pass
     return d
 
 
-def _timedelta_statistics_summary(v,c):
-    v= [vx.days + v.seconds/(24*60*60) for vx in v]
-    d = _numeric_statistics_summary(v,c)
+def _timedelta_statistics_summary(v, c):
+    v = [vx.days + v.seconds / (24 * 60 * 60) for vx in v]
+    d = _numeric_statistics_summary(v, c)
     for k in d.keys():
         d[k] = timedelta(d[k])
     return d
 
 
-def _datetime_statistics_summary(v,c):
+def _datetime_statistics_summary(v, c):
     v = [vx.timestamp() for vx in v]
-    d = _numeric_statistics_summary(v,c)
+    d = _numeric_statistics_summary(v, c)
     for k in d.keys():
-        if k in {'stdev','iqr','sum'}:
+        if k in {"stdev", "iqr", "sum"}:
             d[k] = f"{d[k]/(24*60*60)} days"
         else:
             d[k] = datetime.fromtimestamp(d[k])
     return d
 
 
-def _time_statistics_summary(v,c):
-    v = [ sum(t.hour * 60 * 60,t.minute * 60, t.second, t.microsecond/1e6) for t in v]
-    d = _numeric_statistics_summary(v,c)
+def _time_statistics_summary(v, c):
+    v = [sum(t.hour * 60 * 60, t.minute * 60, t.second, t.microsecond / 1e6) for t in v]
+    d = _numeric_statistics_summary(v, c)
     for k in d.keys():
-        if k in {'min','max','mean','median'}:
+        if k in {"min", "max", "mean", "median"}:
             timestamp = d[k]
             hours = timestamp // (60 * 60)
             timestamp -= hours * 60 * 60
             minutes = timestamp // 60
             timestamp -= minutes * 60
-            d[k] = time.fromtimestamp(hours,minutes,timestamp)
-        elif k in {'stdev','iqr','sum'}:
+            d[k] = time.fromtimestamp(hours, minutes, timestamp)
+        elif k in {"stdev", "iqr", "sum"}:
             d[k] = f"{d[k]} seconds"
         else:
             pass
     return d
 
 
-def _date_statistics_summary(v,c):
-    v = [datetime(d.year,d.month,d.day,0,0,0).timestamp() for d in v]
-    d = _numeric_statistics_summary(v,c)
+def _date_statistics_summary(v, c):
+    v = [datetime(d.year, d.month, d.day, 0, 0, 0).timestamp() for d in v]
+    d = _numeric_statistics_summary(v, c)
     for k in d.keys():
-        if k in {'min','max','mean','median'}:
+        if k in {"min", "max", "mean", "median"}:
             d[k] = date(*datetime.fromtimestamp(d[k]).timetuple()[:3])
-        elif k in {'stdev','iqr','sum'}:
+        elif k in {"stdev", "iqr", "sum"}:
             d[k] = f"{d[k]/(24*60*60)} days"
         else:
             pass
     return d
 
 
-def _string_statistics_summary(v,c):
+def _string_statistics_summary(v, c):
     vx = [len(x) for x in v]
-    d = _numeric_statistics_summary(vx,c)
+    d = _numeric_statistics_summary(vx, c)
+
+    vc_sorted = sorted(zip(v, c), key=lambda t: t[1], reverse=True)
+    mode, _ = vc_sorted[0]
+
     for k in d.keys():
         d[k] = f"{d[k]} characters"
+
+    d["mode"] = mode
+
     return d
 
 
 summary_methods = {
-        bool: _boolean_statistics_summary,
-        int: _numeric_statistics_summary,
-        float: _numeric_statistics_summary,
-        str: _string_statistics_summary,
-        date: _date_statistics_summary,
-        datetime: _datetime_statistics_summary,
-        time: _time_statistics_summary,
-        timedelta: _timedelta_statistics_summary,
-        type(None): _none_type_summary,
-    }
-
+    bool: _boolean_statistics_summary,
+    int: _numeric_statistics_summary,
+    float: _numeric_statistics_summary,
+    str: _string_statistics_summary,
+    date: _date_statistics_summary,
+    datetime: _datetime_statistics_summary,
+    time: _time_statistics_summary,
+    timedelta: _timedelta_statistics_summary,
+    type(None): _none_type_summary,
+}
+
+
+def date_range(start, stop, step):
+    if not isinstance(start, datetime):
+        raise TypeError("start is not datetime")
+    if not isinstance(stop, datetime):
+        raise TypeError("stop is not datetime")
+    if not isinstance(step, timedelta):
+        raise TypeError("step is not timedelta")
+    n = (stop - start) // step
+    return [start + step * i for i in range(n)]
+
+
+def dict_to_rows(d):
+    type_check(d, dict)
+    rows = []
+    max_length = max(len(i) for i in d.values())
+    order = list(d.keys())
+    rows.append(order)
+    for i in range(max_length):
+        row = [d[k][i] for k in order]
+        rows.append(row)
+    return rows
```

## tablite/version.py

```diff
@@ -1,3 +1,3 @@
-major, minor, patch = 2022, 9, 3
+major, minor, patch = 2023, 6, "dev1"
 __version_info__ = (major, minor, patch)
-__version__ = '.'.join(str(i) for i in __version_info__)
+__version__ = ".".join(str(i) for i in __version_info__)
```

## Comparing `tablite-2022.9.3.data/data/LICENSE` & `tablite-2023.6.dev1.data/data/LICENSE`

 * *Files identical despite different names*

## Comparing `tablite-2022.9.3.data/data/README.md` & `tablite-2023.6.dev1.data/data/README.md`

 * *Files 14% similar despite different names*

```diff
@@ -1,26 +1,36 @@
 # Tablite
 
-![Build status](https://github.com/root-11/tablite/actions/workflows/python-package.yml/badge.svg)
-[![Code coverage](https://codecov.io/gh/root-11/tablite/branch/master/graph/badge.svg)](https://codecov.io/gh/root-11/tablite)
+![Build status](https://github.com/root-11/tablite/actions/workflows/python-test.yml/badge.svg)
+[![codecov](https://codecov.io/gh/root-11/tablite/branch/master/graph/badge.svg?token=A0QEWGO9R6)](https://codecov.io/gh/root-11/tablite)
 [![Downloads](https://pepy.tech/badge/tablite)](https://pepy.tech/project/tablite)
 [![Downloads](https://pepy.tech/badge/tablite/month)](https://pepy.tech/project/tablite)
+[![PyPI version](https://badge.fury.io/py/tablite.svg)](https://badge.fury.io/py/tablite)
 
 --------------
 
-## Overview 
+## Contents
 
-`Tablite` seeks to be the go-to library for tabular data with an api that is as close in synxtax to pure python as possible. 
+- [introduction](#introduction)
+- [installation](#installation)
+- [feature overview](#feature_overview)
+- [tutorial](#tutorial)
+- [latest updates](#latest_updates)
+- [credits](#credits)
+
+## <a name="introduction"></a>Introduction 
+
+`Tablite` seeks to be the go-to library for manipulating tabular data with an api that is as close in syntax to pure python as possible. 
 
 
 ### Even smaller memory footprint
 
-Tablite uses HDF5 as a backend with strong abstraction, so that copy, append & repetition of data is handled in pages. This is imperative for [incremental data processing](https://github.com/root-11/tablite/blob/master/images/incremental_dataprocessing.svg). 
+Tablite uses HDF5 as a backend with strong abstraction, so that copy, append & repetition of data is handled in pages. This is imperative for [incremental data processing](https://raw.githubusercontent.com/root-11/tablite/74e7b44cfc314950b7a769316cb48d67cce725d0/images/incremental_dataprocessing.svg).
 
-Tablite tests [for memory footprint](https://github.com/root-11/tablite/blob/master/tests/test_memory_footprint.py). One test compares the memory footprint of 10,000,000 integers where `tablite` will use < 1 Mb RAM in contrast to python which will require around 133.7 Mb of RAM (1M lists with 10 integers). Tablite also tests to assure that working with [1Tb of data](https://github.com/root-11/tablite/blob/master/tests/test_filereader_time.py) is tolerable.
+Tablite tests [for memory footprint](https://github.com/root-11/tablite/blob/master/tests/test_memory_footprint.py). One test compares the memory footprint of 10,000,000 integers where `tablite` will use < 1 Mb RAM in contrast to python which will require around 133.7 Mb of RAM (1M lists with 10 integers). Tablite also tests to assure that working with [1Tb of data](https://github.com/root-11/tablite/blob/9bb6e572538a85aee31ef8a4a60c0945a6f857a4/tests/test_filereader_performance.py#L104) is tolerable.
 
 Tablite achieves this by using `HDF5` as storage which is faster than mmap'ed files for the average case \[[1](https://stackoverflow.com/questions/27710245/is-there-an-analysis-speed-or-memory-usage-advantage-to-using-hdf5-for-large-arr), [2](https://github.com/root-11/root-11.github.io/blob/master/content/short_intro_to_hdf5.ipynb) \] and stores all data in `/tmp/tablite.hdf5` so if your OS (windows/linux/mac) sits on a SSD it will benefit from high IOPS and permit slices of [9,000,000,000 rows in less than a second](https://github.com/root-11/tablite/blob/master/images/1TB_test.png?raw=true).
 
 ### Multiprocessing enabled by default
 
 Tablite uses multiprocessing for bypassing the GIL on all major operations. CSV import is [tested with 96M fields](https://github.com/root-11/tablite/blob/master/tests/test_filereader_time.py) that are imported and type-mapped to native python types in 120 secs.
 
@@ -40,15 +50,15 @@
 
 Tablite is ~200 kB.
 
 ### Helpful
 
 Tablite wants you to be productive, so a number of helpers are available. 
 
-- `Table.import_file` to import csv*, tsv, txt, xls, xlsx, xlsm, ods, zip and logs. There is automatic type detection (see [tutorial.ipynb](https://github.com/root-11/tablite/blob/master/tutorial.ipynb))
+- `Table.import_file` to import csv*, tsv, txt, xls, xlsx, xlsm, ods, zip and logs. There is automatic type detection (see [tutorial.ipynb](https://github.com/root-11/tablite/tree/master/docs/articles/tutorial.ipynb))
 - To peek into any supported file use `get_headers` which shows the first 10 rows.
 - Use `mytable.rows` and `mytable.columns` to iterate over rows or columns.
 - Create multi-key `.index` for quick lookups.
 - Perform multi-key `.sort`,
 - Filter using `.any` and `.all` to select specific rows.
 - use multi-key `.lookup` and `.join` to find data across tables.
 - Perform `.groupby` and reorganise data as a `.pivot` table with max, min, sum, first, last, count, unique, average, st.deviation, median and mode
@@ -56,22 +66,22 @@
 - Should you tables be similar but not the identical you can use `.stack` to "stack" tables on top of each other
 
 If you're still missing something add it to the [wishlist](https://github.com/root-11/tablite/issues)
 
 
 ---------------
 
-## Installation
+## <a name="installation"></a>Installation
 
-[Tablite](https://pypi.org/project/tablite/)
+Get it from pypi: [Tablite](https://pypi.org/project/tablite/) [![PyPI version](https://badge.fury.io/py/tablite.svg)](https://badge.fury.io/py/tablite)
 
 Install: `pip install tablite`  
 Usage:  `>>> from tablite import Table`  
 
-## General overview
+## <a name="feature_overview"></a>Feature overview
 
 |want to...| this way... |
 |---|---|
 |loop over rows| `[ row for row in table.rows ]`|
 |loop over columns| `[ table[col_name] for col_name in table.columns ]`|
 |slice | `myslice = table['A', 'B', slice(0,None,15)]`|
 |get column by name | `my_table['A']` |
@@ -87,16 +97,24 @@
 | filter    | `true, false = unfiltered.filter( [{"column1": 'a', "criteria":">=", 'value2':3}, ... more criteria ... ], filter_type='all' )`|
 | find any  | `any_even_rows = mytable.any('A': lambda x : x%2==0, 'B': lambda x > 0)`|
 | find all  | `all_even_rows = mytable.all('A': lambda x : x%2==0, 'B': lambda x > 0)`|
 | to json   | `json_str = my_table.to_json()`|
 | from json | `Table.from_json(json_str)`|
 
 
-## Tutorial
+## <a name="tutorial"></a>Tutorial
 
 To learn more see the [tutorial.ipynb](https://github.com/root-11/tablite/blob/master/tutorial.ipynb) (Jupyter notebook)
 
-## Credits
+
+## <a name="latest_updates"></a>Latest updates
+
+See [changelog.md](https://github.com/root-11/tablite/blob/master/changelog.md)
+
+
+## <a name="credits"></a>Credits
 
 - Martynas Kaunas - GroupBy functionality.
-- Audrius Kulikajevas - Edge case testing / various bugs.
-- realratchet - Jupyter notebook integration.
+- Audrius Kulikajevas - Edge case testing / various bugs, Jupyter notebook integration.
+- Sergej Sinkarenko - various bugs.
+- Ovidijus Grigas - various bugs, documentation.
+- Lori Cooper - spell checking.
```

## Comparing `tablite-2022.9.3.dist-info/LICENSE` & `tablite-2023.6.dev1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tablite-2022.9.3.dist-info/METADATA` & `tablite-2023.6.dev1.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,60 +1,71 @@
 Metadata-Version: 2.1
 Name: tablite
-Version: 2022.9.3
+Version: 2023.6.dev1
 Summary: multiprocessing enabled out-of-memory data analysis library for tabular data.
 Home-page: https://github.com/root-11/tablite
 Author: https://github.com/root-11
 License: MIT
-Keywords: all,any,average,column,columns,count,csv,excel,filter,first,from,groupby,in-memory,index,indexing,inner join,is sorted,json,last,left join,list on disk,log,max,median,min,mode,ods,out-of-memory,outer join,pivot,pivot table,product,rows,show,sort,standard deviation,stored list,sum,table,tables,tablite,to,txt,unique,use disk,xlsx,zip
+Keywords: all,any,average,column,columns,count,csv,data imputation,date range,dict,excel,filter,first,from,from_pandas,groupby,guess,imputation,in-memory,index,indexing,inner join,is sorted,json,last,left join,list,list on disk,log,max,median,min,mode,numpy,ods,out-of-memory,outer join,pandas,pivot,pivot table,product,read csv,remove duplicates,replace,replace missing values,rows,show,sort,standard deviation,stored list,sum,table,tables,tablite,to,to_pandas,tools,transpose,txt,unique,use disk,xlsx,xround,zip
 Platform: any
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Science/Research
 Classifier: Natural Language :: English
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
-Requires-Python: >=3.7, <4
+Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: tqdm (>=4.63.0)
-Requires-Dist: numpy (>=1.22.3)
-Requires-Dist: h5py (>=3.6.0)
-Requires-Dist: psutil (>=5.9.0)
-Requires-Dist: chardet (==5.0.0)
+Requires-Dist: numpy (==1.24.3)
+Requires-Dist: psutil (>=5.9.5)
+Requires-Dist: chardet (==5.1.0)
 Requires-Dist: pyexcel (==0.7.0)
 Requires-Dist: pyexcel-odsr (==0.6.0)
 Requires-Dist: pyexcel-ods (==0.6.0)
-Requires-Dist: pyperclip (==1.8.2)
 Requires-Dist: pyexcel-xlsx (==0.6.0)
 Requires-Dist: pyexcel-xls (==0.7.0)
 Requires-Dist: pyuca (>=1.2)
-Requires-Dist: mplite (==1.1.0)
+Requires-Dist: mplite (==1.2.2)
+Requires-Dist: PyYAML (==6.0)
+Requires-Dist: openpyxl (==3.0.10)
+Requires-Dist: h5py (>=3.6.0)
 
 # Tablite
 
-![Build status](https://github.com/root-11/tablite/actions/workflows/python-package.yml/badge.svg)
-[![Code coverage](https://codecov.io/gh/root-11/tablite/branch/master/graph/badge.svg)](https://codecov.io/gh/root-11/tablite)
+![Build status](https://github.com/root-11/tablite/actions/workflows/python-test.yml/badge.svg)
+[![codecov](https://codecov.io/gh/root-11/tablite/branch/master/graph/badge.svg?token=A0QEWGO9R6)](https://codecov.io/gh/root-11/tablite)
 [![Downloads](https://pepy.tech/badge/tablite)](https://pepy.tech/project/tablite)
 [![Downloads](https://pepy.tech/badge/tablite/month)](https://pepy.tech/project/tablite)
+[![PyPI version](https://badge.fury.io/py/tablite.svg)](https://badge.fury.io/py/tablite)
 
 --------------
 
-## Overview 
+## Contents
+
+- [introduction](#introduction)
+- [installation](#installation)
+- [feature overview](#feature_overview)
+- [tutorial](#tutorial)
+- [latest updates](#latest_updates)
+- [credits](#credits)
+
+## <a name="introduction"></a>Introduction 
 
-`Tablite` seeks to be the go-to library for tabular data with an api that is as close in synxtax to pure python as possible. 
+`Tablite` seeks to be the go-to library for manipulating tabular data with an api that is as close in syntax to pure python as possible. 
 
 
 ### Even smaller memory footprint
 
-Tablite uses HDF5 as a backend with strong abstraction, so that copy, append & repetition of data is handled in pages. This is imperative for [incremental data processing](https://github.com/root-11/tablite/blob/master/images/incremental_dataprocessing.svg). 
+Tablite uses HDF5 as a backend with strong abstraction, so that copy, append & repetition of data is handled in pages. This is imperative for [incremental data processing](https://raw.githubusercontent.com/root-11/tablite/74e7b44cfc314950b7a769316cb48d67cce725d0/images/incremental_dataprocessing.svg).
 
-Tablite tests [for memory footprint](https://github.com/root-11/tablite/blob/master/tests/test_memory_footprint.py). One test compares the memory footprint of 10,000,000 integers where `tablite` will use < 1 Mb RAM in contrast to python which will require around 133.7 Mb of RAM (1M lists with 10 integers). Tablite also tests to assure that working with [1Tb of data](https://github.com/root-11/tablite/blob/master/tests/test_filereader_time.py) is tolerable.
+Tablite tests [for memory footprint](https://github.com/root-11/tablite/blob/master/tests/test_memory_footprint.py). One test compares the memory footprint of 10,000,000 integers where `tablite` will use < 1 Mb RAM in contrast to python which will require around 133.7 Mb of RAM (1M lists with 10 integers). Tablite also tests to assure that working with [1Tb of data](https://github.com/root-11/tablite/blob/9bb6e572538a85aee31ef8a4a60c0945a6f857a4/tests/test_filereader_performance.py#L104) is tolerable.
 
 Tablite achieves this by using `HDF5` as storage which is faster than mmap'ed files for the average case \[[1](https://stackoverflow.com/questions/27710245/is-there-an-analysis-speed-or-memory-usage-advantage-to-using-hdf5-for-large-arr), [2](https://github.com/root-11/root-11.github.io/blob/master/content/short_intro_to_hdf5.ipynb) \] and stores all data in `/tmp/tablite.hdf5` so if your OS (windows/linux/mac) sits on a SSD it will benefit from high IOPS and permit slices of [9,000,000,000 rows in less than a second](https://github.com/root-11/tablite/blob/master/images/1TB_test.png?raw=true).
 
 ### Multiprocessing enabled by default
 
 Tablite uses multiprocessing for bypassing the GIL on all major operations. CSV import is [tested with 96M fields](https://github.com/root-11/tablite/blob/master/tests/test_filereader_time.py) that are imported and type-mapped to native python types in 120 secs.
 
@@ -74,15 +85,15 @@
 
 Tablite is ~200 kB.
 
 ### Helpful
 
 Tablite wants you to be productive, so a number of helpers are available. 
 
-- `Table.import_file` to import csv*, tsv, txt, xls, xlsx, xlsm, ods, zip and logs. There is automatic type detection (see [tutorial.ipynb](https://github.com/root-11/tablite/blob/master/tutorial.ipynb))
+- `Table.import_file` to import csv*, tsv, txt, xls, xlsx, xlsm, ods, zip and logs. There is automatic type detection (see [tutorial.ipynb](https://github.com/root-11/tablite/tree/master/docs/articles/tutorial.ipynb))
 - To peek into any supported file use `get_headers` which shows the first 10 rows.
 - Use `mytable.rows` and `mytable.columns` to iterate over rows or columns.
 - Create multi-key `.index` for quick lookups.
 - Perform multi-key `.sort`,
 - Filter using `.any` and `.all` to select specific rows.
 - use multi-key `.lookup` and `.join` to find data across tables.
 - Perform `.groupby` and reorganise data as a `.pivot` table with max, min, sum, first, last, count, unique, average, st.deviation, median and mode
@@ -90,22 +101,22 @@
 - Should you tables be similar but not the identical you can use `.stack` to "stack" tables on top of each other
 
 If you're still missing something add it to the [wishlist](https://github.com/root-11/tablite/issues)
 
 
 ---------------
 
-## Installation
+## <a name="installation"></a>Installation
 
-[Tablite](https://pypi.org/project/tablite/)
+Get it from pypi: [Tablite](https://pypi.org/project/tablite/) [![PyPI version](https://badge.fury.io/py/tablite.svg)](https://badge.fury.io/py/tablite)
 
 Install: `pip install tablite`  
 Usage:  `>>> from tablite import Table`  
 
-## General overview
+## <a name="feature_overview"></a>Feature overview
 
 |want to...| this way... |
 |---|---|
 |loop over rows| `[ row for row in table.rows ]`|
 |loop over columns| `[ table[col_name] for col_name in table.columns ]`|
 |slice | `myslice = table['A', 'B', slice(0,None,15)]`|
 |get column by name | `my_table['A']` |
@@ -121,16 +132,24 @@
 | filter    | `true, false = unfiltered.filter( [{"column1": 'a', "criteria":">=", 'value2':3}, ... more criteria ... ], filter_type='all' )`|
 | find any  | `any_even_rows = mytable.any('A': lambda x : x%2==0, 'B': lambda x > 0)`|
 | find all  | `all_even_rows = mytable.all('A': lambda x : x%2==0, 'B': lambda x > 0)`|
 | to json   | `json_str = my_table.to_json()`|
 | from json | `Table.from_json(json_str)`|
 
 
-## Tutorial
+## <a name="tutorial"></a>Tutorial
 
 To learn more see the [tutorial.ipynb](https://github.com/root-11/tablite/blob/master/tutorial.ipynb) (Jupyter notebook)
 
-## Credits
+
+## <a name="latest_updates"></a>Latest updates
+
+See [changelog.md](https://github.com/root-11/tablite/blob/master/changelog.md)
+
+
+## <a name="credits"></a>Credits
 
 - Martynas Kaunas - GroupBy functionality.
-- Audrius Kulikajevas - Edge case testing / various bugs.
-- realratchet - Jupyter notebook integration.
+- Audrius Kulikajevas - Edge case testing / various bugs, Jupyter notebook integration.
+- Sergej Sinkarenko - various bugs.
+- Ovidijus Grigas - various bugs, documentation.
+- Lori Cooper - spell checking.
```

