# Comparing `tmp/nncf-2.4.0.tar.gz` & `tmp/nncf-2.5.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "nncf-2.4.0.tar", last modified: Wed Feb  1 11:09:01 2023, max compression
+gzip compressed data, was "nncf-2.5.0.tar", last modified: Tue Jun  6 16:40:59 2023, max compression
```

## Comparing `nncf-2.4.0.tar` & `nncf-2.5.0.tar`

### file list

```diff
@@ -1,593 +1,680 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.471970 nncf-2.4.0/
--rw-r--r--   0 runner    (1001) docker     (123)    11357 2023-02-01 11:08:42.000000 nncf-2.4.0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)      122 2023-02-01 11:08:42.000000 nncf-2.4.0/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)    27584 2023-02-01 11:09:01.471970 nncf-2.4.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    26691 2023-02-01 11:08:42.000000 nncf-2.4.0/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.419971 nncf-2.4.0/licensing/
--rw-r--r--   0 runner    (1001) docker     (123)    61708 2023-02-01 11:08:42.000000 nncf-2.4.0/licensing/third-party-programs.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.419971 nncf-2.4.0/nncf/
--rw-r--r--   0 runner    (1001) docker     (123)     2255 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.419971 nncf-2.4.0/nncf/api/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14717 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/api/compression.py
--rw-r--r--   0 runner    (1001) docker     (123)      979 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/api/statistics.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.419971 nncf-2.4.0/nncf/common/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.419971 nncf-2.4.0/nncf/common/accuracy_aware_training/
--rw-r--r--   0 runner    (1001) docker     (123)      765 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/accuracy_aware_training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    21273 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/accuracy_aware_training/runner.py
--rw-r--r--   0 runner    (1001) docker     (123)     5614 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/accuracy_aware_training/runner_factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     1535 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/accuracy_aware_training/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)    25140 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/accuracy_aware_training/training_loop.py
--rw-r--r--   0 runner    (1001) docker     (123)      944 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/collector.py
--rw-r--r--   0 runner    (1001) docker     (123)    15141 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/composite_compression.py
--rw-r--r--   0 runner    (1001) docker     (123)    11848 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/compression.py
--rw-r--r--   0 runner    (1001) docker     (123)     1020 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/engine.py
--rw-r--r--   0 runner    (1001) docker     (123)     2401 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2913 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/factory.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.423971 nncf-2.4.0/nncf/common/graph/
--rw-r--r--   0 runner    (1001) docker     (123)      617 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      766 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/definitions.py
--rw-r--r--   0 runner    (1001) docker     (123)    29648 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     6853 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/graph_matching.py
--rw-r--r--   0 runner    (1001) docker     (123)     6682 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/layer_attributes.py
--rw-r--r--   0 runner    (1001) docker     (123)     1360 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5157 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/operator_metatypes.py
--rw-r--r--   0 runner    (1001) docker     (123)    10700 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.423971 nncf-2.4.0/nncf/common/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7957 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/transformations/commands.py
--rw-r--r--   0 runner    (1001) docker     (123)     1936 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/transformations/layout.py
--rw-r--r--   0 runner    (1001) docker     (123)     3271 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/graph/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.423971 nncf-2.4.0/nncf/common/hardware/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11653 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/hardware/config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.423971 nncf-2.4.0/nncf/common/hardware/configs/
--rw-r--r--   0 runner    (1001) docker     (123)     6837 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/hardware/configs/cpu.json
--rw-r--r--   0 runner    (1001) docker     (123)     6311 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/hardware/configs/gpu.json
--rw-r--r--   0 runner    (1001) docker     (123)     3113 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/hardware/configs/template.json
--rw-r--r--   0 runner    (1001) docker     (123)     8499 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/hardware/configs/vpu.json
--rw-r--r--   0 runner    (1001) docker     (123)     1719 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/hardware/opset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.423971 nncf-2.4.0/nncf/common/initialization/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/initialization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4347 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/initialization/batchnorm_adaptation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1188 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/initialization/dataloader.py
--rw-r--r--   0 runner    (1001) docker     (123)    22165 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/insertion_point_graph.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.423971 nncf-2.4.0/nncf/common/logging/
--rw-r--r--   0 runner    (1001) docker     (123)      630 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/logging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2618 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/logging/logger.py
--rw-r--r--   0 runner    (1001) docker     (123)     3280 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/logging/progress_bar.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.423971 nncf-2.4.0/nncf/common/pruning/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5915 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/clusterization.py
--rw-r--r--   0 runner    (1001) docker     (123)     7885 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/mask_propagation.py
--rw-r--r--   0 runner    (1001) docker     (123)    10404 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/model_analysis.py
--rw-r--r--   0 runner    (1001) docker     (123)    20043 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/node_selector.py
--rw-r--r--   0 runner    (1001) docker     (123)    15335 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     8398 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (123)    10562 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/shape_pruning_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     8338 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)      845 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/structs.py
--rw-r--r--   0 runner    (1001) docker     (123)     6328 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/symbolic_mask.py
--rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/tensor_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)    16826 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8521 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/pruning/weights_flops_calculator.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.427971 nncf-2.4.0/nncf/common/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5092 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/collectors.py
--rw-r--r--   0 runner    (1001) docker     (123)     6362 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/config_assignment.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.427971 nncf-2.4.0/nncf/common/quantization/initialization/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/initialization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7621 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/initialization/range.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.427971 nncf-2.4.0/nncf/common/quantization/quantizer_propagation/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/quantizer_propagation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    75590 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/quantizer_propagation/graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     7827 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/quantizer_propagation/grouping.py
--rw-r--r--   0 runner    (1001) docker     (123)    87568 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/quantizer_propagation/solver.py
--rw-r--r--   0 runner    (1001) docker     (123)     4483 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/quantizer_propagation/structs.py
--rw-r--r--   0 runner    (1001) docker     (123)     1643 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/quantizer_propagation/visualizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    23887 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/quantizer_setup.py
--rw-r--r--   0 runner    (1001) docker     (123)     2588 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)     7903 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)    13137 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/quantization/structs.py
--rw-r--r--   0 runner    (1001) docker     (123)    10342 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (123)     5570 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/scopes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.427971 nncf-2.4.0/nncf/common/sparsity/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/sparsity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4309 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/sparsity/collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/sparsity/controller.py
--rw-r--r--   0 runner    (1001) docker     (123)    13359 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/sparsity/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (123)     8034 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/sparsity/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)     4593 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/stateful_classes_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     5138 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)     1421 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.427971 nncf-2.4.0/nncf/common/tensor_statistics/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/tensor_statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4509 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/tensor_statistics/aggregator.py
--rw-r--r--   0 runner    (1001) docker     (123)    16339 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/tensor_statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (123)     2834 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/tensor_statistics/reduction.py
--rw-r--r--   0 runner    (1001) docker     (123)     4247 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/tensor_statistics/statistic_point.py
--rw-r--r--   0 runner    (1001) docker     (123)     3335 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/tensor_statistics/statistics.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.427971 nncf-2.4.0/nncf/common/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2860 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/backend.py
--rw-r--r--   0 runner    (1001) docker     (123)     1072 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/debug.py
--rw-r--r--   0 runner    (1001) docker     (123)     1987 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/decorators.py
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/dot_file_rw.py
--rw-r--r--   0 runner    (1001) docker     (123)     2369 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/helpers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.427971 nncf-2.4.0/nncf/common/utils/logger/
--rw-r--r--   0 runner    (1001) docker     (123)     1231 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/logger/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1159 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/os.py
--rw-r--r--   0 runner    (1001) docker     (123)     1852 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     3279 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/common/utils/tensorboard.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/config/
--rw-r--r--   0 runner    (1001) docker     (123)      621 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5880 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2077 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/definitions.py
--rw-r--r--   0 runner    (1001) docker     (123)     9726 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/extractors.py
--rw-r--r--   0 runner    (1001) docker     (123)    10961 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schema.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/config/schemata/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7571 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/accuracy_aware.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/config/schemata/algo/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/algo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2406 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/algo/binarization.py
--rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/algo/const_sparsity.py
--rw-r--r--   0 runner    (1001) docker     (123)    11690 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/algo/filter_pruning.py
--rw-r--r--   0 runner    (1001) docker     (123)     2659 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/algo/knowledge_distillation.py
--rw-r--r--   0 runner    (1001) docker     (123)     3334 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/algo/magnitude_sparsity.py
--rw-r--r--   0 runner    (1001) docker     (123)    32683 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/algo/quantization.py
--rw-r--r--   0 runner    (1001) docker     (123)     2561 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/algo/rb_sparsity.py
--rw-r--r--   0 runner    (1001) docker     (123)     1976 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/basic.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/config/schemata/common/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1191 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/common/compression.py
--rw-r--r--   0 runner    (1001) docker     (123)     2178 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/common/initialization.py
--rw-r--r--   0 runner    (1001) docker     (123)     7422 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/common/sparsity.py
--rw-r--r--   0 runner    (1001) docker     (123)     2578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/common/targeting.py
--rw-r--r--   0 runner    (1001) docker     (123)     2777 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/defaults.py
--rw-r--r--   0 runner    (1001) docker     (123)    20253 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/schemata/experimental_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     3019 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/structures.py
--rw-r--r--   0 runner    (1001) docker     (123)     1079 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/telemetry_extractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     1244 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/config/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/data/
--rw-r--r--   0 runner    (1001) docker     (123)      617 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5557 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/data/dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     1071 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/definitions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/experimental/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/experimental/openvino_native/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2083 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/experimental/openvino_native/graph/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/graph/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/experimental/openvino_native/graph/metatypes/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/graph/metatypes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11650 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/graph/metatypes/openvino_metatypes.py
--rw-r--r--   0 runner    (1001) docker     (123)     5605 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4657 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/graph/nncf_graph_builder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.431971 nncf-2.4.0/nncf/experimental/openvino_native/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/quantization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/openvino_native/quantization/algorithms/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/quantization/algorithms/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/openvino_native/quantization/algorithms/min_max/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/openvino_native/quantization/algorithms/min_max/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/tensorflow/
--rw-r--r--   0 runner    (1001) docker     (123)      682 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5931 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/context.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/tensorflow/graph/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9340 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/graph/argprovider.py
--rw-r--r--   0 runner    (1001) docker     (123)    18105 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/graph/converter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2306 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2176 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/graph/node_attributes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/tensorflow/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3410 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/graph/transformations/commands.py
--rw-r--r--   0 runner    (1001) docker     (123)     2171 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/graph/transformations/layout.py
--rw-r--r--   0 runner    (1001) docker     (123)     5734 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/nncf_network.py
--rw-r--r--   0 runner    (1001) docker     (123)    10492 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/patch_tf.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/tensorflow/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17215 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/quantization/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     5021 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/quantization/init_range.py
--rw-r--r--   0 runner    (1001) docker     (123)     5650 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/quantization/quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1787 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/tensorflow/scope.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/torch/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/torch/nas/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.435971 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9861 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/base_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    21882 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_depth.py
--rw-r--r--   0 runner    (1001) docker     (123)    24097 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_kernel.py
--rw-r--r--   0 runner    (1001) docker     (123)    55486 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_width.py
--rw-r--r--   0 runner    (1001) docker     (123)     8207 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4285 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_controller.py
--rw-r--r--   0 runner    (1001) docker     (123)      783 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_dim.py
--rw-r--r--   0 runner    (1001) docker     (123)     2840 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/filter_reorder.py
--rw-r--r--   0 runner    (1001) docker     (123)    14258 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/multi_elasticity_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4345 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/visualization.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/search/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/search/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8139 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3566 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    22744 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/search/search.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2826 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/base_training.py
--rw-r--r--   0 runner    (1001) docker     (123)     8511 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5858 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     7159 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    10424 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py
--rw-r--r--   0 runner    (1001) docker     (123)    13498 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (123)     4589 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py
--rw-r--r--   0 runner    (1001) docker     (123)    13698 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/experimental/torch/search_building_blocks/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/search_building_blocks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    30953 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/search_building_blocks/search_blocks.py
--rw-r--r--   0 runner    (1001) docker     (123)    16796 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/search_building_blocks/search_graph.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/experimental/torch/sparsity/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11129 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     1484 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/functions.py
--rw-r--r--   0 runner    (1001) docker     (123)    14896 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/layers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1767 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/loss.py
--rw-r--r--   0 runner    (1001) docker     (123)    16672 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (123)    21024 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/structured_mask_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     8159 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/experimental/torch/sparsity/movement/structured_mask_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/onnx/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1757 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/onnx/graph/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/graph/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/onnx/graph/metatypes/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/graph/metatypes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14732 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/graph/metatypes/onnx_metatypes.py
--rw-r--r--   0 runner    (1001) docker     (123)    19768 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    12433 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/graph/nncf_graph_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    21158 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/graph/onnx_graph.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.439971 nncf-2.4.0/nncf/onnx/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5366 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/graph/transformations/commands.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/onnx/hardware/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      966 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/hardware/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3444 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/hardware/fused_patterns.py
--rw-r--r--   0 runner    (1001) docker     (123)     4291 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/hardware/pattern_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     6636 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/hardware/patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/onnx/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3183 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/quantization/default_quantization.py
--rw-r--r--   0 runner    (1001) docker     (123)     2830 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/quantization/quantize.py
--rw-r--r--   0 runner    (1001) docker     (123)     6735 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/quantization/quantizer_parameters.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/onnx/statistics/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3959 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/statistics/aggregator.py
--rw-r--r--   0 runner    (1001) docker     (123)     5045 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/statistics/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)      899 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/onnx/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/openvino/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/openvino/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7874 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/openvino/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/openvino/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/openvino/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3658 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/openvino/quantization/accuracy_aware.py
--rw-r--r--   0 runner    (1001) docker     (123)     8347 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/openvino/quantization/quantize.py
--rw-r--r--   0 runner    (1001) docker     (123)     3630 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/parameters.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      761 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/quantization/algorithms/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2738 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/algorithm.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/quantization/algorithms/bias_correction/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/bias_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    27465 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/bias_correction/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     8254 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/bias_correction/backend.py
--rw-r--r--   0 runner    (1001) docker     (123)     9259 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/bias_correction/onnx_backend.py
--rw-r--r--   0 runner    (1001) docker     (123)      767 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/definitions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/quantization/algorithms/fast_bias_correction/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/fast_bias_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15908 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/fast_bias_correction/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7361 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/fast_bias_correction/backend.py
--rw-r--r--   0 runner    (1001) docker     (123)     6471 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/quantization/algorithms/min_max/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/min_max/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    25542 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/min_max/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7598 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/min_max/backend.py
--rw-r--r--   0 runner    (1001) docker     (123)     6671 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/min_max/onnx_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.443971 nncf-2.4.0/nncf/quantization/algorithms/post_training/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/post_training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9299 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/algorithms/post_training/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     7084 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/quantize.py
--rw-r--r--   0 runner    (1001) docker     (123)      934 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/quantization/telemetry_extractors.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.447971 nncf-2.4.0/nncf/telemetry/
--rw-r--r--   0 runner    (1001) docker     (123)      736 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/telemetry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3649 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/telemetry/decorator.py
--rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/telemetry/events.py
--rw-r--r--   0 runner    (1001) docker     (123)     2101 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/telemetry/extractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     4225 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/telemetry/wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.447971 nncf-2.4.0/nncf/tensorflow/
--rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.447971 nncf-2.4.0/nncf/tensorflow/accuracy_aware_training/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/accuracy_aware_training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5099 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/accuracy_aware_training/keras_model_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     5518 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/accuracy_aware_training/runner.py
--rw-r--r--   0 runner    (1001) docker     (123)     2666 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/algorithm_selector.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.447971 nncf-2.4.0/nncf/tensorflow/api/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2548 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/api/composite_compression.py
--rw-r--r--   0 runner    (1001) docker     (123)     2855 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/api/compression.py
--rw-r--r--   0 runner    (1001) docker     (123)     2832 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/batchnorm_adaptation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.447971 nncf-2.4.0/nncf/tensorflow/callbacks/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/callbacks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2884 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/callbacks/checkpoint_callback.py
--rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/callbacks/statistics_callback.py
--rw-r--r--   0 runner    (1001) docker     (123)     3926 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)      798 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/functions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.447971 nncf-2.4.0/nncf/tensorflow/graph/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    39653 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/converter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.447971 nncf-2.4.0/nncf/tensorflow/graph/metatypes/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/metatypes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6934 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/metatypes/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    22086 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/metatypes/keras_layers.py
--rw-r--r--   0 runner    (1001) docker     (123)     2723 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/metatypes/matcher.py
--rw-r--r--   0 runner    (1001) docker     (123)    11644 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/metatypes/tf_ops.py
--rw-r--r--   0 runner    (1001) docker     (123)    24655 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/model_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3742 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/pattern_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     5378 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.447971 nncf-2.4.0/nncf/tensorflow/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17405 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/transformations/commands.py
--rw-r--r--   0 runner    (1001) docker     (123)     6007 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/transformations/layout.py
--rw-r--r--   0 runner    (1001) docker     (123)     9120 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/graph/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.451971 nncf-2.4.0/nncf/tensorflow/hardware/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      962 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/hardware/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3455 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/hardware/fused_patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.451971 nncf-2.4.0/nncf/tensorflow/helpers/
--rw-r--r--   0 runner    (1001) docker     (123)      654 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/helpers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2097 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/helpers/callback_creation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6317 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/helpers/model_creation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2453 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/helpers/model_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     1079 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/helpers/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2461 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/initialization.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.451971 nncf-2.4.0/nncf/tensorflow/layers/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      832 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/layers/custom_objects.py
--rw-r--r--   0 runner    (1001) docker     (123)     2672 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/layers/data_layout.py
--rw-r--r--   0 runner    (1001) docker     (123)     3516 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/layers/operation.py
--rw-r--r--   0 runner    (1001) docker     (123)    10959 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/layers/wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)      944 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.451971 nncf-2.4.0/nncf/tensorflow/pruning/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17116 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/base_algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     1719 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/callbacks.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.451971 nncf-2.4.0/nncf/tensorflow/pruning/filter_pruning/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/filter_pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    27177 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/filter_pruning/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     2516 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/filter_pruning/functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     4939 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     2246 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/tensor_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4827 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/pruning/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.451971 nncf-2.4.0/nncf/tensorflow/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      826 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    36049 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     3452 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/collectors.py
--rw-r--r--   0 runner    (1001) docker     (123)     3328 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/default_quantization.py
--rw-r--r--   0 runner    (1001) docker     (123)     2255 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/functions.py
--rw-r--r--   0 runner    (1001) docker     (123)    12296 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/init_range.py
--rw-r--r--   0 runner    (1001) docker     (123)     4248 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/layers.py
--rw-r--r--   0 runner    (1001) docker     (123)     4526 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/quantize.py
--rw-r--r--   0 runner    (1001) docker     (123)    21047 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)     2448 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/quantization/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.451971 nncf-2.4.0/nncf/tensorflow/sparsity/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2298 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/base_algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     2097 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/callbacks.py
--rw-r--r--   0 runner    (1001) docker     (123)     3558 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/collector.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.455971 nncf-2.4.0/nncf/tensorflow/sparsity/magnitude/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/magnitude/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12847 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/magnitude/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     1036 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/magnitude/functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     3021 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/magnitude/operation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.455971 nncf-2.4.0/nncf/tensorflow/sparsity/rb/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/rb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8253 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/rb/algorithm.py
--rw-r--r--   0 runner    (1001) docker     (123)     1036 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/rb/functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     2855 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/rb/loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     5373 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/rb/operation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2212 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/sparsity/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1084 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.455971 nncf-2.4.0/nncf/tensorflow/tensor_statistics/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/tensor_statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6800 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/tensor_statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (123)     2555 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/tensor_statistics/reduction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2774 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/tensor_statistics/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/tf_internals.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.455971 nncf-2.4.0/nncf/tensorflow/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1046 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/utils/hook_handle.py
--rw-r--r--   0 runner    (1001) docker     (123)      972 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/utils/node.py
--rw-r--r--   0 runner    (1001) docker     (123)      950 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/utils/scopes_handle.py
--rw-r--r--   0 runner    (1001) docker     (123)     2837 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/tensorflow/utils/state.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.455971 nncf-2.4.0/nncf/torch/
--rw-r--r--   0 runner    (1001) docker     (123)     2713 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/accuracy_aware_training/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/accuracy_aware_training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7811 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/accuracy_aware_training/runner.py
--rw-r--r--   0 runner    (1001) docker     (123)      938 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/accuracy_aware_training/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3259 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/algo_selector.py
--rw-r--r--   0 runner    (1001) docker     (123)     1275 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/batchnorm_adaptation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/binarization/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/binarization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9576 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/binarization/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     7600 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/binarization/binarize_functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     3812 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/binarization/extensions.py
--rw-r--r--   0 runner    (1001) docker     (123)     4669 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/binarization/layers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3586 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/binarization/reference.py
--rw-r--r--   0 runner    (1001) docker     (123)    23743 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/checkpoint_loading.py
--rw-r--r--   0 runner    (1001) docker     (123)     5667 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/composite_compression.py
--rw-r--r--   0 runner    (1001) docker     (123)     9765 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/compression_method_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     3165 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/debug.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/dynamic_graph/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16002 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/context.py
--rw-r--r--   0 runner    (1001) docker     (123)    30658 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     6516 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/graph_tracer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6216 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/io_handling.py
--rw-r--r--   0 runner    (1001) docker     (123)     1839 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/op_input_processing.py
--rw-r--r--   0 runner    (1001) docker     (123)     1715 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/operation_address.py
--rw-r--r--   0 runner    (1001) docker     (123)    11509 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/patch_pytorch.py
--rw-r--r--   0 runner    (1001) docker     (123)     3816 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/scope.py
--rw-r--r--   0 runner    (1001) docker     (123)     3798 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/trace_functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     5098 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/trace_tensor.py
--rw-r--r--   0 runner    (1001) docker     (123)     9938 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/transform_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)    12186 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/dynamic_graph/wrappers.py
--rw-r--r--   0 runner    (1001) docker     (123)     7601 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/
--rw-r--r--   0 runner    (1001) docker     (123)     2786 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/include/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/include/binarization/
--rw-r--r--   0 runner    (1001) docker     (123)      488 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/include/binarization/functions_cuda_impl.h
--rw-r--r--   0 runner    (1001) docker     (123)      242 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/include/common_cpu_funcs.h
--rw-r--r--   0 runner    (1001) docker     (123)     3822 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/include/common_cuda_defs.cuh
--rw-r--r--   0 runner    (1001) docker     (123)     3891 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/include/common_cuda_funcs.cuh
--rw-r--r--   0 runner    (1001) docker     (123)      311 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/include/common_defs.h
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/include/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      507 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/include/quantization/functions_cuda_impl.h
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.415971 nncf-2.4.0/nncf/torch/extensions/src/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.415971 nncf-2.4.0/nncf/torch/extensions/src/binarization/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/src/binarization/cpu/
--rw-r--r--   0 runner    (1001) docker     (123)     3785 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/src/binarization/cpu/functions_cpu.cpp
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/src/binarization/cuda/
--rw-r--r--   0 runner    (1001) docker     (123)     1469 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (123)    13042 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda_impl.cu
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.415971 nncf-2.4.0/nncf/torch/extensions/src/common/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/src/common/cpu/
--rw-r--r--   0 runner    (1001) docker     (123)      870 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/src/common/cpu/tensor_funcs.cpp
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.415971 nncf-2.4.0/nncf/torch/extensions/src/quantization/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/src/quantization/cpu/
--rw-r--r--   0 runner    (1001) docker     (123)     3723 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/src/quantization/cpu/functions_cpu.cpp
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.459971 nncf-2.4.0/nncf/torch/extensions/src/quantization/cuda/
--rw-r--r--   0 runner    (1001) docker     (123)     1072 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (123)    23467 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu
--rw-r--r--   0 runner    (1001) docker     (123)     1463 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/functions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.463971 nncf-2.4.0/nncf/torch/graph/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3401 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     7219 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/graph_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    29152 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/operator_metatypes.py
--rw-r--r--   0 runner    (1001) docker     (123)     3428 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/pattern_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     6410 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/patterns.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.463971 nncf-2.4.0/nncf/torch/graph/transformations/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/transformations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3743 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/transformations/commands.py
--rw-r--r--   0 runner    (1001) docker     (123)      137 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/graph/transformations/layout.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.463971 nncf-2.4.0/nncf/torch/hardware/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/hardware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      959 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/hardware/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2843 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/hardware/fused_patterns.py
--rw-r--r--   0 runner    (1001) docker     (123)    12062 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/initialization.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.463971 nncf-2.4.0/nncf/torch/knowledge_distillation/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/knowledge_distillation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3952 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/knowledge_distillation/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/knowledge_distillation/knowledge_distillation_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     6447 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/knowledge_distillation/knowledge_distillation_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     3676 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/layer_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    34981 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/layers.py
--rw-r--r--   0 runner    (1001) docker     (123)    15456 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/model_creation.py
--rw-r--r--   0 runner    (1001) docker     (123)     5234 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/module_operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     7385 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/nested_objects_traversal.py
--rw-r--r--   0 runner    (1001) docker     (123)    36828 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/nncf_network.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.463971 nncf-2.4.0/nncf/torch/pruning/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13480 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/base_algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     1144 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/export_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.463971 nncf-2.4.0/nncf/torch/pruning/filter_pruning/
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/filter_pruning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    33867 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/filter_pruning/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     2107 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/filter_pruning/functions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.463971 nncf-2.4.0/nncf/torch/pruning/filter_pruning/global_ranking/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/filter_pruning/global_ranking/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14122 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/filter_pruning/global_ranking/evolutionary_optimization.py
--rw-r--r--   0 runner    (1001) docker     (123)     5237 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/filter_pruning/global_ranking/legr.py
--rw-r--r--   0 runner    (1001) docker     (123)     3672 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/filter_pruning/layers.py
--rw-r--r--   0 runner    (1001) docker     (123)    20380 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/operations.py
--rw-r--r--   0 runner    (1001) docker     (123)     1250 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/structs.py
--rw-r--r--   0 runner    (1001) docker     (123)     2157 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/tensor_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4873 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/pruning/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.467970 nncf-2.4.0/nncf/torch/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      669 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3937 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/adjust_padding.py
--rw-r--r--   0 runner    (1001) docker     (123)   100219 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     3509 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/default_quantization.py
--rw-r--r--   0 runner    (1001) docker     (123)     3813 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/extensions.py
--rw-r--r--   0 runner    (1001) docker     (123)     6770 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/hessian_trace.py
--rw-r--r--   0 runner    (1001) docker     (123)      764 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/init_precision.py
--rw-r--r--   0 runner    (1001) docker     (123)    16218 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/init_range.py
--rw-r--r--   0 runner    (1001) docker     (123)    37915 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/layers.py
--rw-r--r--   0 runner    (1001) docker     (123)    17726 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     2377 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_constraints.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.467970 nncf-2.4.0/nncf/torch/quantization/precision_init/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5604 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/adjacent_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)    22372 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/autoq_init.py
--rw-r--r--   0 runner    (1001) docker     (123)     7208 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/base_init.py
--rw-r--r--   0 runner    (1001) docker     (123)     9089 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/bitwidth_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     3108 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/compression_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)    10488 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/hawq_debug.py
--rw-r--r--   0 runner    (1001) docker     (123)    44372 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/hawq_init.py
--rw-r--r--   0 runner    (1001) docker     (123)     3498 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/manual_init.py
--rw-r--r--   0 runner    (1001) docker     (123)     2243 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/perturbations.py
--rw-r--r--   0 runner    (1001) docker     (123)     3287 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/precision_init/traces_order.py
--rw-r--r--   0 runner    (1001) docker     (123)     8008 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/quantize.py
--rw-r--r--   0 runner    (1001) docker     (123)     9929 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/quantize_functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     3445 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/reference.py
--rw-r--r--   0 runner    (1001) docker     (123)     3724 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/schedulers.py
--rw-r--r--   0 runner    (1001) docker     (123)     4195 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)     1403 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/structs.py
--rw-r--r--   0 runner    (1001) docker     (123)     1809 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/quantization/translator.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.467970 nncf-2.4.0/nncf/torch/sparsity/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5117 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/base_algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     3593 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/collector.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.467970 nncf-2.4.0/nncf/torch/sparsity/const/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/const/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2345 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/const/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)      732 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     1676 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.467970 nncf-2.4.0/nncf/torch/sparsity/magnitude/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/magnitude/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8848 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/magnitude/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)      961 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/magnitude/functions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.467970 nncf-2.4.0/nncf/torch/sparsity/rb/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/rb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7058 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/rb/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     1026 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/rb/functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     2391 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/rb/layers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3797 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/sparsity/rb/loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7668 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/structures.py
--rw-r--r--   0 runner    (1001) docker     (123)     1074 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.471970 nncf-2.4.0/nncf/torch/tensor_statistics/
--rw-r--r--   0 runner    (1001) docker     (123)      583 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/tensor_statistics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4891 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/tensor_statistics/algo.py
--rw-r--r--   0 runner    (1001) docker     (123)     8732 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/tensor_statistics/collectors.py
--rw-r--r--   0 runner    (1001) docker     (123)     2231 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/tensor_statistics/reduction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2785 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/tensor_statistics/statistics.py
--rw-r--r--   0 runner    (1001) docker     (123)    15231 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/torch/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      112 2023-02-01 11:08:42.000000 nncf-2.4.0/nncf/version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-01 11:09:01.419971 nncf-2.4.0/nncf.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    27584 2023-02-01 11:09:01.000000 nncf-2.4.0/nncf.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    20033 2023-02-01 11:09:01.000000 nncf-2.4.0/nncf.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-02-01 11:09:01.000000 nncf-2.4.0/nncf.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      643 2023-02-01 11:09:01.000000 nncf-2.4.0/nncf.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        5 2023-02-01 11:09:01.000000 nncf-2.4.0/nncf.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-02-01 11:09:01.471970 nncf-2.4.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     7435 2023-02-01 11:08:42.000000 nncf-2.4.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.222075 nncf-2.5.0/
+-rw-r--r--   0 runner    (1001) docker     (123)    11357 2023-06-06 16:40:42.000000 nncf-2.5.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)      122 2023-06-06 16:40:42.000000 nncf-2.5.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)    27955 2023-06-06 16:40:59.222075 nncf-2.5.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    27062 2023-06-06 16:40:42.000000 nncf-2.5.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.114074 nncf-2.5.0/licensing/
+-rw-r--r--   0 runner    (1001) docker     (123)    84660 2023-06-06 16:40:42.000000 nncf-2.5.0/licensing/third-party-programs.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.118074 nncf-2.5.0/nncf/
+-rw-r--r--   0 runner    (1001) docker     (123)     2300 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.118074 nncf-2.5.0/nncf/api/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15299 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/api/compression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1005 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/api/statistics.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.122074 nncf-2.5.0/nncf/common/
+-rw-r--r--   0 runner    (1001) docker     (123)      643 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.122074 nncf-2.5.0/nncf/common/accuracy_aware_training/
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/accuracy_aware_training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20689 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/accuracy_aware_training/runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5815 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/accuracy_aware_training/runner_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1575 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/accuracy_aware_training/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27561 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/accuracy_aware_training/training_loop.py
+-rw-r--r--   0 runner    (1001) docker     (123)      961 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15265 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/composite_compression.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12680 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/compression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2771 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/deprecation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1022 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2397 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4554 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.122074 nncf-2.5.0/nncf/common/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      619 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      768 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/definitions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30501 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6684 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/graph_matching.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9471 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/layer_attributes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1362 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5407 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/operator_metatypes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.122074 nncf-2.5.0/nncf/common/graph/patterns/
+-rw-r--r--   0 runner    (1001) docker     (123)      913 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/patterns/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6964 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/patterns/manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18225 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/patterns/patterns.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.122074 nncf-2.5.0/nncf/common/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2319 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/transformations/command_creation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8029 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/transformations/commands.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1938 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/transformations/layout.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4435 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/graph/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.126074 nncf-2.5.0/nncf/common/hardware/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11495 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/hardware/config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.126074 nncf-2.5.0/nncf/common/hardware/configs/
+-rw-r--r--   0 runner    (1001) docker     (123)     7192 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/hardware/configs/cpu.json
+-rw-r--r--   0 runner    (1001) docker     (123)     6311 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/hardware/configs/gpu.json
+-rw-r--r--   0 runner    (1001) docker     (123)     3113 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/hardware/configs/template.json
+-rw-r--r--   0 runner    (1001) docker     (123)     8499 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/hardware/configs/vpu.json
+-rw-r--r--   0 runner    (1001) docker     (123)     1790 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/hardware/opset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.126074 nncf-2.5.0/nncf/common/initialization/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/initialization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4270 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/initialization/batchnorm_adaptation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1358 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/initialization/dataloader.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21510 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/insertion_point_graph.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.126074 nncf-2.5.0/nncf/common/logging/
+-rw-r--r--   0 runner    (1001) docker     (123)      632 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/logging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2775 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/logging/logger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3412 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/logging/progress_bar.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.130074 nncf-2.5.0/nncf/common/pruning/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5827 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/clusterization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7905 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/mask_propagation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10317 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/model_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19598 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/node_selector.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15374 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/operations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8174 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/schedulers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10227 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/shape_pruning_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8118 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)      847 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/structs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6394 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/symbolic_mask.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3008 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/tensor_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16537 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7919 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/pruning/weights_flops_calculator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.130074 nncf-2.5.0/nncf/common/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5039 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5989 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/config_assignment.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.130074 nncf-2.5.0/nncf/common/quantization/initialization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/initialization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7592 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/initialization/range.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.130074 nncf-2.5.0/nncf/common/quantization/quantizer_propagation/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizer_propagation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    75557 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizer_propagation/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7527 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizer_propagation/grouping.py
+-rw-r--r--   0 runner    (1001) docker     (123)    88036 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizer_propagation/solver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4533 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizer_propagation/structs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1504 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizer_propagation/visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5958 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizer_removal.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23514 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizer_setup.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2514 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7672 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13311 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/quantization/structs.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10292 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/schedulers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5525 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/scopes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.130074 nncf-2.5.0/nncf/common/sparsity/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/sparsity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4257 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/sparsity/collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1188 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/sparsity/controller.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13327 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/sparsity/schedulers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7315 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/sparsity/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4683 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/stateful_classes_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5060 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1477 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.134074 nncf-2.5.0/nncf/common/tensor_statistics/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/tensor_statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5382 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/tensor_statistics/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20526 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/tensor_statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2836 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/tensor_statistics/reduction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5951 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/tensor_statistics/statistic_point.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3431 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/tensor_statistics/statistics.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.134074 nncf-2.5.0/nncf/common/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1213 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/api_marker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3225 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1077 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/debug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2013 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)      975 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/dot_file_rw.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2301 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/helpers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.134074 nncf-2.5.0/nncf/common/utils/logger/
+-rw-r--r--   0 runner    (1001) docker     (123)     1233 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/logger/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1280 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/os.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8839 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/patcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1854 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3281 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/tensorboard.py
+-rw-r--r--   0 runner    (1001) docker     (123)      993 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/common/utils/timer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.138074 nncf-2.5.0/nncf/config/
+-rw-r--r--   0 runner    (1001) docker     (123)      690 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7009 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2080 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/definitions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10030 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/extractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10221 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schema.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.138074 nncf-2.5.0/nncf/config/schemata/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6986 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/accuracy_aware.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.138074 nncf-2.5.0/nncf/config/schemata/algo/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/algo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2258 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/algo/binarization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1258 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/algo/const_sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9085 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/algo/filter_pruning.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2434 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/algo/knowledge_distillation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2853 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/algo/magnitude_sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26324 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/algo/quantization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2248 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/algo/rb_sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1820 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/basic.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.138074 nncf-2.5.0/nncf/config/schemata/common/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1118 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/common/compression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1743 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/common/initialization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5491 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/common/sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2141 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/common/targeting.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2779 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/defaults.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17651 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/schemata/experimental_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3133 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/structures.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1046 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/telemetry_extractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1246 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/config/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.138074 nncf-2.5.0/nncf/data/
+-rw-r--r--   0 runner    (1001) docker     (123)      619 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5440 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/data/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1206 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/definitions.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.138074 nncf-2.5.0/nncf/experimental/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.142074 nncf-2.5.0/nncf/experimental/common/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.142074 nncf-2.5.0/nncf/experimental/common/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5875 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/graph/netron.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.142074 nncf-2.5.0/nncf/experimental/common/pruning/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3992 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/pruning/block_hierarchy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6431 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/pruning/nodes_grouping.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35613 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/pruning/operations.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10453 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/pruning/propagation_data.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.142074 nncf-2.5.0/nncf/experimental/common/tensor_statistics/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/tensor_statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22313 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/common/tensor_statistics/collectors.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.142074 nncf-2.5.0/nncf/experimental/openvino/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/openvino/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.142074 nncf-2.5.0/nncf/experimental/openvino/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/openvino/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6676 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/openvino/quantization/quantize_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.142074 nncf-2.5.0/nncf/experimental/tensorflow/
+-rw-r--r--   0 runner    (1001) docker     (123)      684 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5805 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/context.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.146074 nncf-2.5.0/nncf/experimental/tensorflow/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9324 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/graph/argprovider.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17664 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/graph/converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2178 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/graph/node_attributes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.146074 nncf-2.5.0/nncf/experimental/tensorflow/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3330 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/graph/transformations/commands.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2173 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/graph/transformations/layout.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5585 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/nncf_network.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10271 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/patch_tf.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.146074 nncf-2.5.0/nncf/experimental/tensorflow/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16869 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/quantization/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4815 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/quantization/init_range.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5442 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/quantization/quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1791 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/tensorflow/scope.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.146074 nncf-2.5.0/nncf/experimental/torch/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.146074 nncf-2.5.0/nncf/experimental/torch/nas/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.146074 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/
+-rw-r--r--   0 runner    (1001) docker     (123)      977 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.150074 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9588 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/base_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21775 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_depth.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23692 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (123)    54416 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_width.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8001 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4270 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_controller.py
+-rw-r--r--   0 runner    (1001) docker     (123)      786 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_dim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2842 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/filter_reorder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14125 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/multi_elasticity_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4296 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/visualization.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.150074 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/search/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/search/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7980 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3528 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22489 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/search/search.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.150074 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2829 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/base_training.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8170 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5574 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7102 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10323 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13296 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4510 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13371 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.154074 nncf-2.5.0/nncf/experimental/torch/pruning/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7104 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/pruning/operations.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.154074 nncf-2.5.0/nncf/experimental/torch/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5203 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/quantization/quantize_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.154074 nncf-2.5.0/nncf/experimental/torch/search_building_blocks/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/search_building_blocks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29982 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/search_building_blocks/search_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16469 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/search_building_blocks/search_graph.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.154074 nncf-2.5.0/nncf/experimental/torch/sparsity/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/sparsity/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.154074 nncf-2.5.0/nncf/experimental/torch/sparsity/movement/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/sparsity/movement/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10952 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/sparsity/movement/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1434 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/sparsity/movement/functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14845 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/sparsity/movement/layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1770 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/sparsity/movement/loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/sparsity/movement/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22414 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/experimental/torch/sparsity/movement/structured_mask_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.154074 nncf-2.5.0/nncf/onnx/
+-rw-r--r--   0 runner    (1001) docker     (123)      633 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1759 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.158074 nncf-2.5.0/nncf/onnx/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.158074 nncf-2.5.0/nncf/onnx/graph/metatypes/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/metatypes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15385 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/metatypes/onnx_metatypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20275 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10763 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/nncf_graph_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4273 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/node_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16883 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/onnx_graph.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.158074 nncf-2.5.0/nncf/onnx/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/transformations/command_creation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5667 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/graph/transformations/commands.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.158074 nncf-2.5.0/nncf/onnx/hardware/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      950 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/hardware/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14928 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/hardware/fused_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3418 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/hardware/pattern_operations.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.158074 nncf-2.5.0/nncf/onnx/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2262 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/quantization/default_quantization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2692 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/quantization/ignored_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3010 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4683 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/quantization/quantizer_parameters.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.162074 nncf-2.5.0/nncf/onnx/statistics/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4065 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/statistics/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7212 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1397 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/statistics/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)      901 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/onnx/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.162074 nncf-2.5.0/nncf/openvino/
+-rw-r--r--   0 runner    (1001) docker     (123)      637 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3174 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.162074 nncf-2.5.0/nncf/openvino/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.162074 nncf-2.5.0/nncf/openvino/graph/metatypes/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/metatypes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2768 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/metatypes/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17398 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/metatypes/openvino_metatypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20168 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10558 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/nncf_graph_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13403 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/node_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.162074 nncf-2.5.0/nncf/openvino/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2432 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/transformations/command_creation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5946 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/graph/transformations/commands.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.162074 nncf-2.5.0/nncf/openvino/hardware/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      956 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/hardware/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37601 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/hardware/fused_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2763 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/hardware/pattern_operations.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.166075 nncf-2.5.0/nncf/openvino/pot/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/pot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7435 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/pot/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.166075 nncf-2.5.0/nncf/openvino/pot/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/pot/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4051 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/pot/quantization/accuracy_aware.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19824 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/pot/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)      868 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/pot/telemetry_extractors.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.166075 nncf-2.5.0/nncf/openvino/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1516 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/quantization/backend_parameters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2894 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/quantization/default_quantization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2564 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/quantization/ignored_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12225 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/quantization/quantize_model.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.166075 nncf-2.5.0/nncf/openvino/statistics/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6231 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/statistics/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11970 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1391 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/statistics/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1030 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/openvino/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1841 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/parameters.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.170075 nncf-2.5.0/nncf/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14005 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/advanced_parameters.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.170075 nncf-2.5.0/nncf/quantization/algorithms/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.170075 nncf-2.5.0/nncf/quantization/algorithms/accuracy_control/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/accuracy_control/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15788 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/accuracy_control/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4447 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/accuracy_control/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3457 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/accuracy_control/openvino_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1521 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/accuracy_control/rank_functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13862 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/accuracy_control/ranker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2705 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/algorithm.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.170075 nncf-2.5.0/nncf/quantization/algorithms/bias_correction/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/bias_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27638 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/bias_correction/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7667 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/bias_correction/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5782 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/bias_correction/onnx_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6199 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/bias_correction/openvino_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.170075 nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15627 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6562 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4743 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4981 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/openvino_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.174075 nncf-2.5.0/nncf/quantization/algorithms/min_max/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/min_max/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35616 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/min_max/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8339 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/min_max/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11832 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/min_max/onnx_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12520 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/min_max/openvino_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14369 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/min_max/torch_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.174075 nncf-2.5.0/nncf/quantization/algorithms/post_training/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/post_training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8876 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/algorithms/post_training/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11975 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/fake_quantize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4889 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/passes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9145 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6740 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/range_estimator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1138 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/quantization/telemetry_extractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6188 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/scopes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.174075 nncf-2.5.0/nncf/telemetry/
+-rw-r--r--   0 runner    (1001) docker     (123)      737 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/telemetry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3670 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/telemetry/decorator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1183 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/telemetry/events.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2038 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/telemetry/extractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4291 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/telemetry/wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.174075 nncf-2.5.0/nncf/tensorflow/
+-rw-r--r--   0 runner    (1001) docker     (123)     2389 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.178075 nncf-2.5.0/nncf/tensorflow/accuracy_aware_training/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/accuracy_aware_training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4947 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/accuracy_aware_training/keras_model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5292 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/accuracy_aware_training/runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2872 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/algorithm_selector.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.178075 nncf-2.5.0/nncf/tensorflow/api/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2560 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/api/composite_compression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2847 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/api/compression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2825 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/batchnorm_adaptation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.178075 nncf-2.5.0/nncf/tensorflow/callbacks/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/callbacks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2836 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/callbacks/checkpoint_callback.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3179 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/callbacks/statistics_callback.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      801 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/functions.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.178075 nncf-2.5.0/nncf/tensorflow/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38644 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/converter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.182075 nncf-2.5.0/nncf/tensorflow/graph/metatypes/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/metatypes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7058 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/metatypes/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21851 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/metatypes/keras_layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2699 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/metatypes/matcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11930 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/metatypes/tf_ops.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24149 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/model_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2910 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/pattern_operations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5380 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/patterns.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.182075 nncf-2.5.0/nncf/tensorflow/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17119 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/transformations/commands.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5765 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/transformations/layout.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9051 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/graph/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.182075 nncf-2.5.0/nncf/tensorflow/hardware/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      946 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/hardware/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3406 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/hardware/fused_patterns.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.182075 nncf-2.5.0/nncf/tensorflow/helpers/
+-rw-r--r--   0 runner    (1001) docker     (123)      656 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/helpers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2285 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/helpers/callback_creation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5863 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/helpers/model_creation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2455 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/helpers/model_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1081 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/helpers/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2519 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/initialization.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.182075 nncf-2.5.0/nncf/tensorflow/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      834 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/layers/custom_objects.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2675 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/layers/data_layout.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3519 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/layers/operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10778 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/layers/wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)      947 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.186075 nncf-2.5.0/nncf/tensorflow/pruning/
+-rw-r--r--   0 runner    (1001) docker     (123)      644 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16725 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/base_algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1721 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/callbacks.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.186075 nncf-2.5.0/nncf/tensorflow/pruning/filter_pruning/
+-rw-r--r--   0 runner    (1001) docker     (123)      645 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/filter_pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26355 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/filter_pruning/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2475 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/filter_pruning/functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5677 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/operations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2248 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/tensor_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4846 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/pruning/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.186075 nncf-2.5.0/nncf/tensorflow/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      898 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35119 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3212 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3327 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/default_quantization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1830 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11386 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/init_range.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4196 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7270 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20370 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2452 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/quantization/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.190075 nncf-2.5.0/nncf/tensorflow/sparsity/
+-rw-r--r--   0 runner    (1001) docker     (123)      645 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2448 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/base_algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2099 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/callbacks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3562 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/collector.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.190075 nncf-2.5.0/nncf/tensorflow/sparsity/magnitude/
+-rw-r--r--   0 runner    (1001) docker     (123)      644 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/magnitude/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12662 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/magnitude/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/magnitude/functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2946 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/magnitude/operation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.190075 nncf-2.5.0/nncf/tensorflow/sparsity/rb/
+-rw-r--r--   0 runner    (1001) docker     (123)      660 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/rb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8324 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/rb/algorithm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1075 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/rb/functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2813 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/rb/loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5305 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/rb/operation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2186 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/sparsity/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1086 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.190075 nncf-2.5.0/nncf/tensorflow/tensor_statistics/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/tensor_statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7976 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/tensor_statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2433 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/tensor_statistics/reduction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2811 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/tensor_statistics/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/tf_internals.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.194075 nncf-2.5.0/nncf/tensorflow/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1049 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/utils/hook_handle.py
+-rw-r--r--   0 runner    (1001) docker     (123)      982 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/utils/node.py
+-rw-r--r--   0 runner    (1001) docker     (123)      944 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/utils/scopes_handle.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2824 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/tensorflow/utils/state.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.198075 nncf-2.5.0/nncf/torch/
+-rw-r--r--   0 runner    (1001) docker     (123)     3048 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.198075 nncf-2.5.0/nncf/torch/accuracy_aware_training/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/accuracy_aware_training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7440 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/accuracy_aware_training/runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)      940 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/accuracy_aware_training/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/algo_selector.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.198075 nncf-2.5.0/nncf/torch/automl/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/automl/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.198075 nncf-2.5.0/nncf/torch/automl/agent/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/automl/agent/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.198075 nncf-2.5.0/nncf/torch/automl/agent/ddpg/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/automl/agent/ddpg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10037 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/automl/agent/ddpg/ddpg.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10701 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/automl/agent/ddpg/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.198075 nncf-2.5.0/nncf/torch/automl/environment/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/automl/environment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33796 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/automl/environment/quantization_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1277 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/batchnorm_adaptation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.202075 nncf-2.5.0/nncf/torch/binarization/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/binarization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9478 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/binarization/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7207 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/binarization/binarize_functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3872 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/binarization/extensions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4717 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/binarization/layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3584 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/binarization/reference.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24309 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/checkpoint_loading.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5669 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/composite_compression.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9867 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/compression_method_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3302 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/debug.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.202075 nncf-2.5.0/nncf/torch/dynamic_graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18198 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/context.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29884 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6306 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/graph_tracer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6298 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/io_handling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8094 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/layer_attributes_handlers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/op_input_processing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1645 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/operation_address.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14927 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/patch_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3884 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/scope.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1034 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/scope_access.py
+-rw-r--r--   0 runner    (1001) docker     (123)      868 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/structs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3599 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/trace_functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7278 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/trace_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9472 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/dynamic_graph/wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1284 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7789 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.202075 nncf-2.5.0/nncf/torch/extensions/
+-rw-r--r--   0 runner    (1001) docker     (123)     2978 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.202075 nncf-2.5.0/nncf/torch/extensions/include/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/extensions/include/binarization/
+-rw-r--r--   0 runner    (1001) docker     (123)      488 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/include/binarization/functions_cuda_impl.h
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/include/common_cpu_funcs.h
+-rw-r--r--   0 runner    (1001) docker     (123)     3822 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/include/common_cuda_defs.cuh
+-rw-r--r--   0 runner    (1001) docker     (123)     3891 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/include/common_cuda_funcs.cuh
+-rw-r--r--   0 runner    (1001) docker     (123)      311 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/include/common_defs.h
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/extensions/include/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      507 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/include/quantization/functions_cuda_impl.h
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.114074 nncf-2.5.0/nncf/torch/extensions/src/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.114074 nncf-2.5.0/nncf/torch/extensions/src/binarization/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/extensions/src/binarization/cpu/
+-rw-r--r--   0 runner    (1001) docker     (123)     3785 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/src/binarization/cpu/functions_cpu.cpp
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/extensions/src/binarization/cuda/
+-rw-r--r--   0 runner    (1001) docker     (123)     1469 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)    13042 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda_impl.cu
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.114074 nncf-2.5.0/nncf/torch/extensions/src/common/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/extensions/src/common/cpu/
+-rw-r--r--   0 runner    (1001) docker     (123)      870 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/src/common/cpu/tensor_funcs.cpp
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.114074 nncf-2.5.0/nncf/torch/extensions/src/quantization/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/extensions/src/quantization/cpu/
+-rw-r--r--   0 runner    (1001) docker     (123)     3723 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/src/quantization/cpu/functions_cpu.cpp
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/extensions/src/quantization/cuda/
+-rw-r--r--   0 runner    (1001) docker     (123)     1072 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (123)    23467 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu
+-rw-r--r--   0 runner    (1001) docker     (123)     1465 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/functions.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3329 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/graph/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4662 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/graph/graph_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29666 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/graph/operator_metatypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2498 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/graph/pattern_operations.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/graph/transformations/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/graph/transformations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3704 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/graph/transformations/commands.py
+-rw-r--r--   0 runner    (1001) docker     (123)      137 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/graph/transformations/layout.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.206075 nncf-2.5.0/nncf/torch/hardware/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/hardware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      943 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/hardware/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12911 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/hardware/fused_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11669 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/initialization.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.210075 nncf-2.5.0/nncf/torch/knowledge_distillation/
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/knowledge_distillation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4128 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/knowledge_distillation/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3299 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/knowledge_distillation/knowledge_distillation_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7639 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/knowledge_distillation/knowledge_distillation_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3676 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/layer_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35591 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14824 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/model_creation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5209 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/module_operations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7087 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/nested_objects_traversal.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13464 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/nncf_module_replacement.py
+-rw-r--r--   0 runner    (1001) docker     (123)    51532 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/nncf_network.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.210075 nncf-2.5.0/nncf/torch/pruning/
+-rw-r--r--   0 runner    (1001) docker     (123)      644 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12362 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/base_algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1146 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/export_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.210075 nncf-2.5.0/nncf/torch/pruning/filter_pruning/
+-rw-r--r--   0 runner    (1001) docker     (123)      645 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/filter_pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33606 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/filter_pruning/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2110 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/filter_pruning/functions.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.210075 nncf-2.5.0/nncf/torch/pruning/filter_pruning/global_ranking/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/filter_pruning/global_ranking/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14167 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/filter_pruning/global_ranking/evolutionary_optimization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5353 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/filter_pruning/global_ranking/legr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3610 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/filter_pruning/layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31559 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/operations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1230 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/structs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2159 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/tensor_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4833 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/pruning/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.218075 nncf-2.5.0/nncf/torch/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      736 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3994 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/adjust_padding.py
+-rw-r--r--   0 runner    (1001) docker     (123)    94367 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3805 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/default_quantization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3872 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/extensions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6635 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/hessian_trace.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2600 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/ignored_patterns.py
+-rw-r--r--   0 runner    (1001) docker     (123)      764 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/init_precision.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15093 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/init_range.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41044 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17817 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2379 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_constraints.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.218075 nncf-2.5.0/nncf/torch/quantization/precision_init/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5579 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/adjacent_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22552 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/autoq_init.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7066 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/base_init.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8904 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/bitwidth_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3116 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/compression_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10588 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/hawq_debug.py
+-rw-r--r--   0 runner    (1001) docker     (123)    42983 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/hawq_init.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3438 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/manual_init.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2245 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/perturbations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3305 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/precision_init/traces_order.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10271 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/quantize_functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10243 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/quantize_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3707 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/reference.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3726 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/schedulers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4169 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5938 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/strip.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1376 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/structs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1746 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/quantization/translator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.218075 nncf-2.5.0/nncf/torch/sparsity/
+-rw-r--r--   0 runner    (1001) docker     (123)      645 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6123 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/base_algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3642 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/collector.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.218075 nncf-2.5.0/nncf/torch/sparsity/const/
+-rw-r--r--   0 runner    (1001) docker     (123)      668 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/const/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2520 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/const/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)      734 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1678 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.222075 nncf-2.5.0/nncf/torch/sparsity/magnitude/
+-rw-r--r--   0 runner    (1001) docker     (123)      653 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/magnitude/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8734 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/magnitude/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)      948 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/magnitude/functions.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.222075 nncf-2.5.0/nncf/torch/sparsity/rb/
+-rw-r--r--   0 runner    (1001) docker     (123)      660 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/rb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7335 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/rb/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1060 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/rb/functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2473 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/rb/layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3829 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/sparsity/rb/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.222075 nncf-2.5.0/nncf/torch/statistics/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3019 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/statistics/aggregator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7688 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/structures.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1076 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.222075 nncf-2.5.0/nncf/torch/tensor_statistics/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/tensor_statistics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4712 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/tensor_statistics/algo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9869 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/tensor_statistics/collectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2233 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/tensor_statistics/reduction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2770 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/tensor_statistics/statistics.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15309 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/torch/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      113 2023-06-06 16:40:42.000000 nncf-2.5.0/nncf/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-06 16:40:59.118074 nncf-2.5.0/nncf.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    27955 2023-06-06 16:40:59.000000 nncf-2.5.0/nncf.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    22973 2023-06-06 16:40:59.000000 nncf-2.5.0/nncf.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-06 16:40:59.000000 nncf-2.5.0/nncf.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      933 2023-06-06 16:40:59.000000 nncf-2.5.0/nncf.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        5 2023-06-06 16:40:59.000000 nncf-2.5.0/nncf.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-06-06 16:40:59.222075 nncf-2.5.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     7349 2023-06-06 16:40:42.000000 nncf-2.5.0/setup.py
```

### Comparing `nncf-2.4.0/LICENSE` & `nncf-2.5.0/LICENSE`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/PKG-INFO` & `nncf-2.5.0/README.md`

 * *Files 17% similar despite different names*

```diff
@@ -1,186 +1,109 @@
-Metadata-Version: 2.1
-Name: nncf
-Version: 2.4.0
-Summary: Neural Networks Compression Framework
-Home-page: https://github.com/openvinotoolkit/nncf
-Author: Intel
-Author-email: alexander.kozlov@intel.com
-License: Apache-2.0
-Keywords: compression,quantization,sparsity,mixed-precision-training,quantization-aware-training,hawq,classification,pruning,object-detection,semantic-segmentation,nas,nlp,bert,transformers,mmdetection
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Operating System :: OS Independent
-Description-Content-Type: text/markdown
-Provides-Extra: dev
-Provides-Extra: tests
-Provides-Extra: docs
-Provides-Extra: tf
-Provides-Extra: tensorflow
-Provides-Extra: tensorflow2
-Provides-Extra: torch
-Provides-Extra: pytorch
-Provides-Extra: onnx
-Provides-Extra: openvino
-Provides-Extra: all
-License-File: LICENSE
-
 <div align="center">
 
 # Neural Network Compression Framework (NNCF)
 
+[Key Features](#key-features) 
+[Installation](#Installation-guide) 
+[Documentation](#documentation) 
+[Usage](#usage) 
+[Tutorials and Samples](#Model-compression-tutorials-and-samples) 
+[Third-party integration](#Third-party-repository-integration) 
+[Model Zoo](#NNCF-Compressed-Model-Zoo)
+ 
 [![GitHub Release](https://img.shields.io/github/v/release/openvinotoolkit/nncf?color=green)](https://github.com/openvinotoolkit/nncf/releases)
 [![Website](https://img.shields.io/website?up_color=blue&up_message=docs&url=https%3A%2F%2Fdocs.openvino.ai%2Flatest%2Fopenvino_docs_model_optimization_guide.html)](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
 [![Apache License Version 2.0](https://img.shields.io/badge/license-Apache_2.0-green.svg)](LICENSE)
 [![PyPI Downloads](https://static.pepy.tech/badge/nncf)](https://pypi.org/project/nncf/)
  
 </div>
 
-_For the installation instructions, [click here](#installation)._
-
-NNCF provides a suite of advanced algorithms for Neural Networks inference optimization in [OpenVINO&trade;](https://docs.openvino.ai/latest/home.html) with minimal accuracy drop.
+Neural Network Compression Framework (NNCF) provides a suite of post-training and training-time algorithms for neural networks inference optimization in [OpenVINO&trade;](https://docs.openvino.ai) with minimal accuracy drop.
 
 NNCF is designed to work with models from [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), [ONNX](https://onnx.ai/) and [OpenVINO&trade;](https://docs.openvino.ai/latest/home.html).
 
-NNCF provides samples that demonstrate the usage of compression algorithms for three different use cases on public PyTorch and 
-TensorFlow models and datasets: Image Classification, Object Detection and Semantic Segmentation. 
+NNCF provides [samples](#Model-Compression-Samples) that demonstrate the usage of compression algorithms for different use cases and models. 
 [Compression results](#nncf-compressed-model-zoo) achievable with the NNCF-powered samples can be found in a table at 
 the end of this document.
 
 The framework is organized as a Python\* package that can be built and used in a standalone mode. The framework 
 architecture is unified to make it easy to add different compression algorithms for both PyTorch and TensorFlow deep 
 learning frameworks.
 
 ## Key Features
 ### Post-Training Compression Algorithms
 
-| Compression algorithm                                                       |PyTorch|TensorFlow|   ONNX   |       OpenVINO     |
+| Compression algorithm                                                       |OpenVINO|PyTorch|   TensorFlow   |     ONNX       |
 |:----------------------------------------------------------------------------| :---: | :---: |:--------:|:------------------:|
-| [Quantization](./docs/compression_algorithms/post_training/Quantization.md) | Supported | Supported |Supported| Preview |
-
-_Preview means that this is a work in progress and NNCF does not guarantee the full functional support._
+| [Post-Training Quantization](./docs/compression_algorithms/post_training/Quantization.md) | Supported | Supported |Supported| Supported |
 
-### Training-time Compression Algorithms
+### Training-Time Compression Algorithms
 
 |Compression algorithm|PyTorch|TensorFlow|
 | :--- | :---: | :---: |
-|[Quantization](./docs/compression_algorithms/Quantization.md) | Supported | Supported |
-|[Mixed-Precision Quantization](./docs/compression_algorithms/Quantization.md#mixed_precision_quantization) | Supported | Not supported | Not supported |
-|[Binarization](./docs/compression_algorithms/Binarization.md) | Supported | Not supported | Not supported |
-|[Sparsity](./docs/compression_algorithms/Sparsity.md) | Supported | Supported | Not supported |
-|[Filter pruning](./docs/compression_algorithms/Pruning.md) | Supported | Supported | Not supported |
-
-
-
+|[Quantization Aware Training](./docs/compression_algorithms/Quantization.md) | Supported | Supported |
+|[Mixed-Precision Quantization](./docs/compression_algorithms/Quantization.md#mixed_precision_quantization) | Supported | Not supported |
+|[Binarization](./docs/compression_algorithms/Binarization.md) | Supported | Not supported |
+|[Sparsity](./docs/compression_algorithms/Sparsity.md) | Supported | Supported |
+|[Filter pruning](./docs/compression_algorithms/Pruning.md) | Supported | Supported |
+|[Movement pruning](./nncf/experimental/torch/sparsity/movement/MovementSparsity.md) | Experimental | Not supported |
 
 - Automatic, configurable model graph transformation to obtain the compressed model.
   > **NOTE**: Limited support for TensorFlow models. The models created using Sequential or Keras Functional API are only supported.
 - Common interface for compression methods.
 - GPU-accelerated layers for faster compressed model fine-tuning.
 - Distributed training support.
-- Configuration file examples for each supported compression algorithm.
-- Git patches for prominent third-party repositories ([huggingface-transformers](https://github.com/huggingface/transformers)) demonstrating the process of integrating NNCF into custom training pipelines
-- Exporting PyTorch compressed models to ONNX\* checkpoints and TensorFlow compressed models to SavedModel or Frozen Graph format, ready to use with [OpenVINO&trade; toolkit](https://docs.openvino.ai/latest/home.html).
+- Git patch for prominent third-party repository ([huggingface-transformers](https://github.com/huggingface/transformers)) demonstrating the process of integrating NNCF into custom training pipelines
+- Seamless combination of pruning, sparsity and quantization algorithms. Please refer to [optimum-intel](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino) for examples of 
+joint (movement) pruning, quantization and distillation (JPQD), end-to-end from NNCF optimization to compressed OpenVINO IR.
+- Exporting PyTorch compressed models to ONNX\* checkpoints and TensorFlow compressed models to SavedModel or Frozen Graph format, ready to use with [OpenVINO&trade; toolkit](https://docs.openvino.ai).
 - Support for [Accuracy-Aware model training](./docs/Usage.md#accuracy-aware-model-training) pipelines via the [Adaptive Compression Level Training](./docs/accuracy_aware_model_training/AdaptiveCompressionLevelTraining.md) and [Early Exit Training](./docs/accuracy_aware_model_training/EarlyExitTraining.md).
 
-## Usage
-The NNCF is organized as a regular Python package that can be imported in your target training pipeline script.
-The basic workflow is loading a JSON configuration script containing NNCF-specific parameters determining the compression to be applied to your model, and then passing your model along with the configuration script to the `create_compressed_model` function.
-This function returns a model with additional modifications necessary to enable algorithm-specific compression during fine-tuning and handle to the object allowing you to control the compression during the training process:
+## Documentation
 
-### Usage example with PyTorch 
+This documentation covers detailed information about NNCF algorithms and functions needed for the contribution to NNCF.  
 
-```python
-import torch
-import nncf  # Important - must be imported before any other external package that depends on torch
+The latest user documentation for NNCF is available [here](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html).
 
-from nncf import NNCFConfig
-from nncf.torch import create_compressed_model, register_default_init_args
+NNCF API documentation can be found [here](https://openvinotoolkit.github.io/nncf/autoapi/nncf/).
 
-# Instantiate your uncompressed model
-from torchvision.models.resnet import resnet50
-model = resnet50()
-
-# Load a configuration file to specify compression
-nncf_config = NNCFConfig.from_json("resnet50_int8.json")
-
-# Provide data loaders for compression algorithm initialization, if necessary
-import torchvision.datasets as datasets
-representative_dataset = datasets.ImageFolder("/path")
-init_loader = torch.utils.data.DataLoader(representative_dataset)
-nncf_config = register_default_init_args(nncf_config, init_loader)
-
-# Apply the specified compression algorithms to the model
-compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+## Usage
 
-# Now use compressed_model as a usual torch.nn.Module 
-# to fine-tune compression parameters along with the model weights
+### Post-Training Quantization
 
-# ... the rest of the usual PyTorch-powered training pipeline
+The NNCF PTQ is the simplest way to apply 8-bit quantization. To run the algorithm you only need your model and a small (~300 samples) calibration dataset.
 
-# Export to ONNX or .pth when done fine-tuning
-compression_ctrl.export_model("compressed_model.onnx")
-torch.save(compressed_model.state_dict(), "compressed_model.pth")
-```
-
-**NOTE (PyTorch)**: Due to the way NNCF works within the PyTorch backend, `import nncf` must be done before any other import of `torch` in your package _or_ in third-party packages that your code utilizes, otherwise the compression may be applied incompletely.
+[OpenVINO](https://github.com/openvinotoolkit/openvino) is the preferred backend to run PTQ with, and PyTorch, TensorFlow and ONNX are also supported.
 
+<details open><summary><b>OpenVINO</b></summary>
 
-### Usage example with TensorFlow
 ```python
-import tensorflow as tf
-
-from nncf import NNCFConfig
-from nncf.tensorflow import create_compressed_model, register_default_init_args
+import nncf
+import openvino.runtime as ov
+import torch
+from torchvision import datasets
 
 # Instantiate your uncompressed model
-from tensorflow.keras.applications import ResNet50
-model = ResNet50()
-
-# Load a configuration file to specify compression
-nncf_config = NNCFConfig.from_json("resnet50_int8.json")
-
-# Provide dataset for compression algorithm initialization
-representative_dataset = tf.data.Dataset.list_files("/path/*.jpeg")
-nncf_config = register_default_init_args(nncf_config, representative_dataset, batch_size=1)
-
-# Apply the specified compression algorithms to the model
-compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
-
-# Now use compressed_model as a usual Keras model
-# to fine-tune compression parameters along with the model weights
+model = ov.Core().read_model("/model_path")
+# Provide validation part of the dataset to collect statistics needed for the compression algorithm
+val_dataset = datasets.ImageFolder("/path")
+dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
 
-# ... the rest of the usual TensorFlow-powered training pipeline
+# Step 1: Initialize transformation function
+def transform_fn(data_item):
+    images, _ = data_item
+    return images
 
-# Export to Frozen Graph, TensorFlow SavedModel or .h5  when done fine-tuning 
-compression_ctrl.export_model("compressed_model.pb", save_format='frozen_graph')
+# Step 2: Initialize NNCF Dataset
+calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
+# Step 3: Run the quantization pipeline
+quantized_model = nncf.quantize(model, calibration_dataset)
 ```
 
-For a more detailed description of NNCF usage in your training code, see [this tutorial](docs/Usage.md). 
-For in-depth examples of NNCF integration, browse the [sample scripts](#compression-aware-training-samples) code, or the [example patches](#third-party-repository-integration) to third-party repositories.
-For FAQ, visit this [link](./docs/FAQ.md).
-
-### Usage examples of Post-Training Quantization
-
-NNCF provides [samples](#post-training-quantization-samples), which demonstrate Post-Training Quantization usage for PyTorch, TensorFlow, ONNX, OpenVINO.
-
-To start the algorithm, provide the following entities:
-* Original model.
-* Validation part of the dataset.
-* [Data transformation function](./docs/compression_algorithms/post_training/Quantization.md#data-transformation-function) transforming data items from the original dataset to the model input data. 
-
-
-The basic workflow steps:
-1) Create the [data transformation function](./docs/compression_algorithms/post_training/Quantization.md#data-transformation-function).
-2) Create an instance of `nncf.Dataset` class by passing two parameters:
-* `data_source` - Iterable python object that contains data items for model calibration.
-* `transform_fn` - Data transformation function from the Step 1.
-3) Run the quantization pipeline.
-
-Below are the usage examples for every backend.
+</details>
 
 <details><summary><b>PyTorch</b></summary>
 
 ```python
 import nncf
 import torch
 from torchvision import datasets, models
@@ -211,15 +134,15 @@
 import nncf
 import tensorflow as tf
 import tensorflow_datasets as tfds
 
 # Instantiate your uncompressed model
 model = tf.keras.applications.MobileNetV2()
 # Provide validation part of the dataset to collect statistics needed for the compression algorithm
-val_dataset = tfds.load('/path', split='validation', 
+val_dataset = tfds.load("/path", split="validation", 
                         shuffle_files=False, as_supervised=True)
 
 # Step 1: Initialize transformation function
 def transform_fn(data_item):
     images, _ = data_item
     return images
 
@@ -236,15 +159,15 @@
 ```python
 import onnx
 import nncf
 import torch
 from torchvision import datasets
 
 # Instantiate your uncompressed model
-onnx_model = onnx.load_model('/model_path')
+onnx_model = onnx.load_model("/model_path")
 # Provide validation part of the dataset to collect statistics needed for the compression algorithm
 val_dataset = datasets.ImageFolder("/path")
 dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
 
 # Step 1: Initialize transformation function
 input_name = onnx_model.graph.input[0].name
 def transform_fn(data_item):
@@ -255,168 +178,183 @@
 calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
 # Step 3: Run the quantization pipeline
 quantized_model = nncf.quantize(onnx_model, calibration_dataset)
 ```
 
 </details>
 
-<details><summary><b>OpenVINO</b></summary>
+
+[//]: # (NNCF provides full  [samples]&#40;#post-training-quantization-samples&#41;, which demonstrate Post-Training Quantization usage for PyTorch, TensorFlow, ONNX, OpenVINO.)
+
+### Training-Time Compression
+
+Below is an example of Accuracy Aware Quantization pipeline where model weights and compression parameters may be fine-tuned to achieve a higher accuracy.
+
+<details><summary><b>PyTorch</b></summary>
 
 ```python
-import nncf
-import openvino.runtime as ov
 import torch
-from torchvision import datasets
+import nncf.torch  # Important - must be imported before any other external package that depends on torch
+
+from nncf import NNCFConfig
+from nncf.torch import create_compressed_model, register_default_init_args
 
 # Instantiate your uncompressed model
-model = ov.Core().read_model('/model_path')
-# Provide validation part of the dataset to collect statistics needed for the compression algorithm
-val_dataset = datasets.ImageFolder("/path")
-dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
+from torchvision.models.resnet import resnet50
+model = resnet50()
 
-# Step 1: Initialize transformation function
-def transform_fn(data_item):
-    images, _ = data_item
-    return images
+# Load a configuration file to specify compression
+nncf_config = NNCFConfig.from_json("resnet50_int8.json")
 
-# Step 2: Initialize NNCF Dataset
-calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
-# Step 3: Run the quantization pipeline
-quantized_model = nncf.quantize(model, calibration_dataset)
+# Provide data loaders for compression algorithm initialization, if necessary
+import torchvision.datasets as datasets
+representative_dataset = datasets.ImageFolder("/path")
+init_loader = torch.utils.data.DataLoader(representative_dataset)
+nncf_config = register_default_init_args(nncf_config, init_loader)
+
+# Apply the specified compression algorithms to the model
+compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+
+# Now use compressed_model as a usual torch.nn.Module 
+# to fine-tune compression parameters along with the model weights
+
+# ... the rest of the usual PyTorch-powered training pipeline
+
+# Export to ONNX or .pth when done fine-tuning
+compression_ctrl.export_model("compressed_model.onnx")
+torch.save(compressed_model.state_dict(), "compressed_model.pth")
 ```
 
+**NOTE (PyTorch)**: Due to the way NNCF works within the PyTorch backend, `import nncf` must be done before any other import of `torch` in your package _or_ in third-party packages that your code utilizes, otherwise the compression may be applied incompletely.
+
 </details>
 
-## Model Compression Samples
+<details><summary><b>Tensorflow</b></summary>
 
-For a quicker start with NNCF-powered compression, you can also try the sample scripts, each of which provides a basic training pipeline for classification, semantic segmentation and object detection neural network training correspondingly.
+```python
+import tensorflow as tf
 
-To run the samples please refer to the corresponding tutorials:
+from nncf import NNCFConfig
+from nncf.tensorflow import create_compressed_model, register_default_init_args
 
-### Compression-Aware Training Samples
-- PyTorch samples:
-  - [Image Classification sample](examples/torch/classification/README.md)
-  - [Object Detection sample](examples/torch/object_detection/README.md)
-  - [Semantic Segmentation sample](examples/torch/semantic_segmentation/README.md)
-- TensorFlow samples:
-    - [Image Classification sample](examples/tensorflow/classification/README.md)
-    - [Object Detection sample](examples/tensorflow/object_detection/README.md)
-    - [Instance Segmentation sample](examples/tensorflow/segmentation/README.md)
+# Instantiate your uncompressed model
+from tensorflow.keras.applications import ResNet50
+model = ResNet50()
 
-### Post-Training Quantization Samples
+# Load a configuration file to specify compression
+nncf_config = NNCFConfig.from_json("resnet50_int8.json")
+
+# Provide dataset for compression algorithm initialization
+representative_dataset = tf.data.Dataset.list_files("/path/*.jpeg")
+nncf_config = register_default_init_args(nncf_config, representative_dataset, batch_size=1)
+
+# Apply the specified compression algorithms to the model
+compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+
+# Now use compressed_model as a usual Keras model
+# to fine-tune compression parameters along with the model weights
+
+# ... the rest of the usual TensorFlow-powered training pipeline
+
+# Export to Frozen Graph, TensorFlow SavedModel or .h5  when done fine-tuning 
+compression_ctrl.export_model("compressed_model.pb", save_format="frozen_graph")
+```
+
+</details>
 
-- [PyTorch Post-Training Quantization sample](examples/post_training_quantization/torch/mobilenet_v2/README.md)
-- [TensorFlow Post-Training Quantization sample](examples/post_training_quantization/tensorflow/mobilenet_v2/README.md)
-- [ONNX Post-Training Quantization sample](examples/post_training_quantization/onnx/mobilenet_v2/README.md)
-- [OpenVINO Post-Training Quantization sample](examples/post_training_quantization/openvino/mobilenet_v2/README.md)
+For a more detailed description of NNCF usage in your training code, see [this tutorial](docs/Usage.md).
 
-## Model Compression Notebooks 
+## Model Compression Tutorials and Samples
 
-A collection of ready-to-run Jupyter* notebooks are also available to demonstrate how to use NNCF compression algorithms
-to optimize models for inference with the OpenVINO Toolkit.
+For a quicker start with NNCF-powered compression, try sample notebooks and scripts presented below.
+
+### Model Compression Tutorials 
+
+A collection of ready-to-run Jupyter* notebooks are available to demonstrate how to use NNCF compression algorithms to optimize models for inference with the OpenVINO Toolkit:
+- [Accelerate Inference of NLP models with Post-Training Qunatization API of NNCF](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert)
+- [Convert and Optimize YOLOv8 with OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)
+- [Convert and Optimize YOLOv7 with OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)
+- [NNCF Post-Training Optimization of Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)
+- [Quantize a Segmentation Model and Show Live Inference](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)
+- [Training to Deployment with TensorFlow and OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/301-tensorflow-training-openvino)
+- [Migrate quantization from POT API to NNCF API](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration)
+- [Post-Training Quantization of Pytorch model with NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)
 - [Optimizing PyTorch models with NNCF of OpenVINO by 8-bit quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/302-pytorch-quantization-aware-training)
 - [Optimizing TensorFlow models with NNCF of OpenVINO by 8-bit quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/305-tensorflow-quantization-aware-training)
-- [Post-Training Quantization of Pytorch model with NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)
+- [Accelerate Inference of Sparse Transformer Models with OpenVINO and 4th Gen Intel Xeon Scalable Processors](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)
+
+### Post-Training Quantization Samples
+Compact scripts demonstrating quantization and corresponding inference speed boost: 
+- [Post-Training Quantization of MobileNet v2 OpenVINO Model](examples/post_training_quantization/openvino/mobilenet_v2/README.md)
+- [Post-Training Quantization of YOLOv8 OpenVINO Model](examples/post_training_quantization/openvino/yolov8/README.md)
+- [Post-Training Quantization of Anomaly Classification OpenVINO model with control of accuracy metric](examples/post_training_quantization/openvino/quantize_with_accuracy_control/README.md)
+- [Post-Training Quantization of YOLOv8 OpenVINO Model with control of accuracy metric](examples/post_training_quantization/openvino/yolov8_quantize_with_accuracy_control/README.md)
+- [Post-Training Quantization of MobileNet v2 PyTorch Model](examples/post_training_quantization/torch/mobilenet_v2/README.md)
+- [Post-Training Quantization of SSD PyTorch Model](examples/post_training_quantization/torch/ssd300_vgg16/README.md)
+- [Post-Training Quantization of MobileNet v2 ONNX Model](examples/post_training_quantization/onnx/mobilenet_v2/README.md)
+- [Post-Training Quantization of MobileNet v2 TensorFlow Model](examples/post_training_quantization/tensorflow/mobilenet_v2/README.md)
+
+### Training-Time Compression Samples
+These examples provide full pipelines including compression, training and inference for classification, object detection and segmentation tasks.
+- PyTorch samples:
+  - [Image Classification sample](examples/torch/classification/README.md)
+  - [Object Detection sample](examples/torch/object_detection/README.md)
+  - [Semantic Segmentation sample](examples/torch/semantic_segmentation/README.md)
+- TensorFlow samples:
+  - [Image Classification sample](examples/tensorflow/classification/README.md)
+  - [Object Detection sample](examples/tensorflow/object_detection/README.md)
+  - [Instance Segmentation sample](examples/tensorflow/segmentation/README.md)
 
 ## Third-party repository integration
 NNCF may be straightforwardly integrated into training/evaluation pipelines of third-party repositories.
 
 ### Used by
 
 - [OpenVINO Training Extensions](https://github.com/openvinotoolkit/training_extensions)
   
   NNCF is integrated into OpenVINO Training Extensions as model optimization backend. So you can train, optimize and export new models based on the available model templates as well as run exported models with OpenVINO.
 
+- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov) 
+
+  NNCF is used as a compression backend within the renowned `transformers` repository in HuggingFace Optimum Intel.
+
 ### Git patches for third-party repository
 See [third_party_integration](./third_party_integration) for examples of code modifications (Git patches and base commit IDs are provided) that are necessary to integrate NNCF into the following repositories:
   - [huggingface-transformers](third_party_integration/huggingface_transformers/README.md)
 
-## System requirements
-- Ubuntu\* 18.04 or later (64-bit)
-- Python\* 3.7 or later
-- Supported frameworks:
-  - PyTorch\* 1.12.1
-  - TensorFlow\* >=2.4.0, <=2.8.2
-
-This repository is tested on Python* 3.8.10, PyTorch* 1.12.1 (NVidia CUDA\* Toolkit 11.6) and TensorFlow* 2.8.2 (NVidia CUDA\* Toolkit 11.2).
-
-## Installation
-We suggest to install or use the package in the [Python virtual environment](https://docs.python.org/3/tutorial/venv.html).
-
-If you want to optimize a model from PyTorch, install PyTorch by following [PyTorch installation guide](https://pytorch.org/get-started/locally/#start-locally). 
-If you want to optimize a model from TensorFlow, install TensorFlow by following [TensorFlow installation guide](https://www.tensorflow.org/install/).
-
-#### As a package built from a checked-out repository:
-
-Install the package and its dependencies by running the following in the repository root directory:
-```
-pip install .
-```
-
-Note that if you install NNCF in this manner, the backend frameworks supported by NNCF will not be explicitly installed. NNCF will try to work with whatever backend versions you have installed in your Python environment.
-
-If you want to install both NNCF and the supported PyTorch version in one line, you can do this by running:
-```
-pip install .[torch]
-```
-For installation of NNCF along with TensorFlow, run:
-```
-pip install .[tf]
-```
-For installation of NNCF for ONNX, run:
-```
-pip install .[onnx]
-```
-(Preview) For installation of NNCF for OpenVINO, run:
-```
-pip install .[openvino]
-```
-
-
-_NB_: For launching example scripts in this repository, we recommend setting the `PYTHONPATH` variable to the root of the checked-out repository once the installation is completed.
-
-#### As a PyPI package:
+## Installation Guide
+For detailed installation instructions please refer to the [Installation](./docs/Installation.md) page.
 
 NNCF can be installed as a regular PyPI package via pip:
 ```
 pip install nncf
 ```
-Use the same `pip install` syntax as above to install NNCF along with the backend package versions in one go, i.e. for NNCF with PyTorch, run:
+If you want to install both NNCF and the supported PyTorch version in one line, you can do this by simply running:
 ```
 pip install nncf[torch]
 ```
-For installation of NNCF along with TensorFlow, run:
-```
-pip install nncf[tf]
-```
-For installation of NNCF for ONNX, run:
-```
-pip install nncf[onnx]
-```
-(Preview) For installation of NNCF for OpenVINO, run:
-```
-pip install nncf[openvino]
-```
+Other viable options besides `[torch]` are `[tf]`, `[onnx]` and `[openvino]`.
 
 NNCF is also available via [conda](https://anaconda.org/conda-forge/nncf):
 ```
 conda install -c conda-forge nncf
 ```
 
-#### From a specific commit hash using pip:
-```python
-pip install git+https://github.com/openvinotoolkit/nncf@bd189e2#egg=nncf
-```
-Note that in order for this to work for pip versions >= 21.3, your Git version must be at least 2.22.
+You may also use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#model-compression-samples).
 
-#### As a Docker image
-Use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#model-compression-samples).
+### System requirements
+- Ubuntu\* 18.04 or later (64-bit)
+- Python\* 3.7 or later
+- Supported frameworks:
+  - PyTorch\* >=1.9.1, <1.14
+  - TensorFlow\* >=2.4.0, <=2.11.1
+  - ONNX\* ~=1.13.1
+  - OpenVINO\* >=2022.3.0
 
-## Contributing
-Refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines on contributions to the NNCF repository.
+This repository is tested on Python* 3.8.10, PyTorch* 1.13.1 (NVidia CUDA\* Toolkit 11.6) and TensorFlow* 2.11.1 (NVidia CUDA\* Toolkit 11.2).
 
 ## NNCF Compressed Model Zoo
 
 Results achieved using sample scripts, example patches to third-party repositories and NNCF configuration files provided 
 with this repository. See README.md files for [sample scripts](#model-compression-samples) and [example patches](#third-party-repository-integration) 
 to find instruction and links to exact configuration files and final checkpoints.
 - [PyTorch models](#pytorch-models)
@@ -430,63 +368,63 @@
   * [Instance segmentation](#tensorflow_instance_segmentation)
 
 ### PyTorch models
 
 <a name="pytorch_classification"></a>
 #### Classification
 
-|PyTorch Model|<img width="115" height="1">Compression algorithm<img width="115" height="1">|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|Accuracy (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|ResNet-50|INT8|ImageNet|76.42 (-0.26)|
-|ResNet-50|INT8 (per-tensor for weights)|ImageNet|76.37 (-0.21)|
-|ResNet-50|Mixed, 44.8% INT8 / 55.2% INT4|ImageNet|76.2 (-0.04)|
-|ResNet-50|INT8 + Sparsity 61% (RB)|ImageNet|75.43 (0.73)|
-|ResNet-50|INT8 + Sparsity 50% (RB)|ImageNet|75.55 (0.61)|
-|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|75.62 (0.54)|
-|Inception V3|INT8|ImageNet|78.25 (-0.91)|
-|Inception V3|INT8 + Sparsity 61% (RB)|ImageNet|77.58 (-0.24)|
-|MobileNet V2|INT8|ImageNet|71.35 (0.58)|
-|MobileNet V2|INT8 (per-tensor for weights)|ImageNet|71.3 (0.63)|
-|MobileNet V2|Mixed, 46.6% INT8 / 53.4% INT4|ImageNet|70.92 (1.01)|
-|MobileNet V2|INT8 + Sparsity 52% (RB)|ImageNet|71.11 (0.82)|
-|MobileNet V3 small|INT8|ImageNet|66.94 (0.73)|
-|SqueezeNet V1.1|INT8|ImageNet|58.28 (-0.04)|
-|SqueezeNet V1.1|INT8 (per-tensor for weights)|ImageNet|58.26 (-0.02)|
-|SqueezeNet V1.1|Mixed, 54.7% INT8 / 45.3% INT4|ImageNet|58.9 (-0.66)|
-|ResNet-18|XNOR (weights), scale/threshold (activations)|ImageNet|61.63 (8.17)|
-|ResNet-18|DoReFa (weights), scale/threshold (activations)|ImageNet|61.61 (8.19)|
-|ResNet-18|Filter pruning, 40%, magnitude criterion|ImageNet|69.26 (0.54)|
-|ResNet-18|Filter pruning, 40%, geometric median criterion|ImageNet|69.32 (0.48)|
+|ResNet-50|INT8|ImageNet|76.46 (-0.31)|
+|ResNet-50|INT8 (per-tensor only)|ImageNet|76.39 (-0.24)|
+|ResNet-50|Mixed, 43.12% INT8 / 56.88% INT4|ImageNet|76.05 (0.10)|
+|ResNet-50|INT8 + Sparsity 61% (RB)|ImageNet|75.42 (0.73)|
+|ResNet-50|INT8 + Sparsity 50% (RB)|ImageNet|75.50 (0.65)|
+|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|75.57 (0.58)|
+|Inception V3|INT8|ImageNet|77.45 (-0.12)|
+|Inception V3|INT8 + Sparsity 61% (RB)|ImageNet|76.36 (0.97)|
+|MobileNet V2|INT8|ImageNet|71.07 (0.80)|
+|MobileNet V2|INT8 (per-tensor only)|ImageNet|71.24 (0.63)|
+|MobileNet V2|Mixed, 58.88% INT8 / 41.12% INT4|ImageNet|70.95 (0.92)|
+|MobileNet V2|INT8 + Sparsity 52% (RB)|ImageNet|71.09 (0.78)|
+|MobileNet V3 small|INT8|ImageNet|66.98 (0.68)|
+|SqueezeNet V1.1|INT8|ImageNet|58.22 (-0.03)|
+|SqueezeNet V1.1|INT8 (per-tensor only)|ImageNet|58.11 (0.08)|
+|SqueezeNet V1.1|Mixed, 52.83% INT8 / 47.17% INT4|ImageNet|57.57 (0.62)|
+|ResNet-18|XNOR (weights), scale/threshold (activations)|ImageNet|61.67 (8.09)|
+|ResNet-18|DoReFa (weights), scale/threshold (activations)|ImageNet|61.63 (8.13)|
+|ResNet-18|Filter pruning, 40%, magnitude criterion|ImageNet|69.27 (0.49)|
+|ResNet-18|Filter pruning, 40%, geometric median criterion|ImageNet|69.31 (0.45)|
 |ResNet-34|Filter pruning, 50%, geometric median criterion + KD|ImageNet|73.11 (0.19)|
-|GoogLeNet|Filter pruning, 40%, geometric median criterion|ImageNet|68.82 (0.93)|
+|GoogLeNet|Filter pruning, 40%, geometric median criterion|ImageNet|69.47 (0.30)|
 
 <a name="pytorch_object_detection"></a>
 #### Object detection
 
-|PyTorch Model|Compression algorithm|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|SSD300-MobileNet|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|62.94 (-0.71)|
-|SSD300-VGG-BN|INT8|VOC12+07 train, VOC07 eval|77.96 (0.32)|
-|SSD300-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|77.59 (0.69)|
-|SSD300-VGG-BN|Filter pruning, 40%, geometric median criterion|VOC12+07 train, VOC07 eval|77.72 (0.56)|
-|SSD512-VGG-BN|INT8|VOC12+07 train, VOC07 eval|80.12 (0.14)|
-|SSD512-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|79.67 (0.59)|
+|SSD300-MobileNet|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|62.95 (-0.72)|
+|SSD300-VGG-BN|INT8|VOC12+07 train, VOC07 eval|77.81 (0.47)|
+|SSD300-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|77.66 (0.62)|
+|SSD300-VGG-BN|Filter pruning, 40%, geometric median criterion|VOC12+07 train, VOC07 eval|78.35 (-0.07)|
+|SSD512-VGG-BN|INT8|VOC12+07 train, VOC07 eval|80.04 (0.22)|
+|SSD512-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|79.68 (0.58)|
 
 <a name="pytorch_semantic_segmentation"></a>
 #### Semantic segmentation
 
-|PyTorch Model|<img width="125" height="1">Compression algorithm<img width="125" height="1">|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|mIoU (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|UNet|INT8|CamVid|71.8 (0.15)|
-|UNet|INT8 + Sparsity 60% (Magnitude)|CamVid|72.03 (-0.08)|
-|ICNet|INT8|CamVid|67.86 (0.03)|
-|ICNet|INT8 + Sparsity 60% (Magnitude)|CamVid|67.18 (0.71)|
-|UNet|INT8|Mapillary|55.87 (0.36)|
-|UNet|INT8 + Sparsity 60% (Magnitude)|Mapillary|55.65 (0.58)|
-|UNet|Filter pruning, 25%, geometric median criterion|Mapillary|55.62 (0.61)|
+|UNet|INT8|CamVid|71.89 (0.06)|
+|UNet|INT8 + Sparsity 60% (Magnitude)|CamVid|72.46 (-0.51)|
+|ICNet|INT8|CamVid|67.89 (0.00)|
+|ICNet|INT8 + Sparsity 60% (Magnitude)|CamVid|67.16 (0.73)|
+|UNet|INT8|Mapillary|56.09 (0.15)|
+|UNet|INT8 + Sparsity 60% (Magnitude)|Mapillary|55.69 (0.55)|
+|UNet|Filter pruning, 25%, geometric median criterion|Mapillary|55.64 (0.60)|
 
 <a name="pytorch_nlp"></a>
 #### NLP (HuggingFace Transformers-powered models)
 
 |PyTorch Model|<img width="20" height="1">Compression algorithm<img width="20" height="1">|Dataset|Accuracy (Drop) %|
 | :---: | :---: | :---: | :---: |
 |BERT-base-chinese|INT8|XNLI|77.22 (0.46)|
@@ -499,52 +437,52 @@
 |GPT-2|INT8|WikiText-2 (raw)|perplexity: 20.9 (-1.17)|
 
 ### TensorFlow models
 
 <a name="tensorflow_classification"></a>
 #### Classification
 
-|Tensorflow Model|Compression algorithm|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|Accuracy (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|Inception V3|INT8 (per-tensor for weights)|ImageNet|78.36 (-0.44)|
-|Inception V3|Sparsity 54% (Magnitude)|ImageNet|77.87 (0.03)|
-|Inception V3|INT8 (per-tensor for weights) + Sparsity 61% (RB)|ImageNet|77.58 (0.32)|
-|MobileNet V2|INT8 (per-tensor for weights)|ImageNet|71.66 (0.19)|
-|MobileNet V2|Sparsity 50% (RB)|ImageNet|71.34 (0.51)|
-|MobileNet V2|INT8 (per-tensor for weights) + Sparsity 52% (RB)|ImageNet|71.0 (0.85)|
-|MobileNet V3 small|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) |ImageNet|67.75 (0.63)|
-|MobileNet V3 small|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) + Sparsity 42% (RB)|ImageNet|67.59 (0.79)|
-|MobileNet V3 large|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) |ImageNet|75.04 (0.77)|
-|MobileNet V3 large|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) + Sparsity 42% (RB)|ImageNet|75.29 (0.52)|
-|ResNet50|INT8 (per-tensor for weights)|ImageNet|75.0 (0.04)|
-|ResNet50|Sparsity 80% (RB)|ImageNet|74.36 (0.68)|
-|ResNet50|INT8 (per-tensor for weightsy) + Sparsity 65% (RB)|ImageNet|74.3 (0.74)|
-|ResNet50|Filter Pruning 40%, geometric_median criterion|ImageNet|74.98 (0.06)|
-|ResNet50|Filter Pruning 40%, geometric_median criterion + INT8 (per-tensor for weights)|ImageNet|75.08 (-0.04)|
-|TensorFlow Hub MobileNet V2|Sparsity 35% (Magnitude)|ImageNet|71.90 (-0.06)|
+|Inception V3|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|78.39 (-0.48)|
+|Inception V3|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations), Sparsity 61% (RB)|ImageNet|77.52 (0.39)|
+|Inception V3|Sparsity 54% (Magnitude)|ImageNet|77.86 (0.05)|
+|MobileNet V2|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|71.63 (0.22)|
+|MobileNet V2|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations), Sparsity 52% (RB)|ImageNet|70.94 (0.91)|
+|MobileNet V2| Sparsity 50% (RB)|ImageNet|71.34 (0.51)|
+|MobileNet V2 (TensorFlow Hub MobileNet V2)|Sparsity 35% (Magnitude)|ImageNet|71.87 (-0.02)|
+|MobileNet V3 (Small)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|67.79 (0.59)|
+|MobileNet V3 (Small)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 42% (Magnitude)|ImageNet|67.44 (0.94)|
+|MobileNet V3 (Large)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|75.04 (0.76)|
+|MobileNet V3 (Large)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 42% (RB)|ImageNet|75.24 (0.56)|
+|ResNet-50|INT8|ImageNet|74.99 (0.06)|
+|ResNet-50|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 65% (RB)|ImageNet|74.36 (0.69)|
+|ResNet-50|Sparsity 80% (RB)|ImageNet|74.38 (0.67)|
+|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|74.96 (0.09)|
+|ResNet-50|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + Filter pruning, 40%, geometric median criterion|ImageNet|75.09 (-0.04)|
 
 <a name="tensorflow_object_detection"></a>
 #### Object detection
 
-|TensorFlow Model|Compression algorithm|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|RetinaNet|INT8 (per-tensor for weights)|COCO2017|33.18 (0.26)|
-|RetinaNet|Sparsity 50% (Magnitude)|COCO2017|33.13 (0.31)|
-|RetinaNet|Filter Pruning 40%, geometric_median criterion|COCO2017|32.7 (0.74)|
-|RetinaNet|Filter Pruning 40%, geometric_median criterion + INT8 (per-tensor for weights)|COCO2017|32.68 (0.76)|
-|YOLOv4|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations)|COCO2017|46.30 (0.74)|
-|YOLOv4|Sparsity 50% (Magnitude)|COCO2017|46.54 (0.50)|
+|RetinaNet|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|33.12 (0.31)|
+|RetinaNet|Magnitude sparsity (50%)|COCO 2017|33.10 (0.33)|
+|RetinaNet|Filter pruning, 40%|COCO 2017|32.72 (0.71)|
+|RetinaNet|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + filter pruning 40%|COCO 2017|32.67 (0.76)|
+|YOLO v4|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|46.20 (0.87)|
+|YOLO v4|Magnitude sparsity, 50%|COCO 2017|46.49 (0.58)|
 
 <a name="tensorflow_instance_segmentation"></a>
 #### Instance segmentation
 
-|TensorFlow Model|<img width="110" height="1">Compression algorithm<img width="110" height="1">|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|MaskRCNN|INT8 (per-tensor for weights)|COCO2017|bbox: 37.27 (0.06)<br/>segm: 33.54 (0.02)|
-|MaskRCNN|Sparsity 50% (Magnitude)|COCO2017|bbox: 36.93 (0.40)<br/>segm: 33.23 (0.33)|
+|Mask-R-CNN|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|37.19 (0.14)|
+|Mask-R-CNN|Magnitude sparsity, 50%|COCO 2017|36.94 (0.39)|
 
 ### ONNX models
 
 <a name="onnx_classification"></a>
 #### Classification
 
 |   ONNX Model    | Compression algorithm |Dataset|Accuracy (Drop) %|
@@ -572,19 +510,19 @@
     title =   {Neural network compression framework for fast model inference},
     author =  {Kozlov, Alexander and Lazarevich, Ivan and Shamporov, Vasily and Lyalyushkin, Nikolay and Gorbachev, Yury},
     journal = {arXiv preprint arXiv:2002.08679},
     year =    {2020}
 }
 ```
 
+## Contributing Guide
+Refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines on contributions to the NNCF repository.
+
 ## Useful links
 - [Documentation](./docs)
 - Example scripts (model objects available through links in respective README.md files):
     - [PyTorch](./examples/torch)
     - [TensorFlow](./examples/tensorflow)
 - [FAQ](./docs/FAQ.md)
 - [Notebooks](https://github.com/openvinotoolkit/openvino_notebooks#-model-training)
-- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov) utilizes NNCF as a compression backend within the renowned `transformers` repository.
-- [Model Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
-
-## Legal Information
-[*] Other names and brands may be claimed as the property of others.
+- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
+- [OpenVINO Model Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `nncf-2.4.0/README.md` & `nncf-2.5.0/PKG-INFO`

 * *Files 21% similar despite different names*

```diff
@@ -1,160 +1,135 @@
+Metadata-Version: 2.1
+Name: nncf
+Version: 2.5.0
+Summary: Neural Networks Compression Framework
+Home-page: https://github.com/openvinotoolkit/nncf
+Author: Intel
+Author-email: alexander.kozlov@intel.com
+License: Apache-2.0
+Keywords: compression,quantization,sparsity,mixed-precision-training,quantization-aware-training,hawq,classification,pruning,object-detection,semantic-segmentation,nas,nlp,bert,transformers,mmdetection
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: Apache Software License
+Classifier: Operating System :: OS Independent
+Description-Content-Type: text/markdown
+Provides-Extra: dev
+Provides-Extra: tests
+Provides-Extra: docs
+Provides-Extra: tf
+Provides-Extra: tensorflow
+Provides-Extra: tensorflow2
+Provides-Extra: torch
+Provides-Extra: pytorch
+Provides-Extra: onnx
+Provides-Extra: openvino
+Provides-Extra: all
+License-File: LICENSE
+
 <div align="center">
 
 # Neural Network Compression Framework (NNCF)
 
+[Key Features](#key-features) 
+[Installation](#Installation-guide) 
+[Documentation](#documentation) 
+[Usage](#usage) 
+[Tutorials and Samples](#Model-compression-tutorials-and-samples) 
+[Third-party integration](#Third-party-repository-integration) 
+[Model Zoo](#NNCF-Compressed-Model-Zoo)
+ 
 [![GitHub Release](https://img.shields.io/github/v/release/openvinotoolkit/nncf?color=green)](https://github.com/openvinotoolkit/nncf/releases)
 [![Website](https://img.shields.io/website?up_color=blue&up_message=docs&url=https%3A%2F%2Fdocs.openvino.ai%2Flatest%2Fopenvino_docs_model_optimization_guide.html)](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
 [![Apache License Version 2.0](https://img.shields.io/badge/license-Apache_2.0-green.svg)](LICENSE)
 [![PyPI Downloads](https://static.pepy.tech/badge/nncf)](https://pypi.org/project/nncf/)
  
 </div>
 
-_For the installation instructions, [click here](#installation)._
-
-NNCF provides a suite of advanced algorithms for Neural Networks inference optimization in [OpenVINO&trade;](https://docs.openvino.ai/latest/home.html) with minimal accuracy drop.
+Neural Network Compression Framework (NNCF) provides a suite of post-training and training-time algorithms for neural networks inference optimization in [OpenVINO&trade;](https://docs.openvino.ai) with minimal accuracy drop.
 
 NNCF is designed to work with models from [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), [ONNX](https://onnx.ai/) and [OpenVINO&trade;](https://docs.openvino.ai/latest/home.html).
 
-NNCF provides samples that demonstrate the usage of compression algorithms for three different use cases on public PyTorch and 
-TensorFlow models and datasets: Image Classification, Object Detection and Semantic Segmentation. 
+NNCF provides [samples](#Model-Compression-Samples) that demonstrate the usage of compression algorithms for different use cases and models. 
 [Compression results](#nncf-compressed-model-zoo) achievable with the NNCF-powered samples can be found in a table at 
 the end of this document.
 
 The framework is organized as a Python\* package that can be built and used in a standalone mode. The framework 
 architecture is unified to make it easy to add different compression algorithms for both PyTorch and TensorFlow deep 
 learning frameworks.
 
 ## Key Features
 ### Post-Training Compression Algorithms
 
-| Compression algorithm                                                       |PyTorch|TensorFlow|   ONNX   |       OpenVINO     |
+| Compression algorithm                                                       |OpenVINO|PyTorch|   TensorFlow   |     ONNX       |
 |:----------------------------------------------------------------------------| :---: | :---: |:--------:|:------------------:|
-| [Quantization](./docs/compression_algorithms/post_training/Quantization.md) | Supported | Supported |Supported| Preview |
-
-_Preview means that this is a work in progress and NNCF does not guarantee the full functional support._
+| [Post-Training Quantization](./docs/compression_algorithms/post_training/Quantization.md) | Supported | Supported |Supported| Supported |
 
-### Training-time Compression Algorithms
+### Training-Time Compression Algorithms
 
 |Compression algorithm|PyTorch|TensorFlow|
 | :--- | :---: | :---: |
-|[Quantization](./docs/compression_algorithms/Quantization.md) | Supported | Supported |
-|[Mixed-Precision Quantization](./docs/compression_algorithms/Quantization.md#mixed_precision_quantization) | Supported | Not supported | Not supported |
-|[Binarization](./docs/compression_algorithms/Binarization.md) | Supported | Not supported | Not supported |
-|[Sparsity](./docs/compression_algorithms/Sparsity.md) | Supported | Supported | Not supported |
-|[Filter pruning](./docs/compression_algorithms/Pruning.md) | Supported | Supported | Not supported |
-
-
-
+|[Quantization Aware Training](./docs/compression_algorithms/Quantization.md) | Supported | Supported |
+|[Mixed-Precision Quantization](./docs/compression_algorithms/Quantization.md#mixed_precision_quantization) | Supported | Not supported |
+|[Binarization](./docs/compression_algorithms/Binarization.md) | Supported | Not supported |
+|[Sparsity](./docs/compression_algorithms/Sparsity.md) | Supported | Supported |
+|[Filter pruning](./docs/compression_algorithms/Pruning.md) | Supported | Supported |
+|[Movement pruning](./nncf/experimental/torch/sparsity/movement/MovementSparsity.md) | Experimental | Not supported |
 
 - Automatic, configurable model graph transformation to obtain the compressed model.
   > **NOTE**: Limited support for TensorFlow models. The models created using Sequential or Keras Functional API are only supported.
 - Common interface for compression methods.
 - GPU-accelerated layers for faster compressed model fine-tuning.
 - Distributed training support.
-- Configuration file examples for each supported compression algorithm.
-- Git patches for prominent third-party repositories ([huggingface-transformers](https://github.com/huggingface/transformers)) demonstrating the process of integrating NNCF into custom training pipelines
-- Exporting PyTorch compressed models to ONNX\* checkpoints and TensorFlow compressed models to SavedModel or Frozen Graph format, ready to use with [OpenVINO&trade; toolkit](https://docs.openvino.ai/latest/home.html).
+- Git patch for prominent third-party repository ([huggingface-transformers](https://github.com/huggingface/transformers)) demonstrating the process of integrating NNCF into custom training pipelines
+- Seamless combination of pruning, sparsity and quantization algorithms. Please refer to [optimum-intel](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino) for examples of 
+joint (movement) pruning, quantization and distillation (JPQD), end-to-end from NNCF optimization to compressed OpenVINO IR.
+- Exporting PyTorch compressed models to ONNX\* checkpoints and TensorFlow compressed models to SavedModel or Frozen Graph format, ready to use with [OpenVINO&trade; toolkit](https://docs.openvino.ai).
 - Support for [Accuracy-Aware model training](./docs/Usage.md#accuracy-aware-model-training) pipelines via the [Adaptive Compression Level Training](./docs/accuracy_aware_model_training/AdaptiveCompressionLevelTraining.md) and [Early Exit Training](./docs/accuracy_aware_model_training/EarlyExitTraining.md).
 
-## Usage
-The NNCF is organized as a regular Python package that can be imported in your target training pipeline script.
-The basic workflow is loading a JSON configuration script containing NNCF-specific parameters determining the compression to be applied to your model, and then passing your model along with the configuration script to the `create_compressed_model` function.
-This function returns a model with additional modifications necessary to enable algorithm-specific compression during fine-tuning and handle to the object allowing you to control the compression during the training process:
+## Documentation
 
-### Usage example with PyTorch 
+This documentation covers detailed information about NNCF algorithms and functions needed for the contribution to NNCF.  
 
-```python
-import torch
-import nncf  # Important - must be imported before any other external package that depends on torch
+The latest user documentation for NNCF is available [here](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html).
 
-from nncf import NNCFConfig
-from nncf.torch import create_compressed_model, register_default_init_args
+NNCF API documentation can be found [here](https://openvinotoolkit.github.io/nncf/autoapi/nncf/).
 
-# Instantiate your uncompressed model
-from torchvision.models.resnet import resnet50
-model = resnet50()
-
-# Load a configuration file to specify compression
-nncf_config = NNCFConfig.from_json("resnet50_int8.json")
-
-# Provide data loaders for compression algorithm initialization, if necessary
-import torchvision.datasets as datasets
-representative_dataset = datasets.ImageFolder("/path")
-init_loader = torch.utils.data.DataLoader(representative_dataset)
-nncf_config = register_default_init_args(nncf_config, init_loader)
-
-# Apply the specified compression algorithms to the model
-compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+## Usage
 
-# Now use compressed_model as a usual torch.nn.Module 
-# to fine-tune compression parameters along with the model weights
+### Post-Training Quantization
 
-# ... the rest of the usual PyTorch-powered training pipeline
+The NNCF PTQ is the simplest way to apply 8-bit quantization. To run the algorithm you only need your model and a small (~300 samples) calibration dataset.
 
-# Export to ONNX or .pth when done fine-tuning
-compression_ctrl.export_model("compressed_model.onnx")
-torch.save(compressed_model.state_dict(), "compressed_model.pth")
-```
-
-**NOTE (PyTorch)**: Due to the way NNCF works within the PyTorch backend, `import nncf` must be done before any other import of `torch` in your package _or_ in third-party packages that your code utilizes, otherwise the compression may be applied incompletely.
+[OpenVINO](https://github.com/openvinotoolkit/openvino) is the preferred backend to run PTQ with, and PyTorch, TensorFlow and ONNX are also supported.
 
+<details open><summary><b>OpenVINO</b></summary>
 
-### Usage example with TensorFlow
 ```python
-import tensorflow as tf
-
-from nncf import NNCFConfig
-from nncf.tensorflow import create_compressed_model, register_default_init_args
+import nncf
+import openvino.runtime as ov
+import torch
+from torchvision import datasets
 
 # Instantiate your uncompressed model
-from tensorflow.keras.applications import ResNet50
-model = ResNet50()
-
-# Load a configuration file to specify compression
-nncf_config = NNCFConfig.from_json("resnet50_int8.json")
-
-# Provide dataset for compression algorithm initialization
-representative_dataset = tf.data.Dataset.list_files("/path/*.jpeg")
-nncf_config = register_default_init_args(nncf_config, representative_dataset, batch_size=1)
-
-# Apply the specified compression algorithms to the model
-compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
-
-# Now use compressed_model as a usual Keras model
-# to fine-tune compression parameters along with the model weights
+model = ov.Core().read_model("/model_path")
+# Provide validation part of the dataset to collect statistics needed for the compression algorithm
+val_dataset = datasets.ImageFolder("/path")
+dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
 
-# ... the rest of the usual TensorFlow-powered training pipeline
+# Step 1: Initialize transformation function
+def transform_fn(data_item):
+    images, _ = data_item
+    return images
 
-# Export to Frozen Graph, TensorFlow SavedModel or .h5  when done fine-tuning 
-compression_ctrl.export_model("compressed_model.pb", save_format='frozen_graph')
+# Step 2: Initialize NNCF Dataset
+calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
+# Step 3: Run the quantization pipeline
+quantized_model = nncf.quantize(model, calibration_dataset)
 ```
 
-For a more detailed description of NNCF usage in your training code, see [this tutorial](docs/Usage.md). 
-For in-depth examples of NNCF integration, browse the [sample scripts](#compression-aware-training-samples) code, or the [example patches](#third-party-repository-integration) to third-party repositories.
-For FAQ, visit this [link](./docs/FAQ.md).
-
-### Usage examples of Post-Training Quantization
-
-NNCF provides [samples](#post-training-quantization-samples), which demonstrate Post-Training Quantization usage for PyTorch, TensorFlow, ONNX, OpenVINO.
-
-To start the algorithm, provide the following entities:
-* Original model.
-* Validation part of the dataset.
-* [Data transformation function](./docs/compression_algorithms/post_training/Quantization.md#data-transformation-function) transforming data items from the original dataset to the model input data. 
-
-
-The basic workflow steps:
-1) Create the [data transformation function](./docs/compression_algorithms/post_training/Quantization.md#data-transformation-function).
-2) Create an instance of `nncf.Dataset` class by passing two parameters:
-* `data_source` - Iterable python object that contains data items for model calibration.
-* `transform_fn` - Data transformation function from the Step 1.
-3) Run the quantization pipeline.
-
-Below are the usage examples for every backend.
+</details>
 
 <details><summary><b>PyTorch</b></summary>
 
 ```python
 import nncf
 import torch
 from torchvision import datasets, models
@@ -185,15 +160,15 @@
 import nncf
 import tensorflow as tf
 import tensorflow_datasets as tfds
 
 # Instantiate your uncompressed model
 model = tf.keras.applications.MobileNetV2()
 # Provide validation part of the dataset to collect statistics needed for the compression algorithm
-val_dataset = tfds.load('/path', split='validation', 
+val_dataset = tfds.load("/path", split="validation", 
                         shuffle_files=False, as_supervised=True)
 
 # Step 1: Initialize transformation function
 def transform_fn(data_item):
     images, _ = data_item
     return images
 
@@ -210,15 +185,15 @@
 ```python
 import onnx
 import nncf
 import torch
 from torchvision import datasets
 
 # Instantiate your uncompressed model
-onnx_model = onnx.load_model('/model_path')
+onnx_model = onnx.load_model("/model_path")
 # Provide validation part of the dataset to collect statistics needed for the compression algorithm
 val_dataset = datasets.ImageFolder("/path")
 dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
 
 # Step 1: Initialize transformation function
 input_name = onnx_model.graph.input[0].name
 def transform_fn(data_item):
@@ -229,168 +204,183 @@
 calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
 # Step 3: Run the quantization pipeline
 quantized_model = nncf.quantize(onnx_model, calibration_dataset)
 ```
 
 </details>
 
-<details><summary><b>OpenVINO</b></summary>
+
+[//]: # (NNCF provides full  [samples]&#40;#post-training-quantization-samples&#41;, which demonstrate Post-Training Quantization usage for PyTorch, TensorFlow, ONNX, OpenVINO.)
+
+### Training-Time Compression
+
+Below is an example of Accuracy Aware Quantization pipeline where model weights and compression parameters may be fine-tuned to achieve a higher accuracy.
+
+<details><summary><b>PyTorch</b></summary>
 
 ```python
-import nncf
-import openvino.runtime as ov
 import torch
-from torchvision import datasets
+import nncf.torch  # Important - must be imported before any other external package that depends on torch
+
+from nncf import NNCFConfig
+from nncf.torch import create_compressed_model, register_default_init_args
 
 # Instantiate your uncompressed model
-model = ov.Core().read_model('/model_path')
-# Provide validation part of the dataset to collect statistics needed for the compression algorithm
-val_dataset = datasets.ImageFolder("/path")
-dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
+from torchvision.models.resnet import resnet50
+model = resnet50()
 
-# Step 1: Initialize transformation function
-def transform_fn(data_item):
-    images, _ = data_item
-    return images
+# Load a configuration file to specify compression
+nncf_config = NNCFConfig.from_json("resnet50_int8.json")
 
-# Step 2: Initialize NNCF Dataset
-calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
-# Step 3: Run the quantization pipeline
-quantized_model = nncf.quantize(model, calibration_dataset)
+# Provide data loaders for compression algorithm initialization, if necessary
+import torchvision.datasets as datasets
+representative_dataset = datasets.ImageFolder("/path")
+init_loader = torch.utils.data.DataLoader(representative_dataset)
+nncf_config = register_default_init_args(nncf_config, init_loader)
+
+# Apply the specified compression algorithms to the model
+compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+
+# Now use compressed_model as a usual torch.nn.Module 
+# to fine-tune compression parameters along with the model weights
+
+# ... the rest of the usual PyTorch-powered training pipeline
+
+# Export to ONNX or .pth when done fine-tuning
+compression_ctrl.export_model("compressed_model.onnx")
+torch.save(compressed_model.state_dict(), "compressed_model.pth")
 ```
 
+**NOTE (PyTorch)**: Due to the way NNCF works within the PyTorch backend, `import nncf` must be done before any other import of `torch` in your package _or_ in third-party packages that your code utilizes, otherwise the compression may be applied incompletely.
+
 </details>
 
-## Model Compression Samples
+<details><summary><b>Tensorflow</b></summary>
 
-For a quicker start with NNCF-powered compression, you can also try the sample scripts, each of which provides a basic training pipeline for classification, semantic segmentation and object detection neural network training correspondingly.
+```python
+import tensorflow as tf
 
-To run the samples please refer to the corresponding tutorials:
+from nncf import NNCFConfig
+from nncf.tensorflow import create_compressed_model, register_default_init_args
 
-### Compression-Aware Training Samples
-- PyTorch samples:
-  - [Image Classification sample](examples/torch/classification/README.md)
-  - [Object Detection sample](examples/torch/object_detection/README.md)
-  - [Semantic Segmentation sample](examples/torch/semantic_segmentation/README.md)
-- TensorFlow samples:
-    - [Image Classification sample](examples/tensorflow/classification/README.md)
-    - [Object Detection sample](examples/tensorflow/object_detection/README.md)
-    - [Instance Segmentation sample](examples/tensorflow/segmentation/README.md)
+# Instantiate your uncompressed model
+from tensorflow.keras.applications import ResNet50
+model = ResNet50()
 
-### Post-Training Quantization Samples
+# Load a configuration file to specify compression
+nncf_config = NNCFConfig.from_json("resnet50_int8.json")
+
+# Provide dataset for compression algorithm initialization
+representative_dataset = tf.data.Dataset.list_files("/path/*.jpeg")
+nncf_config = register_default_init_args(nncf_config, representative_dataset, batch_size=1)
+
+# Apply the specified compression algorithms to the model
+compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+
+# Now use compressed_model as a usual Keras model
+# to fine-tune compression parameters along with the model weights
+
+# ... the rest of the usual TensorFlow-powered training pipeline
+
+# Export to Frozen Graph, TensorFlow SavedModel or .h5  when done fine-tuning 
+compression_ctrl.export_model("compressed_model.pb", save_format="frozen_graph")
+```
+
+</details>
 
-- [PyTorch Post-Training Quantization sample](examples/post_training_quantization/torch/mobilenet_v2/README.md)
-- [TensorFlow Post-Training Quantization sample](examples/post_training_quantization/tensorflow/mobilenet_v2/README.md)
-- [ONNX Post-Training Quantization sample](examples/post_training_quantization/onnx/mobilenet_v2/README.md)
-- [OpenVINO Post-Training Quantization sample](examples/post_training_quantization/openvino/mobilenet_v2/README.md)
+For a more detailed description of NNCF usage in your training code, see [this tutorial](docs/Usage.md).
 
-## Model Compression Notebooks 
+## Model Compression Tutorials and Samples
 
-A collection of ready-to-run Jupyter* notebooks are also available to demonstrate how to use NNCF compression algorithms
-to optimize models for inference with the OpenVINO Toolkit.
+For a quicker start with NNCF-powered compression, try sample notebooks and scripts presented below.
+
+### Model Compression Tutorials 
+
+A collection of ready-to-run Jupyter* notebooks are available to demonstrate how to use NNCF compression algorithms to optimize models for inference with the OpenVINO Toolkit:
+- [Accelerate Inference of NLP models with Post-Training Qunatization API of NNCF](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert)
+- [Convert and Optimize YOLOv8 with OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)
+- [Convert and Optimize YOLOv7 with OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)
+- [NNCF Post-Training Optimization of Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)
+- [Quantize a Segmentation Model and Show Live Inference](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)
+- [Training to Deployment with TensorFlow and OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/301-tensorflow-training-openvino)
+- [Migrate quantization from POT API to NNCF API](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration)
+- [Post-Training Quantization of Pytorch model with NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)
 - [Optimizing PyTorch models with NNCF of OpenVINO by 8-bit quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/302-pytorch-quantization-aware-training)
 - [Optimizing TensorFlow models with NNCF of OpenVINO by 8-bit quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/305-tensorflow-quantization-aware-training)
-- [Post-Training Quantization of Pytorch model with NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)
+- [Accelerate Inference of Sparse Transformer Models with OpenVINO and 4th Gen Intel Xeon Scalable Processors](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)
+
+### Post-Training Quantization Samples
+Compact scripts demonstrating quantization and corresponding inference speed boost: 
+- [Post-Training Quantization of MobileNet v2 OpenVINO Model](examples/post_training_quantization/openvino/mobilenet_v2/README.md)
+- [Post-Training Quantization of YOLOv8 OpenVINO Model](examples/post_training_quantization/openvino/yolov8/README.md)
+- [Post-Training Quantization of Anomaly Classification OpenVINO model with control of accuracy metric](examples/post_training_quantization/openvino/quantize_with_accuracy_control/README.md)
+- [Post-Training Quantization of YOLOv8 OpenVINO Model with control of accuracy metric](examples/post_training_quantization/openvino/yolov8_quantize_with_accuracy_control/README.md)
+- [Post-Training Quantization of MobileNet v2 PyTorch Model](examples/post_training_quantization/torch/mobilenet_v2/README.md)
+- [Post-Training Quantization of SSD PyTorch Model](examples/post_training_quantization/torch/ssd300_vgg16/README.md)
+- [Post-Training Quantization of MobileNet v2 ONNX Model](examples/post_training_quantization/onnx/mobilenet_v2/README.md)
+- [Post-Training Quantization of MobileNet v2 TensorFlow Model](examples/post_training_quantization/tensorflow/mobilenet_v2/README.md)
+
+### Training-Time Compression Samples
+These examples provide full pipelines including compression, training and inference for classification, object detection and segmentation tasks.
+- PyTorch samples:
+  - [Image Classification sample](examples/torch/classification/README.md)
+  - [Object Detection sample](examples/torch/object_detection/README.md)
+  - [Semantic Segmentation sample](examples/torch/semantic_segmentation/README.md)
+- TensorFlow samples:
+  - [Image Classification sample](examples/tensorflow/classification/README.md)
+  - [Object Detection sample](examples/tensorflow/object_detection/README.md)
+  - [Instance Segmentation sample](examples/tensorflow/segmentation/README.md)
 
 ## Third-party repository integration
 NNCF may be straightforwardly integrated into training/evaluation pipelines of third-party repositories.
 
 ### Used by
 
 - [OpenVINO Training Extensions](https://github.com/openvinotoolkit/training_extensions)
   
   NNCF is integrated into OpenVINO Training Extensions as model optimization backend. So you can train, optimize and export new models based on the available model templates as well as run exported models with OpenVINO.
 
+- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov) 
+
+  NNCF is used as a compression backend within the renowned `transformers` repository in HuggingFace Optimum Intel.
+
 ### Git patches for third-party repository
 See [third_party_integration](./third_party_integration) for examples of code modifications (Git patches and base commit IDs are provided) that are necessary to integrate NNCF into the following repositories:
   - [huggingface-transformers](third_party_integration/huggingface_transformers/README.md)
 
-## System requirements
-- Ubuntu\* 18.04 or later (64-bit)
-- Python\* 3.7 or later
-- Supported frameworks:
-  - PyTorch\* 1.12.1
-  - TensorFlow\* >=2.4.0, <=2.8.2
-
-This repository is tested on Python* 3.8.10, PyTorch* 1.12.1 (NVidia CUDA\* Toolkit 11.6) and TensorFlow* 2.8.2 (NVidia CUDA\* Toolkit 11.2).
-
-## Installation
-We suggest to install or use the package in the [Python virtual environment](https://docs.python.org/3/tutorial/venv.html).
-
-If you want to optimize a model from PyTorch, install PyTorch by following [PyTorch installation guide](https://pytorch.org/get-started/locally/#start-locally). 
-If you want to optimize a model from TensorFlow, install TensorFlow by following [TensorFlow installation guide](https://www.tensorflow.org/install/).
-
-#### As a package built from a checked-out repository:
-
-Install the package and its dependencies by running the following in the repository root directory:
-```
-pip install .
-```
-
-Note that if you install NNCF in this manner, the backend frameworks supported by NNCF will not be explicitly installed. NNCF will try to work with whatever backend versions you have installed in your Python environment.
-
-If you want to install both NNCF and the supported PyTorch version in one line, you can do this by running:
-```
-pip install .[torch]
-```
-For installation of NNCF along with TensorFlow, run:
-```
-pip install .[tf]
-```
-For installation of NNCF for ONNX, run:
-```
-pip install .[onnx]
-```
-(Preview) For installation of NNCF for OpenVINO, run:
-```
-pip install .[openvino]
-```
-
-
-_NB_: For launching example scripts in this repository, we recommend setting the `PYTHONPATH` variable to the root of the checked-out repository once the installation is completed.
-
-#### As a PyPI package:
+## Installation Guide
+For detailed installation instructions please refer to the [Installation](./docs/Installation.md) page.
 
 NNCF can be installed as a regular PyPI package via pip:
 ```
 pip install nncf
 ```
-Use the same `pip install` syntax as above to install NNCF along with the backend package versions in one go, i.e. for NNCF with PyTorch, run:
+If you want to install both NNCF and the supported PyTorch version in one line, you can do this by simply running:
 ```
 pip install nncf[torch]
 ```
-For installation of NNCF along with TensorFlow, run:
-```
-pip install nncf[tf]
-```
-For installation of NNCF for ONNX, run:
-```
-pip install nncf[onnx]
-```
-(Preview) For installation of NNCF for OpenVINO, run:
-```
-pip install nncf[openvino]
-```
+Other viable options besides `[torch]` are `[tf]`, `[onnx]` and `[openvino]`.
 
 NNCF is also available via [conda](https://anaconda.org/conda-forge/nncf):
 ```
 conda install -c conda-forge nncf
 ```
 
-#### From a specific commit hash using pip:
-```python
-pip install git+https://github.com/openvinotoolkit/nncf@bd189e2#egg=nncf
-```
-Note that in order for this to work for pip versions >= 21.3, your Git version must be at least 2.22.
+You may also use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#model-compression-samples).
 
-#### As a Docker image
-Use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#model-compression-samples).
+### System requirements
+- Ubuntu\* 18.04 or later (64-bit)
+- Python\* 3.7 or later
+- Supported frameworks:
+  - PyTorch\* >=1.9.1, <1.14
+  - TensorFlow\* >=2.4.0, <=2.11.1
+  - ONNX\* ~=1.13.1
+  - OpenVINO\* >=2022.3.0
 
-## Contributing
-Refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines on contributions to the NNCF repository.
+This repository is tested on Python* 3.8.10, PyTorch* 1.13.1 (NVidia CUDA\* Toolkit 11.6) and TensorFlow* 2.11.1 (NVidia CUDA\* Toolkit 11.2).
 
 ## NNCF Compressed Model Zoo
 
 Results achieved using sample scripts, example patches to third-party repositories and NNCF configuration files provided 
 with this repository. See README.md files for [sample scripts](#model-compression-samples) and [example patches](#third-party-repository-integration) 
 to find instruction and links to exact configuration files and final checkpoints.
 - [PyTorch models](#pytorch-models)
@@ -404,63 +394,63 @@
   * [Instance segmentation](#tensorflow_instance_segmentation)
 
 ### PyTorch models
 
 <a name="pytorch_classification"></a>
 #### Classification
 
-|PyTorch Model|<img width="115" height="1">Compression algorithm<img width="115" height="1">|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|Accuracy (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|ResNet-50|INT8|ImageNet|76.42 (-0.26)|
-|ResNet-50|INT8 (per-tensor for weights)|ImageNet|76.37 (-0.21)|
-|ResNet-50|Mixed, 44.8% INT8 / 55.2% INT4|ImageNet|76.2 (-0.04)|
-|ResNet-50|INT8 + Sparsity 61% (RB)|ImageNet|75.43 (0.73)|
-|ResNet-50|INT8 + Sparsity 50% (RB)|ImageNet|75.55 (0.61)|
-|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|75.62 (0.54)|
-|Inception V3|INT8|ImageNet|78.25 (-0.91)|
-|Inception V3|INT8 + Sparsity 61% (RB)|ImageNet|77.58 (-0.24)|
-|MobileNet V2|INT8|ImageNet|71.35 (0.58)|
-|MobileNet V2|INT8 (per-tensor for weights)|ImageNet|71.3 (0.63)|
-|MobileNet V2|Mixed, 46.6% INT8 / 53.4% INT4|ImageNet|70.92 (1.01)|
-|MobileNet V2|INT8 + Sparsity 52% (RB)|ImageNet|71.11 (0.82)|
-|MobileNet V3 small|INT8|ImageNet|66.94 (0.73)|
-|SqueezeNet V1.1|INT8|ImageNet|58.28 (-0.04)|
-|SqueezeNet V1.1|INT8 (per-tensor for weights)|ImageNet|58.26 (-0.02)|
-|SqueezeNet V1.1|Mixed, 54.7% INT8 / 45.3% INT4|ImageNet|58.9 (-0.66)|
-|ResNet-18|XNOR (weights), scale/threshold (activations)|ImageNet|61.63 (8.17)|
-|ResNet-18|DoReFa (weights), scale/threshold (activations)|ImageNet|61.61 (8.19)|
-|ResNet-18|Filter pruning, 40%, magnitude criterion|ImageNet|69.26 (0.54)|
-|ResNet-18|Filter pruning, 40%, geometric median criterion|ImageNet|69.32 (0.48)|
+|ResNet-50|INT8|ImageNet|76.46 (-0.31)|
+|ResNet-50|INT8 (per-tensor only)|ImageNet|76.39 (-0.24)|
+|ResNet-50|Mixed, 43.12% INT8 / 56.88% INT4|ImageNet|76.05 (0.10)|
+|ResNet-50|INT8 + Sparsity 61% (RB)|ImageNet|75.42 (0.73)|
+|ResNet-50|INT8 + Sparsity 50% (RB)|ImageNet|75.50 (0.65)|
+|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|75.57 (0.58)|
+|Inception V3|INT8|ImageNet|77.45 (-0.12)|
+|Inception V3|INT8 + Sparsity 61% (RB)|ImageNet|76.36 (0.97)|
+|MobileNet V2|INT8|ImageNet|71.07 (0.80)|
+|MobileNet V2|INT8 (per-tensor only)|ImageNet|71.24 (0.63)|
+|MobileNet V2|Mixed, 58.88% INT8 / 41.12% INT4|ImageNet|70.95 (0.92)|
+|MobileNet V2|INT8 + Sparsity 52% (RB)|ImageNet|71.09 (0.78)|
+|MobileNet V3 small|INT8|ImageNet|66.98 (0.68)|
+|SqueezeNet V1.1|INT8|ImageNet|58.22 (-0.03)|
+|SqueezeNet V1.1|INT8 (per-tensor only)|ImageNet|58.11 (0.08)|
+|SqueezeNet V1.1|Mixed, 52.83% INT8 / 47.17% INT4|ImageNet|57.57 (0.62)|
+|ResNet-18|XNOR (weights), scale/threshold (activations)|ImageNet|61.67 (8.09)|
+|ResNet-18|DoReFa (weights), scale/threshold (activations)|ImageNet|61.63 (8.13)|
+|ResNet-18|Filter pruning, 40%, magnitude criterion|ImageNet|69.27 (0.49)|
+|ResNet-18|Filter pruning, 40%, geometric median criterion|ImageNet|69.31 (0.45)|
 |ResNet-34|Filter pruning, 50%, geometric median criterion + KD|ImageNet|73.11 (0.19)|
-|GoogLeNet|Filter pruning, 40%, geometric median criterion|ImageNet|68.82 (0.93)|
+|GoogLeNet|Filter pruning, 40%, geometric median criterion|ImageNet|69.47 (0.30)|
 
 <a name="pytorch_object_detection"></a>
 #### Object detection
 
-|PyTorch Model|Compression algorithm|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|SSD300-MobileNet|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|62.94 (-0.71)|
-|SSD300-VGG-BN|INT8|VOC12+07 train, VOC07 eval|77.96 (0.32)|
-|SSD300-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|77.59 (0.69)|
-|SSD300-VGG-BN|Filter pruning, 40%, geometric median criterion|VOC12+07 train, VOC07 eval|77.72 (0.56)|
-|SSD512-VGG-BN|INT8|VOC12+07 train, VOC07 eval|80.12 (0.14)|
-|SSD512-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|79.67 (0.59)|
+|SSD300-MobileNet|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|62.95 (-0.72)|
+|SSD300-VGG-BN|INT8|VOC12+07 train, VOC07 eval|77.81 (0.47)|
+|SSD300-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|77.66 (0.62)|
+|SSD300-VGG-BN|Filter pruning, 40%, geometric median criterion|VOC12+07 train, VOC07 eval|78.35 (-0.07)|
+|SSD512-VGG-BN|INT8|VOC12+07 train, VOC07 eval|80.04 (0.22)|
+|SSD512-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|79.68 (0.58)|
 
 <a name="pytorch_semantic_segmentation"></a>
 #### Semantic segmentation
 
-|PyTorch Model|<img width="125" height="1">Compression algorithm<img width="125" height="1">|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|mIoU (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|UNet|INT8|CamVid|71.8 (0.15)|
-|UNet|INT8 + Sparsity 60% (Magnitude)|CamVid|72.03 (-0.08)|
-|ICNet|INT8|CamVid|67.86 (0.03)|
-|ICNet|INT8 + Sparsity 60% (Magnitude)|CamVid|67.18 (0.71)|
-|UNet|INT8|Mapillary|55.87 (0.36)|
-|UNet|INT8 + Sparsity 60% (Magnitude)|Mapillary|55.65 (0.58)|
-|UNet|Filter pruning, 25%, geometric median criterion|Mapillary|55.62 (0.61)|
+|UNet|INT8|CamVid|71.89 (0.06)|
+|UNet|INT8 + Sparsity 60% (Magnitude)|CamVid|72.46 (-0.51)|
+|ICNet|INT8|CamVid|67.89 (0.00)|
+|ICNet|INT8 + Sparsity 60% (Magnitude)|CamVid|67.16 (0.73)|
+|UNet|INT8|Mapillary|56.09 (0.15)|
+|UNet|INT8 + Sparsity 60% (Magnitude)|Mapillary|55.69 (0.55)|
+|UNet|Filter pruning, 25%, geometric median criterion|Mapillary|55.64 (0.60)|
 
 <a name="pytorch_nlp"></a>
 #### NLP (HuggingFace Transformers-powered models)
 
 |PyTorch Model|<img width="20" height="1">Compression algorithm<img width="20" height="1">|Dataset|Accuracy (Drop) %|
 | :---: | :---: | :---: | :---: |
 |BERT-base-chinese|INT8|XNLI|77.22 (0.46)|
@@ -473,52 +463,52 @@
 |GPT-2|INT8|WikiText-2 (raw)|perplexity: 20.9 (-1.17)|
 
 ### TensorFlow models
 
 <a name="tensorflow_classification"></a>
 #### Classification
 
-|Tensorflow Model|Compression algorithm|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|Accuracy (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|Inception V3|INT8 (per-tensor for weights)|ImageNet|78.36 (-0.44)|
-|Inception V3|Sparsity 54% (Magnitude)|ImageNet|77.87 (0.03)|
-|Inception V3|INT8 (per-tensor for weights) + Sparsity 61% (RB)|ImageNet|77.58 (0.32)|
-|MobileNet V2|INT8 (per-tensor for weights)|ImageNet|71.66 (0.19)|
-|MobileNet V2|Sparsity 50% (RB)|ImageNet|71.34 (0.51)|
-|MobileNet V2|INT8 (per-tensor for weights) + Sparsity 52% (RB)|ImageNet|71.0 (0.85)|
-|MobileNet V3 small|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) |ImageNet|67.75 (0.63)|
-|MobileNet V3 small|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) + Sparsity 42% (RB)|ImageNet|67.59 (0.79)|
-|MobileNet V3 large|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) |ImageNet|75.04 (0.77)|
-|MobileNet V3 large|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) + Sparsity 42% (RB)|ImageNet|75.29 (0.52)|
-|ResNet50|INT8 (per-tensor for weights)|ImageNet|75.0 (0.04)|
-|ResNet50|Sparsity 80% (RB)|ImageNet|74.36 (0.68)|
-|ResNet50|INT8 (per-tensor for weightsy) + Sparsity 65% (RB)|ImageNet|74.3 (0.74)|
-|ResNet50|Filter Pruning 40%, geometric_median criterion|ImageNet|74.98 (0.06)|
-|ResNet50|Filter Pruning 40%, geometric_median criterion + INT8 (per-tensor for weights)|ImageNet|75.08 (-0.04)|
-|TensorFlow Hub MobileNet V2|Sparsity 35% (Magnitude)|ImageNet|71.90 (-0.06)|
+|Inception V3|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|78.39 (-0.48)|
+|Inception V3|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations), Sparsity 61% (RB)|ImageNet|77.52 (0.39)|
+|Inception V3|Sparsity 54% (Magnitude)|ImageNet|77.86 (0.05)|
+|MobileNet V2|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|71.63 (0.22)|
+|MobileNet V2|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations), Sparsity 52% (RB)|ImageNet|70.94 (0.91)|
+|MobileNet V2| Sparsity 50% (RB)|ImageNet|71.34 (0.51)|
+|MobileNet V2 (TensorFlow Hub MobileNet V2)|Sparsity 35% (Magnitude)|ImageNet|71.87 (-0.02)|
+|MobileNet V3 (Small)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|67.79 (0.59)|
+|MobileNet V3 (Small)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 42% (Magnitude)|ImageNet|67.44 (0.94)|
+|MobileNet V3 (Large)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|75.04 (0.76)|
+|MobileNet V3 (Large)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 42% (RB)|ImageNet|75.24 (0.56)|
+|ResNet-50|INT8|ImageNet|74.99 (0.06)|
+|ResNet-50|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 65% (RB)|ImageNet|74.36 (0.69)|
+|ResNet-50|Sparsity 80% (RB)|ImageNet|74.38 (0.67)|
+|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|74.96 (0.09)|
+|ResNet-50|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + Filter pruning, 40%, geometric median criterion|ImageNet|75.09 (-0.04)|
 
 <a name="tensorflow_object_detection"></a>
 #### Object detection
 
-|TensorFlow Model|Compression algorithm|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|RetinaNet|INT8 (per-tensor for weights)|COCO2017|33.18 (0.26)|
-|RetinaNet|Sparsity 50% (Magnitude)|COCO2017|33.13 (0.31)|
-|RetinaNet|Filter Pruning 40%, geometric_median criterion|COCO2017|32.7 (0.74)|
-|RetinaNet|Filter Pruning 40%, geometric_median criterion + INT8 (per-tensor for weights)|COCO2017|32.68 (0.76)|
-|YOLOv4|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations)|COCO2017|46.30 (0.74)|
-|YOLOv4|Sparsity 50% (Magnitude)|COCO2017|46.54 (0.50)|
+|RetinaNet|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|33.12 (0.31)|
+|RetinaNet|Magnitude sparsity (50%)|COCO 2017|33.10 (0.33)|
+|RetinaNet|Filter pruning, 40%|COCO 2017|32.72 (0.71)|
+|RetinaNet|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + filter pruning 40%|COCO 2017|32.67 (0.76)|
+|YOLO v4|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|46.20 (0.87)|
+|YOLO v4|Magnitude sparsity, 50%|COCO 2017|46.49 (0.58)|
 
 <a name="tensorflow_instance_segmentation"></a>
 #### Instance segmentation
 
-|TensorFlow Model|<img width="110" height="1">Compression algorithm<img width="110" height="1">|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|MaskRCNN|INT8 (per-tensor for weights)|COCO2017|bbox: 37.27 (0.06)<br/>segm: 33.54 (0.02)|
-|MaskRCNN|Sparsity 50% (Magnitude)|COCO2017|bbox: 36.93 (0.40)<br/>segm: 33.23 (0.33)|
+|Mask-R-CNN|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|37.19 (0.14)|
+|Mask-R-CNN|Magnitude sparsity, 50%|COCO 2017|36.94 (0.39)|
 
 ### ONNX models
 
 <a name="onnx_classification"></a>
 #### Classification
 
 |   ONNX Model    | Compression algorithm |Dataset|Accuracy (Drop) %|
@@ -546,19 +536,19 @@
     title =   {Neural network compression framework for fast model inference},
     author =  {Kozlov, Alexander and Lazarevich, Ivan and Shamporov, Vasily and Lyalyushkin, Nikolay and Gorbachev, Yury},
     journal = {arXiv preprint arXiv:2002.08679},
     year =    {2020}
 }
 ```
 
+## Contributing Guide
+Refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines on contributions to the NNCF repository.
+
 ## Useful links
 - [Documentation](./docs)
 - Example scripts (model objects available through links in respective README.md files):
     - [PyTorch](./examples/torch)
     - [TensorFlow](./examples/tensorflow)
 - [FAQ](./docs/FAQ.md)
 - [Notebooks](https://github.com/openvinotoolkit/openvino_notebooks#-model-training)
-- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov) utilizes NNCF as a compression backend within the renowned `transformers` repository.
-- [Model Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
-
-## Legal Information
-[*] Other names and brands may be claimed as the property of others.
+- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
+- [OpenVINO Model Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `nncf-2.4.0/licensing/third-party-programs.txt` & `nncf-2.5.0/licensing/third-party-programs.txt`

 * *Files 17% similar despite different names*

```diff
@@ -1127,8 +1127,420 @@
 FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
--------------------------------------------------------------
+-------------------------------------------------------------
+
+keras-team/keras
+Copyright 2015 The TensorFlow Authors. All Rights Reserved.
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+-------------------------------------------------------------
+rwightman/pytorch-image-models
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+-------------------------------------------------------------
```

### Comparing `nncf-2.4.0/nncf/__init__.py` & `nncf-2.5.0/nncf/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,38 +1,34 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Neural Network Compression Framework (NNCF) for enhanced OpenVINO inference.
 """
-from nncf.common.logging.logger import set_log_level
 from nncf.common.logging.logger import disable_logging
-from nncf.version import __version__
-
+from nncf.common.logging.logger import set_log_level
 from nncf.config import NNCFConfig
 from nncf.data import Dataset
-from nncf.parameters import IgnoredScope
+from nncf.parameters import DropType
 from nncf.parameters import ModelType
 from nncf.parameters import TargetDevice
 from nncf.quantization import QuantizationPreset
 from nncf.quantization import quantize
 from nncf.quantization import quantize_with_accuracy_control
+from nncf.scopes import IgnoredScope
+from nncf.version import __version__
 
-_LOADED_FRAMEWORKS = {
-    "torch": True,
-    "tensorflow": True,
-    "onnx": True,
-    "openvino": True
-}
+_LOADED_FRAMEWORKS = {"torch": True, "tensorflow": True, "onnx": True, "openvino": True}  # fmt: off
 
 try:
     import torch
 except ImportError:
     _LOADED_FRAMEWORKS["torch"] = False
 
 try:
@@ -43,20 +39,24 @@
 try:
     import onnx
 except ImportError:
     _LOADED_FRAMEWORKS["onnx"] = False
 
 try:
     import openvino.runtime as ov_runtime
-    import openvino.tools.pot as ov_pot
 except ImportError:
     _LOADED_FRAMEWORKS["openvino"] = False
 
 from nncf.common.logging import nncf_logger
+
 if not any(_LOADED_FRAMEWORKS.values()):
-    nncf_logger.error("Neither PyTorch, TensorFlow, ONNX or OpenVINO Python packages have been found in your Python "
-                      "environment.\n"
-                      "Please install one of the supported frameworks above in order to use NNCF on top of it.\n"
-                      "See the installation guide at https://github.com/openvinotoolkit/nncf#installation for help.")
+    nncf_logger.error(
+        "Neither PyTorch, TensorFlow, ONNX or OpenVINO Python packages have been found in your Python "
+        "environment.\n"
+        "Please install one of the supported frameworks above in order to use NNCF on top of it.\n"
+        "See the installation guide at https://github.com/openvinotoolkit/nncf#installation for help."
+    )
 else:
-    nncf_logger.info(f"NNCF initialized successfully. Supported frameworks detected: "
-                     f"{', '.join([name for name, loaded in _LOADED_FRAMEWORKS.items() if loaded])}")
+    nncf_logger.info(
+        f"NNCF initialized successfully. Supported frameworks detected: "
+        f"{', '.join([name for name, loaded in _LOADED_FRAMEWORKS.items() if loaded])}"
+    )
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `nncf-2.4.0/nncf/api/__init__.py` & `nncf-2.5.0/nncf/common/graph/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,12 +1,12 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from nncf.common.graph.graph import *
```

### Comparing `nncf-2.4.0/nncf/api/compression.py` & `nncf-2.5.0/nncf/api/compression.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,90 +1,88 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
 from enum import IntEnum
 from typing import Any, Dict, List, Optional, Tuple, TypeVar
 
 from nncf.api.statistics import Statistics
 from nncf.common.graph.transformations.layout import TransformationLayout
+from nncf.common.utils.api_marker import api
+from nncf.common.utils.backend import copy_model
 
-TModel = TypeVar('TModel')
+TModel = TypeVar("TModel")
 
 
+@api()
 class CompressionLoss(ABC):
     """
     Used to calculate the additional loss to be added to the base loss during the
     training process. It uses the model graph to measure variables and activations
     values of the layers during the loss construction. For example, the $L_0$-based
     sparsity algorithm calculates the number of non-zero weights in convolutional
     and fully-connected layers to construct the loss function.
     """
 
     @abstractmethod
     def calculate(self, *args, **kwargs) -> Any:
         """
-        Calculates the compression loss value.
-
-        :return: The compression loss value.
+        Calculates and returns the compression loss value.
         """
 
     @abstractmethod
     def load_state(self, state: Dict[str, Any]) -> None:
         """
         Loads the compression loss state.
 
-        :param state: Output of `get_state()` method.
+        :param state: The state of the compression loss, most likely obtained as the result of a `.get_state()` method
+            call.
         """
 
     @abstractmethod
     def get_state(self) -> Dict[str, Any]:
         """
         Returns the compression loss state.
-
-        :return: The compression loss state.
         """
 
     def __call__(self, *args, **kwargs) -> Any:
         """
-        Invokes the `CompressionLoss` instance.
-
-        :return: The compression loss value.
+        Calculates and returns the compression loss value. Same as `.calculate()`.
         """
         return self.calculate(*args, **kwargs)
 
 
+@api()
 class CompressionScheduler(ABC):
     """
     Implements the logic of compression method control during the training process.
-    May change the method hyperparameters in regards to the current training step
+    May change the method hyperparameters in regard to the current training step
     or epoch. For example, the sparsity method can smoothly increase the sparsity
     rate over several epochs.
 
     The `step()` and `epoch_step()` methods of the compression scheduler must be
-    called at the beginning of each training step and epoch, respectively.
+    called at the beginning of each training step and epoch, respectively:
+
+    ..  code-block:: python
+
+        for epoch in range(0, num_epochs):
+            scheduler.epoch_step()
+            for i, (x, y) in enumerate(dataset):
+                 scheduler.step()
+                 ...
 
-    ```
-    for epoch in range(0, num_epochs):
-        scheduler.epoch_step()
-        for i, (x, y) in enumerate(dataset):
-             scheduler.step()
-             ...
-    ```
     """
 
     @abstractmethod
     def step(self, next_step: Optional[int] = None) -> None:
         """
         Should be called at the beginning of each training step to prepare
         the compression method to continue training the model in the `next_step`.
@@ -112,120 +110,113 @@
         :param state: Output of `get_state()` method.
         """
 
     @abstractmethod
     def get_state(self) -> Dict[str, Any]:
         """
         Returns the compression scheduler state.
-
-        :return: The compression scheduler state.
         """
 
 
+@api()
 class CompressionStage(IntEnum):
     """
     Specifies the compression stage for the model.
     """
 
     UNCOMPRESSED = 0
     PARTIALLY_COMPRESSED = 1
     FULLY_COMPRESSED = 2
 
-    def __add__(self, other: 'CompressionStage') -> 'CompressionStage':
+    def __add__(self, other: "CompressionStage") -> "CompressionStage":
         """
         Defines compression stage of a composite compression controller, consist of
         two algorithms, where `self` is the compression stage of the first algorithm
         and other - compression stage of the second one.
-            UNCOMPRESSED    & UNCOMPRESSED    = UNCOMPRESSED
-            PARTIALLY_COMPRESSED & PARTIALLY_COMPRESSED = PARTIALLY_COMPRESSED
-            FULLY_COMPRESSED    & FULLY_COMPRESSED    = FULLY_COMPRESSED
-            UNCOMPRESSED    & PARTIALLY_COMPRESSED = PARTIALLY_COMPRESSED
-            UNCOMPRESSED    & FULLY_COMPRESSED    = PARTIALLY_COMPRESSED
-            PARTIALLY_COMPRESSED & FULLY_COMPRESSED    = PARTIALLY_COMPRESSED
+
+        * ``UNCOMPRESSED         & UNCOMPRESSED         == UNCOMPRESSED``
+        * ``PARTIALLY_COMPRESSED & PARTIALLY_COMPRESSED == PARTIALLY_COMPRESSED``
+        * ``FULLY_COMPRESSED     & FULLY_COMPRESSED     == FULLY_COMPRESSED``
+        * ``UNCOMPRESSED         & PARTIALLY_COMPRESSED == PARTIALLY_COMPRESSED``
+        * ``UNCOMPRESSED         & FULLY_COMPRESSED     == PARTIALLY_COMPRESSED``
+        * ``PARTIALLY_COMPRESSED & FULLY_COMPRESSED     == PARTIALLY_COMPRESSED``
 
         :param other: An instance of another compression stage.
         :return: The common compression stage of the two algorithms.
         """
         if self == other:
             return self
         return CompressionStage.PARTIALLY_COMPRESSED
 
 
+@api()
 class CompressionAlgorithmController(ABC):
     """
-    Serves as a handle to the additional modules, parameters and hooks inserted
-    into the original uncompressed model to enable algorithm-specific compression.
-    Hosts entities that are to be used during the training process, such as
-    compression scheduler and compression loss.
+    A handle to the compression-specific modifications made to the model.
+    Hosts entities that are to be used during the training process, such as compression scheduler and compression loss.
+
+    :param target_model: The model with additional modifications necessary
+        to enable algorithm-specific compression during fine-tuning built by the `CompressionAlgorithmBuilder`.
     """
 
     def __init__(self, target_model: TModel):
-        """
-        Initializes the internal state of the compression algorithm controller.
-
-        :param target_model: The model with additional modifications necessary
-            to enable algorithm-specific compression during fine-tuning built
-            by the `CompressionAlgorithmBuilder`.
-        """
         self._model = target_model
 
     @property
     def model(self) -> TModel:
         """
-        :return: The target model.
+        The compressed model object with which this controller is associated.
         """
         return self._model
 
     @property
     @abstractmethod
     def loss(self) -> CompressionLoss:
         """
-        :return: The instance of the `CompressionLoss`.
+        The compression loss for this particular algorithm combination.
         """
 
     @property
     @abstractmethod
     def scheduler(self) -> CompressionScheduler:
         """
-        :return: The instance of the `CompressionScheduler`.
+        The compression scheduler for this particular algorithm combination.
         """
 
     @property
     @abstractmethod
     def name(self) -> str:
         """
-        :return: name of the compression algorithm that is being controlled. Should be unique to identify the controller
-        and its state among other controllers and their states.
+        Name of the compression algorithm that is being controlled.
+        Should be unique to identify the controller and its state among other controllers and their states.
         """
 
     @abstractmethod
     def load_state(self, state: Dict[str, Dict[str, Any]]) -> None:
         """
         Loads the compression controller state from the map of algorithm name to the dictionary with state attributes.
 
         :param state: map of the algorithm name to the dictionary with the corresponding state attributes.
         """
 
     @abstractmethod
     def get_state(self) -> Dict[str, Dict[str, Any]]:
         """
-        Returns compression controller state, which is the map of the algorithm name to the dictionary with the
+        Returns the compression controller state, which is the map of the algorithm name to the dictionary with the
         corresponding state attributes.
-
-        :return: The compression controller state.
         """
 
     @abstractmethod
     def get_compression_state(self) -> Dict[str, Any]:
         """
-        Returns compression state - builder and controller state.
-        This state should be used to resume compression via `compression_state` argument of `create_compressed_model`
-        method.
+        Returns the compression state - builder and controller state.
+        This state should be used to unambiguously resume compression via `compression_state` argument of
+        `create_compressed_model` method.
 
-        :return: Compression state of the model to unambiguously resume compression from it.
+        :return: Compression state of the model to  resume compression from it.
         """
 
     def compression_stage(self) -> CompressionStage:
         """
         Returns the compression stage. Should be used on saving best checkpoints
         to distinguish between uncompressed, partially compressed, and fully
         compressed models.
@@ -237,30 +228,55 @@
     def statistics(self, quickly_collected_only: bool = False) -> Statistics:
         """
         Returns a `Statistics` class instance that contains compression algorithm statistics.
 
         :param quickly_collected_only: Enables collection of the statistics that
             don't take too much time to compute. Can be helpful for the case when
             need to keep track of statistics on each training batch/step/iteration.
-        :return: A `Statistics` class instance that contains compression algorithm statistics.
         """
 
+    def strip_model(self, model: TModel, do_copy: bool = False) -> TModel:
+        """
+        Strips auxiliary layers that were used for the model compression, as it's
+        only needed for training. The method is used before exporting the model
+        in the target format.
+
+        :param model: The compressed model.
+        :param do_copy: Modify copy of the model, defaults to False.
+        :return: The stripped model.
+        """
+        if do_copy:
+            model = copy_model(model)
+        return model
+
     def prepare_for_export(self) -> None:
         """
-        Prepare the compressed model for deployment.
+        Prepare the compressed model for exporting to a backend-specific model serialization format.
         """
         self._model = self.strip_model(self._model)
 
+    def strip(self, do_copy: bool = True) -> TModel:
+        """
+        Returns the model object with as much custom NNCF additions as possible removed
+        while still preserving the functioning of the model object as a compressed model.
+
+        :param do_copy: If True (default), will return a copy of the currently associated model object. If False,
+        will return the currently associated model object "stripped" in-place.
+        """
+        return self.strip_model(self.model, do_copy)
+
     @abstractmethod
-    def export_model(self,
-                     save_path: str,
-                     save_format: Optional[str] = None,
-                     input_names: Optional[List[str]] = None,
-                     output_names: Optional[List[str]] = None,
-                     model_args: Optional[Tuple[Any, ...]] = None) -> None:
+    def export_model(
+        self,
+        save_path: str,
+        save_format: Optional[str] = None,
+        input_names: Optional[List[str]] = None,
+        output_names: Optional[List[str]] = None,
+        model_args: Optional[Tuple[Any, ...]] = None,
+    ) -> None:
         """
         Exports the compressed model to the specified format for deployment.
 
         Makes method-specific preparations of the model, (e.g. removing auxiliary
         layers that were used for the model compression), then exports the model to
         the specified path.
 
@@ -268,38 +284,26 @@
         :param save_format: Saving format. The default format will
             be used if `save_format` is not specified.
         :param input_names: Names to be assigned to the input tensors of the model.
         :param output_names: Names to be assigned to the output tensors of the model.
         :param model_args: Tuple of additional positional and keyword arguments
             which are required for the model's forward during export. Should be
             specified in the following format:
-                - (a, b, {'x': None, 'y': y}) for positional and keyword arguments.
-                - (a, b, {}) for positional arguments only.
-                - ({'x': None, 'y': y},) for keyword arguments only.
-        """
 
-    def strip_model(self, model: TModel) -> TModel:
+            * (a, b, {'x': None, 'y': y}) for positional and keyword arguments.
+            * (a, b, {}) for positional arguments only.
+            * ({'x': None, 'y': y},) for keyword arguments only.
         """
-        Strips auxiliary layers that were used for the model compression, as it's
-        only needed for training. The method is used before exporting the model
-        in the target format.
-
-        :param model: The compressed model.
-        :return: The stripped model.
-        """
-        return model
 
     @property
     @abstractmethod
     def compression_rate(self) -> float:
         """
-        Returns a float compression rate value ranging from 0 to 1 (e.g. the sparsity level or
-        the ratio of filters pruned).
-
-        :return: Compression rate value
+        Returns a float compression rate value ranging from 0 to 1 (e.g. the sparsity level,
+        or the ratio of filters pruned).
         """
 
     @compression_rate.setter
     @abstractmethod
     def compression_rate(self, compression_rate: float) -> None:
         """
         Set a float compression rate value in the model (e.g. the sparsity
@@ -307,30 +311,31 @@
 
         :param compression_rate: The compressed rate value to be set.
         """
 
     @abstractmethod
     def disable_scheduler(self) -> None:
         """
-        Disables current compression scheduler during training by changing
-        it to a dummy one that does not change the compression rate.
+        Disables current compression scheduler during training by changing it to a dummy one that does not change
+        the compression rate.
         """
 
     @property
     @abstractmethod
     def maximal_compression_rate(self) -> float:
         """
         Returns the maximal model compression rate supported by the compression controller.
         """
 
 
+@api()
 class CompressionAlgorithmBuilder(ABC):
     """
-    Determines which modifications should be made to the original model in
-    order to enable algorithm-specific compression during fine-tuning.
+    Determines which modifications should be made to the original model in order to enable algorithm-specific
+    compression during fine-tuning.
     """
 
     @property
     @abstractmethod
     def name(self) -> str:
         """
         :return: name of the compression algorithm that is being built. Should be unique to identify the builder
@@ -346,21 +351,21 @@
         :return: The model with additional modifications necessary to enable
             algorithm-specific compression during fine-tuning.
         """
 
     @abstractmethod
     def build_controller(self, model: TModel) -> CompressionAlgorithmController:
         """
-        Builds `CompressionAlgorithmController` to handle the additional modules,
-        parameters, and hooks inserted into the model to enable algorithm-specific
+        Builds an instance of algorithm-specific `nncf.api.compression.CompressionAlgorithmController`
+        to handle the additional modules, parameters, and hooks inserted into the model to enable algorithm-specific
         compression.
 
         :param model: The model with additional modifications necessary to enable
             algorithm-specific compression during fine-tuning.
-        :return: The instance of the `CompressionAlgorithmController`.
+        :return: The instance of a `CompressionAlgorithmController`-derived class, specific for this algorithm.
         """
 
     @abstractmethod
     def get_transformation_layout(self, model: TModel) -> TransformationLayout:
         """
         Computes necessary model transformations to enable algorithm-specific
         compression.
@@ -369,35 +374,33 @@
         :return: The instance of the `TransformationLayout` class containing
             a list of algorithm-specific modifications.
         """
 
     @abstractmethod
     def initialize(self, model: TModel) -> None:
         """
-        Initialize model parameters before training
+        Initialize model parameters before training.
 
         :param model: The model with additional modifications necessary to enable
             algorithm-specific compression during fine-tuning.
         """
 
     @abstractmethod
     def load_state(self, state: Dict[str, Any]) -> None:
         """
-        Initializes object from the state.
+        Initializes object from the supplied state.
 
-        :param state: Output of `get_state()` method.
+        :param state: The state of the builder, most likely obtained as the result of a `.get_state()` call.
         """
 
     @abstractmethod
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
-
-        :return: state of the object
         """
 
 
 class CompressionLevel(IntEnum):
     """
     Legacy class, now replaced by CompressionStage.
     Supports backward compatibility of older checkpoints produced with NNCF.
```

### Comparing `nncf-2.4.0/nncf/api/statistics.py` & `nncf-2.5.0/nncf/common/collector.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,28 +1,27 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
 
+from nncf.api.statistics import Statistics
 
-class Statistics(ABC):
+
+class StatisticsCollector(ABC):
     """
-    Contains a data collection and provides a way for its human-readable representation.
+    Encapsulates the logic of the statistics collection.
     """
 
     @abstractmethod
-    def to_str(self) -> str:
+    def collect(self) -> Statistics:
         """
-        Returns a representation of the statistics as a human-readable string.
-
-        :return: A representation of the statistics as a human-readable string.
+        Collects statistics. The logic of the statistics collection should be implemented here.
         """
```

### Comparing `nncf-2.4.0/nncf/common/__init__.py` & `nncf-2.5.0/nncf/tensorflow/helpers/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,12 +1,12 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from nncf.tensorflow.helpers.model_creation import create_compressed_model
```

### Comparing `nncf-2.4.0/nncf/common/accuracy_aware_training/__init__.py` & `nncf-2.5.0/nncf/common/accuracy_aware_training/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,16 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Accuracy Aware Training functionality.
 """
 
-from nncf.common.accuracy_aware_training.training_loop import create_accuracy_aware_training_loop
 from nncf.common.accuracy_aware_training.training_loop import AccuracyAwareTrainingMode
+from nncf.common.accuracy_aware_training.training_loop import create_accuracy_aware_training_loop
```

### Comparing `nncf-2.4.0/nncf/common/accuracy_aware_training/runner.py` & `nncf-2.5.0/nncf/common/accuracy_aware_training/runner.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,45 +1,43 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import io
 import os
 import os.path as osp
 import pathlib
 from abc import ABC
 from abc import abstractmethod
-from typing import Callable, Dict, List, Optional, Union, Tuple, TypeVar
+from typing import Callable, Dict, List, Optional, Tuple, TypeVar, Union
 
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.api.compression import CompressionStage
-from nncf.common.utils.helpers import configure_accuracy_aware_paths
 from nncf.common.logging import nncf_logger
+from nncf.common.utils.helpers import configure_accuracy_aware_paths
 from nncf.common.utils.tensorboard import prepare_for_tensorboard
 from nncf.config.schemata.defaults import AA_COMPRESSION_RATE_STEP_REDUCTION_FACTOR
 from nncf.config.schemata.defaults import AA_INITIAL_COMPRESSION_RATE_STEP
 from nncf.config.schemata.defaults import AA_INITIAL_TRAINING_PHASE_EPOCHS
 from nncf.config.schemata.defaults import AA_LR_REDUCTION_FACTOR
 from nncf.config.schemata.defaults import AA_MAXIMAL_TOTAL_EPOCHS
 from nncf.config.schemata.defaults import AA_MINIMAL_COMPRESSION_RATE_STEP
 from nncf.config.schemata.defaults import AA_PATIENCE_EPOCHS
 
-TModel = TypeVar('TModel')
-OptimizerType = TypeVar('OptimizerType')
-LRSchedulerType = TypeVar('LRSchedulerType')
-TensorboardWriterType = TypeVar('TensorboardWriterType')
+TModel = TypeVar("TModel")
+OptimizerType = TypeVar("OptimizerType")
+LRSchedulerType = TypeVar("LRSchedulerType")
+TensorboardWriterType = TypeVar("TensorboardWriterType")
 
 try:
     import matplotlib.pyplot as plt
     import PIL.Image
 
     IMG_PACKAGES_AVAILABLE = True
 except ImportError:
@@ -48,18 +46,14 @@
 
 class TrainingRunner(ABC):
     """
     Runner is an object that is used by a TrainingLoop instance to control the training process
     via wrapping user-supplied functions such as `train_epoch_fn` and `validate_fn`.
     """
 
-    uncompressed_model_accuracy: float
-    maximal_total_epochs: int
-    minimal_tolerable_accuracy: float
-
     @abstractmethod
     def train_epoch(self, model: TModel, compression_controller: CompressionAlgorithmController) -> None:
         """
         Calls train_epoch_fn and compression_controller.scheduler.epoch_step()
 
         :param model: The model to be fine-tuned
         :param compression_controller: The compression controller to be used during
@@ -110,56 +104,52 @@
     @abstractmethod
     def reset_training(self) -> None:
         """
         Initialize all-training related parameters (e.g. epoch count, optimizer, learning rate scheduler).
         """
 
     @abstractmethod
-    def retrieve_uncompressed_model_accuracy(self, model: TModel) -> None:
-        """
-        :param model: The model object to retrieve the original accuracy value from.
-
-        Retrive the original uncompressed model accuracy from the model instance and
-        set the obtained value to the `uncompressed_model_accuracy` attribute of the TrainingRunner.
-        """
-
-    @abstractmethod
     def calculate_minimal_tolerable_accuracy(self, uncompressed_model_accuracy: float) -> None:
         """
         :param uncompressed_model_accuracy: The uncompressed model accuracy.
 
         Calculate the minimal tolerable accuracy from thr uncompressed_model_accuracy and
         set the obtained value to the `minimal_tolerable_accuracy` attribute of the TrainingRunner.
         """
 
     @abstractmethod
-    def initialize_training_loop_fns(self, train_epoch_fn: Callable[[CompressionAlgorithmController, TModel,
-                                                                     Optional[int],
-                                                                     Optional[OptimizerType],
-                                                                     Optional[LRSchedulerType]], float],
-                                     validate_fn: Callable[[TModel, Optional[int]], float],
-                                     configure_optimizers_fn: Callable[[], Tuple[OptimizerType, LRSchedulerType]],
-                                     dump_checkpoint_fn: Callable[
-                                         [TModel, CompressionAlgorithmController, 'TrainingRunner', str], None],
-                                     **kwargs):
+    def initialize_training_loop_fns(
+        self,
+        train_epoch_fn: Callable[
+            [CompressionAlgorithmController, TModel, Optional[int], Optional[OptimizerType], Optional[LRSchedulerType]],
+            float,
+        ],
+        validate_fn: Callable[[TModel, Optional[int]], float],
+        configure_optimizers_fn: Callable[[], Tuple[OptimizerType, LRSchedulerType]],
+        dump_checkpoint_fn: Callable[[TModel, CompressionAlgorithmController, "TrainingRunner", str], None],
+        **kwargs,
+    ):
         """
         Register the user-supplied functions to be used to control the training process.
 
         :param train_epoch_fn: a method to fine-tune the model for a single epoch
         (to be called inside the `train_epoch` of the TrainingRunner).
         :param validate_fn: a method to evaluate the model on the validation dataset
         (to be called inside the `train_epoch` of the TrainingRunner).
         :param configure_optimizers_fn: a method to instantiate an optimizer and a learning
         rate scheduler (to be called inside the `configure_optimizers` of the TrainingRunner).
         :param dump_checkpoint_fn: a method to dump a checkpoint.
         """
 
     @abstractmethod
-    def initialize_logging(self, log_dir: Optional[Union[str, pathlib.Path]] = None,
-                           tensorboard_writer: Optional[TensorboardWriterType] = None):
+    def initialize_logging(
+        self,
+        log_dir: Optional[Union[str, pathlib.Path]] = None,
+        tensorboard_writer: Optional[TensorboardWriterType] = None,
+    ):
         """
         Initialize logging related variables
 
         :param log_dir: The path to be used for logging and checkpoint saving.
         :param tensorboard_writer: The tensorboard object to be used for logging.
         """
 
@@ -184,21 +174,30 @@
 
 class BaseAccuracyAwareTrainingRunner(TrainingRunner):
     """
     The base accuracy-aware training Runner object,
     initialized with the default parameters unless specified in the config.
     """
 
-    def __init__(self, accuracy_aware_training_params: Dict[str, object], verbose=True,
-                 dump_checkpoints=True, lr_updates_needed=True):
+    def __init__(
+        self,
+        accuracy_aware_training_params: Dict[str, object],
+        uncompressed_model_accuracy: float,
+        verbose: bool = True,
+        dump_checkpoints: bool = True,
+        lr_updates_needed: bool = True,
+    ):
+        self.uncompressed_model_accuracy = uncompressed_model_accuracy
         self.maximal_relative_accuracy_drop = accuracy_aware_training_params.get(
-            'maximal_relative_accuracy_degradation', 1.0)
+            "maximal_relative_accuracy_degradation", 1.0
+        )
         self.maximal_absolute_accuracy_drop = accuracy_aware_training_params.get(
-            'maximal_absolute_accuracy_degradation')
-        self.maximal_total_epochs = accuracy_aware_training_params.get('maximal_total_epochs', AA_MAXIMAL_TOTAL_EPOCHS)
+            "maximal_absolute_accuracy_degradation"
+        )
+        self.maximal_total_epochs = accuracy_aware_training_params.get("maximal_total_epochs", AA_MAXIMAL_TOTAL_EPOCHS)
 
         self.verbose = verbose
         self.dump_checkpoints = dump_checkpoints
 
         self.base_lr_reduction_factor_during_search = 1.0
         self.lr_updates_needed = lr_updates_needed
 
@@ -227,48 +226,55 @@
         self._log_dir = None
         self._checkpoint_save_dir = None
         self._tensorboard_writer = None
 
     def train_epoch(self, model, compression_controller):
         compression_controller.scheduler.epoch_step()
         # assuming that epoch number is only used for logging in train_fn:
-        self.current_loss = self._train_epoch_fn(compression_controller,
-                                                 model,
-                                                 epoch=self.cumulative_epoch_count,
-                                                 optimizer=self.optimizer,
-                                                 lr_scheduler=self.lr_scheduler)
+        self.current_loss = self._train_epoch_fn(
+            compression_controller,
+            model,
+            epoch=self.cumulative_epoch_count,
+            optimizer=self.optimizer,
+            lr_scheduler=self.lr_scheduler,
+        )
         self.training_epoch_count += 1
         self.cumulative_epoch_count += 1
 
     def dump_statistics(self, model, compression_controller):
         statistics = compression_controller.statistics()
 
         if self.verbose:
             nncf_logger.info(statistics.to_str())
 
-        self.add_tensorboard_scalar('val/accuracy_aware/metric_value',
-                                    self.current_val_metric_value, self.cumulative_epoch_count)
+        self.add_tensorboard_scalar(
+            "val/accuracy_aware/metric_value", self.current_val_metric_value, self.cumulative_epoch_count
+        )
 
         for key, value in prepare_for_tensorboard(statistics).items():
             if isinstance(value, (int, float)):
-                self.add_tensorboard_scalar('compression/statistics/{0}'.format(key),
-                                            value, self.cumulative_epoch_count)
+                self.add_tensorboard_scalar(
+                    "compression/statistics/{0}".format(key), value, self.cumulative_epoch_count
+                )
 
         self.dump_checkpoint(model, compression_controller)
 
     def calculate_minimal_tolerable_accuracy(self, uncompressed_model_accuracy: float):
         if self.maximal_absolute_accuracy_drop is not None:
             self.minimal_tolerable_accuracy = uncompressed_model_accuracy - self.maximal_absolute_accuracy_drop
         else:
-            self.minimal_tolerable_accuracy = uncompressed_model_accuracy * \
-                                              (1 - 0.01 * self.maximal_relative_accuracy_drop)
+            self.minimal_tolerable_accuracy = uncompressed_model_accuracy * (
+                1 - 0.01 * self.maximal_relative_accuracy_drop
+            )
 
     def dump_checkpoint(self, model, compression_controller):
-        is_best_checkpoint = (self.best_val_metric_value == self.current_val_metric_value and
-                              self.is_model_fully_compressed(compression_controller))
+        is_best_checkpoint = (
+            self.best_val_metric_value == self.current_val_metric_value
+            and self.is_model_fully_compressed(compression_controller)
+        )
         if not self.dump_checkpoints and not is_best_checkpoint:
             return
 
         if self._dump_checkpoint_fn is not None:
             checkpoint_path = self._dump_checkpoint_fn(model, compression_controller, self, self._checkpoint_save_dir)
         else:
             checkpoint_path = self._make_checkpoint_path(is_best=False)
@@ -277,44 +283,52 @@
 
         if is_best_checkpoint:
             self._save_best_checkpoint(model, compression_controller)
 
     def configure_optimizers(self):
         self.optimizer, self.lr_scheduler = self._configure_optimizers_fn()
 
-    def initialize_training_loop_fns(self, train_epoch_fn, validate_fn, configure_optimizers_fn, dump_checkpoint_fn,
-                                     load_checkpoint_fn=None, early_stopping_fn=None, update_learning_rate_fn=None):
+    def initialize_training_loop_fns(
+        self,
+        train_epoch_fn,
+        validate_fn,
+        configure_optimizers_fn,
+        dump_checkpoint_fn,
+        load_checkpoint_fn=None,
+        early_stopping_fn=None,
+        update_learning_rate_fn=None,
+    ):
         self._train_epoch_fn = train_epoch_fn
         self._validate_fn = validate_fn
         self._configure_optimizers_fn = configure_optimizers_fn
         self._dump_checkpoint_fn = dump_checkpoint_fn
         self._load_checkpoint_fn = load_checkpoint_fn
         self._early_stopping_fn = early_stopping_fn
         self._update_learning_rate_fn = update_learning_rate_fn
 
     def initialize_logging(self, log_dir=None, tensorboard_writer=None):
-        self._log_dir = log_dir if log_dir is not None else osp.join(os.getcwd(), 'runs')
+        self._log_dir = log_dir if log_dir is not None else osp.join(os.getcwd(), "runs")
         self._log_dir = configure_accuracy_aware_paths(self._log_dir)
         self._checkpoint_save_dir = self._log_dir
         self._tensorboard_writer = tensorboard_writer
 
     def stop_training(self, compression_controller):
         if self.is_model_fully_compressed(compression_controller) and self._early_stopping_fn is not None:
             return self._early_stopping_fn(self.current_val_metric_value)
         return False
 
     def _save_best_checkpoint(self, model, compression_controller):
         best_path = self._make_checkpoint_path(is_best=True)
         self._best_checkpoint = (best_path, compression_controller.compression_rate)
         self._save_checkpoint(model, compression_controller, best_path)
-        nncf_logger.info(f'Saved the best model to {best_path}')
+        nncf_logger.info(f"Saved the best model to {best_path}")
 
     def load_best_checkpoint(self, model):
         resuming_checkpoint_path, compression_rate = self._best_checkpoint
-        nncf_logger.info(f'Loading the best checkpoint found during training: {resuming_checkpoint_path}')
+        nncf_logger.info(f"Loading the best checkpoint found during training: {resuming_checkpoint_path}")
         self._load_checkpoint(model, resuming_checkpoint_path)
         return compression_rate
 
     def is_model_fully_compressed(self, compression_controller) -> bool:
         return compression_controller.compression_stage() == CompressionStage.FULLY_COMPRESSED
 
     @abstractmethod
@@ -334,16 +348,17 @@
 
         :param key: Image key name
         :param data: Image data
         :param step: Logging step
         """
 
     @abstractmethod
-    def _save_checkpoint(self, model: TModel, compression_controller: CompressionAlgorithmController,
-                         checkpoint_path: str) -> None:
+    def _save_checkpoint(
+        self, model: TModel, compression_controller: CompressionAlgorithmController, checkpoint_path: str
+    ) -> None:
         """
         Save a model to the disk.
 
         :param model: The model to be saved
         :param checkpoint_path: The path to save the checkpoint to
         :param compression_controller: The compression controller to be used during
             model fine-tuning
@@ -370,29 +385,42 @@
 
 class BaseAdaptiveCompressionLevelTrainingRunner(BaseAccuracyAwareTrainingRunner):
     """
     The base adaptive compression level accuracy-aware training Runner object,
     initialized with the default parameters unless specified in the config.
     """
 
-    def __init__(self, accuracy_aware_training_params: Dict[str, object], verbose=True,
-                 dump_checkpoints=True, lr_updates_needed=True,
-                 minimal_compression_rate=0.0, maximal_compression_rate=0.95):
-        super().__init__(accuracy_aware_training_params, verbose, dump_checkpoints, lr_updates_needed)
-
-        self.compression_rate_step = accuracy_aware_training_params.get('initial_compression_rate_step',
-                                                                        AA_INITIAL_COMPRESSION_RATE_STEP)
+    def __init__(
+        self,
+        accuracy_aware_training_params: Dict[str, object],
+        uncompressed_model_accuracy: float,
+        verbose: bool = True,
+        dump_checkpoints: bool = True,
+        lr_updates_needed: bool = True,
+        minimal_compression_rate: float = 0.0,
+        maximal_compression_rate: float = 0.95,
+    ):
+        super().__init__(
+            accuracy_aware_training_params, uncompressed_model_accuracy, verbose, dump_checkpoints, lr_updates_needed
+        )
+
+        self.compression_rate_step = accuracy_aware_training_params.get(
+            "initial_compression_rate_step", AA_INITIAL_COMPRESSION_RATE_STEP
+        )
         self.compression_rate_step_reduction_factor = accuracy_aware_training_params.get(
-            'compression_rate_step_reduction_factor', AA_COMPRESSION_RATE_STEP_REDUCTION_FACTOR)
-        self.lr_reduction_factor = accuracy_aware_training_params.get('lr_reduction_factor', AA_LR_REDUCTION_FACTOR)
-        self.minimal_compression_rate_step = accuracy_aware_training_params.get('minimal_compression_rate_step',
-                                                                                AA_MINIMAL_COMPRESSION_RATE_STEP)
-        self.patience_epochs = accuracy_aware_training_params.get('patience_epochs', AA_PATIENCE_EPOCHS)
-        self.initial_training_phase_epochs = accuracy_aware_training_params.get('initial_training_phase_epochs',
-                                                                                AA_INITIAL_TRAINING_PHASE_EPOCHS)
+            "compression_rate_step_reduction_factor", AA_COMPRESSION_RATE_STEP_REDUCTION_FACTOR
+        )
+        self.lr_reduction_factor = accuracy_aware_training_params.get("lr_reduction_factor", AA_LR_REDUCTION_FACTOR)
+        self.minimal_compression_rate_step = accuracy_aware_training_params.get(
+            "minimal_compression_rate_step", AA_MINIMAL_COMPRESSION_RATE_STEP
+        )
+        self.patience_epochs = accuracy_aware_training_params.get("patience_epochs", AA_PATIENCE_EPOCHS)
+        self.initial_training_phase_epochs = accuracy_aware_training_params.get(
+            "initial_training_phase_epochs", AA_INITIAL_TRAINING_PHASE_EPOCHS
+        )
 
         self.minimal_compression_rate = minimal_compression_rate
         self.maximal_compression_rate = maximal_compression_rate
 
         self._best_checkpoints = {}
         self._compression_rate_target = None
         self.adaptive_controller = None
@@ -402,43 +430,48 @@
         self.update_training_history(self.compression_rate_target, self.current_val_metric_value)
         super().dump_statistics(model, compression_controller)
 
     def _save_best_checkpoint(self, model, compression_controller):
         best_path = self._make_checkpoint_path(is_best=True, compression_rate=self.compression_rate_target)
 
         accuracy_budget = self.best_val_metric_value - self.minimal_tolerable_accuracy
-        if self.compression_rate_target in self._best_checkpoints and \
-                self._best_checkpoints[self.compression_rate_target][1] >= accuracy_budget:
+        if (
+            self.compression_rate_target in self._best_checkpoints
+            and self._best_checkpoints[self.compression_rate_target][1] >= accuracy_budget
+        ):
             return
 
         self._best_checkpoints[self.compression_rate_target] = (best_path, accuracy_budget)
         self._save_checkpoint(model, compression_controller, best_path)
-        nncf_logger.info(f'Saved the best model to {best_path}')
+        nncf_logger.info(f"Saved the best model to {best_path}")
 
     def load_best_checkpoint(self, model):
         # load checkpoint with the highest compression rate and positive acc budget
         possible_checkpoint_rates = self.get_compression_rates_with_positive_acc_budget()
         if len(possible_checkpoint_rates) == 0:
-            nncf_logger.warning('Could not produce a compressed model satisfying the set accuracy '
-                                'degradation criterion during training. Increasing the number of training '
-                                'epochs')
+            nncf_logger.warning(
+                "Could not produce a compressed model satisfying the set accuracy "
+                "degradation criterion during training. Increasing the number of training "
+                "epochs"
+            )
             return self.compression_rate_target
 
         best_checkpoint_compression_rate = None
         for checkpoint_rate in sorted(possible_checkpoint_rates, key=lambda x: -x):
             if checkpoint_rate in self._best_checkpoints:
                 best_checkpoint_compression_rate = checkpoint_rate
                 break
         if best_checkpoint_compression_rate is None:
-            nncf_logger.error('Could not load the model - no models with positive accuracy budget in '
-                              'compression training history.')
+            nncf_logger.error(
+                "Could not load the model - no models with positive accuracy budget in compression training history."
+            )
             return self.compression_rate_target
 
         resuming_checkpoint_path = self._best_checkpoints[best_checkpoint_compression_rate][0]
-        nncf_logger.info(f'Loading the best checkpoint found during training: {resuming_checkpoint_path}')
+        nncf_logger.info(f"Loading the best checkpoint found during training: {resuming_checkpoint_path}")
         self._load_checkpoint(model, resuming_checkpoint_path)
         return best_checkpoint_compression_rate
 
     @property
     def compression_rate_target(self):
         if self._compression_rate_target is None:
             return self.adaptive_controller.compression_rate
@@ -449,23 +482,28 @@
         self._compression_rate_target = value
 
     def update_training_history(self, compression_rate, metric_value):
         accuracy_budget = metric_value - self.minimal_tolerable_accuracy
         self._compressed_training_history.append((compression_rate, accuracy_budget))
 
         if IMG_PACKAGES_AVAILABLE:
-            plt.figure()
-            plt.plot(self.compressed_training_history.keys(),
-                     self.compressed_training_history.values())
+            backend = plt.get_backend()
+            plt.switch_backend("agg")
+            plt.ioff()
+            fig = plt.figure()
+            plt.plot(self.compressed_training_history.keys(), self.compressed_training_history.values())
             buf = io.BytesIO()
-            plt.savefig(buf, format='jpeg')
+            plt.savefig(buf, format="jpeg")
             buf.seek(0)
             image = PIL.Image.open(buf)
-            self.add_tensorboard_image('compression/accuracy_aware/acc_budget_vs_comp_rate', image,
-                                       len(self.compressed_training_history))
+            self.add_tensorboard_image(
+                "compression/accuracy_aware/acc_budget_vs_comp_rate", image, len(self.compressed_training_history)
+            )
+            plt.close(fig)
+            plt.switch_backend(backend)
 
     @property
     def compressed_training_history(self):
         return dict(self._compressed_training_history)
 
     def get_compression_rates_with_positive_acc_budget(self) -> List[float]:
         return [comp_rate for (comp_rate, acc_budget) in self._compressed_training_history if acc_budget >= 0]
```

### Comparing `nncf-2.4.0/nncf/common/accuracy_aware_training/runner_factory.py` & `nncf-2.5.0/nncf/common/accuracy_aware_training/runner_factory.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,30 +1,28 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
 from typing import Dict
 
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.accuracy_aware_training.runner import BaseAccuracyAwareTrainingRunner
 from nncf.common.accuracy_aware_training.runner import BaseAdaptiveCompressionLevelTrainingRunner
 from nncf.common.accuracy_aware_training.runner import TrainingRunner
-from nncf.common.utils.backend import infer_backend_from_compression_controller
 from nncf.common.utils.backend import BackendType
+from nncf.common.utils.backend import get_backend
 
 
 class TrainingRunnerCreator(ABC):
     """
     Declares the factory method returning TrainingRunner object
     """
 
@@ -34,74 +32,111 @@
 
 
 class EarlyExitTrainingRunnerCreator(TrainingRunnerCreator):
     """
     Class creates an Early Exit Training Runner depending on an used backend.
     """
 
-    def __init__(self, accuracy_aware_training_params: Dict[str, object],
-                 compression_controller: CompressionAlgorithmController,
-                 verbose: bool, dump_checkpoints: bool, lr_updates_needed: bool):
+    def __init__(
+        self,
+        accuracy_aware_training_params: Dict[str, object],
+        compression_controller: CompressionAlgorithmController,
+        uncompressed_model_accuracy: float,
+        verbose: bool,
+        dump_checkpoints: bool,
+        lr_updates_needed: bool,
+    ):
         self.accuracy_aware_training_params = accuracy_aware_training_params
         self.compression_controller = compression_controller
         self.lr_updates_needed = lr_updates_needed
         self.verbose = verbose
         self.dump_checkpoints = dump_checkpoints
+        self.uncompressed_model_accuracy = uncompressed_model_accuracy
 
     def create_training_loop(self) -> BaseAccuracyAwareTrainingRunner:
         """
         Creates an object of AccuracyAwareTrainingRunner depending on user backend
 
         :return: AccuracyAwareTrainingRunner object
         """
-        nncf_backend = infer_backend_from_compression_controller(self.compression_controller)
+        nncf_backend = get_backend(self.compression_controller.model)
         if nncf_backend is BackendType.TORCH:
             from nncf.torch.accuracy_aware_training.runner import PTAccuracyAwareTrainingRunner
-            return PTAccuracyAwareTrainingRunner(self.accuracy_aware_training_params, self.verbose,
-                                                 self.dump_checkpoints, self.lr_updates_needed)
+
+            return PTAccuracyAwareTrainingRunner(
+                self.accuracy_aware_training_params,
+                self.uncompressed_model_accuracy,
+                self.verbose,
+                self.dump_checkpoints,
+                self.lr_updates_needed,
+            )
         if nncf_backend == BackendType.TENSORFLOW:
             from nncf.tensorflow.accuracy_aware_training.runner import TFAccuracyAwareTrainingRunner
-            return TFAccuracyAwareTrainingRunner(self.accuracy_aware_training_params, self.verbose,
-                                                 self.dump_checkpoints, self.lr_updates_needed)
-        raise RuntimeError('Got an unsupported value of nncf_backend')
+
+            return TFAccuracyAwareTrainingRunner(
+                self.accuracy_aware_training_params,
+                self.uncompressed_model_accuracy,
+                self.verbose,
+                self.dump_checkpoints,
+                self.lr_updates_needed,
+            )
+        raise RuntimeError("Got an unsupported value of nncf_backend")
 
 
 class AdaptiveCompressionLevelTrainingRunnerCreator(TrainingRunnerCreator):
     """
     Class creates an Adaptive Compression Level Training Runner depending on an used backend.
     """
 
-    def __init__(self, accuracy_aware_training_params: Dict[str, object],
-                 compression_controller: CompressionAlgorithmController,
-                 verbose: bool, dump_checkpoints: bool, lr_updates_needed: bool,
-                 minimal_compression_rate: float, maximal_compression_rate: float):
+    def __init__(
+        self,
+        accuracy_aware_training_params: Dict[str, object],
+        compression_controller: CompressionAlgorithmController,
+        uncompressed_model_accuracy: float,
+        verbose: bool,
+        dump_checkpoints: bool,
+        lr_updates_needed: bool,
+        minimal_compression_rate: float,
+        maximal_compression_rate: float,
+    ):
         self.accuracy_aware_training_params = accuracy_aware_training_params
         self.compression_controller = compression_controller
+        self.uncompressed_model_accuracy = uncompressed_model_accuracy
         self.lr_updates_needed = lr_updates_needed
         self.verbose = verbose
         self.minimal_compression_rate = minimal_compression_rate
         self.maximal_compression_rate = maximal_compression_rate
         self.dump_checkpoints = dump_checkpoints
 
     def create_training_loop(self) -> BaseAdaptiveCompressionLevelTrainingRunner:
         """
         Creates an object of AdaptiveCompressionLevelTrainingRunner depending on user backend
 
         :return: AdaptiveCompressionLevelTrainingRunner object
         """
-        nncf_backend = infer_backend_from_compression_controller(self.compression_controller)
+        nncf_backend = get_backend(self.compression_controller.model)
 
         if nncf_backend is BackendType.TORCH:
             from nncf.torch.accuracy_aware_training.runner import PTAdaptiveCompressionLevelTrainingRunner
-            return PTAdaptiveCompressionLevelTrainingRunner(self.accuracy_aware_training_params,
-                                                            self.verbose, self.dump_checkpoints,
-                                                            self.lr_updates_needed,
-                                                            self.minimal_compression_rate,
-                                                            self.maximal_compression_rate)
+
+            return PTAdaptiveCompressionLevelTrainingRunner(
+                self.accuracy_aware_training_params,
+                self.uncompressed_model_accuracy,
+                self.verbose,
+                self.dump_checkpoints,
+                self.lr_updates_needed,
+                self.minimal_compression_rate,
+                self.maximal_compression_rate,
+            )
         if nncf_backend == BackendType.TENSORFLOW:
             from nncf.tensorflow.accuracy_aware_training.runner import TFAdaptiveCompressionLevelTrainingRunner
-            return TFAdaptiveCompressionLevelTrainingRunner(self.accuracy_aware_training_params,
-                                                            self.verbose, self.dump_checkpoints,
-                                                            self.lr_updates_needed,
-                                                            self.minimal_compression_rate,
-                                                            self.maximal_compression_rate)
-        raise RuntimeError('Got an unsupported value of nncf_backend')
+
+            return TFAdaptiveCompressionLevelTrainingRunner(
+                self.accuracy_aware_training_params,
+                self.uncompressed_model_accuracy,
+                self.verbose,
+                self.dump_checkpoints,
+                self.lr_updates_needed,
+                self.minimal_compression_rate,
+                self.maximal_compression_rate,
+            )
+        raise RuntimeError("Got an unsupported value of nncf_backend")
```

### Comparing `nncf-2.4.0/nncf/common/accuracy_aware_training/statistics.py` & `nncf-2.5.0/nncf/common/accuracy_aware_training/statistics.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,39 +1,41 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from dataclasses import dataclass
 
 from nncf.api.statistics import Statistics
+from nncf.common.utils.api_marker import api
 
 
+@api()
 @dataclass
 class TrainingLoopStatistics(Statistics):
     """
     Contains statistics related to Accuracy Aware Training Loop
     """
 
-    original_accuracy: float
+    uncompressed_accuracy: float
     compression_rate: float
     compressed_accuracy: float
     absolute_accuracy_degradation: float
     relative_accuracy_degradation: float
     accuracy_budget: float
 
     def to_str(self):
-        stats_str = f'Original model accuracy: {self.original_accuracy:.4f}\n' \
-                    f'Compressed model accuracy: {self.compressed_accuracy:.4f}\n' \
-                    f'Model compression rate: {self.compression_rate:.4f}\n' \
-                    f'Absolute accuracy drop: {self.absolute_accuracy_degradation:.4f}\n' \
-                    f'Relative accuracy drop: {self.relative_accuracy_degradation:.2f}%\n' \
-                    f'Accuracy budget: {self.accuracy_budget:.4f}'
+        stats_str = (
+            f"Uncompressed model accuracy: {self.uncompressed_accuracy:.4f}\n"
+            f"Compressed model accuracy: {self.compressed_accuracy:.4f}\n"
+            f"Model compression rate: {self.compression_rate:.4f}\n"
+            f"Absolute accuracy drop: {self.absolute_accuracy_degradation:.4f}\n"
+            f"Relative accuracy drop: {self.relative_accuracy_degradation:.2f}%\n"
+            f"Accuracy budget: {self.accuracy_budget:.4f}"
+        )
         return stats_str
```

### Comparing `nncf-2.4.0/nncf/common/accuracy_aware_training/training_loop.py` & `nncf-2.5.0/nncf/common/accuracy_aware_training/training_loop.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,152 +1,187 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Implementations of training loops to be used for accuracy aware training.
 """
-
+import pathlib
 from abc import ABC
 from abc import abstractmethod
 from functools import partial
-from typing import TypeVar
+from typing import Callable, Optional, TypeVar, Union
 
 import numpy as np
 from scipy.interpolate import interp1d
 
 from nncf.api.compression import CompressionAlgorithmController
+from nncf.common.accuracy_aware_training.runner_factory import AdaptiveCompressionLevelTrainingRunnerCreator
+from nncf.common.accuracy_aware_training.runner_factory import EarlyExitTrainingRunnerCreator
+from nncf.common.accuracy_aware_training.statistics import TrainingLoopStatistics
 from nncf.common.composite_compression import CompositeCompressionAlgorithmController
 from nncf.common.logging import nncf_logger
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.registry import Registry
 from nncf.config.config import NNCFConfig
 from nncf.config.extractors import extract_accuracy_aware_training_params
-from nncf.common.accuracy_aware_training.runner_factory import EarlyExitTrainingRunnerCreator
-from nncf.common.accuracy_aware_training.runner_factory import AdaptiveCompressionLevelTrainingRunnerCreator
-from nncf.common.accuracy_aware_training.statistics import TrainingLoopStatistics
+from nncf.config.structures import ModelEvaluationArgs
 
-TModel = TypeVar('TModel')
-ADAPTIVE_COMPRESSION_CONTROLLERS = Registry('adaptive_compression_controllers')
+TModel = TypeVar("TModel")
+TensorboardWriterType = TypeVar("TensorboardWriterType")
+ADAPTIVE_COMPRESSION_CONTROLLERS = Registry("adaptive_compression_controllers")
 
 
+@api()
 class TrainingLoop(ABC):
     """
-    The training loop object is instantiated by the user, the training process
-    is launched via the `run` method.
+    The training loop object that launches the training process via the `run` method.
     """
 
     @abstractmethod
-    def run(self, model: TModel, train_epoch_fn, validate_fn, configure_optimizers_fn=None,
-            dump_checkpoint_fn=None, load_checkpoint_fn=None, early_stopping_fn=None,
-            tensorboard_writer=None, log_dir=None):
+    def run(
+        self,
+        model: TModel,
+        train_epoch_fn: Callable,
+        validate_fn: Callable,
+        configure_optimizers_fn: Callable = None,
+        dump_checkpoint_fn: Callable = None,
+        load_checkpoint_fn: Callable = None,
+        early_stopping_fn: Callable = None,
+        tensorboard_writer: Optional[TensorboardWriterType] = None,
+        log_dir: Union[pathlib.Path, str] = None,
+        update_learning_rate_fn: Callable = None,
+    ):
         """
-        Implements the custom logic to run a training loop for model fine-tuning
-        by using the provided `train_epoch_fn`, `validate_fn` and `configure_optimizers_fn` methods.
-        The passed methods are registered in the `TrainingRunner` instance and the training logic
-        is implemented by calling the corresponding `TrainingRunner` methods
+        Implements the custom logic to run a training loop for model fine-tuning by using the provided
+        `train_epoch_fn`, `validate_fn` and `configure_optimizers_fn` methods.
 
         :param model: The model instance before fine-tuning
-        :param train_epoch_fn: a method to fine-tune the model for a single epoch
-        (to be called inside the `train_epoch` of the TrainingRunner)
-        :param validate_fn: a method to evaluate the model on the validation dataset
-        (to be called inside the `train_epoch` of the TrainingRunner)
-        :param configure_optimizers_fn: a method to instantiate an optimizer and a learning
-        rate scheduler (to be called inside the `configure_optimizers` of the TrainingRunner)
-        :param dump_checkpoint_fn: a method to dump a checkpoint
-        :param load_checkpoint_fn: a method to load a checkpoint
-        :param early_stopping_fn: a method to check for an early stopping condition
-        :return: The fine-tuned model
+        :param train_epoch_fn: a callback to fine-tune the model for a single epoch
+        :param validate_fn: a callback to evaluate the model on the validation dataset
+        :param configure_optimizers_fn: a callback to instantiate an optimizer and a learning rate scheduler
+        :param dump_checkpoint_fn: a callback to dump a checkpoint
+        :param load_checkpoint_fn: a callback to load a checkpoint
+        :param early_stopping_fn: a callback to check for an early stopping condition
+        :param tensorboard_writer: The tensorboard object to be used for logging.
+        :param log_dir: The path to be used for logging and checkpoint saving.
+        :param update_learning_rate_fn: The callback to update the learning rate after each epoch
+          of the training loop.
+        :return: The fine-tuned model.
         """
 
     @property
     @abstractmethod
-    def statistics(self):
+    def statistics(self) -> TrainingLoopStatistics:
         """
         Returns statistics of the compressed model.
         """
 
 
+@api()
 class BaseEarlyExitCompressionTrainingLoop(TrainingLoop, ABC):
-    def __init__(self):
-        self.runner = None
-        self.compression_controller = None
+    """
+    Base class to generalize functionality of derived training loop classes.
+    """
+
+    def __init__(self, compression_controller: CompressionAlgorithmController):
+        self.runner = None  # type: BaseAccuracyAwareTrainingRunner
+        self.compression_controller = compression_controller
         self._current_compression_rate = None
 
-    def run(self, model: TModel, train_epoch_fn, validate_fn, configure_optimizers_fn=None,
-            dump_checkpoint_fn=None, load_checkpoint_fn=None, early_stopping_fn=None,
-            tensorboard_writer=None, log_dir=None, update_learning_rate_fn=None):
-        self.runner.initialize_training_loop_fns(train_epoch_fn, validate_fn, configure_optimizers_fn,
-                                                 dump_checkpoint_fn, load_checkpoint_fn, early_stopping_fn,
-                                                 update_learning_rate_fn)
+    def run(
+        self,
+        model: TModel,
+        train_epoch_fn: Callable,
+        validate_fn: Callable,
+        configure_optimizers_fn: Callable = None,
+        dump_checkpoint_fn: Callable = None,
+        load_checkpoint_fn: Callable = None,
+        early_stopping_fn: Callable = None,
+        tensorboard_writer: Optional[TensorboardWriterType] = None,
+        log_dir: Union[pathlib.Path, str] = None,
+        update_learning_rate_fn: Callable = None,
+    ):
+        self.runner.initialize_training_loop_fns(
+            train_epoch_fn,
+            validate_fn,
+            configure_optimizers_fn,
+            dump_checkpoint_fn,
+            load_checkpoint_fn,
+            early_stopping_fn,
+            update_learning_rate_fn,
+        )
         self.runner.initialize_logging(log_dir, tensorboard_writer)
         return self._run_early_exit_training_loop(model)
 
     def _run_early_exit_training_loop(self, model):
-        self.runner.retrieve_uncompressed_model_accuracy(model)
         uncompressed_model_accuracy = self.runner.uncompressed_model_accuracy
         self.runner.calculate_minimal_tolerable_accuracy(uncompressed_model_accuracy)
 
         self.runner.configure_optimizers()
 
         self.runner.validate(model)
         self._current_compression_rate = self.compression_controller.compression_rate
         self.runner.dump_statistics(model, self.compression_controller)
 
-        nncf_logger.info('Initialization step results:')
+        nncf_logger.info("Initialization step results:")
         self.log_accuracy_statistics()
 
         if self._accuracy_criterion_satisfied():
-            nncf_logger.info('\nReached the accuracy criteria after the initialization step.\n')
+            nncf_logger.info("\nReached the accuracy criteria after the initialization step.\n")
             return model
 
         for epoch in range(1, self.runner.maximal_total_epochs + 1):
             self.runner.train_epoch(model, self.compression_controller)
             self.runner.validate(model)
             self._current_compression_rate = self.compression_controller.compression_rate
             self.runner.dump_statistics(model, self.compression_controller)
             if self._accuracy_criterion_satisfied():
-                nncf_logger.info(f'Reached the accuracy criteria after epoch {epoch}.')
+                nncf_logger.info(f"Reached the accuracy criteria after epoch {epoch}.")
                 self.log_accuracy_statistics()
                 break
 
-            nncf_logger.info(f'Epoch {epoch} results:')
+            nncf_logger.info(f"Epoch {epoch} results:")
             self.log_accuracy_statistics()
 
             if self.runner.stop_training(self.compression_controller):
-                nncf_logger.info('Training stopped - early stopping criterion satisfied')
+                nncf_logger.info("Training stopped - early stopping criterion satisfied")
                 break
 
             self.runner.update_learning_rate()
 
         self._current_compression_rate = self.runner.load_best_checkpoint(model)
         return model
 
     def log_accuracy_statistics(self):
-        for log_str in self.statistics.to_str().split('\n'):
+        for log_str in self.statistics.to_str().split("\n"):
             nncf_logger.info(log_str)
 
     @property
     def statistics(self) -> TrainingLoopStatistics:
         compression_rate = self._current_compression_rate or 0.0
         compressed_accuracy = self.runner.current_val_metric_value
         uncompressed_accuracy = self.runner.uncompressed_model_accuracy
         accuracy_drop = self._calculate_accuracy_drop(uncompressed_accuracy, compressed_accuracy)
         relative_accuracy_drop = self._calculate_rel_accuracy_drop(uncompressed_accuracy, compressed_accuracy)
         accuracy_budget = self._calculate_accuracy_budget(self.runner.minimal_tolerable_accuracy, compressed_accuracy)
-        stats = TrainingLoopStatistics(uncompressed_accuracy,
-                                       compression_rate,
-                                       compressed_accuracy,
-                                       accuracy_drop,
-                                       relative_accuracy_drop,
-                                       accuracy_budget)
+        stats = TrainingLoopStatistics(
+            uncompressed_accuracy,
+            compression_rate,
+            compressed_accuracy,
+            accuracy_drop,
+            relative_accuracy_drop,
+            accuracy_budget,
+        )
         return stats
 
     @staticmethod
     def _calculate_accuracy_drop(uncompressed_model_accuracy, compressed_model_accuracy):
         return uncompressed_model_accuracy - compressed_model_accuracy
 
     @staticmethod
@@ -159,152 +194,223 @@
             rel_accuracy_drop = 100 * (1.0 - compressed_model_accuracy / uncompressed_model_accuracy)
         except ZeroDivisionError:
             rel_accuracy_drop = 0
 
         return rel_accuracy_drop
 
     def _accuracy_criterion_satisfied(self):
-        accuracy_budget = self._calculate_accuracy_budget(self.runner.minimal_tolerable_accuracy,
-                                                          self.runner.current_val_metric_value)
+        accuracy_budget = self._calculate_accuracy_budget(
+            self.runner.minimal_tolerable_accuracy, self.runner.current_val_metric_value
+        )
         return accuracy_budget >= 0 and self.runner.is_model_fully_compressed(self.compression_controller)
 
 
+@api()
 class EarlyExitCompressionTrainingLoop(BaseEarlyExitCompressionTrainingLoop):
     """
-    Adaptive compression training loop allows an accuracy-aware training process
-    to reach the maximal accuracy drop
-    (the maximal allowed accuracy degradation criterion is satisfied).
+    Training loop that does not modify compression parameters and exits as soon as (and if) the accuracy drop criterion
+    is reached.
+
+    :param nncf_config: The configuration object.
+    :type nncf_config: nncf.NNCFConfig
+    :param compression_controller: The controller for the compression algorithm that is currently applied to the model
+        to be trained.
+    :param uncompressed_model_accuracy: The uncompressed model accuracy, measured outside of this training loop to
+        serve as the point of reference for fine-tuning the compressed model.
+    :param lr_updates_needed:
+    :param verbose: Whether to post additional data to TensorBoard.
+    :param dump_checkpoints: If true, will dump all checkpoints obtained during the training process, otherwise will
+      only keep the best checkpoint (accuracy-wise).
     """
 
-    def __init__(self,
-                 nncf_config: NNCFConfig,
-                 compression_controller: CompressionAlgorithmController,
-                 lr_updates_needed=True, verbose=True,
-                 dump_checkpoints=True):
-        super().__init__()
+    def __init__(
+        self,
+        nncf_config: NNCFConfig,
+        compression_controller: CompressionAlgorithmController,
+        uncompressed_model_accuracy: float,
+        lr_updates_needed: bool = True,
+        verbose: bool = True,
+        dump_checkpoints: bool = True,
+    ):
+        super().__init__(compression_controller)
         accuracy_aware_training_params = extract_accuracy_aware_training_params(nncf_config)
-        runner_factory = EarlyExitTrainingRunnerCreator(accuracy_aware_training_params,
-                                                        compression_controller,
-                                                        verbose, dump_checkpoints, lr_updates_needed)
+        runner_factory = EarlyExitTrainingRunnerCreator(
+            accuracy_aware_training_params,
+            compression_controller,
+            uncompressed_model_accuracy,
+            verbose,
+            dump_checkpoints,
+            lr_updates_needed,
+        )
 
         self.runner = runner_factory.create_training_loop()
-        self.compression_controller = compression_controller
 
 
+@api()
 class AdaptiveCompressionTrainingLoop(BaseEarlyExitCompressionTrainingLoop):
     """
-    Adaptive compression training loop allows an accuracy-aware training process whereby
-    the compression rate is automatically varied during training to reach the maximal
-    possible compression rate with a positive accuracy budget
-    (the maximal allowed accuracy degradation criterion is satisfied).
+    A training loop that automatically adjusts compression rate to reach maximum compression within accuracy budget.
+
+    :param nncf_config: The configuration object.
+    :type nncf_config: nncf.NNCFConfig
+    :param compression_controller: The controller for the compression algorithm that is currently applied to the model
+        to be trained.
+    :param uncompressed_model_accuracy: The uncompressed model accuracy, measured outside of this training loop to
+        serve as the point of reference for fine-tuning the compressed model.
+    :param lr_updates_needed:
+    :param verbose: Whether to post additional data to TensorBoard.
+    :param minimal_compression_rate: Sets the minimal compression rate to be considered during the training loop.
+    :param maximal_compression_rate: Sets the maximal compression rate to be considered during the training loop.
+    :param dump_checkpoints: If true, will dump all checkpoints obtained during the training process, otherwise will
+      only keep the best checkpoint (accuracy-wise).
     """
 
-    def __init__(self,
-                 nncf_config: NNCFConfig,
-                 compression_controller: CompressionAlgorithmController,
-                 lr_updates_needed=True, verbose=True,
-                 minimal_compression_rate=0.0,
-                 maximal_compression_rate=0.95,
-                 dump_checkpoints=True):
-        super().__init__()
-        self.compression_controller = compression_controller
+    def __init__(
+        self,
+        nncf_config: NNCFConfig,
+        compression_controller: CompressionAlgorithmController,
+        uncompressed_model_accuracy: float,
+        lr_updates_needed: bool = True,
+        verbose: bool = True,
+        minimal_compression_rate: float = 0.0,
+        maximal_compression_rate: float = 0.95,
+        dump_checkpoints: bool = True,
+    ):
+        super().__init__(compression_controller)
         self.adaptive_controller = self._get_adaptive_compression_ctrl(compression_controller)
         if self.adaptive_controller is None:
-            raise RuntimeError('No compression algorithm supported by the accuracy-aware training '
-                               'runner was specified in the config')
+            raise RuntimeError(
+                "No compression algorithm supported by the accuracy-aware training "
+                "runner was specified in the config"
+            )
 
         maximal_compression_rate = min(maximal_compression_rate, self.adaptive_controller.maximal_compression_rate)
 
         accuracy_aware_training_params = extract_accuracy_aware_training_params(nncf_config)
-        runner_factory = AdaptiveCompressionLevelTrainingRunnerCreator(accuracy_aware_training_params,
-                                                                       self.adaptive_controller,
-                                                                       verbose, dump_checkpoints, lr_updates_needed,
-                                                                       minimal_compression_rate,
-                                                                       maximal_compression_rate)
+        runner_factory = AdaptiveCompressionLevelTrainingRunnerCreator(
+            accuracy_aware_training_params,
+            self.adaptive_controller,
+            uncompressed_model_accuracy,
+            verbose,
+            dump_checkpoints,
+            lr_updates_needed,
+            minimal_compression_rate,
+            maximal_compression_rate,
+        )
         self.runner = runner_factory.create_training_loop()
         self.runner.adaptive_controller = self.adaptive_controller
 
     def _get_adaptive_compression_ctrl(self, compression_controller):
         def _adaptive_compression_controllers():
             def remove_registry_prefix(algo_name):
-                for prefix in ('pt_', 'tf_'):
+                for prefix in ("pt_", "tf_"):
                     if algo_name.startswith(prefix):
-                        return algo_name[len(prefix):]
-                raise RuntimeError('Compression algorithm names in the adaptive controllers '
-                                   'registry should be prefixed with "pt_" or "tf_" depending on the '
-                                   'backend framework')
-
-            return {remove_registry_prefix(algo_name): controller_cls for algo_name, controller_cls in
-                    ADAPTIVE_COMPRESSION_CONTROLLERS.registry_dict.items()}
+                        return algo_name[len(prefix) :]
+                raise RuntimeError(
+                    "Compression algorithm names in the adaptive controllers "
+                    'registry should be prefixed with "pt_" or "tf_" depending on the '
+                    "backend framework"
+                )
+
+            return {
+                remove_registry_prefix(algo_name): controller_cls
+                for algo_name, controller_cls in ADAPTIVE_COMPRESSION_CONTROLLERS.registry_dict.items()
+            }
 
         adaptive_compression_controllers = _adaptive_compression_controllers()
 
         if isinstance(compression_controller, CompositeCompressionAlgorithmController):
             for controller in compression_controller.child_ctrls:
                 for ctrl_type in adaptive_compression_controllers.values():
                     if isinstance(controller, ctrl_type):
                         return controller
         elif isinstance(compression_controller, CompressionAlgorithmController):
             if compression_controller.name in adaptive_compression_controllers:
                 return compression_controller
 
-        raise RuntimeError('No compression algorithm that supports adaptive compression '
-                           'accuracy-aware training was specified')
-
-    def run(self, model: TModel, train_epoch_fn, validate_fn, configure_optimizers_fn=None,
-            dump_checkpoint_fn=None, load_checkpoint_fn=None, early_stopping_fn=None,
-            tensorboard_writer=None, log_dir=None, update_learning_rate_fn=None):
-        self.runner.initialize_training_loop_fns(train_epoch_fn, validate_fn, configure_optimizers_fn,
-                                                 dump_checkpoint_fn, load_checkpoint_fn, early_stopping_fn,
-                                                 update_learning_rate_fn)
+        raise RuntimeError(
+            "No compression algorithm that supports adaptive compression accuracy-aware training was specified"
+        )
+
+    def run(
+        self,
+        model: TModel,
+        train_epoch_fn: Callable,
+        validate_fn: Callable,
+        configure_optimizers_fn: Callable = None,
+        dump_checkpoint_fn: Callable = None,
+        load_checkpoint_fn: Callable = None,
+        early_stopping_fn: Callable = None,
+        tensorboard_writer: Optional[TensorboardWriterType] = None,
+        log_dir: Union[pathlib.Path, str] = None,
+        update_learning_rate_fn: Callable = None,
+    ):
+        self.runner.initialize_training_loop_fns(
+            train_epoch_fn,
+            validate_fn,
+            configure_optimizers_fn,
+            dump_checkpoint_fn,
+            load_checkpoint_fn,
+            early_stopping_fn,
+            update_learning_rate_fn,
+        )
         self.runner.initialize_logging(log_dir, tensorboard_writer)
         model = self._run_initial_training_phase(model)
         self.runner.reset_training()
         self.runner.validate(model)
 
-        nncf_logger.info('Search for the optimal compression rate started.')
-        accuracy_budget = self._calculate_accuracy_budget(self.runner.minimal_tolerable_accuracy,
-                                                          self.runner.best_val_metric_value)
-        self.runner.add_tensorboard_scalar('val/accuracy_aware/accuracy_budget',
-                                           accuracy_budget,
-                                           self.runner.cumulative_epoch_count)
-        self.runner.add_tensorboard_scalar('compression/accuracy_aware/target_compression_rate',
-                                           self.runner.compression_rate_target,
-                                           self.runner.cumulative_epoch_count)
+        nncf_logger.info("Search for the optimal compression rate started.")
+        accuracy_budget = self._calculate_accuracy_budget(
+            self.runner.minimal_tolerable_accuracy, self.runner.best_val_metric_value
+        )
+        self.runner.add_tensorboard_scalar(
+            "val/accuracy_aware/accuracy_budget", accuracy_budget, self.runner.cumulative_epoch_count
+        )
+        self.runner.add_tensorboard_scalar(
+            "compression/accuracy_aware/target_compression_rate",
+            self.runner.compression_rate_target,
+            self.runner.cumulative_epoch_count,
+        )
 
         self.compression_controller.disable_scheduler()
         force_updating_target_compression_rate = True
-        while self.runner.compression_rate_step >= self.runner.minimal_compression_rate_step and \
-                self.runner.cumulative_epoch_count < self.runner.maximal_total_epochs:
+        while (
+            self.runner.compression_rate_step >= self.runner.minimal_compression_rate_step
+            and self.runner.cumulative_epoch_count < self.runner.maximal_total_epochs
+        ):
             prev_compression_rate_target = self.runner.compression_rate_target
             prev_compression_rate_step = self.runner.compression_rate_step
             was_compression_rate_changed = self._update_target_compression_rate(
-                self.runner,
-                force_updating_target_compression_rate
+                self.runner, force_updating_target_compression_rate
             )
 
             self.log_accuracy_statistics()
 
             if was_compression_rate_changed:
-                nncf_logger.info(f'Target compression rate value changed: {prev_compression_rate_target:.3f} -> '
-                                 f'{self.runner.compression_rate_target:.3f}')
+                nncf_logger.info(
+                    f"Target compression rate value changed: {prev_compression_rate_target:.3f} -> "
+                    f"{self.runner.compression_rate_target:.3f}"
+                )
                 if prev_compression_rate_step == self.runner.compression_rate_step:
-                    nncf_logger.info(f'Compression rate step value is kept unchanged: '
-                                     f'{self.runner.compression_rate_step:.3f}')
+                    nncf_logger.info(
+                        f"Compression rate step value is kept unchanged: " f"{self.runner.compression_rate_step:.3f}"
+                    )
                 else:
-                    nncf_logger.info(f'The compression rate step value was changed {prev_compression_rate_step:.3f} -> '
-                                     f'{self.runner.compression_rate_step:.3f}')
+                    nncf_logger.info(
+                        f"The compression rate step value was changed {prev_compression_rate_step:.3f} -> "
+                        f"{self.runner.compression_rate_step:.3f}"
+                    )
                 if self.runner.compression_rate_target < self.runner.minimal_compression_rate:
-                    nncf_logger.warning('Cannot produce a compressed model with a specified '
-                                        'minimal tolerable accuracy')
+                    nncf_logger.warning("Cannot produce a compressed model with a specified minimal tolerable accuracy")
                     break
                 if self.runner.compression_rate_target > self.runner.maximal_compression_rate:
                     self.runner.compression_rate_target = self.runner.maximal_compression_rate
-                    nncf_logger.info(f'Reached maximal possible compression rate: '
-                                     f'{self.runner.maximal_compression_rate}')
+                    nncf_logger.info(
+                        f"Reached maximal possible compression rate: " f"{self.runner.maximal_compression_rate}"
+                    )
                     break
 
                 force_updating_target_compression_rate = False
 
                 self.runner.load_best_checkpoint(model)
                 # set compression rate for model
                 self.adaptive_controller.compression_rate = self.runner.compression_rate_target
@@ -312,131 +418,161 @@
                 self.runner.reset_training()
 
                 # workaround for compression statistics
                 self.adaptive_controller.scheduler.current_sparsity_level = self.runner.compression_rate_target
                 self.adaptive_controller.scheduler.current_pruning_level = self.runner.compression_rate_target
                 self.adaptive_controller.scheduler.target_level = self.runner.compression_rate_target
 
-                self.runner.add_tensorboard_scalar('compression/accuracy_aware/target_compression_rate',
-                                                   self.runner.compression_rate_target,
-                                                   self.runner.cumulative_epoch_count)
-                self.runner.add_tensorboard_scalar('compression/accuracy_aware/compression_rate_step',
-                                                   self.runner.compression_rate_step,
-                                                   self.runner.cumulative_epoch_count)
+                self.runner.add_tensorboard_scalar(
+                    "compression/accuracy_aware/target_compression_rate",
+                    self.runner.compression_rate_target,
+                    self.runner.cumulative_epoch_count,
+                )
+                self.runner.add_tensorboard_scalar(
+                    "compression/accuracy_aware/compression_rate_step",
+                    self.runner.compression_rate_step,
+                    self.runner.cumulative_epoch_count,
+                )
             else:
-                nncf_logger.info(f'Current target compression rate value: {self.runner.compression_rate_target:.3f}')
-                nncf_logger.info(f'Current compression rate step value: {self.runner.compression_rate_target:.3f}')
+                nncf_logger.info(f"Current target compression rate value: {self.runner.compression_rate_target:.3f}")
+                nncf_logger.info(f"Current compression rate step value: {self.runner.compression_rate_target:.3f}")
 
             self.runner.train_epoch(model, self.compression_controller)
             compressed_model_accuracy = self.runner.validate(model)
             self.runner.dump_statistics(model, self.compression_controller)
-            accuracy_budget = self._calculate_accuracy_budget(self.runner.minimal_tolerable_accuracy,
-                                                              compressed_model_accuracy)
-            self.runner.add_tensorboard_scalar('val/accuracy_aware/accuracy_budget', accuracy_budget,
-                                               self.runner.cumulative_epoch_count)
+            accuracy_budget = self._calculate_accuracy_budget(
+                self.runner.minimal_tolerable_accuracy, compressed_model_accuracy
+            )
+            self.runner.add_tensorboard_scalar(
+                "val/accuracy_aware/accuracy_budget", accuracy_budget, self.runner.cumulative_epoch_count
+            )
             if self.runner.stop_training(self.compression_controller):
-                nncf_logger.info('Training stopped - early stopping criterion satisfied.')
+                nncf_logger.info("Training stopped - early stopping criterion satisfied.")
                 force_updating_target_compression_rate = True
 
             self.runner.update_learning_rate()
 
         self._current_compression_rate = self.runner.load_best_checkpoint(model)
         compressed_model_accuracy = self.runner.validate(model)
-        nncf_logger.info(f'Compression rate for the final compressed model: {self._current_compression_rate}, '
-                         f'accuracy: {compressed_model_accuracy} '
-                         f'(vs. original model accuracy: {self.runner.uncompressed_model_accuracy})')
+        nncf_logger.info(
+            f"Compression rate for the final compressed model: {self._current_compression_rate}, "
+            f"accuracy: {compressed_model_accuracy} "
+            f"(vs. uncompressed model accuracy: {self.runner.uncompressed_model_accuracy})"
+        )
         return model
 
     def _run_initial_training_phase(self, model):
-        nncf_logger.info('Initial training phase started...')
+        nncf_logger.info("Initial training phase started...")
 
         maximal_total_epochs = self.runner.maximal_total_epochs
         self.runner.maximal_total_epochs = self.runner.initial_training_phase_epochs
         model = self._run_early_exit_training_loop(model)
         self.runner.maximal_total_epochs = maximal_total_epochs
 
-        nncf_logger.info('Initial training phase finished.')
+        nncf_logger.info("Initial training phase finished.")
         return model
 
     def _update_target_compression_rate(self, runner, force_update=False):
         best_accuracy_budget = runner.best_val_metric_value - runner.minimal_tolerable_accuracy
-        nncf_logger.info(f'Training epoch count: {runner.training_epoch_count}, '
-                         f'patience epochs: {runner.patience_epochs}')
+        nncf_logger.info(
+            f"Training epoch count: {runner.training_epoch_count}, " f"patience epochs: {runner.patience_epochs}"
+        )
         if runner.training_epoch_count >= runner.patience_epochs or best_accuracy_budget >= 0.0 or force_update:
             runner.compression_rate_target += self._determine_compression_rate_step_value(runner)
             runner.was_compression_increased_on_prev_step = 1.0 if best_accuracy_budget >= 0.0 else -1.0
             return True
         return False
 
-    def _determine_compression_rate_step_value(self, runner, stepping_mode='uniform_decrease', **kwargs):
+    def _determine_compression_rate_step_value(self, runner, stepping_mode="uniform_decrease", **kwargs):
         if stepping_mode == "uniform_decrease":
             compression_step_updater = self._uniform_decrease_compression_step_update
         elif stepping_mode == "interpolate":
-            compression_step_updater = partial(self._interpolate_compression_step_update,
-                                               current_compression_rate=runner.compression_rate_target)
+            compression_step_updater = partial(
+                self._interpolate_compression_step_update, current_compression_rate=runner.compression_rate_target
+            )
         else:
             raise ValueError("Wrong stepping mode to determine compression rate step value provided")
         return compression_step_updater(runner, **kwargs)
 
     @staticmethod
     def _uniform_decrease_compression_step_update(runner):
         best_accuracy_budget = runner.best_val_metric_value - runner.minimal_tolerable_accuracy
         best_accuracy_budget_sign = 1.0 if best_accuracy_budget >= 0.0 else -1.0
         # if we don't fit the accuracy budget now and before we did fit or vice versa, we reduce the compression rate
         # step and the learning rate
-        if runner.was_compression_increased_on_prev_step is not None and \
-                runner.was_compression_increased_on_prev_step != best_accuracy_budget_sign:
+        if (
+            runner.was_compression_increased_on_prev_step is not None
+            and runner.was_compression_increased_on_prev_step != best_accuracy_budget_sign
+        ):
             runner.compression_rate_step *= runner.compression_rate_step_reduction_factor
             runner.base_lr_reduction_factor_during_search *= runner.lr_reduction_factor
         # if we don't fit the accuracy budget, then we decrease the compression rate, and if otherwise we increase it
         compression_step_update = best_accuracy_budget_sign * runner.compression_rate_step
         return compression_step_update
 
     @staticmethod
-    def _interpolate_compression_step_update(runner,
-                                             current_compression_rate,
-                                             num_curve_pts=1000,
-                                             full_compression_factor=20,
-                                             minimal_compression_rate=0.0,
-                                             maximal_compression_rate=1.0):
+    def _interpolate_compression_step_update(
+        runner,
+        current_compression_rate,
+        num_curve_pts=1000,
+        full_compression_factor=20,
+        minimal_compression_rate=0.0,
+        maximal_compression_rate=1.0,
+    ):
         training_history = runner.compressed_training_history
-        nncf_logger.info(f'Compressed training history: {training_history}')
+        nncf_logger.info(f"Compressed training history: {training_history}")
         training_history[minimal_compression_rate] = runner.maximal_accuracy_drop
         training_history[maximal_compression_rate] = -full_compression_factor * runner.maximal_accuracy_drop
         compression_rates, evaluated_acc_budgets = list(training_history.keys()), list(training_history.values())
-        interp_kind = 'linear' if len(compression_rates) < 4 else 'cubic'
-        acc_budget_vs_comp_rate_curve = interp1d(compression_rates, evaluated_acc_budgets,
-                                                 kind=interp_kind)
-        rate_interval = np.linspace(minimal_compression_rate, maximal_compression_rate,
-                                    num=num_curve_pts, endpoint=True)
+        interp_kind = "linear" if len(compression_rates) < 4 else "cubic"
+        acc_budget_vs_comp_rate_curve = interp1d(compression_rates, evaluated_acc_budgets, kind=interp_kind)
+        rate_interval = np.linspace(
+            minimal_compression_rate, maximal_compression_rate, num=num_curve_pts, endpoint=True
+        )
         acc_budget_values = acc_budget_vs_comp_rate_curve(rate_interval)
         target_compression_rate = rate_interval[np.argmin(np.abs(acc_budget_values))]
-        nncf_logger.info(f'Predicted compression rate: {target_compression_rate}, '
-                         f'current compression rate: {current_compression_rate}')
+        nncf_logger.info(
+            f"Predicted compression rate: {target_compression_rate}, "
+            f"current compression rate: {current_compression_rate}"
+        )
         if runner.compression_rate_target is None:
             runner.compression_rate_step = np.abs(target_compression_rate - current_compression_rate)
             return target_compression_rate - current_compression_rate
         runner.compression_rate_step = np.abs(target_compression_rate - runner.compression_rate_target)
         return target_compression_rate - runner.compression_rate_target
 
 
 class AccuracyAwareTrainingMode:
-    EARLY_EXIT = 'early_exit'
-    ADAPTIVE_COMPRESSION_LEVEL = 'adaptive_compression_level'
+    EARLY_EXIT = "early_exit"
+    ADAPTIVE_COMPRESSION_LEVEL = "adaptive_compression_level"
 
 
-def create_accuracy_aware_training_loop(nncf_config: NNCFConfig,
-                                        compression_ctrl: CompressionAlgorithmController,
-                                        **additional_runner_args) -> BaseEarlyExitCompressionTrainingLoop:
+def create_accuracy_aware_training_loop(
+    nncf_config: NNCFConfig,
+    compression_ctrl: CompressionAlgorithmController,
+    uncompressed_model_accuracy: float = None,
+    **additional_runner_args,
+) -> BaseEarlyExitCompressionTrainingLoop:
     """
     Creates an accuracy aware training loop corresponding to NNCFConfig and CompressionAlgorithmController.
     :param: nncf_config: An instance of the NNCFConfig.
-    :compression_ctrl: An instance of thr CompressionAlgorithmController.
+    :param: compression_ctrl: An instance of CompressionAlgorithmController.
+    :param: uncompressed_model_accuracy: If provided, will take this as the value of the target accuracy metric for the
+    original (uncompressed) model for purposes of matching the compressed model metric to this baseline. If not
+    provided, the uncompressed model accuracy will be measured during this function using `ModelEvaluationArgs.eval_fn`
+    callable as provided by the nncf_config.
     :return: Accuracy aware training loop.
     """
     accuracy_aware_training_params = extract_accuracy_aware_training_params(nncf_config)
-    accuracy_aware_training_mode = accuracy_aware_training_params.get('mode')
+    accuracy_aware_training_mode = accuracy_aware_training_params.get("mode")
+    if uncompressed_model_accuracy is None:
+        eval_fn = nncf_config.get_extra_struct(ModelEvaluationArgs).eval_fn
+        uncompressed_model_accuracy = eval_fn(compression_ctrl.model)
     if accuracy_aware_training_mode == AccuracyAwareTrainingMode.EARLY_EXIT:
-        return EarlyExitCompressionTrainingLoop(nncf_config, compression_ctrl, **additional_runner_args)
+        return EarlyExitCompressionTrainingLoop(
+            nncf_config, compression_ctrl, uncompressed_model_accuracy, **additional_runner_args
+        )
     if accuracy_aware_training_mode == AccuracyAwareTrainingMode.ADAPTIVE_COMPRESSION_LEVEL:
-        return AdaptiveCompressionTrainingLoop(nncf_config, compression_ctrl, **additional_runner_args)
-    raise RuntimeError('Incorrect accuracy aware mode in the config file')
+        return AdaptiveCompressionTrainingLoop(
+            nncf_config, compression_ctrl, uncompressed_model_accuracy, **additional_runner_args
+        )
+    raise RuntimeError("Incorrect accuracy aware mode in the config file")
```

### Comparing `nncf-2.4.0/nncf/common/collector.py` & `nncf-2.5.0/nncf/common/engine.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,28 +1,30 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
+from typing import Any
 
-from nncf.api.statistics import Statistics
 
-
-class StatisticsCollector(ABC):
+class Engine(ABC):
     """
-    Encapsulates the logic of the statistics collection.
+    The basic class aims to provide the interface to infer the model.
     """
 
     @abstractmethod
-    def collect(self) -> Statistics:
+    def infer(self, input_data: Any) -> Any:
         """
-        Collects statistics. The logic of the statistics collection should be implemented here.
+        Runs model on the provided input data.
+        Returns the raw model outputs.
+
+        :param input_data: inputs for the model
+        :return: raw model outputs
         """
```

### Comparing `nncf-2.4.0/nncf/common/composite_compression.py` & `nncf-2.5.0/nncf/common/composite_compression.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,35 +1,30 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Any, Dict, List, Optional, Tuple
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionAlgorithmBuilder
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.api.compression import TModel
 from nncf.common.statistics import NNCFStatistics
 from nncf.common.utils.backend import BackendType
+from nncf.common.utils.backend import copy_model
 from nncf.common.utils.backend import get_backend
 
 
 class CompositeCompressionLoss(CompressionLoss):
     """
     The `CompositeCompressionLoss` class stores a group of `CompressionLoss`
     instances as a list of children that are treated the same way as a single
@@ -77,15 +72,15 @@
         Traverses through all children and calculates the total compression
         loss value.
 
         :return: The compression loss value.
         """
 
         if len(self._child_losses) == 0:
-            raise RuntimeError('Cannot calculate the loss value because the number of child loss is 0.')
+            raise RuntimeError("Cannot calculate the loss value because the number of child loss is 0.")
 
         result_loss = 0
         for loss in self._child_losses:
             result_loss += loss()
         return result_loss
 
 
@@ -159,16 +154,16 @@
 class CompositeCompressionAlgorithmController(CompressionAlgorithmController):
     """
     The `CompositeCompressionAlgorithmController` class stores a group of
     `CompressionAlgorithmController` instances as a list of children that are
     treated the same way as a single `CompressionAlgorithmController` instance.
     """
 
-    BUILDER_STATE = 'builder_state'
-    CONTROLLER_STATE = 'ctrl_state'
+    BUILDER_STATE = "builder_state"
+    CONTROLLER_STATE = "ctrl_state"
 
     def __init__(self, target_model: TModel):
         """
         Initializes the internal state of the composite compression algorithm
         controller.
 
         :param target_model: The model with additional modifications necessary
@@ -201,16 +196,15 @@
     def add(self, child_ctrl: CompressionAlgorithmController) -> None:
         """
         Add `CompressionAlgorithmController` instance to the list of children.
 
         :param child_ctrl: A `CompressionAlgorithmController` instance.
         """
         if child_ctrl.model is not self.model:
-            raise RuntimeError('Cannot create a composite controller '
-                               'from controllers belonging to different models!')
+            raise RuntimeError("Cannot create a composite controller from controllers belonging to different models!")
 
         self._child_ctrls.append(child_ctrl)
         self._loss.add(child_ctrl.loss)
         self._scheduler.add(child_ctrl.scheduler)
 
     def compression_stage(self) -> CompressionStage:
         """
@@ -276,32 +270,42 @@
         Prepare the compressed model for deployment.
         """
         stripped_model = self._model
         for ctrl in self.child_ctrls:
             stripped_model = ctrl.strip_model(stripped_model)
         self._model = stripped_model
 
+    def strip(self, do_copy: bool = True) -> TModel:
+        model = self.model
+        if do_copy:
+            model = copy_model(model)
+        for ctrl in self.child_ctrls:
+            model = ctrl.strip_model(model, do_copy=False)
+        return model
+
     @property
     def compression_rate(self) -> float:
         raise NotImplementedError
 
     @compression_rate.setter
     def compression_rate(self, compression_rate: float) -> None:
         raise NotImplementedError
 
     @property
     def maximal_compression_rate(self) -> float:
         return min(child_ctrl.maximal_compression_rate for child_ctrl in self.child_ctrls)
 
-    def export_model(self,
-                     save_path: str,
-                     save_format: Optional[str] = None,
-                     input_names: Optional[List[str]] = None,
-                     output_names: Optional[List[str]] = None,
-                     model_args: Optional[Tuple[Any, ...]] = None) -> None:
+    def export_model(
+        self,
+        save_path: str,
+        save_format: Optional[str] = None,
+        input_names: Optional[List[str]] = None,
+        output_names: Optional[List[str]] = None,
+        model_args: Optional[Tuple[Any, ...]] = None,
+    ) -> None:
         """
         Exports the compressed model to the specified format for deployment.
 
         Makes method-specific preparations of the model, (e.g. removing auxiliary
         layers that were used for the model compression), then exports the model to
         the specified path.
 
@@ -316,39 +320,38 @@
                 - (a, b, {'x': None, 'y': y}) for positional and keyword arguments.
                 - (a, b, {}) for positional arguments only.
                 - ({'x': None, 'y': y},) for keyword arguments only.
         """
         self.prepare_for_export()
         backend = get_backend(self.model)
         if backend is BackendType.TENSORFLOW:
-            from nncf.tensorflow.exporter import TFExporter #pylint: disable=cyclic-import
+            from nncf.tensorflow.exporter import TFExporter  # pylint: disable=cyclic-import
+
             exporter = TFExporter(self.model, input_names, output_names, model_args)
         else:
             assert backend is BackendType.TORCH
-            from nncf.torch.exporter import PTExporter #pylint: disable=cyclic-import
+            from nncf.torch.exporter import PTExporter  # pylint: disable=cyclic-import
+
             exporter = PTExporter(self.model, input_names, output_names, model_args)
         if save_format is not None:
             exporter.export_model(save_path, save_format)
         else:
             exporter.export_model(save_path)
 
     def disable_scheduler(self) -> None:
         self._scheduler = CompositeCompressionScheduler()
         for ctrl in self.child_ctrls:
             ctrl.disable_scheduler()
             self._scheduler.add(ctrl.scheduler)
 
     def get_compression_state(self) -> Dict[str, Any]:
         if self._builder_state is None:
-            raise RuntimeError('Internal error: builder state is not set for the controller')
+            raise RuntimeError("Internal error: builder state is not set for the controller")
 
-        return {
-            self.BUILDER_STATE: self._builder_state,
-            self.CONTROLLER_STATE: self.get_state()
-        }
+        return {self.BUILDER_STATE: self._builder_state, self.CONTROLLER_STATE: self.get_state()}
 
     def set_builder_state_with_name(self, name: str, builder_state: Dict):
         """
         Sets state of the builder and the corresponding algorithm name. Should be called by the builder to set its
         state and registered algorithm key.
 
         :param name: algorithm name, the string that was used to register the builder
```

### Comparing `nncf-2.4.0/nncf/common/compression.py` & `nncf-2.5.0/nncf/common/compression.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,58 +1,55 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from abc import ABC
 from abc import abstractmethod
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
-from typing import TypeVar
+from typing import Any, Dict, List, Optional, Tuple, TypeVar
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionAlgorithmBuilder
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.logging import nncf_logger
 from nncf.common.schedulers import StubCompressionScheduler
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
 from nncf.common.utils.registry import Registry
+from nncf.config.extractors import BNAdaptDataLoaderNotFoundError
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.extractors import extract_bn_adaptation_init_params
+from nncf.config.extractors import has_bn_section
 
-TModel = TypeVar('TModel')
+TModel = TypeVar("TModel")
 
-NO_COMPRESSION_ALGORITHM_NAME = 'NoCompressionAlgorithm'
+NO_COMPRESSION_ALGORITHM_NAME = "NoCompressionAlgorithm"
 
 
 class BaseControllerStateNames:
-    LOSS = 'loss_state'
-    SCHEDULER = 'scheduler_state'
-    COMPRESSION_STAGE = 'compression_stage'
-    COMPRESSION_LEVEL = 'compression_level'
+    LOSS = "loss_state"
+    SCHEDULER = "scheduler_state"
+    COMPRESSION_STAGE = "compression_stage"
+    COMPRESSION_LEVEL = "compression_level"
 
 
-class BaseCompressionAlgorithmController(CompressionAlgorithmController):
+@api()
+class BaseCompressionAlgorithmController(CompressionAlgorithmController, ABC):
     """
     Contains the implementation of the basic functionality of the compression controller.
     """
 
-    BUILDER_STATE = 'builder_state'
-    CONTROLLER_STATE = 'ctrl_state'
+    BUILDER_STATE = "builder_state"
+    CONTROLLER_STATE = "ctrl_state"
     _state_names = BaseControllerStateNames
 
     def __init__(self, target_model: TModel):
         """
         Initializes the internal state of the compression algorithm controller.
 
         :param target_model: The model with additional modifications necessary
@@ -62,31 +59,33 @@
         super().__init__(target_model)
         self._name = None
         self._builder_state = None
 
     @property
     def name(self):
         if self._name is None:
-            raise RuntimeError('Internal error: name of the controller is not set!')
+            raise RuntimeError("Internal error: name of the controller is not set!")
         return self._name
 
     @property
     def compression_rate(self) -> float:
         return None
 
     @compression_rate.setter
     def compression_rate(self) -> float:
         pass
 
-    def export_model(self,
-                     save_path: str,
-                     save_format: Optional[str] = None,
-                     input_names: Optional[List[str]] = None,
-                     output_names: Optional[List[str]] = None,
-                     model_args: Optional[Tuple[Any, ...]] = None) -> None:
+    def export_model(
+        self,
+        save_path: str,
+        save_format: Optional[str] = None,
+        input_names: Optional[List[str]] = None,
+        output_names: Optional[List[str]] = None,
+        model_args: Optional[Tuple[Any, ...]] = None,
+    ) -> None:
         """
         Exports the compressed model to the specified format for deployment.
 
         Makes method-specific preparations of the model, (e.g. removing auxiliary
         layers that were used for the model compression), then exports the model to
         the specified path.
 
@@ -102,18 +101,20 @@
                 - (a, b, {}) for positional arguments only.
                 - ({'x': None, 'y': y},) for keyword arguments only.
         """
         self.prepare_for_export()
         backend = get_backend(self.model)
         if backend is BackendType.TENSORFLOW:
             from nncf.tensorflow.exporter import TFExporter  # pylint: disable=cyclic-import
+
             exporter = TFExporter(self.model, input_names, output_names, model_args)
         else:
             assert backend is BackendType.TORCH
             from nncf.torch.exporter import PTExporter  # pylint: disable=cyclic-import
+
             exporter = PTExporter(self.model, input_names, output_names, model_args)
         if save_format is not None:
             exporter.export_model(save_path, save_format)
         else:
             exporter.export_model(save_path)
 
     def disable_scheduler(self) -> None:
@@ -138,49 +139,47 @@
         """
         if self.name in state:
             algo_state = state[self.name]
             if self._state_names.COMPRESSION_STAGE in state:
                 compression_stage = state[self._state_names.COMPRESSION_STAGE]
                 if self.compression_stage() != compression_stage:
                     nncf_logger.warning(
-                        f'Current CompressionStage ({self.compression_stage()}) of the compression controller '
-                        f'does not correspond to the value found in the checkpoint ({compression_stage})')
+                        f"Current CompressionStage ({self.compression_stage()}) of the compression controller "
+                        f"does not correspond to the value found in the checkpoint ({compression_stage})"
+                    )
             self.loss.load_state(algo_state[self._state_names.LOSS])
             self.scheduler.load_state(algo_state[self._state_names.SCHEDULER])
 
     def get_state(self) -> Dict[str, Dict[str, Any]]:
         """
         Returns compression controller state, which is the map of the algorithm name to the dictionary with the
         corresponding state attributes.
 
         :return: The compression controller state.
         """
         return {
             self.name: {
                 self._state_names.LOSS: self.loss.get_state(),
                 self._state_names.SCHEDULER: self.scheduler.get_state(),
-                self._state_names.COMPRESSION_STAGE: self.compression_stage()
+                self._state_names.COMPRESSION_STAGE: self.compression_stage(),
             }
         }
 
     def get_compression_state(self) -> Dict[str, Any]:
         """
         Returns compression state - builder and controller state.
         This state should be used to resume compression via `compression_state` argument of `create_compressed_model`
         method.
 
         :return: The compression state.
         """
         if self._builder_state is None:
-            raise RuntimeError('Internal error: builder state is not set for the controller')
+            raise RuntimeError("Internal error: builder state is not set for the controller")
 
-        return {
-            self.BUILDER_STATE: self._builder_state,
-            self.CONTROLLER_STATE: self.get_state()
-        }
+        return {self.BUILDER_STATE: self._builder_state, self.CONTROLLER_STATE: self.get_state()}
 
     @property
     def maximal_compression_rate(self) -> float:
         return 1.0
 
 
 class BaseCompressionAlgorithmBuilder(CompressionAlgorithmBuilder):
@@ -198,36 +197,36 @@
             skipped during building.
         """
         super().__init__()
         self.config = config
         self.should_init = should_init
         self._algo_config = self._get_algo_specific_config_section()
 
-        self.ignored_scopes = self.config.get('ignored_scopes')
+        self.ignored_scopes = self.config.get("ignored_scopes")
 
-        if 'ignored_scopes' in self._algo_config:
-            algo_ignored_scopes = self._algo_config['ignored_scopes']
+        if "ignored_scopes" in self._algo_config:
+            algo_ignored_scopes = self._algo_config["ignored_scopes"]
             if self.ignored_scopes is not None:
                 self.ignored_scopes.extend(algo_ignored_scopes)
             else:
                 self.ignored_scopes = algo_ignored_scopes
 
-        self._global_target_scopes = self.config.get('target_scopes')
+        self._global_target_scopes = self.config.get("target_scopes")
         self.target_scopes = self._global_target_scopes
-        if 'target_scopes' in self._algo_config:
-            algo_target_scopes = self._algo_config['target_scopes']
+        if "target_scopes" in self._algo_config:
+            algo_target_scopes = self._algo_config["target_scopes"]
             if self.target_scopes is None:
                 self.target_scopes = algo_target_scopes
 
     def _get_algo_specific_config_section(self) -> Dict:
         return extract_algo_specific_config(self.config, self.name)
 
     @property
     def name(self) -> str:
-        return getattr(self, Registry.REGISTERED_NAME_ATTR, 'NOT_REGISTERED_' + self.__class__.__name__)
+        return getattr(self, Registry.REGISTERED_NAME_ATTR, "NOT_REGISTERED_" + self.__class__.__name__)
 
     def load_state(self, state: Dict[str, Any]) -> None:
         """
         Initializes object from the state.
 
         :param state: Output of `get_state()` method.
         """
@@ -281,8 +280,20 @@
         Implementation of get_state that returns state without builder name.
 
         :return: Returns a dictionary with Python data structures
             (dict, list, tuple, str, int, float, True, False, None) that represents state of the object.
         """
 
     def _parse_bn_adapt_params(self) -> Optional[Dict]:
-        return extract_bn_adaptation_init_params(self.config, self.name)
+        try:
+            return extract_bn_adaptation_init_params(self.config, self.name)
+        except BNAdaptDataLoaderNotFoundError as e:
+            if not has_bn_section(self.config, self.name):
+                nncf_logger.info(
+                    "Data loader for batchnorm adaptation not found in NNCFConfig and no explicit batchnorm adaptation"
+                    "parameters were passed in config - will not perform batchnorm adaptation.\n"
+                    "It is recommended to do batchnorm adaptation after creating a compressed model - use "
+                    "`register_default_init_args` or `nncf.NNCFConfig.register_extra_structs` directly to register a "
+                    "dataloader and NNCF will do batchnorm adaptation automatically at compressed model creation."
+                )
+                return None
+            raise e
```

### Comparing `nncf-2.4.0/nncf/common/engine.py` & `nncf-2.5.0/nncf/common/graph/definitions.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,32 +1,18 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from abc import ABC
-from abc import abstractmethod
-from typing import Any
+MODEL_INPUT_OP_NAME = "nncf_model_input"
+MODEL_OUTPUT_OP_NAME = "nncf_model_output"
 
 
-class Engine(ABC):
-    """
-    The basic class aims to provide the interface to infer the model.
-    """
-
-    @abstractmethod
-    def infer(self, input_data: Any) -> Any:
-        """
-        Runs model on the provided input data.
-        Returns the raw model outputs.
-
-        :param input_data: inputs for the model
-        :return: raw model outputs
-        """
+class NNCFGraphNodeType:
+    INPUT_NODE = MODEL_INPUT_OP_NAME
+    OUTPUT_NODE = MODEL_OUTPUT_OP_NAME
```

### Comparing `nncf-2.4.0/nncf/common/exporter.py` & `nncf-2.5.0/nncf/common/exporter.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,39 +1,40 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from abc import ABC, abstractmethod
-from typing import Optional, List, Tuple, Any
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from abc import ABC
+from abc import abstractmethod
+from typing import Any, List, Optional, Tuple
 
 from nncf.api.compression import TModel
 
 
 class Exporter(ABC):
     """
     This is the class from which all framework-specific exporters inherit.
 
     An exporter is an object which provides export of the compressed model
     for deployment.
     """
 
-    def __init__(self,
-                 model: TModel,
-                 input_names: Optional[List[str]] = None,
-                 output_names: Optional[List[str]] = None,
-                 model_args: Optional[Tuple[Any, ...]] = None):
+    def __init__(
+        self,
+        model: TModel,
+        input_names: Optional[List[str]] = None,
+        output_names: Optional[List[str]] = None,
+        model_args: Optional[Tuple[Any, ...]] = None,
+    ):
         """
         Initializes an exporter.
 
         :param model: The model to be exported.
         :param input_names: Names to be assigned to the input tensors of the model.
         :param output_names: Names to be assigned to the output tensors of the model.
         :param model_args: Tuple of additional positional and keyword arguments
```

### Comparing `nncf-2.4.0/nncf/common/factory.py` & `nncf-2.5.0/nncf/common/factory.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,29 +1,28 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import TypeVar
 
+from nncf.common.engine import Engine
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.model_transformer import ModelTransformer
+from nncf.common.graph.transformations.command_creation import CommandCreator
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
-from nncf.common.engine import Engine
 
-TModel = TypeVar('TModel')
+TModel = TypeVar("TModel")
 
 
 class NNCFGraphFactory:
     @staticmethod
     def create(model: TModel) -> NNCFGraph:
         """
         Factory method to create backend-specific NNCFGraph instance based on the input model.
@@ -32,41 +31,85 @@
         :return: backend-specific NNCFGraph instance
         """
         model_backend = get_backend(model)
         if model_backend == BackendType.ONNX:
             from nncf.onnx.graph.nncf_graph_builder import GraphConverter
 
             return GraphConverter.create_nncf_graph(model)
-        raise RuntimeError('Cannot create backend-specific graph'
-                           'because {} is not supported!'.format(model_backend))
+        if model_backend == BackendType.OPENVINO:
+            from nncf.openvino.graph.nncf_graph_builder import GraphConverter
+
+            return GraphConverter.create_nncf_graph(model)
+        if model_backend == BackendType.TORCH:
+            return model.nncf.get_original_graph()
+        raise RuntimeError("Cannot create backend-specific graph because {} is not supported!".format(model_backend))
+
 
 class ModelTransformerFactory:
     @staticmethod
     def create(model: TModel) -> ModelTransformer:
         """
         Factory method to create backend-specific ModelTransformer instance based on the input model.
 
         :param model: backend-specific model instance
         :return: backend-specific ModelTransformer instance
         """
         model_backend = get_backend(model)
         if model_backend == BackendType.ONNX:
             from nncf.onnx.graph.model_transformer import ONNXModelTransformer
+
             return ONNXModelTransformer(model)
-        raise RuntimeError('Cannot create backend-specific model transformer'
-                           'because {} is not supported!'.format(model_backend))
+        if model_backend == BackendType.OPENVINO:
+            from nncf.openvino.graph.model_transformer import OVModelTransformer
+
+            return OVModelTransformer(model)
+        if model_backend == BackendType.TORCH:
+            from nncf.torch.nncf_network import PTModelTransformer
+
+            return PTModelTransformer(model)
+        raise RuntimeError(
+            "Cannot create backend-specific model transformer because {} is not supported!".format(model_backend)
+        )
+
 
 class EngineFactory:
     @staticmethod
     def create(model: TModel) -> Engine:
         """
         Factory method to create backend-specific Engine instance based on the input model.
 
         :param model: backend-specific model instance
         :return: backend-specific Engine instance
         """
         model_backend = get_backend(model)
         if model_backend == BackendType.ONNX:
             from nncf.onnx.engine import ONNXEngine
+
             return ONNXEngine(model)
-        raise RuntimeError('Cannot create backend-specific engine'
-                           'because {} is not supported!'.format(model_backend))
+        if model_backend == BackendType.OPENVINO:
+            from nncf.openvino.engine import OVNativeEngine
+
+            return OVNativeEngine(model)
+        if model_backend == BackendType.TORCH:
+            from nncf.torch.engine import PTEngine
+
+            return PTEngine(model)
+        raise RuntimeError("Cannot create backend-specific engine because {} is not supported!".format(model_backend))
+
+
+class CommandCreatorFactory:
+    @staticmethod
+    def create(model: TModel) -> CommandCreator:
+        """
+        Factory method to create backend-specific `CommandCreator` instance based on the input model.
+
+        :param model: backend-specific model instance
+        :return: backend-specific CommandCreator instance
+        """
+        model_backend = get_backend(model)
+        if model_backend == BackendType.OPENVINO:
+            from nncf.openvino.graph.transformations.command_creation import OVCommandCreator
+
+            return OVCommandCreator()
+        raise RuntimeError(
+            "Cannot create backend-specific command creator because {} is not supported!".format(model_backend)
+        )
```

### Comparing `nncf-2.4.0/nncf/common/graph/definitions.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/rb/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,20 +1,13 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Backend-specific implementations of regularization-based (RB) sparsity.
 """
-
-MODEL_INPUT_OP_NAME = 'nncf_model_input'
-MODEL_OUTPUT_OP_NAME = 'nncf_model_output'
-
-
-class NNCFGraphNodeType:
-    INPUT_NODE = MODEL_INPUT_OP_NAME
-    OUTPUT_NODE = MODEL_OUTPUT_OP_NAME
```

### Comparing `nncf-2.4.0/nncf/common/graph/graph.py` & `nncf-2.5.0/nncf/common/graph/graph.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,47 +1,43 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import defaultdict
 from copy import deepcopy
-from typing import Any, Callable, Dict, KeysView, List, Tuple, Type, ValuesView
-from typing import Generator
+from typing import Any, Callable, Dict, Generator, KeysView, List, Tuple, Type, ValuesView
 
 import networkx as nx
 import networkx.algorithms.isomorphism as iso
 
+from nncf.common.graph.graph_matching import find_subgraphs_matching_pattern
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.graph.layer_attributes import Dtype
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
+from nncf.common.graph.patterns import GraphPattern
 from nncf.common.utils.dot_file_rw import write_dot_graph
 
 NNCFNodeName = str
 LayerName = str
 
 
 class NNCFNode:
     """
     Class describing nodes used in NNCFGraph.
     """
 
-    def __init__(self,
-                 node_id: int,
-                 node_name: NNCFNodeName,
-                 data: dict = None):
+    def __init__(self, node_id: int, node_name: NNCFNodeName, data: dict = None):
         self.node_id = node_id
         self.data = data if data else {}
         self.data[NNCFGraph.NODE_NAME_ATTR] = node_name
 
     @property
     def node_name(self) -> NNCFNodeName:
         return self.data.get(NNCFGraph.NODE_NAME_ATTR)
@@ -54,14 +50,18 @@
     def node_type(self) -> str:
         return self.data.get(NNCFGraph.NODE_TYPE_ATTR)
 
     @property
     def layer_name(self) -> LayerName:
         return self.data.get(NNCFGraph.LAYER_NAME_ATTR)
 
+    @layer_name.setter
+    def layer_name(self, data: Any) -> None:
+        self.data[NNCFGraph.LAYER_NAME_ATTR] = data
+
     @property
     def layer_attributes(self) -> BaseLayerAttributes:
         return self.data.get(NNCFGraph.LAYER_ATTRIBUTES)
 
     @layer_attributes.setter
     def layer_attributes(self, data: Any) -> None:
         self.data[NNCFGraph.LAYER_ATTRIBUTES] = data
@@ -79,39 +79,45 @@
     def is_shared(self) -> bool:
         return self.data.get(NNCFGraph.IS_SHARED_ATTR, False)
 
     def __repr__(self):
         return str(self)
 
     def __str__(self):
-        return ' '.join([str(self.node_id), self.node_name, self.node_type])
+        return " ".join([str(self.node_id), self.node_name, self.node_type])
 
     def __hash__(self):
         return hash(str(self))
 
     def __eq__(self, other):
-        return isinstance(other, NNCFNode) \
-               and self.node_id == other.node_id \
-               and self.data == other.data \
-               and self.node_type == other.node_type \
-               and self.layer_attributes == other.layer_attributes
+        return (
+            isinstance(other, NNCFNode)
+            and self.node_id == other.node_id
+            and self.data == other.data
+            and self.node_type == other.node_type
+            and self.layer_attributes == other.layer_attributes
+        )
 
 
 class NNCFGraphEdge:
     """
     A structure describing an edge in NNCFGraph. Since nodes of the NNCFGraph are operations
     on (activation) tensors, an edge in NNCFGraph is a representation of an activation tensor produced or
     consumed by an operation.
     """
 
-    def __init__(self, from_node: NNCFNode, to_node: NNCFNode,
-                 input_port_id: int,
-                 output_port_id: int,
-                 tensor_shape: List[int],
-                 dtype: Dtype):
+    def __init__(
+        self,
+        from_node: NNCFNode,
+        to_node: NNCFNode,
+        input_port_id: int,
+        output_port_id: int,
+        tensor_shape: List[int],
+        dtype: Dtype,
+    ):
         """
         :param from_node: An NNCFNode that sources the directed edge.
         :param to_node: An NNCFNode that sinks the directed edge.
         :param input_port_id: The ID of the tensor input to the `to_node` that this edge corresponds to.
         :param output_port_id: The ID of the tensor output of the `from_node` that this edge corresponds to..
         :param tensor_shape: The shape of the activation tensor the edge represents.
         :param dtype: The data type of the activation tensor the edge represents.
@@ -120,56 +126,59 @@
         self.to_node = to_node
         self.input_port_id = input_port_id
         self.output_port_id = output_port_id
         self.tensor_shape = tensor_shape
         self.dtype = dtype
 
     def __str__(self):
-        return str(self.from_node) + ' -> ' + str(self.tensor_shape) + ' -> ' + str(self.to_node)
+        return str(self.from_node) + " -> " + str(self.tensor_shape) + " -> " + str(self.to_node)
 
     def __hash__(self):
         return hash(str(self))
 
     def __eq__(self, other):
-        return self.from_node == other.from_node and self.to_node == other.to_node \
-               and self.tensor_shape == other.tensor_shape
+        return (
+            self.from_node == other.from_node
+            and self.to_node == other.to_node
+            and self.tensor_shape == other.tensor_shape
+        )
 
 
 class NNCFGraphPatternIO:
     """
     Describes the inputs and outputs of a subgraph in NNCFGraph.
     """
 
     def __init__(self, input_edges: List[NNCFGraphEdge], output_edges: List[NNCFGraphEdge]):
         self.input_edges = input_edges
         self.output_edges = output_edges
 
 
-#pylint:disable=too-many-public-methods
+# pylint:disable=too-many-public-methods
 class NNCFGraph:
     """
     Wrapper over a regular directed acyclic graph that represents a control flow/execution graph of a DNN
     providing some useful methods for graph traversal.
     """
 
-    ID_NODE_ATTR = 'id'
-    KEY_NODE_ATTR = 'key'
-    NODE_NAME_ATTR = 'node_name'
-    NODE_TYPE_ATTR = 'type'
-    METATYPE_ATTR = 'metatype'
-    LAYER_NAME_ATTR = 'layer_name'
-    LAYER_ATTRIBUTES = 'layer_attributes'
-    ACTIVATION_SHAPE_EDGE_ATTR = 'activation_shape'
-    INPUT_PORT_ID_EDGE_ATTR = 'input_port_id'
-    OUTPUT_PORT_ID_EDGE_ATTR = 'output_port_id'
-    IGNORED_ALGOS_ATTR = 'ignored_algos'
-    IS_IN_ITERATION_SCOPE_NODE_ATTR = 'is_in_iteration_scope'
-    IS_INTEGER_INPUT_NODE_ATTR = 'is_integer_input'
-    DTYPE_EDGE_ATTR = 'dtype'
-    IS_SHARED_ATTR = 'is_shared'
+    ID_NODE_ATTR = "id"
+    KEY_NODE_ATTR = "key"
+    NODE_NAME_ATTR = "node_name"
+    NODE_TYPE_ATTR = "type"
+    METATYPE_ATTR = "metatype"
+    LAYER_NAME_ATTR = "layer_name"
+    LAYER_ATTRIBUTES = "layer_attributes"
+    ACTIVATION_SHAPE_EDGE_ATTR = "activation_shape"
+    INPUT_PORT_ID_EDGE_ATTR = "input_port_id"
+    OUTPUT_PORT_ID_EDGE_ATTR = "output_port_id"
+    IGNORED_ALGOS_ATTR = "ignored_algos"
+    IS_IN_ITERATION_SCOPE_NODE_ATTR = "is_in_iteration_scope"
+    IS_INTEGER_INPUT_NODE_ATTR = "is_integer_input"
+    DTYPE_EDGE_ATTR = "dtype"
+    IS_SHARED_ATTR = "is_shared"
 
     def __init__(self):
         self._nx_graph = nx.DiGraph()
         self._node_id_to_key_dict = {}
         self._input_nncf_nodes = {}  # type: Dict[int, NNCFNode]
         self._output_nncf_nodes = {}  # type: Dict[int, NNCFNode]
 
@@ -245,17 +254,17 @@
         all_nodes = []
         for node_key in self.get_all_node_keys():
             nx_node = self._nx_graph.nodes[node_key]
             nncf_node = self._nx_node_to_nncf_node(nx_node)
             all_nodes.append(nncf_node)
         return all_nodes
 
-    def get_all_simple_paths(self,
-                             start_node_name: NNCFNodeName,
-                             end_node_name: NNCFNodeName) -> Generator[List[NNCFNodeName], None, None]:
+    def get_all_simple_paths(
+        self, start_node_name: NNCFNodeName, end_node_name: NNCFNodeName
+    ) -> Generator[List[NNCFNodeName], None, None]:
         """
         Generates all simple paths in the NNCFGraph from start node to end node.
         A simple path is a path with no repeated nodes.
 
         :param start_node_name: a name of starting node for path
         :param end_node_name: a name of node at which to end path
         :return: A generator that produces lists of simple paths. If there are no paths between the start and end nodes
@@ -265,21 +274,22 @@
         end_node = self.get_node_by_name(end_node_name)
         start_node_key = self.get_node_key_by_id(start_node.node_id)
         end_node_key = self.get_node_key_by_id(end_node.node_id)
         return nx.all_simple_paths(self._nx_graph, start_node_key, end_node_key)
 
     @staticmethod
     def _nx_node_to_nncf_node(nx_node: dict) -> NNCFNode:
-        return NNCFNode(node_id=nx_node[NNCFGraph.ID_NODE_ATTR],
-                        node_name=nx_node[NNCFGraph.NODE_NAME_ATTR],
-                        data=nx_node)
+        return NNCFNode(
+            node_id=nx_node[NNCFGraph.ID_NODE_ATTR], node_name=nx_node[NNCFGraph.NODE_NAME_ATTR], data=nx_node
+        )
 
     @staticmethod
-    def _get_edge_boundaries(match: List[str], graph: nx.DiGraph) -> Tuple[
-        List[Tuple[str, str]], List[Tuple[str, str]]]:
+    def _get_edge_boundaries(
+        match: List[str], graph: nx.DiGraph
+    ) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:
         out_edge_boundary = list(nx.edge_boundary(graph, match, data=True))
         complement = list(filter(lambda x: x not in match, graph.nodes.keys()))
         in_edge_boundary = list(nx.edge_boundary(graph, complement, data=True))
         return sorted(in_edge_boundary), sorted(out_edge_boundary)  # must be sorted for determinism
 
     def get_node_key_by_id(self, node_id: id) -> str:
         """
@@ -330,49 +340,58 @@
         :return:  List of output edges for the node sorted by output port ID.
         """
 
         output_nodes = self.get_next_nodes(node)
         edges = [self.get_edge(node, to_node) for to_node in output_nodes]
         return sorted(edges, key=lambda x: x.output_port_id)
 
-    def traverse_graph(self,
-                       curr_node: NNCFNode,
-                       traverse_function: Callable[[NNCFNode, List[Any]], Tuple[bool, List[Any]]],
-                       traverse_forward: bool = True):
+    def traverse_graph(
+        self,
+        curr_node: NNCFNode,
+        traverse_function: Callable[[NNCFNode, List[Any]], Tuple[bool, List[Any]]],
+        traverse_forward: bool = True,
+    ):
         """
         Traverses graph up or down starting form `curr_node` node.
 
         :param curr_node: Node from which traversal is started.
         :param traverse_function: Function describing condition of traversal continuation/termination.
         :param traverse_forward: Flag specifying direction of traversal.
         :return:
         """
         output = []
         return self._traverse_graph_recursive_helper(curr_node, traverse_function, output, traverse_forward)
 
-    def _traverse_graph_recursive_helper(self, curr_node: NNCFNode,
-                                         traverse_function: Callable[[NNCFNode, List[Any]], Tuple[bool, List[Any]]],
-                                         output: List[Any], traverse_forward: bool):
+    def _traverse_graph_recursive_helper(
+        self,
+        curr_node: NNCFNode,
+        traverse_function: Callable[[NNCFNode, List[Any]], Tuple[bool, List[Any]]],
+        output: List[Any],
+        traverse_forward: bool,
+    ):
         is_finished, output = traverse_function(curr_node, output)
         get_nodes_fn = self.get_next_nodes if traverse_forward else self.get_previous_nodes
         if not is_finished:
             for node in get_nodes_fn(curr_node):
                 self._traverse_graph_recursive_helper(node, traverse_function, output, traverse_forward)
         return output
 
-    def add_nncf_node(self, node_name: str,
-                      node_type: str,
-                      node_metatype: Type[OperatorMetatype],
-                      layer_attributes: BaseLayerAttributes = None,
-                      node_id_override: int = None,
-                      layer_name: LayerName = None,
-                      ignored_algorithms: List[str] = None,
-                      is_in_iteration_scope: bool = False,
-                      is_integer_input: bool = False,
-                      is_shared: bool = False) -> NNCFNode:
+    def add_nncf_node(
+        self,
+        node_name: str,
+        node_type: str,
+        node_metatype: Type[OperatorMetatype],
+        layer_attributes: BaseLayerAttributes = None,
+        node_id_override: int = None,
+        layer_name: LayerName = None,
+        ignored_algorithms: List[str] = None,
+        is_in_iteration_scope: bool = False,
+        is_integer_input: bool = False,
+        is_shared: bool = False,
+    ) -> NNCFNode:
         """
         Adds a node into the graph. A node represents an operation being performed on tensors.
         :param node_name: The name of the node to add - will serve as a human-readable and specifiable ID.
         :param node_type: The type of the node - usually a framework-specific string representation of the operation.
         :param node_metatype: The metatype of the node - a framework-abstract definition of what the operation
             actually means.
         :param layer_attributes: Must be passed for operations that, in order to be processed
@@ -401,29 +420,29 @@
             node_ids = self.get_all_node_ids()
             if node_ids:
                 node_id = max(self.get_all_node_ids()) + 1
             else:
                 node_id = 0
 
         if node_id in self._node_id_to_key_dict:
-            raise ValueError(f'NNCF node with id {node_id} is already in the NNCFGraph')
+            raise ValueError(f"NNCF node with id {node_id} is already in the NNCFGraph")
 
-        node_key = f'{node_id} {node_name}'
+        node_key = f"{node_id} {node_name}"
 
         self._node_id_to_key_dict[node_id] = node_key
         attrs = {
             NNCFGraph.ID_NODE_ATTR: node_id,
             NNCFGraph.NODE_NAME_ATTR: node_name,
             NNCFGraph.KEY_NODE_ATTR: node_key,
             NNCFGraph.NODE_TYPE_ATTR: node_type,
             NNCFGraph.LAYER_NAME_ATTR: layer_name,
             NNCFGraph.METATYPE_ATTR: node_metatype,
             NNCFGraph.IS_SHARED_ATTR: is_shared,
             NNCFGraph.IS_IN_ITERATION_SCOPE_NODE_ATTR: is_in_iteration_scope,
-            NNCFGraph.IS_INTEGER_INPUT_NODE_ATTR: is_integer_input
+            NNCFGraph.IS_INTEGER_INPUT_NODE_ATTR: is_integer_input,
         }
         if layer_attributes is not None:
             attrs[NNCFGraph.LAYER_ATTRIBUTES] = layer_attributes
 
         if ignored_algorithms is None:
             ignored_algorithms = []
         attrs[NNCFGraph.IGNORED_ALGOS_ATTR] = ignored_algorithms
@@ -439,19 +458,23 @@
 
         if layer_name is not None:
             self._node_ids_vs_layer_names[node.node_id] = layer_name
             self._layer_name_vs_shared_nodes[layer_name].append(node)
 
         return node
 
-    def add_edge_between_nncf_nodes(self, from_node_id: int, to_node_id: int,
-                                    tensor_shape: List[int],
-                                    input_port_id: int,
-                                    output_port_id: int,
-                                    dtype: Dtype):
+    def add_edge_between_nncf_nodes(
+        self,
+        from_node_id: int,
+        to_node_id: int,
+        tensor_shape: List[int],
+        input_port_id: int,
+        output_port_id: int,
+        dtype: Dtype,
+    ):
         """
         Adds a directed edge between two `NNCFNode`s that are already present in the graph.
         The edge represents an activation tensor, produced or consumed by an operation (which is represented by a node)
         :param from_node_id: The `NNCFNode.node_id` of the node that produces the tensor represented by the edge.
         :param to_node_id: The `NNCFNode.node_id` of the node that consumes the tensor represented by the edge.
         :param tensor_shape: The shape of the tensor represented by the edge.
         :param input_port_id: Specifies the index among the possible inputs of the `to_node_id` node' that this tensor
@@ -462,41 +485,43 @@
         """
         from_node_key = self._node_id_to_key_dict[from_node_id]
         to_node_key = self._node_id_to_key_dict[to_node_id]
 
         err_reason = None
 
         if from_node_key not in self._nx_graph.nodes:
-            err_reason = f'node {from_node_key} not in NNCFGraph'
+            err_reason = f"node {from_node_key} not in NNCFGraph"
         if to_node_key not in self._nx_graph.nodes:
-            err_reason = f'node {from_node_key} not in NNCFGraph'
+            err_reason = f"node {from_node_key} not in NNCFGraph"
         if from_node_id in self._output_nncf_nodes:
-            err_reason = 'cannot add edges *from* output nodes'
+            err_reason = "cannot add edges *from* output nodes"
         if to_node_id in self._input_nncf_nodes:
-            err_reason = 'cannot add edges *to* input nodes'
+            err_reason = "cannot add edges *to* input nodes"
 
         if err_reason is not None:
-            raise ValueError(f'Cannot add edge from {from_node_key} to {to_node_key} - {err_reason}!')
+            raise ValueError(f"Cannot add edge from {from_node_key} to {to_node_key} - {err_reason}!")
 
         attrs = {
             NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR: tensor_shape,
             NNCFGraph.INPUT_PORT_ID_EDGE_ATTR: input_port_id,
             NNCFGraph.OUTPUT_PORT_ID_EDGE_ATTR: output_port_id,
-            NNCFGraph.DTYPE_EDGE_ATTR: dtype
+            NNCFGraph.DTYPE_EDGE_ATTR: dtype,
         }
         self._nx_graph.add_edge(from_node_key, to_node_key, **attrs)
 
     def topological_sort(self) -> List[NNCFNode]:
         """
         Returns nodes in topologically sorted order, additionally sorted in ascending node ID order.
         """
-        return [self._nx_node_to_nncf_node(self._nx_graph.nodes[node_name])
-                for node_name in
-                nx.lexicographical_topological_sort(self._nx_graph,
-                                                    key=lambda x: self._nx_graph.nodes[x][NNCFGraph.ID_NODE_ATTR])]
+        return [
+            self._nx_node_to_nncf_node(self._nx_graph.nodes[node_name])
+            for node_name in nx.lexicographical_topological_sort(
+                self._nx_graph, key=lambda x: self._nx_graph.nodes[x][NNCFGraph.ID_NODE_ATTR]
+            )
+        ]
 
     def dump_graph(self, path: str):
         write_dot_graph(self.get_graph_for_structure_analysis(), path)
 
     def visualize_graph(self, path: str):
         out_graph = self._get_graph_for_visualization()
         write_dot_graph(out_graph, path)
@@ -507,94 +532,95 @@
         The new graph has certain node attributes omitted, compared to the graph stored inside NNCFGraph.
         If the node name consists of a special reserved character, this character will be replaced.
 
         :param extended: whether the graph edges should have attributes: shape of the tensor and tensor primitive type.
         :return: An nx.DiGraph to be used for structure analysis
         """
         # .dot format reserves ':' character in node names
-        __RESERVED_DOT_CHARACTER = ':'
-        __CHARACTER_REPLACE_TO = '^'
+        __RESERVED_DOT_CHARACTER = ":"
+        __CHARACTER_REPLACE_TO = "^"
 
         out_graph = nx.DiGraph()
         for node_name, node in self._nx_graph.nodes.items():
             visualization_node_name = node_name.replace(__RESERVED_DOT_CHARACTER, __CHARACTER_REPLACE_TO)
-            attrs_node = {
-                'id': node[NNCFGraph.ID_NODE_ATTR],
-                'type': node[NNCFGraph.NODE_TYPE_ATTR]
-            }
-            for attr in ['color', 'label', 'style']:
+            attrs_node = {"id": node[NNCFGraph.ID_NODE_ATTR], "type": node[NNCFGraph.NODE_TYPE_ATTR]}
+            for attr in ["color", "label", "style"]:
                 if attr in node:
                     attrs_node[attr] = node[attr]
             # If the node_name has reserved character, use visualization_node_name as node name.
             # While use 'label' attribute with original node name for visualization.
-            if 'label' not in attrs_node and __RESERVED_DOT_CHARACTER in node_name:
-                attrs_node['label'] = node_name
+            if "label" not in attrs_node and __RESERVED_DOT_CHARACTER in node_name:
+                attrs_node["label"] = node_name
 
             out_graph.add_node(visualization_node_name, **attrs_node)
 
         for u, v in self._nx_graph.edges:
             edge = self._nx_graph.edges[u, v]
             attrs_edge = {}
             u = u.replace(__RESERVED_DOT_CHARACTER, __CHARACTER_REPLACE_TO)
             v = v.replace(__RESERVED_DOT_CHARACTER, __CHARACTER_REPLACE_TO)
             if extended:
                 if edge[NNCFGraph.DTYPE_EDGE_ATTR] is Dtype.INTEGER:
-                    attrs_edge['style'] = 'dashed'
+                    attrs_edge["style"] = "dashed"
                 else:
-                    attrs_edge['style'] = 'solid'
-                attrs_edge['label'] = edge[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR]
+                    attrs_edge["style"] = "solid"
+                attrs_edge["label"] = edge[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR]
             out_graph.add_edge(u, v, **attrs_edge)
         return out_graph
 
     def _get_graph_for_visualization(self) -> nx.DiGraph:
         """
         :return: A user-friendly graph .dot file, making it easier to debug the network and setup
         ignored/target scopes.
         """
         out_graph = nx.DiGraph()
         for node in self.get_all_nodes():
             attrs_node = {}
-            attrs_node['label'] = f"{node.node_id} {node.node_name}"
+            attrs_node["label"] = f"{node.node_id} {node.node_name}"
             node_key = self.get_node_key_by_id(node.node_id)
             out_graph.add_node(node_key, **attrs_node)
 
         for u, v in self._nx_graph.edges:
             edge = self._nx_graph.edges[u, v]
             if edge[NNCFGraph.DTYPE_EDGE_ATTR] is Dtype.INTEGER:
-                style = 'dashed'
+                style = "dashed"
             else:
-                style = 'solid'
-            edge_label = f"{edge[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR]} \\n" \
-                         f"{edge[NNCFGraph.OUTPUT_PORT_ID_EDGE_ATTR]} -> {edge[NNCFGraph.INPUT_PORT_ID_EDGE_ATTR]}"
+                style = "solid"
+            edge_label = (
+                f"{edge[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR]} \\n"
+                f"{edge[NNCFGraph.OUTPUT_PORT_ID_EDGE_ATTR]} -> {edge[NNCFGraph.INPUT_PORT_ID_EDGE_ATTR]}"
+            )
             out_graph.add_edge(u, v, label=edge_label, style=style)
 
-        mapping = {k: v['label'] for k, v in out_graph.nodes.items()}
+        mapping = {k: v["label"] for k, v in out_graph.nodes.items()}
         out_graph = nx.relabel_nodes(out_graph, mapping)
         for node in out_graph.nodes.values():
-            node.pop('label')
+            node.pop("label")
 
         return out_graph
 
     def get_node_by_name(self, name: NNCFNodeName) -> NNCFNode:
         matches = [node for node in self.get_all_nodes() if node.node_name == name]
         if not matches:
-            raise RuntimeError('Could not find a node {} in NNCFGraph!'.format(name))
+            raise RuntimeError("Could not find a node {} in NNCFGraph!".format(name))
         if len(matches) > 1:
-            raise RuntimeError('More than one node in NNCFGraph matches name {}:\n{}'.
-                               format(name,
-                                      '\t\n'.join(
-                                          [str(n.node_id) for n in matches])))
+            raise RuntimeError(
+                "More than one node in NNCFGraph matches name {}:\n{}".format(
+                    name, "\t\n".join([str(n.node_id) for n in matches])
+                )
+            )
         return next(iter(matches))
 
-    def __eq__(self, other: 'NNCFGraph'):
-        nm = iso.categorical_node_match([NNCFGraph.ID_NODE_ATTR,
-                                         NNCFGraph.KEY_NODE_ATTR,
-                                         NNCFGraph.LAYER_ATTRIBUTES], [None, None, None])
-        em = iso.categorical_edge_match([NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR,
-                                         NNCFGraph.INPUT_PORT_ID_EDGE_ATTR], [None, None])
+    def __eq__(self, other: "NNCFGraph"):
+        nm = iso.categorical_node_match(
+            [NNCFGraph.ID_NODE_ATTR, NNCFGraph.KEY_NODE_ATTR, NNCFGraph.LAYER_ATTRIBUTES], [None, None, None]
+        )
+        em = iso.categorical_edge_match(
+            [NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR, NNCFGraph.INPUT_PORT_ID_EDGE_ATTR], [None, None]
+        )
         return nx.is_isomorphic(self._nx_graph, other._nx_graph, node_match=nm, edge_match=em)
 
     def get_nx_graph_copy(self) -> nx.DiGraph:
         return deepcopy(self._nx_graph)
 
     def get_nncf_graph_pattern_io(self, match: List[str]) -> NNCFGraphPatternIO:
         """
@@ -612,50 +638,82 @@
         input_nncf_edges = []
         output_nncf_edges = []
 
         for nx_edge in boundary:
             from_node_key = nx_edge[0]
             to_node_key = nx_edge[1]
             data = nx_edge[2]
-            nncf_edge = NNCFGraphEdge(self._nx_node_to_nncf_node(self._nx_graph.nodes[from_node_key]),
-                                      self._nx_node_to_nncf_node(self._nx_graph.nodes[to_node_key]),
-                                      input_port_id=data[NNCFGraph.INPUT_PORT_ID_EDGE_ATTR],
-                                      output_port_id=data[NNCFGraph.OUTPUT_PORT_ID_EDGE_ATTR],
-                                      tensor_shape=data[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR],
-                                      dtype=data[NNCFGraph.DTYPE_EDGE_ATTR])
+            nncf_edge = NNCFGraphEdge(
+                self._nx_node_to_nncf_node(self._nx_graph.nodes[from_node_key]),
+                self._nx_node_to_nncf_node(self._nx_graph.nodes[to_node_key]),
+                input_port_id=data[NNCFGraph.INPUT_PORT_ID_EDGE_ATTR],
+                output_port_id=data[NNCFGraph.OUTPUT_PORT_ID_EDGE_ATTR],
+                tensor_shape=data[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR],
+                dtype=data[NNCFGraph.DTYPE_EDGE_ATTR],
+            )
             if from_node_key in match:
                 output_nncf_edges.append(nncf_edge)
             elif to_node_key in match:
                 input_nncf_edges.append(nncf_edge)
             else:
-                raise RuntimeError('Invalid graph expression supplied!')
+                raise RuntimeError("Invalid graph expression supplied!")
 
         return NNCFGraphPatternIO(input_nncf_edges, output_nncf_edges)
 
     def get_nx_edge(self, node_u: NNCFNode, node_v: NNCFNode):
         nx_node_u = self._nx_graph.nodes[self._node_id_to_key_dict[node_u.node_id]]
         nx_node_v = self._nx_graph.nodes[self._node_id_to_key_dict[node_v.node_id]]
-        return self._nx_graph.edges[nx_node_u['key'], nx_node_v['key']]
+        return self._nx_graph.edges[nx_node_u["key"], nx_node_v["key"]]
 
     def get_nodes_count(self):
         return self._nx_graph.number_of_nodes()
 
     def get_edge(self, from_node: NNCFNode, to_node: NNCFNode) -> NNCFGraphEdge:
         """
         Returns an NNCFGraphEdge object that corresponds to an edge connecting two given NNCFNodes in this
         graph.
         :param from_node: The NNCFNode in this graph that sources the edge.
         :param to_node: The NNCFNode in this graph that sinks the edge.
         :return: The NNCFGraphEdge object representing the edge between `from_node` and `to_node`.
         """
         data = self.get_nx_edge(from_node, to_node)
-        return NNCFGraphEdge(from_node,
-                             to_node,
-                             data[NNCFGraph.INPUT_PORT_ID_EDGE_ATTR],
-                             data[NNCFGraph.OUTPUT_PORT_ID_EDGE_ATTR],
-                             data[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR],
-                             data[NNCFGraph.DTYPE_EDGE_ATTR])
+        return NNCFGraphEdge(
+            from_node,
+            to_node,
+            data[NNCFGraph.INPUT_PORT_ID_EDGE_ATTR],
+            data[NNCFGraph.OUTPUT_PORT_ID_EDGE_ATTR],
+            data[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR],
+            data[NNCFGraph.DTYPE_EDGE_ATTR],
+        )
 
     def get_all_edges(self) -> Generator[NNCFGraphEdge, None, None]:
         for nx_edge in self._nx_graph.in_edges:
-            yield self.get_edge(self.get_node_by_key(nx_edge[0]),
-                                self.get_node_by_key(nx_edge[1]))
+            yield self.get_edge(self.get_node_by_key(nx_edge[0]), self.get_node_by_key(nx_edge[1]))
+
+    def remove_nodes_from(self, nodes: List[NNCFNode]) -> None:
+        """
+        Removes nodes from the current NNCFGraph instance.
+        We use the remove_node method here because remove_nodes_from uses a silent fail instead of an exception.
+
+        :param nodes: List of NNCFNodes to remove.
+        """
+        for node in nodes:
+            self._nx_graph.remove_node(node.data["key"])
+
+        self._node_id_to_key_dict = {}
+        for node_key, node in self._nx_graph.nodes.items():
+            self._node_id_to_key_dict[node["id"]] = node_key
+
+    def find_matching_nodes(self, patterns: GraphPattern) -> List[NNCFNode]:
+        """
+        Returns nodes of matched pattern in patterns.
+
+        :param patterns: Instance of GraphPattern containing all patterns.
+        :return: Nodes that are matched patterns.
+        The returned nodes order relies on DiGraphMatcher isomorphic subgraphs matching logic from networkX package.
+        DiGraphMatcher does not guarantee a specific order for returning isomorphic subgraphs.
+        """
+        output = []
+        for matched_subgraph in find_subgraphs_matching_pattern(self._nx_graph, patterns):
+            for node_key in matched_subgraph:
+                output.append(self.get_node_by_key(node_key))
+        return output
```

### Comparing `nncf-2.4.0/nncf/common/graph/graph_matching.py` & `nncf-2.5.0/nncf/common/graph/graph_matching.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,30 +1,29 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import List, Set
 
-from typing import List
-from typing import Set
 import networkx as nx
 import networkx.algorithms.isomorphism as ism
+
 from nncf.common.graph.patterns import GraphPattern
-from nncf.common.graph.graph import NNCFGraph
 
 
-def is_subgraph_has_inner_outgoing_edges(graph: nx.DiGraph, full_subgraph_with_non_pattern_nodes: List[str],
-                                         pattern_subgraph: List[str]) -> bool:
+def is_subgraph_has_inner_outgoing_edges(
+    graph: nx.DiGraph, full_subgraph_with_non_pattern_nodes: List[str], pattern_subgraph: List[str]
+) -> bool:
     """
     Checks out whether the 'pattern_subgraph' has outgoing edges,
     that aren't connected with nodes from full_subgraph_with_non_pattern_nodes.
     Example:
     (conv2d + BN + ReLU pattern):
             ...
              |
@@ -77,22 +76,24 @@
         for attr in node_2:
             if attr == GraphPattern.LABEL_ATTR:
                 continue
             if attr == GraphPattern.METATYPE_ATTR:
                 # GraphPattern.ANY_PATTERN_NODE_TYPE and GraphPattern.NON_PATTERN_NODE_TYPE
                 # are matched to any node type.
 
-                if GraphPattern.ANY_PATTERN_NODE_TYPE in node_2[attr] or \
-                        GraphPattern.NON_PATTERN_NODE_TYPE in node_2[attr]:
+                if (
+                    GraphPattern.ANY_PATTERN_NODE_TYPE in node_2[attr]
+                    or GraphPattern.NON_PATTERN_NODE_TYPE in node_2[attr]
+                ):
                     continue
                 # Torch and TF pattern mapping based on 'type' section,
                 # While ONNX mapping based on metatypes -
                 # to support all of them, we need to check the existane of the attributes
-                if NNCFGraph.METATYPE_ATTR in node_1:
-                    if node_1[NNCFGraph.METATYPE_ATTR] in node_2[attr]:
+                if GraphPattern.NODE_TYPE_ATTR in node_1:
+                    if node_1[GraphPattern.NODE_TYPE_ATTR] in node_2[attr]:
                         continue
             if node_1[attr] not in node_2[attr]:
                 return False
         return True
 
     def are_edges_matching(edge_1, edge_2):
         for attr in edge_2:
@@ -119,21 +120,20 @@
 
     # Get all patterns sorted by their lengths
     # as we want match the longest patterns first
 
     patterns = sorted(patterns, key=sort_patterns, reverse=True)
 
     for pattern in patterns:
-        matcher = ism.DiGraphMatcher(graph, pattern,
-                                     node_match=are_nodes_matching,
-                                     edge_match=are_edges_matching)
+        matcher = ism.DiGraphMatcher(graph, pattern, node_match=are_nodes_matching, edge_match=are_edges_matching)
         for subgraph in matcher.subgraph_isomorphisms_iter():
             # Bottleneck that need to sort by id for result consistency
-            pattern_subgraph = list(nx.lexicographical_topological_sort(graph.subgraph(subgraph),
-                                                                        key=lambda x: int(x.split()[0])))
+            pattern_subgraph = list(
+                nx.lexicographical_topological_sort(graph.subgraph(subgraph), key=lambda x: int(x.split()[0]))
+            )
 
             full_subgraph_with_non_pattern_nodes = pattern_subgraph[:]
             outside_pattern_nodes = []
 
             # If some nodes are outside the pattern - remove them from pattern_subgraph
 
             for node, pattern_node_id in matcher.mapping.items():
```

### Comparing `nncf-2.4.0/nncf/common/graph/layer_attributes.py` & `nncf-2.5.0/nncf/common/graph/layer_attributes.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,80 +1,77 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from abc import ABC
 from abc import abstractmethod
+from dataclasses import dataclass
 from enum import Enum
-from typing import List, Tuple, Any, Union
+from typing import Any, List, Tuple, Union
 
 
 class Dtype(Enum):
-    FLOAT = 'float'
-    INTEGER = 'int'
+    FLOAT = "float"
+    INTEGER = "int"
 
 
 class BaseLayerAttributes(ABC):
     """
     This class stores base useful for some algorithms attributes
     of modules/layers.
     """
 
 
 class MultipleInputLayerAttributes(BaseLayerAttributes):
-    """
-    Represents a layer with multiple inputs.
-    """
+    def __init__(self, axis: int):
+        """
 
-    def __init__(self,
-                 axis: int):
+        :param axis: the dimension over which the inputs are combined (e.g. concatenated).
+        """
         self.axis = axis
 
     def __eq__(self, other: Any):
-        return isinstance(other, MultipleInputLayerAttributes) \
-               and self.axis == other.axis
+        return isinstance(other, MultipleInputLayerAttributes) and self.axis == other.axis
 
 
 class MultipleOutputLayerAttributes(BaseLayerAttributes):
-    """
-    Represents a layer with multiple outputs.
-    """
+    def __init__(self, chunks: Union[int, List], axis: int):
+        """
 
-    def __init__(self,
-                 chunks: Union[int, List],
-                 axis: int):
+        :param chunks:  number of chunks (outputs)
+        :param axis: the dimension along which to make multiple outputs (e.g. split the tensor).
+        """
         self.chunks = chunks
         self.axis = axis
 
     def __eq__(self, other: Any):
-        return isinstance(other, MultipleOutputLayerAttributes) \
-               and self.chunks == other.chunks \
-               and self.axis == other.axis
+        return (
+            isinstance(other, MultipleOutputLayerAttributes) and self.chunks == other.chunks and self.axis == other.axis
+        )
 
 
 class WeightedLayerAttributes(BaseLayerAttributes):
-    """
-    Represents a layer with weights.
-    """
-
     def __init__(self, weight_requires_grad: bool, dtype: Dtype = Dtype.FLOAT):
+        """
+
+        :param weight_requires_grad: Is True if gradients need to be computed for the corresponding Tensor,
+        False otherwise.
+        :param dtype: is an object that represents the type of data.
+        """
         self.weight_requires_grad = weight_requires_grad
         self.dtype = dtype
 
     def __eq__(self, other: Any):
-        return isinstance(other, WeightedLayerAttributes) \
-               and self.weight_requires_grad == other.weight_requires_grad
+        return isinstance(other, WeightedLayerAttributes) and self.weight_requires_grad == other.weight_requires_grad
 
     @abstractmethod
     def get_weight_shape(self) -> List[int]:
         pass
 
     def get_num_filters(self) -> int:
         weight_shape = self.get_weight_shape()
@@ -87,33 +84,43 @@
 
 class GenericWeightedLayerAttributes(WeightedLayerAttributes):
     """
     Represents a weighted layer for which there is no information ahead of time
     of the exact meaning of the weight indices.
     """
 
-    def __init__(self, weight_requires_grad: bool, weight_shape: List[int],
-                 filter_dimension_idx: int = 0):
+    def __init__(self, weight_requires_grad: bool, weight_shape: List[int], filter_dimension_idx: int = 0):
+        """
+
+        :param weight_requires_grad: Is True if gradients need to be computed for the corresponding Tensor,
+        False otherwise.
+        :param weight_shape: shape of weight tensor.
+        :param filter_dimension_idx: the axis along which the filters are stored.
+        """
         super().__init__(weight_requires_grad)
         self.weight_shape = weight_shape
         self.filter_dimension_idx = filter_dimension_idx
 
     def get_weight_shape(self) -> List[int]:
         return self.weight_shape
 
     def get_target_dim_for_compression(self) -> int:
         return 0
 
 
 class LinearLayerAttributes(WeightedLayerAttributes):
-    def __init__(self,
-                 weight_requires_grad: bool,
-                 in_features: int,
-                 out_features: int,
-                 bias: bool = True):
+    def __init__(self, weight_requires_grad: bool, in_features: int, out_features: int, bias: bool = True):
+        """
+
+        :param weight_requires_grad: Is True if gradients need to be computed for the corresponding Tensor,
+        False otherwise.
+        :param in_features: number of input channels in the layer's input.
+        :param out_features: number of channels produced by the layer.
+        :param bias: If set to ``False``, the layer doesn't learn an additive bias.
+        """
         super().__init__(weight_requires_grad)
         self.in_features = in_features
         self.out_features = out_features
         self.bias = bias
 
     def get_weight_shape(self) -> List[int]:
         return [self.out_features, self.in_features]
@@ -122,90 +129,156 @@
         return self.out_features if self.bias is True else 0
 
     def get_target_dim_for_compression(self) -> int:
         return 0
 
 
 class ConvolutionLayerAttributes(WeightedLayerAttributes):
-    """
-    This class stores attributes of convolution modules/layers
-    that are useful for some algorithms.
-    """
-
-    def __init__(self,
-                 weight_requires_grad: bool,
-                 in_channels: int,
-                 out_channels: int,
-                 kernel_size: Tuple[int, ...],
-                 stride: Tuple[int, ...],
-                 groups: int,
-                 transpose: bool,
-                 padding_values: Tuple[int, ...]):
+    def __init__(
+        self,
+        weight_requires_grad: bool,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Tuple[int, ...],
+        stride: Tuple[int, ...],
+        groups: int,
+        transpose: bool,
+        padding_values: Tuple[int, ...],
+    ):
+        """
+
+        :param weight_requires_grad: Is True if gradients need to be computed for the corresponding Tensor,
+        False otherwise.
+        :param in_channels: number of input channels in the layer's input.
+        :param out_channels: number of channels produced by the layer.
+        :param kernel_size: size of the convolving kernel.
+        :param stride: stride of the convolution.
+        :param groups: number of blocked connections from input channels to output channels.
+        :param transpose: If set to `True`, the layer is an ordinary convolution, otherwise - transpose one.
+        :param padding_values: defines the amount of padding applied to the layer's input.
+        """
         super().__init__(weight_requires_grad)
         self.in_channels = in_channels
         self.out_channels = out_channels
         self.kernel_size = kernel_size
         self.stride = stride
         self.groups = groups
         self.transpose = transpose
         self.padding_values = padding_values
 
     def __eq__(self, other: Any):
-        return isinstance(other, ConvolutionLayerAttributes) \
-               and super().__eq__(other) \
-               and self.in_channels == other.in_channels \
-               and self.out_channels == other.out_channels \
-               and self.kernel_size == other.kernel_size \
-               and self.stride == other.stride \
-               and self.groups == other.groups \
-               and self.transpose == other.transpose
+        return (
+            isinstance(other, ConvolutionLayerAttributes)
+            and super().__eq__(other)
+            and self.in_channels == other.in_channels
+            and self.out_channels == other.out_channels
+            and self.kernel_size == other.kernel_size
+            and self.stride == other.stride
+            and self.groups == other.groups
+            and self.transpose == other.transpose
+        )
 
     def get_weight_shape(self) -> List[int]:
         if not self.transpose:
             return [self.out_channels, self.in_channels // self.groups, *self.kernel_size]
         return [self.in_channels, self.out_channels // self.groups, *self.kernel_size]
 
     def get_target_dim_for_compression(self) -> int:
         # Always quantize per each "out" channel
         if self.transpose:
             return 1
         return 0
 
 
 class GroupNormLayerAttributes(WeightedLayerAttributes):
-    """
-    This class stores attributes of group normalization modules/layers
-    that are useful for some algorithms.
-    """
+    def __init__(self, weight_requires_grad: bool, num_channels: int, num_groups: int):
+        """
 
-    def __init__(self,
-                 weight_requires_grad: bool,
-                 num_channels: int,
-                 num_groups: int):
+        :param weight_requires_grad: Is True if gradients need to be computed for the corresponding Tensor,
+        False otherwise.
+        :param num_channels: number of channels expected in the layer's input.
+        :param num_groups: number of groups to separate the channels into.
+        """
         super().__init__(weight_requires_grad)
         self.num_channels = num_channels
         self.num_groups = num_groups
 
     def __eq__(self, other: Any):
-        return isinstance(other, GroupNormLayerAttributes) \
-               and super().__eq__(other) \
-               and self.num_channels == other.num_channels \
-               and self.num_groups == other.num_groups
+        return (
+            isinstance(other, GroupNormLayerAttributes)
+            and super().__eq__(other)
+            and self.num_channels == other.num_channels
+            and self.num_groups == other.num_groups
+        )
 
     def get_weight_shape(self) -> List[int]:
         return [self.num_channels]
 
     def get_target_dim_for_compression(self) -> int:
         return 0
 
 
+@dataclass
 class ReshapeLayerAttributes(BaseLayerAttributes):
     """
-    This class stores attributes of reshape modules/layers
-    that are useful for some algorithms.
+    :param input_shape: number of elements of each of the axes of a input tensor.
+    :param output_shape: number of elements of each of the axes of a output tensor.
+    """
+
+    input_shape: List[int]
+    output_shape: List[int]
+
+
+@dataclass
+class TransposeLayerAttributes(BaseLayerAttributes):
+    """
+    :param dim0: the first dimension to be transposed.
+    :param dim1: the second dimension to be transposed.
+    """
+
+    dim0: int
+    dim1: int
+
+    def __eq__(self, other: Any) -> bool:
+        return (
+            isinstance(other, TransposeLayerAttributes)
+            and super().__eq__(other)
+            and self.dim0 == other.dim0
+            and self.dim1 == other.dim1
+        )
+
+
+@dataclass
+class PermuteLayerAttributes(BaseLayerAttributes):
+    """
+    :param permutation: the desired ordering of dimensions.
+    """
+
+    permutation: List[int]
+
+    def __eq__(self, other: Any) -> bool:
+        return (
+            isinstance(other, PermuteLayerAttributes)
+            and super().__eq__(other)
+            and len(self.permutation) == len(other.permutation)
+            and (l == r for l, r in zip(self.permutation, other.permutation))
+        )
+
+
+@dataclass
+class GetItemLayerAttributes(BaseLayerAttributes):
+    """
+    :param key: usually int, tuple of int or slice.
+    """
+
+    key: Any
+
+
+@dataclass
+class PadLayerAttributes(BaseLayerAttributes):
+    """
+    :param mode: mode of the padding operation.
+    :param value: fill value of the padding operation.
     """
 
-    def __init__(self,
-                 input_shape: List[int],
-                 output_shape: List[int]):
-        self.input_shape = input_shape
-        self.output_shape = output_shape
+    mode: str = "constant"
+    value: float = 0
```

### Comparing `nncf-2.4.0/nncf/common/graph/model_transformer.py` & `nncf-2.5.0/nncf/common/graph/model_transformer.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,25 +1,23 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import TypeVar
 
 from nncf.common.graph.transformations.layout import TransformationLayout
 
-TModel = TypeVar('TModel')
+TModel = TypeVar("TModel")
 
 
 class ModelTransformer:
     """
     Applies transformations to the model.
     """
```

### Comparing `nncf-2.4.0/nncf/common/graph/operator_metatypes.py` & `nncf-2.5.0/nncf/common/graph/operator_metatypes.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,56 +1,58 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import List
-from typing import Optional
-from typing import Type
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import List, Optional, Type
 
 from nncf.common.graph.definitions import NNCFGraphNodeType
 from nncf.common.utils.registry import Registry
 
 
 class OperatorMetatype:
     """
     Base class for grouping framework operators based on their semantic meaning.
+
+    :param name: The name of the operator.
+    :param hw_config_names: The names of the hardware configurations.
+    :param output_channel_axis: The axis along which the output channels of the operator are arranged.
     """
 
-    name = ''  # type: str
-    hw_config_names = []  # type: List[str]
+    name: str = ""
+    hw_config_names: List[str] = []
+    output_channel_axis: Optional[int] = None
+    ignored_input_ports: List[int] = []
 
     @classmethod
     def get_all_aliases(cls) -> List[str]:
         """
         Returns a list of the framework operator aliases.
 
         :return: A list of the framework operator aliases.
         """
         return []
 
     @classmethod
-    def get_subtypes(cls) -> List[Type['OperatorMetatype']]:
+    def get_subtypes(cls) -> List[Type["OperatorMetatype"]]:
         """
         Returns a list of 'OperatorMetatype' that are subtypes.
 
         :return: A subtype list.
         """
         return []
 
     @classmethod
-    def subtype_check(cls, metatype: Type['OperatorMetatype']) -> bool:
+    def subtype_check(cls, metatype: Type["OperatorMetatype"]) -> bool:
         """
         Check if a metatype is a subtype.
 
         :param metatype: An operator metatype.
         :return: True if metatype is a subtype otherwise False
         """
         subtypes = cls.get_subtypes()
@@ -93,19 +95,19 @@
             """
             cls_name = name_
             if cls_name is None:
                 cls_name = obj.__name__
             super_register(obj, cls_name)
             op_names = obj.get_all_aliases()
             for name in op_names:
-                if name in self._op_name_to_op_meta_dict \
-                        and not obj.subtype_check(self._op_name_to_op_meta_dict[name]):
+                if name in self._op_name_to_op_meta_dict and not obj.subtype_check(self._op_name_to_op_meta_dict[name]):
                     raise RuntimeError(
-                        'Inconsistent operator metatype registry - single patched '
-                        'op name maps to multiple metatypes!')
+                        "Inconsistent operator metatype registry - single patched "
+                        "op name maps to multiple metatypes!"
+                    )
 
                 self._op_name_to_op_meta_dict[name] = obj
             return obj
 
         return wrap
 
     def get_operator_metatype_by_op_name(self, op_name: str) -> Type[OperatorMetatype]:
@@ -116,38 +118,40 @@
         :return: The operator metatype.
         """
         if op_name not in self._op_name_to_op_meta_dict:
             return UnknownMetatype
         return self._op_name_to_op_meta_dict[op_name]
 
 
-NOOP_METATYPES = Registry('noop_metatypes')
-INPUT_NOOP_METATYPES = Registry('input_noop_metatypes')
-OUTPUT_NOOP_METATYPES = Registry('output_noop_metatypes')
+NOOP_METATYPES = Registry("noop_metatypes")
+INPUT_NOOP_METATYPES = Registry("input_noop_metatypes")
+OUTPUT_NOOP_METATYPES = Registry("output_noop_metatypes")
 
 
 class UnknownMetatype(OperatorMetatype):
     """
     UnknownMetatype is mapped to operations in NNCFGraph, which are unknown for algorithms,
     typically these are the operations that haven't been discovered before.
     Algorithms should avoid processing graph nodes with this metatype.
     """
+
     name = "unknown"
 
     @classmethod
     def get_all_aliases(cls) -> List[str]:
         return [cls.name]
 
 
 @NOOP_METATYPES.register()
 class NoopMetatype(OperatorMetatype):
     """
     NoopMetatype is mapped to operations in NNCFGraph, that doesn't influence an input tensor.
     The compression algorithms can safely ignore this node.
     """
+
     name = "noop"
 
     @classmethod
     def get_all_aliases(cls) -> List[str]:
         return [cls.name]
```

### Comparing `nncf-2.4.0/nncf/common/graph/transformations/__init__.py` & `nncf-2.5.0/nncf/tensorflow/functions.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,12 +1,25 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import tensorflow as tf
+
+
+@tf.function
+def logit(x):
+    return tf.math.log(x / (1 - x))
+
+
+@tf.custom_gradient
+def st_threshold(input_):
+    def grad(upstream):
+        return upstream
+
+    return tf.round(input_), grad
```

### Comparing `nncf-2.4.0/nncf/common/graph/transformations/commands.py` & `nncf-2.5.0/nncf/common/graph/transformations/commands.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from enum import IntEnum
-from typing import Any
-from typing import Dict
+from typing import Any, Dict
 
 from nncf.common.stateful_classes_registry import CommonStatefulClassesRegistry
 
 
 class TransformationPriority(IntEnum):
     """
     Defines priorities for compression and service operations that are
@@ -37,15 +34,15 @@
     DEFAULT_PRIORITY = 0
     FP32_TENSOR_STATISTICS_OBSERVATION = 1
     PRUNING_PRIORITY = 2
     SPARSIFICATION_PRIORITY = 3
     QUANTIZATION_PRIORITY = 11
 
 
-TARGET_TYPE_STATE_ATTR = 'name'
+TARGET_TYPE_STATE_ATTR = "name"
 
 
 class TargetType(IntEnum):
     """
     Describes the types of locations in the model that can be modified using NNCF
     in order to create a compressed model.
 
@@ -89,15 +86,15 @@
         represents state of the object.
 
         :return: state of the object
         """
         return {TARGET_TYPE_STATE_ATTR: self.name}
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TargetType':
+    def from_state(cls, state: Dict[str, Any]) -> "TargetType":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return TargetType[state[TARGET_TYPE_STATE_ATTR]]
 
@@ -114,29 +111,30 @@
     MULTI_INSERT = 1
     REMOVE = 2
     CHANGE = 3
     EXTRACT = 4
 
 
 class TargetPointStateNames:
-    TARGET_TYPE = 'target_type'
+    TARGET_TYPE = "target_type"
 
 
 @CommonStatefulClassesRegistry.register()
 class TargetPoint:
     """
     The base class for all target points.
 
     A target point is an object or spot in the model graph. It can be a layer,
     weights, position before or after layer and etc.
 
     For example, the transformation commands use `TargetPoint` to specify
     the target point in the model graph to which the transformation command
     will be applied.
     """
+
     _state_names = TargetPointStateNames
 
     def __init__(self, target_type: TargetType):
         """
         Constructor.
 
         :param target_type: Type of the target point.
@@ -144,16 +142,15 @@
         self._target_type = target_type
 
     @property
     def type(self) -> TargetType:
         return self._target_type
 
     def __eq__(self, other: Any) -> bool:
-        return isinstance(other, TargetPoint) and \
-            self.type == other.type
+        return isinstance(other, TargetPoint) and self.type == other.type
 
     def __str__(self) -> str:
         return str(self.type)
 
     def __hash__(self) -> int:
         return hash(str(self))
 
@@ -162,24 +159,25 @@
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
         return {self._state_names.TARGET_TYPE: self._target_type.get_state()}
 
+    def is_weight_target_point(self):
+        return self._target_type == TargetType.OPERATION_WITH_WEIGHTS
+
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TargetPoint':
+    def from_state(cls, state: Dict[str, Any]) -> "TargetPoint":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
-        kwargs = {
-            cls._state_names.TARGET_TYPE: TargetType.from_state(state[cls._state_names.TARGET_TYPE])
-        }
+        kwargs = {cls._state_names.TARGET_TYPE: TargetType.from_state(state[cls._state_names.TARGET_TYPE])}
         return cls(**kwargs)
 
 
 class Command:
     """
     The base class for non-target transformation commands.
     """
@@ -213,17 +211,19 @@
         super().__init__(command_type)
         self._target_point = target_point
 
     @property
     def target_point(self) -> TargetPoint:
         return self._target_point
 
-    def check_command_compatibility(self, command: 'TransformationCommand') -> bool:
-        return isinstance(command, TransformationCommand) and \
-               self.type == command.type and \
-               self.target_point == command.target_point
+    def check_command_compatibility(self, command: "TransformationCommand") -> bool:
+        return (
+            isinstance(command, TransformationCommand)
+            and self.type == command.type
+            and self.target_point == command.target_point
+        )
 
-    def union(self, other: 'TransformationCommand') -> 'TransformationCommand':
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
         raise NotImplementedError()
 
-    def __add__(self, other: 'TransformationCommand') -> 'TransformationCommand':
+    def __add__(self, other: "TransformationCommand") -> "TransformationCommand":
         return self.union(other)
```

### Comparing `nncf-2.4.0/nncf/common/graph/transformations/layout.py` & `nncf-2.5.0/nncf/common/graph/transformations/layout.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 from nncf.common.graph.transformations.commands import TransformationCommand
 
 
 class TransformationLayout:
@@ -40,15 +38,15 @@
         Registers the transformation command in the transformation layout.
 
         :param transformation: The transformation command to be registered in
             the transformation layout.
         """
         self.transformations.append(transformation)
 
-    def update(self, other: 'TransformationLayout') -> None:
+    def update(self, other: "TransformationLayout") -> None:
         """
         D.update(other), updates D from other.
 
         :param other: Another transformation layout.
         """
         for transformation in other.transformations:
             self.register(transformation)
```

### Comparing `nncf-2.4.0/nncf/common/hardware/__init__.py` & `nncf-2.5.0/nncf/config/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,12 +1,14 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+NNCF configuration file schemata and associated structures.
 """
+from nncf.config.config import NNCFConfig
```

### Comparing `nncf-2.4.0/nncf/common/hardware/config.py` & `nncf-2.5.0/nncf/common/hardware/config.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,133 +1,125 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from abc import ABC
 from abc import abstractmethod
 from collections import OrderedDict
 from enum import Enum
 from pathlib import Path
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Set
-from typing import Type
+from typing import Any, Dict, List, Optional, Set, Type
 
 import jstyleson as json
 
 from nncf.common.graph.operator_metatypes import OperatorMetatype
+from nncf.common.logging import nncf_logger
 from nncf.common.quantization import quantizers as quant
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.utils.helpers import product_dict
 from nncf.common.utils.os import safe_open
 from nncf.definitions import HW_CONFIG_RELATIVE_DIR
 from nncf.definitions import NNCF_PACKAGE_ROOT_DIR
-from nncf.common.logging import nncf_logger
 
 
 class HWConfigType(Enum):
-    CPU = 'CPU'
-    GPU = 'GPU'
-    VPU = 'VPU'
+    CPU = "CPU"
+    GPU = "GPU"
+    VPU = "VPU"
 
 
 HW_CONFIG_TYPE_TARGET_DEVICE_MAP = {
-    'ANY': HWConfigType.CPU.value,
-    'CPU': HWConfigType.CPU.value,
-    'VPU': HWConfigType.VPU.value,
-    'GPU': HWConfigType.GPU.value
+    "ANY": HWConfigType.CPU.value,
+    "CPU": HWConfigType.CPU.value,
+    "VPU": HWConfigType.VPU.value,
+    "GPU": HWConfigType.GPU.value,
 }
 
 
 HWConfigOpName = str
 
 
 def get_hw_config_type(target_device: str) -> Optional[HWConfigType]:
     """
     Returns hardware configuration type for target device
 
     :param target_device: A target device
     :raises ValueError: if target device is not supported yet
     :return: hardware configuration type or None for the 'TRIAL' target device
     """
-    if target_device == 'TRIAL':
+    if target_device == "TRIAL":
         return None
-    if target_device == 'CPU_SPR':
-        raise ValueError(f'{target_device} target device is not supported yet')
+    if target_device == "CPU_SPR":
+        raise ValueError(f"{target_device} target device is not supported yet")
     return HWConfigType(HW_CONFIG_TYPE_TARGET_DEVICE_MAP[target_device])
 
 
 class HWConfig(list, ABC):
-    QUANTIZATION_ALGORITHM_NAME = 'quantization'
-    ATTRIBUTES_NAME = 'attributes'
-    SCALE_ATTRIBUTE_NAME = 'scales'
-    UNIFIED_TYPE_NAME = 'unified'
-    ADJUST_PADDING_ATTRIBUTE_NAME = 'adjust_padding'
-
-    TYPE_TO_CONF_NAME_DICT = {
-        HWConfigType.CPU: 'cpu.json',
-        HWConfigType.VPU: 'vpu.json',
-        HWConfigType.GPU: 'gpu.json'
-    }
+    QUANTIZATION_ALGORITHM_NAME = "quantization"
+    ATTRIBUTES_NAME = "attributes"
+    SCALE_ATTRIBUTE_NAME = "scales"
+    UNIFIED_TYPE_NAME = "unified"
+    ADJUST_PADDING_ATTRIBUTE_NAME = "adjust_padding"
+
+    TYPE_TO_CONF_NAME_DICT = {HWConfigType.CPU: "cpu.json", HWConfigType.VPU: "vpu.json", HWConfigType.GPU: "gpu.json"}
 
     def __init__(self):
         super().__init__()
         self.registered_algorithm_configs = {}
         self.target_device = None
 
     @abstractmethod
     def _get_available_operator_metatypes_for_matching(self) -> List[Type[OperatorMetatype]]:
         pass
 
     @staticmethod
     def get_path_to_hw_config(hw_config_type: HWConfigType):
-        return '/'.join([NNCF_PACKAGE_ROOT_DIR, HW_CONFIG_RELATIVE_DIR,
-                         HWConfig.TYPE_TO_CONF_NAME_DICT[hw_config_type]])
+        return "/".join(
+            [NNCF_PACKAGE_ROOT_DIR, HW_CONFIG_RELATIVE_DIR, HWConfig.TYPE_TO_CONF_NAME_DICT[hw_config_type]]
+        )
 
     @classmethod
     def from_dict(cls, dct: dict):
         # pylint:disable=too-many-nested-blocks,too-many-branches
         hw_config = cls()
-        hw_config.target_device = dct['target_device']
+        hw_config.target_device = dct["target_device"]
 
-        for algorithm_name, algorithm_configs in dct.get('config', {}).items():
+        for algorithm_name, algorithm_configs in dct.get("config", {}).items():
             hw_config.registered_algorithm_configs[algorithm_name] = {}
             for algo_config_alias, algo_config in algorithm_configs.items():
                 for key, val in algo_config.items():
                     if not isinstance(val, list):
                         algo_config[key] = [val]
 
                 hw_config.registered_algorithm_configs[algorithm_name][algo_config_alias] = list(
-                    product_dict(algo_config))
+                    product_dict(algo_config)
+                )
 
-        for op_dict in dct.get('operations', []):
+        for op_dict in dct.get("operations", []):
             for algorithm_name in op_dict:
                 if algorithm_name not in hw_config.registered_algorithm_configs:
                     continue
                 tmp_config = {}
                 for algo_and_op_specific_field_name, algorithm_configs in op_dict[algorithm_name].items():
                     if not isinstance(algorithm_configs, list):
                         algorithm_configs = [algorithm_configs]
 
                     tmp_config[algo_and_op_specific_field_name] = []
                     for algorithm_config in algorithm_configs:
                         if isinstance(algorithm_config, str):  # Alias was supplied
                             tmp_config[algo_and_op_specific_field_name].extend(
-                                hw_config.registered_algorithm_configs[algorithm_name][algorithm_config])
+                                hw_config.registered_algorithm_configs[algorithm_name][algorithm_config]
+                            )
                         else:
                             for key, val in algorithm_config.items():
                                 if not isinstance(val, list):
                                     algorithm_config[key] = [val]
 
                             tmp_config[algo_and_op_specific_field_name].extend(list(product_dict(algorithm_config)))
 
@@ -142,113 +134,117 @@
         file_path = Path(path).resolve()
         with safe_open(file_path) as f:
             json_config = json.load(f, object_pairs_hook=OrderedDict)
             return cls.from_dict(json_config)
 
     @staticmethod
     def get_quantization_mode_from_config_value(str_val: str):
-        if str_val == 'symmetric':
+        if str_val == "symmetric":
             return QuantizationMode.SYMMETRIC
-        if str_val == 'asymmetric':
+        if str_val == "asymmetric":
             return QuantizationMode.ASYMMETRIC
-        raise RuntimeError('Invalid quantization type specified in HW config')
+        raise RuntimeError("Invalid quantization type specified in HW config")
 
     @staticmethod
     def get_is_per_channel_from_config_value(str_val: str):
-        if str_val == 'perchannel':
+        if str_val == "perchannel":
             return True
-        if str_val == 'pertensor':
+        if str_val == "pertensor":
             return False
-        raise RuntimeError('Invalid quantization granularity specified in HW config')
+        raise RuntimeError("Invalid quantization granularity specified in HW config")
 
     @staticmethod
     def get_qconf_from_hw_config_subdict(quantization_subdict: Dict):
-        bits = quantization_subdict['bits']
-        mode = HWConfig.get_quantization_mode_from_config_value(quantization_subdict['mode'])
-        is_per_channel = HWConfig.get_is_per_channel_from_config_value(quantization_subdict['granularity'])
+        bits = quantization_subdict["bits"]
+        mode = HWConfig.get_quantization_mode_from_config_value(quantization_subdict["mode"])
+        is_per_channel = HWConfig.get_is_per_channel_from_config_value(quantization_subdict["granularity"])
         signedness_to_force = None
-        if 'level_low' in quantization_subdict and 'level_high' in quantization_subdict:
+        if "level_low" in quantization_subdict and "level_high" in quantization_subdict:
             signedness_to_force = False
             if mode == QuantizationMode.SYMMETRIC:
-                if quantization_subdict['level_low'] < 0 < quantization_subdict['level_high']:
+                if quantization_subdict["level_low"] < 0 < quantization_subdict["level_high"]:
                     signedness_to_force = True
-                true_level_low, true_level_high, _ = quant.calculate_symmetric_level_ranges(bits, signed=True)
+                true_level_low, true_level_high = quant.calculate_symmetric_level_ranges(bits, signed=True)
             else:
                 signedness_to_force = True
-                true_level_low, true_level_high, _ = quant.calculate_asymmetric_level_ranges(bits)
+                true_level_low, true_level_high = quant.calculate_asymmetric_level_ranges(bits)
 
-            assert quantization_subdict['level_low'] == true_level_low, \
-                    'Invalid value of quantizer parameter `level_low`.\
-                         The parameter must be consistent with other parameters!'
-            assert quantization_subdict['level_high'] == true_level_high, \
-                    'Invalid value of quantizer parameter `level_high`.\
-                         The parameter must be consistent with other parameters!'
-
-        return QuantizerConfig(num_bits=bits,
-                               mode=mode,
-                               per_channel=is_per_channel,
-                               signedness_to_force=signedness_to_force)
+            assert (
+                quantization_subdict["level_low"] == true_level_low
+            ), "Invalid value of quantizer parameter `level_low`.\
+                         The parameter must be consistent with other parameters!"
+            assert (
+                quantization_subdict["level_high"] == true_level_high
+            ), "Invalid value of quantizer parameter `level_high`.\
+                         The parameter must be consistent with other parameters!"
+
+        return QuantizerConfig(
+            num_bits=bits, mode=mode, per_channel=is_per_channel, signedness_to_force=signedness_to_force
+        )
 
     @staticmethod
     def is_qconf_list_corresponding_to_unspecified_op(qconf_list: Optional[List[QuantizerConfig]]):
         return qconf_list is None
 
     @staticmethod
     def is_wildcard_quantization(qconf_list: Optional[List[QuantizerConfig]]):
         # Corresponds to an op itself being specified in the HW config, but having no associated quantization
         # configs specified
         return qconf_list is not None and len(qconf_list) == 0
 
-    def get_metatype_vs_quantizer_configs_map(self, for_weights=False) -> Dict[Type[OperatorMetatype],
-                                                                               Optional[List[QuantizerConfig]]]:
+    def get_metatype_vs_quantizer_configs_map(
+        self, for_weights=False
+    ) -> Dict[Type[OperatorMetatype], Optional[List[QuantizerConfig]]]:
         # 'None' for ops unspecified in HW config, empty list for wildcard quantization ops
         retval = {k: None for k in self._get_available_operator_metatypes_for_matching()}
-        config_key = 'weights' if for_weights else 'activations'
+        config_key = "weights" if for_weights else "activations"
         for op_dict in self:
             hw_config_op_name = op_dict["type"]
 
             metatypes = self._get_metatypes_for_hw_config_op(hw_config_op_name)
             if not metatypes:
                 nncf_logger.debug(
-                    'Operation name {} in HW config is not registered in NNCF under any supported operation '
-                    'metatype - will be ignored'.format(hw_config_op_name))
+                    "Operation name {} in HW config is not registered in NNCF under any supported operation "
+                    "metatype - will be ignored".format(hw_config_op_name)
+                )
 
             if self.QUANTIZATION_ALGORITHM_NAME in op_dict:
                 allowed_qconfs = op_dict[self.QUANTIZATION_ALGORITHM_NAME].get(config_key, [])
             else:
                 allowed_qconfs = []
 
             qconf_list_with_possible_duplicates = []
             for hw_config_qconf_dict in allowed_qconfs:
-                qconf_list_with_possible_duplicates.append(
-                    self.get_qconf_from_hw_config_subdict(hw_config_qconf_dict))
+                qconf_list_with_possible_duplicates.append(self.get_qconf_from_hw_config_subdict(hw_config_qconf_dict))
 
             qconf_list = list(OrderedDict.fromkeys(qconf_list_with_possible_duplicates))
 
             for meta in metatypes:
                 retval[meta] = qconf_list
 
         return retval
 
-    def _get_operations_with_attribute_values(self, attribute_name_vs_required_value: Dict[str, Any]) -> \
-        Set[Type[OperatorMetatype]]:
+    def _get_operations_with_attribute_values(
+        self, attribute_name_vs_required_value: Dict[str, Any]
+    ) -> Set[Type[OperatorMetatype]]:
         result = set()
         for op_dict in self:
             if self.ATTRIBUTES_NAME not in op_dict:
                 continue
             for attr_name, attr_value in attribute_name_vs_required_value.items():
                 is_value_matched = op_dict[self.ATTRIBUTES_NAME].get(attr_name) == attr_value
                 is_attr_set = attr_name in op_dict[self.ATTRIBUTES_NAME]
                 if is_value_matched and is_attr_set:
                     hw_config_op_name = op_dict["type"]
                     metatypes = self._get_metatypes_for_hw_config_op(hw_config_op_name)
                     if not metatypes:
                         nncf_logger.debug(
-                            'Operation name {} in HW config is not registered in NNCF under any supported '
-                            'operation metatype - will be ignored'.format(hw_config_op_name))
+                            "Operation name {} in HW config is not registered in NNCF under any supported "
+                            "operation metatype - will be ignored".format(hw_config_op_name)
+                        )
                     result.update(metatypes)
         return result
 
     def get_operations_with_unified_scales(self) -> Set[Type[OperatorMetatype]]:
         return self._get_operations_with_attribute_values({self.SCALE_ATTRIBUTE_NAME: self.UNIFIED_TYPE_NAME})
 
     def get_operations_with_adjusted_paddings(self) -> Set[Type[OperatorMetatype]]:
@@ -257,10 +253,11 @@
     def _get_metatypes_for_hw_config_op(self, hw_config_op: HWConfigOpName) -> Set[Type[OperatorMetatype]]:
         metatypes = set()
         for op_meta in self._get_available_operator_metatypes_for_matching():
             if hw_config_op in op_meta.hw_config_names:
                 metatypes.add(op_meta)
         if not metatypes:
             nncf_logger.debug(
-                'Operation name {} in HW config is not registered in NNCF under any supported '
-                'operation metatype - will be ignored'.format(hw_config_op))
+                "Operation name {} in HW config is not registered in NNCF under any supported "
+                "operation metatype - will be ignored".format(hw_config_op)
+            )
         return metatypes
```

### Comparing `nncf-2.4.0/nncf/common/hardware/configs/cpu.json` & `nncf-2.5.0/nncf/common/hardware/configs/cpu.json`

 * *Files 1% similar despite different names*

```diff
@@ -234,14 +234,28 @@
         {"type": "Reshape"},
         {
             "type": "Concat",
             "attributes": {
                 "scales": "unified"
             }
         },
+        {
+            "type": "LSTMSequence",
+            "quantization": {
+                "activations": "q8_a",
+                "weights": "q8_w_sym"
+            }
+        },
+        {
+            "type": "GRUSequence",
+            "quantization": {
+                "activations": "q8_a",
+                "weights": "q8_w_sym"
+            }
+        },
         {"type": "Flatten"},
         {"type": "Squeeze"},
         {"type": "Unsqueeze"},
         {"type": "Split"},
         {"type": "VariadicSplit"},
         {"type": "Crop"},
         {"type": "Transpose"},
```

### Comparing `nncf-2.4.0/nncf/common/hardware/configs/gpu.json` & `nncf-2.5.0/nncf/common/hardware/configs/gpu.json`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/common/hardware/configs/template.json` & `nncf-2.5.0/nncf/common/hardware/configs/template.json`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/common/hardware/configs/vpu.json` & `nncf-2.5.0/nncf/common/hardware/configs/vpu.json`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/common/initialization/batchnorm_adaptation.py` & `nncf-2.5.0/nncf/common/initialization/batchnorm_adaptation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,42 +1,36 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+import math
 from abc import ABC
 from abc import abstractmethod
 from typing import Optional
 
-import math
-
 from nncf.api.compression import TModel
 from nncf.common.initialization.dataloader import NNCFDataLoader
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
 
 
 class BatchnormAdaptationAlgorithmImpl(ABC):
     """
     This is the class from which all framework-specific implementations of
     the batch-norm statistics adaptation algorithm inherit.
     """
 
-    def __init__(self,
-                 data_loader: NNCFDataLoader,
-                 num_bn_adaptation_steps: int,
-                 device: Optional[str] = None):
+    def __init__(self, data_loader: NNCFDataLoader, num_bn_adaptation_steps: int, device: Optional[str] = None):
         """
         Initializes the batch-norm statistics adaptation algorithm implementation.
 
         :param data_loader: NNCF data loader.
         :param num_bn_adaptation_steps: Number of batches from the training dataset to pass
             through the model at initialization in order to update batch-norm statistics of
             the original model.
@@ -59,47 +53,48 @@
     """
     This algorithm updates the statistics of the batch normalization layers
     passing several batches of data through the model. This allows to correct
     the compression-induced bias in the model and reduce the corresponding
     accuracy drop even before model training.
     """
 
-    def __init__(self,
-                 data_loader: NNCFDataLoader,
-                 num_bn_adaptation_samples: int,
-                 device: Optional[str] = None):
+    def __init__(self, data_loader: NNCFDataLoader, num_bn_adaptation_samples: int, device: Optional[str] = None):
         """
         Initializes the batch-norm statistics adaptation algorithm.
 
         :param data_loader: NNCF data loader.
         :param num_bn_adaptation_samples: Number of samples from the training
             dataset to pass through the model at initialization in order to update
             batch-norm statistics of the original model. The actual number of samples
             will be a closest multiple of the batch size.
         :param device: Device to perform initialization. If `device` is `None` then the device
             of the model parameters will be used.
         """
         if num_bn_adaptation_samples < 0:
-            raise ValueError('Number of adaptation samples must be >= 0')
+            raise ValueError("Number of adaptation samples must be >= 0")
 
         self._device = device
         self._data_loader = data_loader
         self._num_bn_adaptation_steps = math.ceil(num_bn_adaptation_samples / data_loader.batch_size)
 
     def run(self, model: TModel) -> None:
         """
         Runs the batch-norm statistics adaptation algorithm.
 
         :param model: A model for which the algorithm will be applied.
         """
         backend = get_backend(model)
         if backend is BackendType.TORCH:
-            from nncf.torch.batchnorm_adaptation import PTBatchnormAdaptationAlgorithmImpl #pylint: disable=cyclic-import
+            from nncf.torch.batchnorm_adaptation import (
+                PTBatchnormAdaptationAlgorithmImpl,  # pylint: disable=cyclic-import
+            )
+
             impl_cls = PTBatchnormAdaptationAlgorithmImpl
         else:
             assert backend is BackendType.TENSORFLOW
-            from nncf.tensorflow.batchnorm_adaptation import TFBatchnormAdaptationAlgorithmImpl #pylint: disable=cyclic-import
+            from nncf.tensorflow.batchnorm_adaptation import (
+                TFBatchnormAdaptationAlgorithmImpl,  # pylint: disable=cyclic-import
+            )
+
             impl_cls = TFBatchnormAdaptationAlgorithmImpl
-        impl = impl_cls(self._data_loader,
-                        self._num_bn_adaptation_steps,
-                        self._device)
+        impl = impl_cls(self._data_loader, self._num_bn_adaptation_steps, self._device)
         impl.run(model)
```

### Comparing `nncf-2.4.0/nncf/common/initialization/dataloader.py` & `nncf-2.5.0/nncf/common/initialization/dataloader.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,23 +1,25 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Interface for user-defined data usage during the compression algorithm initialization process."""
+from abc import ABC
+from abc import abstractmethod
 
-from abc import ABC, abstractmethod
+from nncf.common.utils.api_marker import api
 
 
+@api()
 class NNCFDataLoader(ABC):
     """
     Wraps a custom data source.
     """
 
     @property
     @abstractmethod
```

### Comparing `nncf-2.4.0/nncf/common/insertion_point_graph.py` & `nncf-2.5.0/nncf/common/insertion_point_graph.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,34 +1,31 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Set, Dict, List, Optional
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from collections import defaultdict
 from copy import deepcopy
 from enum import Enum
+from typing import Dict, List, Optional, Set
 
 import networkx as nx
 
 from nncf.common.graph import Dtype
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.graph_matching import find_subgraphs_matching_pattern
-from nncf.common.graph.patterns import GraphPattern
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
+from nncf.common.graph.patterns import GraphPattern
 from nncf.common.logging import nncf_logger
 
 
 class InsertionPointGraphNodeType(Enum):
     PRE_HOOK = 0
     POST_HOOK = 1
     OPERATOR = 2
@@ -36,15 +33,15 @@
 
 class PreHookInsertionPoint:
     def __init__(self, target_node_name: str, input_port_id: int):
         self.target_node_name = target_node_name
         self.input_port_id = input_port_id
 
     def __str__(self):
-        return str(self.input_port_id) + ' ' + self.target_node_name
+        return str(self.input_port_id) + " " + self.target_node_name
 
 
 class PostHookInsertionPoint:
     def __init__(self, target_node_name: str):
         self.target_node_name = target_node_name
 
     def __str__(self):
@@ -57,29 +54,33 @@
     "insertion point nodes" into the NNCF model graph representation corresponding to operator pre- and
     post-hooks. Module pre-op and post-op insertion points are currently not reflected here, but they are
     probably not required for quantizing activations, for which the quantizer propagation makes sense.
     This "insertion point graph" representation is useful for quantizer propagation and for referencing
     the compression algorithm hooks to the model operations to which they are applied to.
     """
 
-    NODE_TYPE_NODE_ATTR = 'node_type'
-    INSERTION_POINT_NODE_ATTR = 'insertion_point'
-    IS_IN_NNCF_MODULE_NODE_ATTR = 'is_in_nncf_module'
-    REGULAR_NODE_REF_NODE_ATTR = 'regular_node_data'
-    ASSOCIATED_IP_NODE_KEYS_NODE_ATTR = 'associated_ip_node_keys'
-    IS_MERGED_NODE_ATTR = 'is_merged'
-    MERGED_NNCF_NODE_LIST_NODE_ATTR = 'merged_node_list'
-    IS_INTEGER_PATH_EDGE_ATTR = 'is_integer'
-
-    PRE_HOOK_ID_PREFIX = 'PRE HOOK '  # NB: Do not use colon (':') in node keys! Causes trouble for .dot file export.
-    POST_HOOK_ID_PREFIX = 'POST HOOK '
-
-    def __init__(self, nncf_graph: NNCFGraph, weight_modifiable_node_names: List[NNCFNodeName] = None,
-                 allowed_pre_hook_insertion_points: List[PreHookInsertionPoint] = None,
-                 allowed_post_hook_insertion_points: List[PostHookInsertionPoint] = None):
+    NODE_TYPE_NODE_ATTR = "node_type"
+    INSERTION_POINT_NODE_ATTR = "insertion_point"
+    IS_IN_NNCF_MODULE_NODE_ATTR = "is_in_nncf_module"
+    REGULAR_NODE_REF_NODE_ATTR = "regular_node_data"
+    ASSOCIATED_IP_NODE_KEYS_NODE_ATTR = "associated_ip_node_keys"
+    IS_MERGED_NODE_ATTR = "is_merged"
+    MERGED_NNCF_NODE_LIST_NODE_ATTR = "merged_node_list"
+    IS_INTEGER_PATH_EDGE_ATTR = "is_integer"
+
+    PRE_HOOK_ID_PREFIX = "PRE HOOK "  # NB: Do not use colon (':') in node keys! Causes trouble for .dot file export.
+    POST_HOOK_ID_PREFIX = "POST HOOK "
+
+    def __init__(
+        self,
+        nncf_graph: NNCFGraph,
+        weight_modifiable_node_names: List[NNCFNodeName] = None,
+        allowed_pre_hook_insertion_points: List[PreHookInsertionPoint] = None,
+        allowed_post_hook_insertion_points: List[PostHookInsertionPoint] = None,
+    ):
         """
         Initializes the insertion point graph.
 
         :param nncf_graph: The base NNCFGraph representing the model structure.
         :param weight_modifiable_node_names: Names of the nodes in `nncf_graph` that correspond to operations with
           modifiable weights.
         :param allowed_pre_hook_insertion_points: A list of pre-hook insertion points for this graph to allow.
@@ -115,25 +116,24 @@
 
         for node_key in nx.lexicographical_topological_sort(self._base_nx_graph):
             nncf_node = nncf_graph.get_node_by_key(node_key)
             attrs = {
                 InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR: nncf_node,
                 InsertionPointGraph.NODE_TYPE_NODE_ATTR: InsertionPointGraphNodeType.OPERATOR,
                 InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR: set(),
-                InsertionPointGraph.IS_MERGED_NODE_ATTR: False
+                InsertionPointGraph.IS_MERGED_NODE_ATTR: False,
             }
             self.add_node(node_key, **attrs)
 
         INPUT_PORT_ID = "input_port_id"
         for edge in self._base_nx_graph.edges:
             input_port_id = self._base_nx_graph.edges[edge][NNCFGraph.INPUT_PORT_ID_EDGE_ATTR]
             dtype = self._base_nx_graph.edges[edge][NNCFGraph.DTYPE_EDGE_ATTR]
             from_node, to_node = edge
-            attrs = {INPUT_PORT_ID: input_port_id,
-                     self.IS_INTEGER_PATH_EDGE_ATTR: dtype is Dtype.INTEGER}
+            attrs = {INPUT_PORT_ID: input_port_id, self.IS_INTEGER_PATH_EDGE_ATTR: dtype is Dtype.INTEGER}
             self.add_edge(from_node, to_node, **attrs)
 
         node_keys_working_set = [deepcopy(node_key) for node_key in nx.lexicographical_topological_sort(self)]
 
         # TODO (vshampor): Add insertion points for module pre- and post-ops.
         # Should roughly look so: first, determine subsets of nodes belonging to each
         # separate NNCF module (via scope analysis), then for each subset find input/output
@@ -169,15 +169,15 @@
 
             if original_node.node_name in target_node_name_vs_post_hook_ips:
                 post_hook_ips = target_node_name_vs_post_hook_ips[original_node.node_name]
                 assert len(post_hook_ips) == 1, "Multiple post-hooks for a single NNCFGraph node are not supported!"
                 post_hook_ip = next(iter(post_hook_ips))
                 post_hook_ip_attrs = {
                     InsertionPointGraph.NODE_TYPE_NODE_ATTR: InsertionPointGraphNodeType.POST_HOOK,
-                    InsertionPointGraph.INSERTION_POINT_NODE_ATTR: post_hook_ip
+                    InsertionPointGraph.INSERTION_POINT_NODE_ATTR: post_hook_ip,
                 }
                 ip_node_key = self.get_post_hook_node_key(str(operator_node_key))
                 self.add_node(ip_node_key, **post_hook_ip_attrs)
                 out_edges = list(self.out_edges(operator_node_key))
                 has_integer_outputs = False
                 for out_edge in out_edges:
                     # Need to preserve original edge attributes in order not to lose
@@ -205,16 +205,18 @@
             # Until output ports are ready, the post-hook for output will treat op as having a single
             # tensor output. In multi-output case when some of tensors are integer, need to make
             # sure that the propagation won't happen from a pre-hook of the op consuming the floating part
             # of the output into the post-hook of the operation that produces both int and float tensors.
             from_node_key, to_node_key = edge
             from_node = self.nodes[from_node_key]
             to_node = self.nodes[to_node_key]
-            if from_node[self.NODE_TYPE_NODE_ATTR] is InsertionPointGraphNodeType.POST_HOOK and \
-                    to_node[self.NODE_TYPE_NODE_ATTR] is InsertionPointGraphNodeType.PRE_HOOK:
+            if (
+                from_node[self.NODE_TYPE_NODE_ATTR] is InsertionPointGraphNodeType.POST_HOOK
+                and to_node[self.NODE_TYPE_NODE_ATTR] is InsertionPointGraphNodeType.PRE_HOOK
+            ):
                 post_hook_has_integer_outputs = False
                 for follower_node_key in self.successors(from_node_key):
                     if self.edges[from_node_key, follower_node_key][self.IS_INTEGER_PATH_EDGE_ATTR]:
                         post_hook_has_integer_outputs = True
                 if post_hook_has_integer_outputs:
                     for follower_node_key in self.successors(from_node_key):
                         self.edges[from_node_key, follower_node_key][self.IS_INTEGER_PATH_EDGE_ATTR] = True
@@ -228,16 +230,17 @@
         # Pre-hook all input ports of all nodes
         allowed_pre_hook_insertion_points = []
         for nncf_node in nncf_graph.get_all_nodes():
             pred_nodes = nncf_graph.get_previous_nodes(nncf_node)
 
             for pred_node in pred_nodes:
                 input_edge = nncf_graph.get_edge(pred_node, nncf_node)
-                allowed_pre_hook_insertion_points.append(PreHookInsertionPoint(nncf_node.node_name,
-                                                                               input_edge.input_port_id))
+                allowed_pre_hook_insertion_points.append(
+                    PreHookInsertionPoint(nncf_node.node_name, input_edge.input_port_id)
+                )
         return allowed_pre_hook_insertion_points
 
     @staticmethod
     def _get_default_post_hook_ip_list(nncf_graph: NNCFGraph) -> List[PostHookInsertionPoint]:
         # Post-hook all nodes, post hook applies to the entire op output
         allowed_post_hook_insertion_points = []
         for nncf_node in nncf_graph.get_all_nodes():
@@ -298,37 +301,30 @@
             if data[InsertionPointGraph.IS_MERGED_NODE_ATTR]:
                 for nncf_node in data[InsertionPointGraph.MERGED_NNCF_NODE_LIST_NODE_ATTR]:
                     node_k = nncf_node.data[NNCFGraph.KEY_NODE_ATTR]
                     if node_key == node_k:
                         return node
         return node_key
 
-    def get_ip_graph_with_merged_hw_optimized_operations(self,
-                                                         full_fusing_pattern: GraphPattern,
-                                                         known_non_constant_node_keys: Optional[List[str]] = None) \
-            -> 'InsertionPointGraph':
+    def get_ip_graph_with_merged_hw_optimized_operations(
+        self, full_fusing_pattern: GraphPattern
+    ) -> "InsertionPointGraph":
         """
         Returns an InsertionPointGraph in which the nodes that match a HW-specific list of patterns are fused into a
         single node; the resulting InsertionPointGraph no longer has accessible the pre- and post-hooks that were
         located in  the middle of the fused pattern.
         If the InsertionPointGraph should be filtered from constant nodes before the node fusing,
         then 'known_non_constant_node_keys' should be pass. This is the list of the node known that are non constansts.
 
         :param full_fusing_pattern: The GraphPatttern object representing a composition of fusing pattern variants.
-        :param known_non_constant_node_keys: Keys of the nodes which known to be non constant.
         :return: The InsertionPointGraph with nodes fused according to pattern matching.
         """
         # pylint:disable=too-many-branches
         merged_ip_graph = deepcopy(self)
-        filtered_ip_graph = deepcopy(self)
-        if known_non_constant_node_keys is not None:
-            start_traversing_node_keys = [node.node.data[NNCFGraph.KEY_NODE_ATTR] for node in
-                                          known_non_constant_node_keys]
-            filtered_ip_graph = ConstantNodesFilter.filter(filtered_ip_graph, start_traversing_node_keys)
-        matches = find_subgraphs_matching_pattern(filtered_ip_graph.get_base_nx_graph(), full_fusing_pattern)
+        matches = find_subgraphs_matching_pattern(merged_ip_graph.get_base_nx_graph(), full_fusing_pattern)
         for match in matches:
             if len(match) == 1:
                 continue
 
             input_node_key = match[0]
             output_node_key = match[-1]
 
@@ -359,30 +355,30 @@
                             break
                     if should_keep_ip_node:
                         merged_node_attrs[InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR].add(ip_node_key)
                     else:
                         merged_ip_graph.remove_node(ip_node_key)
                 merged_nncf_nodes.append(self.nodes[node_key][InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR])
                 merged_ip_graph.remove_node(node_key)
-                merged_node_key += node_key + '\n'
+                merged_node_key += node_key + "\n"
 
             # The first node in the merged node list will be considered a "primary" node for purposes
             # of further ignored/target scope application.
             merged_node_attrs[InsertionPointGraph.MERGED_NNCF_NODE_LIST_NODE_ATTR] = merged_nncf_nodes
             merged_ip_graph.add_node(merged_node_key, **merged_node_attrs)
             for in_edge_key, in_edge_attrs in in_edge_copies_dict.items():
                 merged_ip_graph.add_edge(in_edge_key[0], merged_node_key, **in_edge_attrs)
             for out_edge_key, out_edge_attrs in out_edge_copies_dict.items():
                 merged_ip_graph.add_edge(merged_node_key, out_edge_key[1], **out_edge_attrs)
 
         return merged_ip_graph
 
     @staticmethod
     def get_pre_hook_node_key(node_key: str, input_port_id: int = 0) -> str:
-        return InsertionPointGraph.PRE_HOOK_ID_PREFIX + str(input_port_id) + ' ' + node_key
+        return InsertionPointGraph.PRE_HOOK_ID_PREFIX + str(input_port_id) + " " + node_key
 
     @staticmethod
     def get_post_hook_node_key(node_key: str) -> str:
         return InsertionPointGraph.POST_HOOK_ID_PREFIX + node_key
 
 
 class ConstantNodesFilter:
@@ -394,20 +390,21 @@
 
         :param ip_graph: The original InsertionPointGraph.
         :param start_traversing_node_keys: Keys of the nodes from which the traversing will be start.
         :return: InsertionPointGraph without Constant nodes.
         """
         input_nodes = ip_graph.get_input_nodes()
         if not input_nodes:
-            nncf_logger.debug('Skipped filtering - no input nodes found')
+            nncf_logger.debug("Skipped filtering - no input nodes found")
             return ip_graph
         weight_nodes = []
         if start_traversing_node_keys is not None:
-            weight_nodes = [ip_graph.get_merged_node_from_single_node_key(weight_node) for weight_node in
-                            start_traversing_node_keys]
+            weight_nodes = [
+                ip_graph.get_merged_node_from_single_node_key(weight_node) for weight_node in start_traversing_node_keys
+            ]
         visited_nodes = set()
         start_nodes = input_nodes + weight_nodes
         for node in start_nodes:
             for node_from, node_to in nx.bfs_edges(ip_graph, source=node):
                 visited_nodes.add(node_from)
                 visited_nodes.add(node_to)
         constant_nodes = [node for node in ip_graph.nodes if node not in visited_nodes]
```

### Comparing `nncf-2.4.0/nncf/common/logging/__init__.py` & `nncf-2.5.0/nncf/common/logging/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,14 +1,12 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.common.logging.logger import nncf_logger
```

### Comparing `nncf-2.4.0/nncf/common/logging/logger.py` & `nncf-2.5.0/nncf/common/logging/logger.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,34 +1,33 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import logging
 import sys
-import warnings
 from contextlib import contextmanager
 
-NNCF_LOGGER_NAME = 'nncf'
+NNCF_LOGGER_NAME = "nncf"
 
 nncf_logger = logging.getLogger(NNCF_LOGGER_NAME)
 nncf_logger.propagate = False
 
 stdout_handler = logging.StreamHandler(sys.stdout)
-fmt_string = '%(levelname)s:%(name)s:%(message)s'
+fmt_string = "%(levelname)s:%(name)s:%(message)s"
 fmt = logging.Formatter(fmt_string)
-fmt_no_newline = logging.Formatter(fmt_string, )
+fmt_no_newline = logging.Formatter(
+    fmt_string,
+)
 stdout_handler.setFormatter(fmt)
 stdout_handler.setLevel(logging.INFO)
 nncf_logger.addHandler(stdout_handler)
 nncf_logger.setLevel(logging.INFO)
 
 
 def set_log_level(level: int):
@@ -39,14 +38,25 @@
       or `logging.DEBUG`.
     """
     nncf_logger.setLevel(level)
     for handler in nncf_logger.handlers:
         handler.setLevel(level)
 
 
+def set_log_file(filename: str):
+    """
+    Sets the log file for the NNCF logging.
+
+    :param filename: Path to the file to save the log.
+    """
+    file_handler = logging.FileHandler(filename, encoding="utf-8")
+    file_handler.setFormatter(logging.Formatter(logging.BASIC_FORMAT))
+    nncf_logger.addHandler(file_handler)
+
+
 def disable_logging():
     """
     Disables NNCF logging entirely. `FutureWarning`s are still shown.
     """
     nncf_logger.handlers = []
 
 
@@ -59,24 +69,20 @@
         self.msgs.add(rec.msg)
         return retval
 
 
 NNCFDeprecationWarning = FutureWarning
 
 
-def warning_deprecated(msg):
-    # Note: must use FutureWarning in order not to get suppressed by default
-    warnings.warn(msg, NNCFDeprecationWarning, stacklevel=2)
-
-
 @contextmanager
 def extension_is_loading_info_log(extension_name: str):
     nncf_logger.info(f"Compiling and loading torch extension: {extension_name}...")
     yield
     nncf_logger.info(f"Finished loading torch extension: {extension_name}")
 
 
 def warn_bkc_version_mismatch(backend: str, bkc_version: str, current_version: str):
     nncf_logger.warning(
         f"NNCF provides best results with {backend}=={bkc_version}, "
         f"while current {backend} version is {current_version}. "
-        f"If you encounter issues, consider switching to {backend}=={bkc_version}")
+        f"If you encounter issues, consider switching to {backend}=={bkc_version}"
+    )
```

### Comparing `nncf-2.4.0/nncf/common/logging/progress_bar.py` & `nncf-2.5.0/nncf/common/logging/progress_bar.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.common.logging import nncf_logger
 
 
 class ProgressBar:
     """
     A basic progress bar specifically for the logging.
@@ -22,42 +20,48 @@
     :param iterable: iterable to decorate with a progressbar.
     :param logger: logging to print a progress, nncf_logger by default
     :param desc: prefix for the printed line, empty by default
     :param num_lines:  defines the number of lines to log
     :param total: the expected total number of iterations
     """
 
-    def __init__(self, iterable, logger=nncf_logger, desc='', num_lines=10, total=None):
+    def __init__(self, iterable, logger=nncf_logger, desc="", num_lines=10, total=None):
         self._logger = logger
         self._iterable = iterable
         self._desc = desc
         self._num_lines = num_lines
 
         self._index = 0
         self._width = 16
         self._is_enabled = False
         self._total = None
         if total is not None:
             if not isinstance(total, (int, float)) or total <= 0:
-                logger.error('Progress bar is disabled because the expected total number of iterations is invalid: '
-                             'it should be an integer and more than 0')
+                logger.error(
+                    "Progress bar is disabled because the expected total number of iterations is invalid: "
+                    "it should be an integer and more than 0"
+                )
                 return
             self._total = int(total)
 
         if iterable is not None and self._total is None:
             try:
                 self._total = len(iterable)
             except (TypeError, AttributeError):
-                logger.error('Progress bar is disabled because the given iterable is invalid: '
-                             'it does not implement __len__ method')
+                logger.error(
+                    "Progress bar is disabled because the given iterable is invalid: "
+                    "it does not implement __len__ method"
+                )
                 return
 
         if not isinstance(num_lines, int) or num_lines <= 1:
-            logger.error('Progress bar is disabled because the given number of lines for logging is invalid: '
-                         'it should be an integer and more than 1')
+            logger.error(
+                "Progress bar is disabled because the given number of lines for logging is invalid: "
+                "it should be an integer and more than 1"
+            )
             return
 
         self._step = max(1, self._total // (self._num_lines - 1))
         self._is_enabled = True
 
     def __iter__(self):
         for obj in self._iterable:
@@ -69,11 +73,14 @@
         self._index += 1
         if self._index > self._total:
             return
 
         if self._index % self._step == 0 or self._index == self._total:
             num_filled = int(self._index * self._width / self._total)
             num_empty = self._width - num_filled
-            filled = '' * num_filled
-            empty = ' ' * num_empty
-            self._logger.info('{desc} |{filled}{empty}| {index} / {total}'.format(
-                desc=self._desc, filled=filled, empty=empty, index=self._index, total=self._total))
+            filled = "" * num_filled
+            empty = " " * num_empty
+            self._logger.info(
+                "{desc} |{filled}{empty}| {index} / {total}".format(
+                    desc=self._desc, filled=filled, empty=empty, index=self._index, total=self._total
+                )
+            )
```

### Comparing `nncf-2.4.0/nncf/common/pruning/clusterization.py` & `nncf-2.5.0/nncf/common/pruning/clusterization.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,21 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Callable
-from typing import Dict
-from typing import Generic
-from typing import Hashable
-from typing import List
-from typing import TypeVar
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-T = TypeVar('T')
+from typing import Callable, Dict, Generic, Hashable, List, TypeVar
+
+T = TypeVar("T")
 
 
 class Cluster(Generic[T]):
     """
     Represents element of lusterization. Groups together elements.
     """
 
@@ -58,26 +51,26 @@
         """
         Returns cluster according to provided cluster_id.
 
         :param cluster_id: Id of the cluster.
         :return: Cluster according to provided `cluster_id`.
         """
         if cluster_id not in self.clusters:
-            raise IndexError('No cluster with id = {}'.format(cluster_id))
+            raise IndexError("No cluster with id = {}".format(cluster_id))
         return self.clusters[cluster_id]
 
     def get_cluster_containing_element(self, element_id: Hashable) -> Cluster[T]:
         """
         Returns cluster containing element with provided `element_id`.
 
         :param element_id: Id of the element which is in cluster.
         :return: Cluster containing element with provided `element_id`.
         """
         if element_id not in self._element_to_cluster:
-            raise IndexError('No cluster for node with id = {}'.format(element_id))
+            raise IndexError("No cluster for node with id = {}".format(element_id))
         return self.get_cluster_by_id(self._element_to_cluster[element_id])
 
     def is_node_in_clusterization(self, node_id: int) -> bool:
         """
         Returns whether node with provided `node_id` is in clusterization.
 
         :param node_id: Id of the node to test.
@@ -89,27 +82,27 @@
         """
         Adds provided cluster to clusterization.
 
         :param cluster: Cluster to add.
         """
         cluster_id = cluster.id
         if cluster_id in self.clusters:
-            raise IndexError('Cluster with index = {} already exist'.format(cluster_id))
+            raise IndexError("Cluster with index = {} already exist".format(cluster_id))
         self.clusters[cluster_id] = cluster
         for elt in cluster.elements:
             self._element_to_cluster[self._id_fn(elt)] = cluster_id
 
     def delete_cluster(self, cluster_id: int):
         """
         Removes cluster with `cluster_id` from clusterization.
 
         :param cluster_id: Id of a cluster to delete.
         """
         if cluster_id not in self.clusters:
-            raise IndexError('No cluster with index = {} to delete'.format(cluster_id))
+            raise IndexError("No cluster with index = {} to delete".format(cluster_id))
         for elt in self.clusters[cluster_id].elements:
             node_id = self._id_fn(elt)
             self._element_to_cluster.pop(node_id)
         self.clusters.pop(cluster_id)
 
     def get_all_clusters(self) -> List[Cluster[T]]:
         """
```

### Comparing `nncf-2.4.0/nncf/common/pruning/mask_propagation.py` & `nncf-2.5.0/nncf/common/pruning/mask_propagation.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,47 +1,48 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict, List, Type, Optional
+from typing import Dict, List, Optional, Type
 
 from nncf.common.graph import NNCFGraph
+from nncf.common.pruning.operations import BasePruningOp
+from nncf.common.pruning.symbolic_mask import SymbolicMask
+from nncf.common.pruning.symbolic_mask import SymbolicMaskProcessor
 from nncf.common.pruning.tensor_processor import NNCFPruningBaseTensorProcessor
+from nncf.common.pruning.utils import PruningAnalysisDecision
+from nncf.common.pruning.utils import PruningAnalysisReason
 from nncf.common.pruning.utils import PruningOperationsMetatypeRegistry
-from nncf.common.pruning.utils import get_input_masks
 from nncf.common.pruning.utils import get_input_channels
+from nncf.common.pruning.utils import get_input_masks
 from nncf.common.pruning.utils import get_output_channels
-from nncf.common.pruning.utils import is_grouped_conv
 from nncf.common.pruning.utils import is_batched_linear
-from nncf.common.pruning.utils import PruningAnalysisDecision
-from nncf.common.pruning.utils import PruningAnalysisReason
-from nncf.common.pruning.symbolic_mask import SymbolicMask
-from nncf.common.pruning.symbolic_mask import SymbolicMaskProcessor
-from nncf.common.pruning.operations import BasePruningOp
+from nncf.common.pruning.utils import is_grouped_conv
 
 
 class MaskPropagationAlgorithm:
     """
     Algorithm responsible for propagation masks across all nodes in the graph.
     Before call mask_propagation() you need set node.data['output_masks']
     for nodes that have masks already defined.
     """
 
-    def __init__(self, graph: NNCFGraph,
-                 pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
-                 tensor_processor: Optional[Type[NNCFPruningBaseTensorProcessor]] = None):
+    def __init__(
+        self,
+        graph: NNCFGraph,
+        pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
+        tensor_processor: Optional[Type[NNCFPruningBaseTensorProcessor]] = None,
+    ):
         """
         Initializes MaskPropagationAlgorithm.
 
         :param graph: Graph to work with.
         :param pruning_operator_metatypes: Registry with operation metatypes pruning algorithm is aware of, i.e.
                metatypes describing operations with common pruning mask application and propagation properties.
         :param tensor_processor: Framework-specific tensor processor.
@@ -55,29 +56,29 @@
         Returns class of metaop that corresponds to `type_name` type.
 
         :param type_name: Name of type of layer
         :return: Class of metaop that corresponds to `type_name` type.
         """
         cls = self._pruning_operator_metatypes.get_operator_metatype_by_op_name(type_name)
         if cls is None:
-            cls = self._pruning_operator_metatypes.registry_dict['stop_propagation_ops']
+            cls = self._pruning_operator_metatypes.registry_dict["stop_propagation_ops"]
         return cls
 
     def mask_propagation(self):
         """
         Mask propagation in graph:
         to propagate masks run method mask_propagation (of metaop of current node) on all nodes in topological order.
         """
         for node in self._graph.topological_sort():
             cls = self.get_meta_operation_by_type_name(node.node_type)
             cls.mask_propagation(node, self._graph, self._tensor_processor)
 
-    def symbolic_mask_propagation(self, prunable_layers_types: List[str],
-                                  can_prune_after_analysis: Dict[int, PruningAnalysisDecision]) \
-            -> Dict[int, PruningAnalysisDecision]:
+    def symbolic_mask_propagation(
+        self, prunable_layers_types: List[str], can_prune_after_analysis: Dict[int, PruningAnalysisDecision]
+    ) -> Dict[int, PruningAnalysisDecision]:
         """
         Check all nodes that were marked as prunable after the model analysis and compatibility check vs.
         pruning algo have a correct correspondent closing node on each path from self to outputs;
         the check entails verifying that every convolution prunable by the output channel dimension
         has a corresponding convolution that is prunable by its input channel dimension (output channel
         dimension equal to closing convolution input channel dimension) in every path from self to outputs.
         If the check fails, the entire groups containing such nodes will be marked as unprunable.
@@ -95,56 +96,59 @@
         """
 
         can_be_closing_convs = self._get_can_closing_convs(prunable_layers_types)
         can_prune_by_dim = {k: None for k in can_be_closing_convs}
         for node in self._graph.topological_sort():
             if node.node_id in can_be_closing_convs and can_prune_after_analysis[node.node_id]:
                 # Set output mask
-                node.data['output_mask'] = SymbolicMask(get_output_channels(node), node.node_id)
+                node.data["output_mask"] = SymbolicMask(get_output_channels(node), node.node_id)
             # Propagate masks
             cls = self.get_meta_operation_by_type_name(node.node_type)
             cls.mask_propagation(node, self._graph, SymbolicMaskProcessor)
             if node.node_id in can_be_closing_convs:
                 # Check input mask producers out channel dimension
                 input_masks = get_input_masks(node, self._graph)
                 if any(input_masks):
                     assert len(input_masks) == 1
-                    input_mask = input_masks[0] # type: SymbolicMask
+                    input_mask = input_masks[0]  # type: SymbolicMask
 
                     for producer in input_mask.mask_producers:
-                        previously_dims_equal = True if can_prune_by_dim[producer.id] is None \
-                            else can_prune_by_dim[producer.id]
+                        previously_dims_equal = (
+                            True if can_prune_by_dim[producer.id] is None else can_prune_by_dim[producer.id]
+                        )
 
                         is_dims_equal = get_input_channels(node) == input_mask.shape[0]
                         decision = previously_dims_equal and is_dims_equal
                         can_prune_by_dim[producer.id] = PruningAnalysisDecision(
-                            decision, PruningAnalysisReason.DIMENSION_MISMATCH)
+                            decision, PruningAnalysisReason.DIMENSION_MISMATCH
+                        )
         # Remove all convolutions with masks
         # that were propagated to output node
         for out_node in self._graph.get_output_nodes():
             for input_mask in get_input_masks(out_node, self._graph):
                 if input_mask:
                     for producer in input_mask.mask_producers:
-                        can_prune_by_dim[producer.id] = PruningAnalysisDecision(
-                                False, PruningAnalysisReason.LAST_CONV)
+                        can_prune_by_dim[producer.id] = PruningAnalysisDecision(False, PruningAnalysisReason.LAST_CONV)
         # Update decision for nodes which
         # have no closing convolution
         convs_without_closing_conv = {}
         for k, v in can_prune_by_dim.items():
             if v is None:
-                convs_without_closing_conv[k] = \
-                    PruningAnalysisDecision(False, PruningAnalysisReason.CLOSING_CONV_MISSING)
+                convs_without_closing_conv[k] = PruningAnalysisDecision(
+                    False, PruningAnalysisReason.CLOSING_CONV_MISSING
+                )
         can_prune_by_dim.update(convs_without_closing_conv)
 
         # Clean nodes masks
         for node in self._graph.get_all_nodes():
-            node.data['output_mask'] = None
+            node.data["output_mask"] = None
 
         return can_prune_by_dim
 
     def _get_can_closing_convs(self, prunable_layers_types) -> Dict:
         retval = set()
         for node in self._graph.get_all_nodes():
-            if node.node_type in prunable_layers_types and \
-                not (is_grouped_conv(node) or is_batched_linear(node, self._graph)):
+            if node.node_type in prunable_layers_types and not (
+                is_grouped_conv(node) or is_batched_linear(node, self._graph)
+            ):
                 retval.add(node.node_id)
         return retval
```

### Comparing `nncf-2.4.0/nncf/common/pruning/model_analysis.py` & `nncf-2.5.0/nncf/common/pruning/model_analysis.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,31 +1,29 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List, Dict
+from typing import Dict, List
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.pruning.clusterization import Cluster
 from nncf.common.pruning.clusterization import Clusterization
 from nncf.common.pruning.operations import BasePruningOp
-from nncf.common.pruning.utils import find_next_nodes_not_of_types
-from nncf.common.pruning.utils import PruningOperationsMetatypeRegistry
 from nncf.common.pruning.utils import PruningAnalysisDecision
 from nncf.common.pruning.utils import PruningAnalysisReason
+from nncf.common.pruning.utils import PruningOperationsMetatypeRegistry
+from nncf.common.pruning.utils import find_next_nodes_not_of_types
 from nncf.common.pruning.utils import is_prunable_depthwise_conv
 
 
 def get_position(nodes_list: List[NNCFNode], idx: int):
     for i, node in enumerate(nodes_list):
         if node.node_id == idx:
             return i
@@ -55,42 +53,41 @@
     for node in nodes_to_merge:
         if node.node_id != max_importance_node_id:
             current_node_cluster_id = clusterization.get_cluster_containing_element(node.node_id).id
             if current_node_cluster_id != max_importance_cluster_id:
                 clusterization.merge_clusters(max_importance_cluster_id, current_node_cluster_id)
 
 
-def cluster_special_ops(graph: NNCFGraph, special_types: List[str],
-                        identity_types: List[str]) -> Clusterization[NNCFNode]:
+def cluster_special_ops(
+    graph: NNCFGraph, special_types: List[str], identity_types: List[str]
+) -> Clusterization[NNCFNode]:
     """
     This model will cluster all operations with type from special_types. Connected nodes is nodes that:
         1. Have path between nodes with only identity type nodes on it
         2. Have common input (identity type nodes can be on path from this input)
 
     :param graph: Graph to work with.
     :param special_types: List of types that should be grouped to groups of dependent nodes.
     :return: Clusterization of `special_types` nodes to the dependent groups.
     """
     topologically_sorted_nodes = graph.topological_sort()
-    all_special_nodes = [node for node in graph.get_all_nodes()
-                         if node.node_type in special_types]
+    all_special_nodes = [node for node in graph.get_all_nodes() if node.node_type in special_types]
 
     # 0. Initially all nodes is a separate clusters
     clusterization = Clusterization[NNCFNode](lambda x: x.node_id)
     for i, node in enumerate(all_special_nodes):
         cluster = Cluster[NNCFNode](i, [node], [get_position(topologically_sorted_nodes, node.node_id)])
         clusterization.add_cluster(cluster)
 
     for node in topologically_sorted_nodes:
         if node.node_type in identity_types:
             continue
 
         all_outputs = find_next_nodes_not_of_types(graph, node, identity_types)
-        all_output_special_nodes = [node for node in all_outputs
-                                    if node.node_type in special_types]
+        all_output_special_nodes = [node for node in all_outputs if node.node_type in special_types]
         if node.node_type in special_types:
             all_output_special_nodes.append(node)
         merge_clusters_for_nodes(all_output_special_nodes, clusterization)
 
     return clusterization
 
 
@@ -106,31 +103,34 @@
         (from the result of the network to the inputs). Node can be pruned if all outputs of this node accept
         pruned input and all outputs can be pruned.
         3. Propagates `can_prune` down from input nodes to the outputs.
 
     As a result, all nodes are marked by the `can_prune` attribute as potentially prunable or not.
     """
 
-    def __init__(self, graph: NNCFGraph,
-                 pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
-                 prune_operations_types: List[str]):
+    def __init__(
+        self,
+        graph: NNCFGraph,
+        pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
+        prune_operations_types: List[str],
+    ):
         """
         :param pruning_operator_metatypes: registry with operation metatypes pruning algorithm is aware of, i.e.
         metatypes describing operations with common pruning mask application and propagation properties, e.g.
         IdentityMaskForwardOps unifies operations that propagate pruning masks as is (relu, swish etc.), whereas
         Convolution unifies different convolution operations (conv1d, conv2d, conv3d) which accepts some input masks
         and provide some output masks.
         :param prune_operations_types: Types of operations with prunable parameters.
         """
         self.graph = graph
         self._pruning_operator_metatypes = pruning_operator_metatypes
         self._prune_operations_types = prune_operations_types
         pruning_op_metatypes_dict = self._pruning_operator_metatypes.registry_dict
-        self._stop_propagation_op_metatype = pruning_op_metatypes_dict['stop_propagation_ops']
-        self._concat_op_metatype = pruning_op_metatypes_dict['concat']
+        self._stop_propagation_op_metatype = pruning_op_metatypes_dict["stop_propagation_ops"]
+        self._concat_op_metatype = pruning_op_metatypes_dict["concat"]
 
         self.can_prune = {idx: True for idx in self.graph.get_all_node_ids()}
         self.accept_pruned_input = {idx: True for idx in self.graph.get_all_node_ids()}
 
     def node_propagate_can_prune_attr(self, nncf_node: NNCFNode) -> bool:
         """
         Whether the node can propagate the `can_prune` attr. That means a node can propagate pruning mask
@@ -172,16 +172,17 @@
         reversed_sorted_nodes = reversed(self.graph.topological_sort())
         for node in reversed_sorted_nodes:
             # Check all output nodes accept_pruned_input attribute
             out_nodes = self.graph.get_next_nodes(node)
             outputs_accept_pruned_input = all(self.accept_pruned_input[node.node_id] for node in out_nodes)
 
             # Check all output nodes can_prune attribute
-            outputs_will_be_pruned = all(self.can_prune[node.node_id]
-                                         for node in out_nodes if self.node_propagate_can_prune_attr(node))
+            outputs_will_be_pruned = all(
+                self.can_prune[node.node_id] for node in out_nodes if self.node_propagate_can_prune_attr(node)
+            )
             self.can_prune[node.node_id] = outputs_accept_pruned_input and outputs_will_be_pruned
 
     def propagate_can_prune_attr_down(self):
         """
         Propagating can_prune attribute down to fix all branching cases with one pruned and one not pruned
         branches.
         """
@@ -189,22 +190,22 @@
         for node in sorted_nodes:
             # Propagate attribute only in not conv case
             if self.node_propagate_can_prune_attr(node):
                 in_nodes = self.graph.get_previous_nodes(node)
                 can_prune = all(self.can_prune[node.node_id] for node in in_nodes)
                 can_prune_any = any(self.can_prune[node.node_id] for node in in_nodes)
 
-                if (not self.node_accept_different_inputs(node) and not can_prune) or \
-                        (self.node_accept_different_inputs(node) and not can_prune_any):
+                if (not self.node_accept_different_inputs(node) and not can_prune) or (
+                    self.node_accept_different_inputs(node) and not can_prune_any
+                ):
                     self.can_prune[node.node_id] = can_prune
 
     def set_accept_pruned_input_attr(self):
         for nncf_node in self.graph.get_all_nodes():
             cls = self.get_meta_operation_by_type_name(nncf_node.node_type)
             self.accept_pruned_input[nncf_node.node_id] = cls.accept_pruned_input(nncf_node)
 
     def analyse_model_before_pruning(self) -> Dict[int, PruningAnalysisDecision]:
         self.set_accept_pruned_input_attr()
         self.propagate_can_prune_attr_up()
         self.propagate_can_prune_attr_down()
-        return {k: PruningAnalysisDecision(v, PruningAnalysisReason.MODEL_ANALYSIS)
-                for k, v in self.can_prune.items()}
+        return {k: PruningAnalysisDecision(v, PruningAnalysisReason.MODEL_ANALYSIS) for k, v in self.can_prune.items()}
```

### Comparing `nncf-2.4.0/nncf/common/pruning/node_selector.py` & `nncf-2.5.0/nncf/common/pruning/node_selector.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,20 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from collections import defaultdict
-from typing import Dict
-from typing import List
-from typing import Optional
+from typing import Dict, List, Optional
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.utils import get_first_nodes_of_type
 from nncf.common.logging import nncf_logger
 from nncf.common.pruning.clusterization import Cluster
 from nncf.common.pruning.clusterization import Clusterization
@@ -40,22 +36,24 @@
 
 class PruningNodeSelector:
     """
     Determines which of the nodes with pruning types should be pruned
     and which of them should be pruned together.
     """
 
-    def __init__(self,
-                 pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
-                 prune_operations_types: List[str],
-                 grouping_operations_types: List[str],
-                 ignored_scopes: Optional[List[str]],
-                 target_scopes: Optional[List[str]],
-                 prune_first: bool,
-                 prune_downsample_convs: bool):
+    def __init__(
+        self,
+        pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
+        prune_operations_types: List[str],
+        grouping_operations_types: List[str],
+        ignored_scopes: Optional[List[str]],
+        target_scopes: Optional[List[str]],
+        prune_first: bool,
+        prune_downsample_convs: bool,
+    ):
         """
         :param pruning_operator_metatypes: registry with operation metatypes pruning algorithm is aware of, i.e.
             metatypes describing operations with common pruning mask application and propagation properties, e.g.
             IdentityMaskForwardOps unifies operations that propagate pruning masks as is (relu, swish etc.), whereas
             Convolution unifies different convolution operations (conv1d, conv2d, conv3d) which accepts some input masks
             and provide some output masks.
         :param prune_operations_types: Types of operations with prunable parameters.
@@ -64,16 +62,16 @@
         :param ignored_scopes: Ignored scopes.
         :param target_scopes: Target scopes.
         :param prune_first: Whether to prune first convolution or not.
         :param prune_downsample_convs: Whether to prune downsample Convolutional layers (with stride > 1) or not.
         """
         self._pruning_operator_metatypes = pruning_operator_metatypes
         pruning_op_metatypes_dict = self._pruning_operator_metatypes.registry_dict
-        self._identity_mask_propagation_op_metatype = pruning_op_metatypes_dict['identity_mask_propagation']
-        self._stop_propagation_op_metatype = pruning_op_metatypes_dict['stop_propagation_ops']
+        self._identity_mask_propagation_op_metatype = pruning_op_metatypes_dict["identity_mask_propagation"]
+        self._stop_propagation_op_metatype = pruning_op_metatypes_dict["stop_propagation_ops"]
 
         self._prune_operations_types = prune_operations_types
         self._grouping_operations_types = grouping_operations_types
 
         self._ignored_scopes = ignored_scopes
         self._target_scopes = target_scopes
 
@@ -94,16 +92,15 @@
         :return: Clusterization of pruned nodes.
         """
         # pylint:disable=too-many-branches
         all_nodes_to_prune = graph.get_nodes_by_types(self._prune_operations_types)  # NNCFNodes here
 
         # 1. Clusters for special ops
         identity_like_types = self._identity_mask_propagation_op_metatype.get_all_op_aliases()
-        special_ops_clusterization = cluster_special_ops(graph, self._grouping_operations_types,
-                                                         identity_like_types)
+        special_ops_clusterization = cluster_special_ops(graph, self._grouping_operations_types, identity_like_types)
 
         pruned_nodes_clusterization = Clusterization[NNCFNode](lambda x: x.node_id)
 
         # 2. Clusters for nodes that should be pruned together (taking into account clusters for special ops)
         for i, cluster in enumerate(special_ops_clusterization.get_all_clusters()):
             all_pruned_inputs = {}
             clusters_to_merge = []
@@ -148,24 +145,26 @@
                     for node in previous_convs
                 ]
                 pruned_nodes_clusterization.merge_list_of_clusters([cluster_id] + previous_clusters)
 
         # 5. Merge nodes into one cluster if some module forwards several times
         multiforward_nodes = self._get_multiforward_nodes(graph)
         for list_of_nodes in multiforward_nodes:
-            clusters_to_merge = [pruned_nodes_clusterization.get_cluster_containing_element(node.node_id).id
-                                 for node in list_of_nodes]
+            clusters_to_merge = [
+                pruned_nodes_clusterization.get_cluster_containing_element(node.node_id).id for node in list_of_nodes
+            ]
             pruned_nodes_clusterization.merge_list_of_clusters(clusters_to_merge)
 
             # Merge previous convolutions into one cluster
             all_previous_convs = []
             for node in list_of_nodes:
                 nncf_node = graph.get_node_by_id(node.node_id)
-                previous_convs = get_previous_convs(graph, nncf_node, self._prune_operations_types,
-                                                    stop_propagation_ops)
+                previous_convs = get_previous_convs(
+                    graph, nncf_node, self._prune_operations_types, stop_propagation_ops
+                )
                 # Check if previous node isn't multiforward,
                 # in case of multiforward nodes cycle
                 for previous_conv in previous_convs:
                     if previous_conv not in list_of_nodes:
                         all_previous_convs.append(previous_conv)
 
             previous_clusters = [
@@ -173,19 +172,20 @@
                 for node in all_previous_convs
             ]
             pruned_nodes_clusterization.merge_list_of_clusters(previous_clusters)
 
         # 6. Checks for groups (all nodes in group can be pruned or all group can't be pruned).
         model_analyser = ModelAnalyzer(graph, self._pruning_operator_metatypes, self._prune_operations_types)
         can_prune_analysis = model_analyser.analyse_model_before_pruning()
-        can_prune_and_should_prune_analysis = self._should_prune_groups_analysis(graph,
-                                                                                 pruned_nodes_clusterization,
-                                                                                 can_prune_analysis)
-        can_prune_final_analysis = self._pruning_dimensions_analysis(graph, pruned_nodes_clusterization,
-                                                                     can_prune_and_should_prune_analysis)
+        can_prune_and_should_prune_analysis = self._should_prune_groups_analysis(
+            graph, pruned_nodes_clusterization, can_prune_analysis
+        )
+        can_prune_final_analysis = self._pruning_dimensions_analysis(
+            graph, pruned_nodes_clusterization, can_prune_and_should_prune_analysis
+        )
         self._filter_groups(pruned_nodes_clusterization, can_prune_final_analysis)
         return pruned_nodes_clusterization
 
     def _get_multiforward_nodes(self, graph: NNCFGraph) -> List[List[NNCFNode]]:
         """
         Groups nodes based on their `layer_name` property to determine groups of nodes belonging to
         a single weighted layer object in the model, i.e. the group of operations in the graph that reuse one and
@@ -195,18 +195,20 @@
          underlying layer object of the original model.
         """
         ret = defaultdict(list)
         for node in graph.get_nodes_by_types(self._prune_operations_types):
             ret[node.layer_name].append(node)
         return [ret[module_identifier] for module_identifier in ret if len(ret[module_identifier]) > 1]
 
-    def _pruning_dimensions_analysis(self, graph: NNCFGraph,
-                                     pruned_nodes_clusterization: Clusterization,
-                                     can_prune_after_check: Dict[int, PruningAnalysisDecision]) ->\
-                                     Dict[int, PruningAnalysisDecision]:
+    def _pruning_dimensions_analysis(
+        self,
+        graph: NNCFGraph,
+        pruned_nodes_clusterization: Clusterization,
+        can_prune_after_check: Dict[int, PruningAnalysisDecision],
+    ) -> Dict[int, PruningAnalysisDecision]:
         """
         Checks:
         1) All nodes that were marked as prunable after the model analysis and compatibility check vs.
         pruning algo have a correct correspondent closing node on each path from self to outputs;
         2) Pruning dimensions of all nodes in all cluster groups are equal.
 
         :param graph: Graph to work with.
@@ -220,61 +222,69 @@
         nodes_of_group_with_non_eq_pruning_dim = self._check_internal_groups_dim(pruned_nodes_clusterization)
         can_prune_after_check_updated = can_prune_after_check.copy()
         for node_id, val in nodes_of_group_with_non_eq_pruning_dim.items():
             can_prune_after_check_updated[node_id] = can_prune_after_check_updated[node_id].join(val)
 
         return self._check_all_closing_nodes_are_feasible(graph, can_prune_after_check_updated)
 
-    def _check_all_closing_nodes_are_feasible(self, graph: NNCFGraph,
-                                              can_prune_after_check: Dict[int, PruningAnalysisDecision]) ->\
-                                              Dict[int, PruningAnalysisDecision]:
+    def _check_all_closing_nodes_are_feasible(
+        self, graph: NNCFGraph, can_prune_after_check: Dict[int, PruningAnalysisDecision]
+    ) -> Dict[int, PruningAnalysisDecision]:
         """
         Check all nodes that were marked as prunable after the model analysis and compatibility check vs.
         pruning algo have a correct correspondent closing node on each path from self to outputs.
 
         :param graph: Graph to work with.
         :param can_prune_after_check: Dict of node indices vs the decision made by previous steps;
             the decision is true only for the nodes that do not conflict with mask propagation and
             are supported by the NNCF pruning algorithm
         :return: Pruning node analysis after model analyzer, pruning algo compatibility and pruning dimensions checks.
         """
         mask_prop_algo = MaskPropagationAlgorithm(graph, self._pruning_operator_metatypes)
         can_prune_by_dim = mask_prop_algo.symbolic_mask_propagation(self._prune_operations_types, can_prune_after_check)
 
-        can_prune_for_prunable_layers = \
-            {node_id: can_prune_after_check[node_id].join(can_prune_by_dim[node_id]) for node_id in can_prune_by_dim}
+        can_prune_for_prunable_layers = {
+            node_id: can_prune_after_check[node_id].join(can_prune_by_dim[node_id]) for node_id in can_prune_by_dim
+        }
 
         can_prune_updated = can_prune_after_check.copy()
         can_prune_updated.update(can_prune_for_prunable_layers)
         return can_prune_updated
 
-    def _check_internal_groups_dim(self, pruned_nodes_clusterization: Clusterization) ->\
-                                   Dict[int, PruningAnalysisDecision]:
+    def _check_internal_groups_dim(
+        self, pruned_nodes_clusterization: Clusterization
+    ) -> Dict[int, PruningAnalysisDecision]:
         """
         Checks pruning dimensions of all nodes in each cluster group are equal and
         returns nodes of clusters that failed the check.
 
         :param pruned_nodes_clusterization: Pruned nodes clusterization.
         :returns: Pruning analysis decisions for nodes which have
             not equal pruning dimensions in a cluster they are belong to.
         """
         retval = {}
         for cluster in pruned_nodes_clusterization.get_all_clusters():
-            has_equal_amount_of_channel = \
-                all(get_output_channels(cluster.elements[0]) == get_output_channels(node)
-                                        for node in cluster.elements[1:])
+            has_equal_amount_of_channel = all(
+                get_output_channels(cluster.elements[0]) == get_output_channels(node) for node in cluster.elements[1:]
+            )
             if not has_equal_amount_of_channel:
                 retval.update(
-                    {node.node_id: PruningAnalysisDecision(False, PruningAnalysisReason.INCOMPATIBLE_DIMS_IN_CLUSTER)
-                     for node in cluster.elements})
+                    {
+                        node.node_id: PruningAnalysisDecision(False, PruningAnalysisReason.INCOMPATIBLE_DIMS_IN_CLUSTER)
+                        for node in cluster.elements
+                    }
+                )
         return retval
 
-    def _should_prune_groups_analysis(self, graph: NNCFGraph, pruned_nodes_clusterization: Clusterization,
-                                      can_prune: Dict[int, PruningAnalysisDecision]) \
-            -> Dict[int, PruningAnalysisDecision]:
+    def _should_prune_groups_analysis(
+        self,
+        graph: NNCFGraph,
+        pruned_nodes_clusterization: Clusterization,
+        can_prune: Dict[int, PruningAnalysisDecision],
+    ) -> Dict[int, PruningAnalysisDecision]:
         """
         Check whether all nodes in group can be pruned based on user-defined constraints and
         model analysis. Otherwise the whole group cannot be pruned.
 
         :param graph: Graph to work with.
         :param pruned_nodes_clusterization: Clusterization with pruning nodes groups.
         :param can_prune: Complete pruning analysis about each graph node.
@@ -285,27 +295,29 @@
             should_prune_decisions = [self._is_module_prunable(graph, node) for node in cluster.elements]
             can_prune_decisions = [can_prune[node.node_id] for node in cluster.elements]
             decisions = [can.join(should) for can, should in zip(can_prune_decisions, should_prune_decisions)]
             if not all(decisions):
                 updated_decisions = {}
                 for node, decision in zip(cluster.elements, decisions):
                     if decision:
-                        updated_decisions[node.node_id] = \
-                            PruningAnalysisDecision(False, PruningAnalysisReason.IN_GROUP_OF_UNPRUNABLE)
+                        updated_decisions[node.node_id] = PruningAnalysisDecision(
+                            False, PruningAnalysisReason.IN_GROUP_OF_UNPRUNABLE
+                        )
                     else:
                         updated_decisions[node.node_id] = decision
 
                 should_prune.update(updated_decisions)
 
         can_prune_updated = can_prune.copy()
         can_prune_updated.update(should_prune)
         return can_prune_updated
 
-    def _filter_groups(self, pruned_nodes_clusterization: Clusterization,
-                       can_prune: Dict[int, PruningAnalysisDecision]) -> None:
+    def _filter_groups(
+        self, pruned_nodes_clusterization: Clusterization, can_prune: Dict[int, PruningAnalysisDecision]
+    ) -> None:
         """
         Check whether all nodes in group can be pruned based on user-defined constraints and
         connections inside the network. Otherwise the whole group cannot be pruned and will be
         removed the clusterization.
 
         :param pruned_nodes_clusterization: Clusterization with pruning nodes groups.
         :param can_prune: Can this node be pruned or not.
@@ -317,16 +329,18 @@
                 cannot_prune_messages = []
                 for name, decision in zip(nodes_names, nodes_decisions):
                     if not decision:
                         message = PruningAnalysisReason.message(name, decision)
                         if message:
                             cannot_prune_messages.append(message)
 
-                nncf_logger.debug(f'Could not prune node group [{", ".join(nodes_names)}], '
-                                  f'reason: {", ".join(cannot_prune_messages)}.')
+                nncf_logger.debug(
+                    f'Could not prune node group [{", ".join(nodes_names)}], '
+                    f'reason: {", ".join(cannot_prune_messages)}.'
+                )
                 pruned_nodes_clusterization.delete_cluster(cluster.id)
             else:
                 nncf_logger.debug(f'Node group [{", ".join(nodes_names)}] will be pruned together.')
 
     def _is_module_prunable(self, graph: NNCFGraph, node: NNCFNode) -> PruningAnalysisDecision:
         """
         Check whether we should prune module corresponding to provided node
```

### Comparing `nncf-2.4.0/nncf/common/pruning/operations.py` & `nncf-2.5.0/nncf/common/pruning/operations.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,33 +1,31 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Optional, List, Type, Dict
+from typing import Dict, List, Optional, Type
 
 from nncf.common.graph import NNCFGraph
+from nncf.common.graph import NNCFGraphEdge
 from nncf.common.graph import NNCFNode
+from nncf.common.graph.layer_attributes import GroupNormLayerAttributes
+from nncf.common.pruning.symbolic_mask import SymbolicMask
 from nncf.common.pruning.tensor_processor import NNCFPruningBaseTensorProcessor
-from nncf.common.pruning.utils import is_grouped_conv
 from nncf.common.pruning.utils import get_input_masks
-from nncf.common.pruning.utils import is_prunable_depthwise_conv
 from nncf.common.pruning.utils import identity_mask_propagation
+from nncf.common.pruning.utils import is_grouped_conv
+from nncf.common.pruning.utils import is_prunable_depthwise_conv
 from nncf.common.tensor import NNCFTensor
-from nncf.common.pruning.symbolic_mask import SymbolicMask
-from nncf.common.graph import NNCFGraphEdge
-from nncf.common.graph.layer_attributes import GroupNormLayerAttributes
 
 
 class BasePruningOp:
     """
     Determines meta operations which aggregate operations having common
     properties of interaction with pruning masks
     """
@@ -39,16 +37,17 @@
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         """
         :return: accept_pruned_input - can this operation work with pruned input or not
         """
         raise NotImplementedError
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         """
         Propagates the pruning mask through a node using pruning masks of all inputs and the current node (if any).
 
         :param node: The graph node to propagate mask through it
         :param graph: The model graph to prune
         :param tensor_processor: Interface with tensor processing methods
         """
@@ -68,156 +67,169 @@
 
 class InputPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         return False
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
-        node.data['output_mask'] = None
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
+        node.data["output_mask"] = None
 
 
 class OutputPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         return True
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
-        node.data['output_mask'] = None
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
+        node.data["output_mask"] = None
 
 
 class IdentityMaskForwardPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         return True
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         identity_mask_propagation(node, graph)
 
 
 class ConvolutionPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         if is_grouped_conv(node) and not is_prunable_depthwise_conv(node):
             return False
         return True
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         input_masks = get_input_masks(node, graph)
-        output_mask = node.data.get('output_mask', None)
+        output_mask = node.data.get("output_mask", None)
 
         if is_grouped_conv(node):
             output_mask = None
             if is_prunable_depthwise_conv(node):
                 output_mask = input_masks[0]
 
-        node.data['output_mask'] = output_mask
+        node.data["output_mask"] = output_mask
 
 
 class TransposeConvolutionPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         if is_grouped_conv(node) and not is_prunable_depthwise_conv(node):
             return False
         return True
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         input_masks = get_input_masks(node, graph)
-        output_mask = node.data.get('output_mask', None)
+        output_mask = node.data.get("output_mask", None)
 
         # In case of group convs we can't prune by output filters
         if is_grouped_conv(node):
             output_mask = None
             if is_prunable_depthwise_conv(node):
                 output_mask = input_masks[0]
 
-        node.data['output_mask'] = output_mask
+        node.data["output_mask"] = output_mask
 
 
 class LinearPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         return True
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
-        output_mask = node.data.get('output_mask', None)
-        node.data['output_mask'] = output_mask
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
+        output_mask = node.data.get("output_mask", None)
+        node.data["output_mask"] = output_mask
 
 
 class BatchNormPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         return True
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         identity_mask_propagation(node, graph)
 
 
 class GroupNormPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         # For Instance Normalization
-        return isinstance(node.layer_attributes, GroupNormLayerAttributes) \
-               and node.layer_attributes.num_groups == node.layer_attributes.num_channels
+        return (
+            isinstance(node.layer_attributes, GroupNormLayerAttributes)
+            and node.layer_attributes.num_groups == node.layer_attributes.num_channels
+        )
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         if cls.accept_pruned_input(node):
             identity_mask_propagation(node, graph)
         else:
-            node.data['output_mask'] = None
+            node.data["output_mask"] = None
 
 
 class LayerNormPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         return True
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         identity_mask_propagation(node, graph)
 
 
 class ConcatPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode):
         return True
 
     @classmethod
-    def generate_output_mask(cls, node: NNCFNode, graph: NNCFGraph,
-                             tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> Optional[NNCFTensor]:
+    def generate_output_mask(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> Optional[NNCFTensor]:
         """
         Generate output mask from input masks with all None replaced by identity masks.
         If all input masks is None return None.
 
         :param node: Node to determine it's sources.
         :param graph: NNCF graph to work with.
         :param tensor_processor: Interface with tensor processing methods.
         :return: Filled input masks.
         """
         input_edges = graph.get_input_edges(node)
         previous_nodes = [edge.from_node for edge in input_edges]
-        input_masks = [input_node.data['output_mask'] for input_node in previous_nodes]
-        input_masks = [input_mask[node.node_name] if isinstance(input_mask, dict) else input_mask
-                       for input_mask in input_masks]
+        input_masks = [input_node.data["output_mask"] for input_node in previous_nodes]
+        input_masks = [
+            input_mask[node.node_name] if isinstance(input_mask, dict) else input_mask for input_mask in input_masks
+        ]
 
         not_empty_masks = [mask for mask in input_masks if mask is not None]  # type: List[NNCFTensor]
         if not not_empty_masks:
             return None
 
         first_non_empty_mask = not_empty_masks[0]
 
@@ -229,24 +241,26 @@
                 concat_dim = input_edges[i].tensor_shape[concat_axis]
                 mask = tensor_processor.ones(concat_dim, device)
             filled_input_masks.append(mask)
         result_mask = tensor_processor.concatenate(filled_input_masks, 0)
         return result_mask
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         result_mask = cls.generate_output_mask(node, graph, tensor_processor)
-        node.data['output_mask'] = result_mask
+        node.data["output_mask"] = result_mask
 
 
 class SplitPruningOp(BasePruningOp):
     @classmethod
-    def match_multiple_output_masks(cls, output_masks: List[SymbolicMask], output_edges: List[NNCFGraphEdge],
-                                    chunk_axis: int) -> Dict['str', SymbolicMask]:
+    def match_multiple_output_masks(
+        cls, output_masks: List[SymbolicMask], output_edges: List[NNCFGraphEdge], chunk_axis: int
+    ) -> Dict["str", SymbolicMask]:
         """
         Match multiple input mask to each next nodes.
 
         :param output_masks: Given output masks.
         :param output_edges: Given output edges of the node.
         :param chunk_axis: Given the axis on which operation was performed.
         :return: Matched output mask for each next node.
@@ -265,16 +279,17 @@
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode):
         if node.layer_attributes is not None:
             return True
         return False
 
     @classmethod
-    def generate_output_masks(cls, node: NNCFNode, graph: NNCFGraph,
-                              tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> Optional[NNCFTensor]:
+    def generate_output_masks(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> Optional[NNCFTensor]:
         """
         Generate output mask from input masks for split/chunk operations.
         If input mask is None return None
 
         :param node: Node to determine its sources.
         :param graph: NNCF graph to work with.
         :param tensor_processor: Interface with tensor processing methods.
@@ -305,81 +320,90 @@
 
         split_masks = tensor_processor.split(input_mask, output_shapes)
         result_masks = cls.match_multiple_output_masks(split_masks, output_edges, chunk_axis)
 
         return result_masks
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         result_masks = cls.generate_output_masks(node, graph, tensor_processor)
-        node.data['output_mask'] = result_masks
+        node.data["output_mask"] = result_masks
+
+
+class PadPruningOp(IdentityMaskForwardPruningOp):
+    @classmethod
+    def accept_pruned_input(cls, node: NNCFNode) -> bool:
+        mode, value = node.layer_attributes.mode, node.layer_attributes.value
+        if mode == "constant" and value != 0:
+            return False
+        return True
 
 
 class ElementwisePruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         return True
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         input_masks = get_input_masks(node, graph)
         output_mask = input_masks[0]
         if output_mask is not None:
             output_mask = tensor_processor.elementwise_mask_propagation(input_masks)
 
-        node.data['output_mask'] = output_mask
+        node.data["output_mask"] = output_mask
 
 
 class ReshapePruningOp(BasePruningOp):
     @staticmethod
     def _is_flatten(node: NNCFNode) -> bool:
         return len(node.layer_attributes.output_shape) == 2
 
     @staticmethod
     def _is_not_mixing_dim(node: NNCFNode) -> bool:
         input_shape = node.layer_attributes.input_shape
         output_shape = node.layer_attributes.output_shape
 
         # TODO(dlyakhov): Cover all corner cases that appear here (ticket 90976)
-        if len(input_shape)==len(output_shape) and set(input_shape)==set(output_shape):
+        if len(input_shape) == len(output_shape) and set(input_shape) == set(output_shape):
             return input_shape == output_shape
         return True
 
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
-        if node.layer_attributes is None:
-            return False
-        return True
+        return node.layer_attributes is not None
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
         if cls.accept_pruned_input(node):
             if cls._is_flatten(node):
                 FlattenPruningOp.mask_propagation(node, graph, tensor_processor)
             elif cls._is_not_mixing_dim(node):
                 identity_mask_propagation(node, graph)
             else:
-                node.data['output_mask'] = None
+                node.data["output_mask"] = None
         else:
-            node.data['output_mask'] = None
+            node.data["output_mask"] = None
 
 
 class FlattenPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         if node.layer_attributes is not None:
             return True
         return False
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]):
+    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]):
         output_mask = None
         input_masks = get_input_masks(node, graph)
         assert len(input_masks) == 1
         input_mask = input_masks[0]
         if input_mask is not None and node.layer_attributes is not None:
             # We assume all layers known by the mask propagation algo except
             # StopMaskForwardOp have input/output batch dim == 0.
@@ -387,19 +411,20 @@
             # was in the path from mask producer node to this node. As all
             # known nodes have input/output batch dim == 0 previous has too.
             flatten_channels = node.layer_attributes.output_shape[1]
             mask_len = input_mask.shape[0]
             assert flatten_channels % mask_len == 0
             output_mask = tensor_processor.repeat(input_mask, repeats=flatten_channels // mask_len)
 
-        node.data['output_mask'] = output_mask
+        node.data["output_mask"] = output_mask
 
 
 class StopMaskForwardPruningOp(BasePruningOp):
     @classmethod
     def accept_pruned_input(cls, node: NNCFNode) -> bool:
         return False
 
     @classmethod
-    def mask_propagation(cls, node: NNCFNode, graph: NNCFGraph,
-                         tensor_processor: Type[NNCFPruningBaseTensorProcessor]) -> None:
-        node.data['output_mask'] = None
+    def mask_propagation(
+        cls, node: NNCFNode, graph: NNCFGraph, tensor_processor: Type[NNCFPruningBaseTensorProcessor]
+    ) -> None:
+        node.data["output_mask"] = None
```

### Comparing `nncf-2.4.0/nncf/common/pruning/schedulers.py` & `nncf-2.5.0/nncf/common/pruning/schedulers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,82 +1,73 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Optional
+
 import numpy as np
 import scipy.optimize
 
+from nncf.api.compression import CompressionAlgorithmController
+from nncf.common.schedulers import BaseCompressionScheduler
+from nncf.common.schedulers import ExponentialDecaySchedule
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.registry import Registry
-from nncf.common.schedulers import ExponentialDecaySchedule, BaseCompressionScheduler
 from nncf.config.schemata.defaults import PRUNING_NUM_INIT_STEPS
 from nncf.config.schemata.defaults import PRUNING_STEPS
 from nncf.config.schemata.defaults import PRUNING_TARGET
 
-PRUNING_SCHEDULERS = Registry('pruning_schedulers')
+PRUNING_SCHEDULERS = Registry("pruning_schedulers")
 
 
+@api()
 class PruningScheduler(BaseCompressionScheduler):
     """
     This is the class from which all pruning schedulers inherit.
 
     A pruning scheduler is an object which specifies the pruning
     level at each training epoch. It involves a scheduling algorithm,
     defined in the `_calculate_pruning_level()` method and a state
     (some parameters required for current pruning level calculation)
     defined in the `__init__()` method.
 
-    :param initial_level: Pruning level which already has been
-        applied to the model. It is the level at which the schedule begins.
-    :param target_level: Pruning level at which the schedule ends.
-    :param num_warmup_epochs: Number of epochs for model pre-training before pruning.
-    :param num_pruning_epochs: Number of epochs during which the pruning level
-        is increased from `initial_level` to `target_level`.
-    :param freeze_epoch: Zero-based index of the epoch from which the pruning
-        mask will be frozen and will not be trained.
+    :param controller: Pruning algorithm controller.
+    :param params: Parameters of the scheduler in the JSON-like dictionary form. Passed as-is from the corresponding
+      section of the NNCF config file .json section (https://openvinotoolkit.github.io/nncf/schema).
     """
 
-    def __init__(self, controller, params: dict):
-        """
-        Initializes the internal state of the pruning scheduler.
-
-        :param controller: Pruning algorithm controller.
-        :param params: Parameters of the scheduler.
-        """
+    def __init__(self, controller: CompressionAlgorithmController, params: dict):
         super().__init__()
         self._controller = controller
         self.initial_level = self._controller.pruning_init
 
         if self._controller.prune_flops:
-            self.target_level = params.get('pruning_flops_target')
+            self.target_level = params.get("pruning_flops_target")
         else:
-            self.target_level = params.get('pruning_target', PRUNING_TARGET)
+            self.target_level = params.get("pruning_target", PRUNING_TARGET)
 
-        self.num_warmup_epochs = params.get('num_init_steps', PRUNING_NUM_INIT_STEPS)
-        self.num_pruning_epochs = params.get('pruning_steps', PRUNING_STEPS)
+        self.num_warmup_epochs = params.get("num_init_steps", PRUNING_NUM_INIT_STEPS)
+        self.num_pruning_epochs = params.get("pruning_steps", PRUNING_STEPS)
         self.freeze_epoch = self.num_warmup_epochs + self.num_pruning_epochs
 
     def _calculate_pruning_level(self) -> float:
         """
         Calculates a pruning level that should be applied to the model
         for the `current_epoch`.
 
         :return: Pruning level that should be applied to the model.
         """
-        raise NotImplementedError(
-            'PruningScheduler implementation must override _calculate_pruning_level method.')
+        raise NotImplementedError("PruningScheduler implementation must override _calculate_pruning_level method.")
 
     def epoch_step(self, next_epoch: Optional[int] = None) -> None:
         """
         Should be called at the beginning of each training epoch to prepare
         the pruning method to continue training the model in the `next_epoch`.
 
         :param next_epoch: The epoch index for which the pruning scheduler
@@ -106,44 +97,44 @@
         :return: Current sparsity level.
         """
         if self.current_epoch >= self.num_warmup_epochs:
             return self._calculate_pruning_level()
         return 0
 
 
-@PRUNING_SCHEDULERS.register('baseline')
+@PRUNING_SCHEDULERS.register("baseline")
 class BaselinePruningScheduler(PruningScheduler):
     """
     Pruning scheduler which applies the same pruning level for each epoch.
 
     The model is trained without pruning during `num_warmup_epochs` epochs.
     Then scheduler sets `target_level` and freezes the algorithm.
     """
 
-    def __init__(self, controller, params: dict):
+    def __init__(self, controller: CompressionAlgorithmController, params: dict):
         super().__init__(controller, params)
         self.freeze_epoch = self.num_warmup_epochs
 
     def _calculate_pruning_level(self) -> float:
         return self.target_level
 
 
-@PRUNING_SCHEDULERS.register('exponential')
+@PRUNING_SCHEDULERS.register("exponential")
 class ExponentialPruningScheduler(PruningScheduler):
     """
     Pruning scheduler with an exponential decay schedule.
 
     This scheduler applies exponential decay to the density level
     to calculate the pruning level for the `current_epoch`.
     The density level for the `current_epoch` is calculated as
 
         current_density = 1.0 - current_level
     """
 
-    def __init__(self, controller, params: dict):
+    def __init__(self, controller: CompressionAlgorithmController, params: dict):
         """
         Initializes a pruning scheduler with an exponential decay schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
@@ -154,26 +145,26 @@
 
     def _calculate_pruning_level(self) -> float:
         current_density = self.schedule(self.current_epoch - self.num_warmup_epochs)
         current_level = 1.0 - current_density
         return min(current_level, self.target_level)
 
 
-@PRUNING_SCHEDULERS.register('exponential_with_bias')
+@PRUNING_SCHEDULERS.register("exponential_with_bias")
 class ExponentialWithBiasPruningScheduler(PruningScheduler):
     """
     Pruning scheduler which calculates pruning level for the current epoch
     according to the formula:
 
         current_level = a * exp(-k * epoch_idx) + b,
 
     where a, b, k is a params.
     """
 
-    def __init__(self, controller, params: dict):
+    def __init__(self, controller: CompressionAlgorithmController, params: dict):
         """
         Initializes a pruning scheduler with an exponential (with bias) decay schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
@@ -194,22 +185,23 @@
         For more details see [paper](https://arxiv.org/pdf/1808.07471.pdf).
 
         :param epoch_idx: Zero-based index of the epoch for which the a * exp(-k * epoch_idx) + b = p_max.
         :param p_min: Initial pruning level at which the schedule begins.
         :param p_max: Target pruning level at which the schedule ends.
         :param factor: Hyperparameter.
         """
+
         def get_b(a):
             return p_min - a
 
         def get_a(k):
             return (p_max - p_min) / (np.exp(-k * epoch_idx) - 1)
 
         def f_to_solve(x):
             c = (0.75 * p_max - p_min) / (p_max - p_min)
             y = np.exp(-x * epoch_idx)
-            return y ** factor - c * y + c - 1
+            return y**factor - c * y + c - 1
 
         k = scipy.optimize.fsolve(f_to_solve, [1])[0]
         a = get_a(k)
         b = get_b(a)
         return a, b, k
```

### Comparing `nncf-2.4.0/nncf/common/pruning/shape_pruning_processor.py` & `nncf-2.5.0/nncf/common/pruning/shape_pruning_processor.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,93 +1,86 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Tuple
-from typing import Any
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Any, Callable, Dict, List, Tuple
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.pruning.clusterization import Cluster
 from nncf.common.pruning.clusterization import Clusterization
+from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
 from nncf.common.pruning.structs import PrunedLayerInfoBase
 from nncf.common.pruning.symbolic_mask import SymbolicMask
 from nncf.common.pruning.symbolic_mask import SymbolicMaskProcessor
-from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
-from nncf.common.pruning.utils import get_prunable_layers_in_out_channels
-from nncf.common.pruning.utils import get_rounded_pruned_element_number
+from nncf.common.pruning.utils import get_input_masks
 from nncf.common.pruning.utils import get_next_nodes_of_types
 from nncf.common.pruning.utils import get_output_channels
-from nncf.common.pruning.utils import get_input_masks
+from nncf.common.pruning.utils import get_prunable_layers_in_out_channels
+from nncf.common.pruning.utils import get_rounded_pruned_element_number
 
 
 class ShapePruningProcessor:
     """
     Collection of shape pruning functions. Class instance keeps
     only parameters that are constant during
     compression algorithms execution.
     """
 
-    def __init__(self,
-                 prunable_types: List[str],
-                 pruning_operations_metatype: List[str]):
+    def __init__(self, prunable_types: List[str], pruning_operations_metatype: List[str]):
         """
         Constructor.
 
         :param prunable_types: Types of nodes that will be returned.
         :param pruning_operations_metatype: Metatypes of nodes that will be returned.
         """
         self._prunable_types = prunable_types
         self._pruning_operations_metatype = pruning_operations_metatype
 
     def calculate_in_out_channels_by_masks(
         self,
         graph: NNCFGraph,
         pruning_groups: List[Cluster[PrunedLayerInfoBase]],
         pruning_groups_next_nodes: Dict[int, List[Dict[str, Any]]],
-        num_of_sparse_elements_by_node: Dict[NNCFNodeName, int]) -> \
-        Tuple[Dict[str, int], Dict[str, int]]:
+        num_of_sparse_elements_by_node: Dict[NNCFNodeName, int],
+    ) -> Tuple[Dict[str, int], Dict[str, int]]:
         """
         Imitates filters pruning by removing output filters zeroed by pruning masks in each pruning group
         and updating corresponding input channels number in `pruning_groups_next_nodes` nodes.
 
         :param graph: NNCFGraph.
         :param pruning_groups: A list of pruning groups.
         :param pruning_groups_next_nodes: A dictionary of next nodes of each pruning group.
         :param num_of_sparse_elements_by_node: A dictionary of num_of_sparse_elements of each pruning node.
         :return Dictionary of new input channels number {node_name: channels_num}
         """
 
         def get_sparser(full_output_channels):
             def get_num_of_sparse_elements_by_node(node_name: str) -> int:
                 return num_of_sparse_elements_by_node[node_name]
+
             return get_num_of_sparse_elements_by_node
 
         return self._calculate_in_out_channels(get_sparser, graph, pruning_groups, pruning_groups_next_nodes)
 
     def calculate_in_out_channels_in_uniformly_pruned_model(
         self,
         graph: NNCFGraph,
         pruning_groups: List[Cluster[PrunedLayerInfoBase]],
         pruning_groups_next_nodes: Dict[int, List[Dict[str, Any]]],
-        pruning_level: float) -> \
-        Tuple[Dict[str, int], Dict[str, int]]:
+        pruning_level: float,
+    ) -> Tuple[Dict[str, int], Dict[str, int]]:
         """
         Imitates filters pruning by removing `pruning_level` percent of output filters in each pruning group
         and updating corresponding input channels number in `pruning_groups_next_nodes` nodes.
 
         :param graph: NNCFGraph.
         :param pruning_groups: A list of pruning groups.
         :param pruning_groups_next_nodes: A dictionary of next nodes of each pruning group.
@@ -95,24 +88,27 @@
         :return Tuple of dictionarise of new input and output channels number {node_name: channels_num}
         """
 
         def get_sparser(full_output_channels):
             def get_num_of_sparse_elements_by_node(node_name: str) -> int:
                 old_out_channels = full_output_channels[node_name]
                 return get_rounded_pruned_element_number(old_out_channels, pruning_level)
+
             return get_num_of_sparse_elements_by_node
 
         return self._calculate_in_out_channels(get_sparser, graph, pruning_groups, pruning_groups_next_nodes)
 
-    def prune_cluster_shapes(self,
-                             cluster: Cluster[PrunedLayerInfoBase],
-                             pruned_elems: int,
-                             pruning_groups_next_nodes: Dict[int, List[Dict[str, Any]]],
-                             input_channels: Dict[NNCFNodeName, int],
-                             output_channels: Dict[NNCFNodeName, int]) -> None:
+    def prune_cluster_shapes(
+        self,
+        cluster: Cluster[PrunedLayerInfoBase],
+        pruned_elems: int,
+        pruning_groups_next_nodes: Dict[int, List[Dict[str, Any]]],
+        input_channels: Dict[NNCFNodeName, int],
+        output_channels: Dict[NNCFNodeName, int],
+    ) -> None:
         """
         Imitates filter pruning by removing `pruned_elems` elements from
         input/output channels corresponded to feeded cluster.
 
         :param cluster:  A PrunedLayerInfoBase cluster.
         :param pruned_elems: Amount of channels/elements to prune.
         :param pruning_groups_next_nodes: A dictionary of next nodes of each pruning group.
@@ -128,71 +124,73 @@
             output_channels[node.node_name] -= pruned_elems
             if node.is_depthwise:
                 input_channels[node.node_name] -= pruned_elems
 
         # Prune in channels in all next nodes
         next_nodes_info = pruning_groups_next_nodes[cluster.id]
         for next_node_info in next_nodes_info:
-            input_channels[next_node_info['node_name']] -= pruned_elems * next_node_info['sparse_multiplier']
+            input_channels[next_node_info["node_name"]] -= pruned_elems * next_node_info["sparse_multiplier"]
 
     def _calculate_in_out_channels(
         self,
         sparse_elements_counter_getter: Callable[[Dict[NNCFNodeName, int]], Callable[[NNCFNodeName], int]],
         graph: NNCFGraph,
         pruning_groups: List[Cluster[PrunedLayerInfoBase]],
-        pruning_groups_next_nodes: Dict[int, List[Dict[str, Any]]]) -> Tuple[Dict[str, int], Dict[str, int]]:
-
+        pruning_groups_next_nodes: Dict[int, List[Dict[str, Any]]],
+    ) -> Tuple[Dict[str, int], Dict[str, int]]:
         full_input_channels, full_output_channels = get_prunable_layers_in_out_channels(graph)
         tmp_in_channels, tmp_out_channels = full_input_channels.copy(), full_output_channels.copy()
         sparse_elements_counter = sparse_elements_counter_getter(full_output_channels)
 
         for group in pruning_groups.get_all_clusters():
             layer_name = group.elements[0].node_name
-            assert all(tmp_out_channels[layer_name] == tmp_out_channels[node.node_name] for node in
-                       group.elements)
+            assert all(tmp_out_channels[layer_name] == tmp_out_channels[node.node_name] for node in group.elements)
             # Prune all nodes in cluster (by output channels)
             num_of_sparse_elems = sparse_elements_counter(layer_name)
 
-            self.prune_cluster_shapes(cluster=group, pruned_elems=num_of_sparse_elems,
-                                      input_channels=tmp_in_channels,
-                                      output_channels=tmp_out_channels,
-                                      pruning_groups_next_nodes=pruning_groups_next_nodes)
+            self.prune_cluster_shapes(
+                cluster=group,
+                pruned_elems=num_of_sparse_elems,
+                input_channels=tmp_in_channels,
+                output_channels=tmp_out_channels,
+                pruning_groups_next_nodes=pruning_groups_next_nodes,
+            )
 
         return tmp_in_channels, tmp_out_channels
 
-    def _get_next_node_sparse_multiplier(self, graph: NNCFGraph, next_node: NNCFNode,
-                                         cluster: Clusterization[PrunedLayerInfoBase]) -> int:
+    def _get_next_node_sparse_multiplier(
+        self, graph: NNCFGraph, next_node: NNCFNode, cluster: Clusterization[PrunedLayerInfoBase]
+    ) -> int:
         cluster_nodes_idxs = {node.nncf_node_id for node in cluster.elements}
         for input_mask in get_input_masks(next_node, graph):
             if not input_mask:
                 continue
             for mask_producer in input_mask.mask_producers:
                 if mask_producer.id in cluster_nodes_idxs:
                     return mask_producer.sparse_multiplier
 
-        raise RuntimeError(f'Next node for cluster {cluster.elements} doesn\'t have closing mask')
+        raise RuntimeError(f"Next node for cluster {cluster.elements} doesn't have closing mask")
 
-    def get_next_nodes(self, graph: NNCFGraph, pruning_groups: Clusterization[PrunedLayerInfoBase]) ->\
-        Dict[int, Dict[str, Any]]:
+    def get_next_nodes(
+        self, graph: NNCFGraph, pruning_groups: Clusterization[PrunedLayerInfoBase]
+    ) -> Dict[int, Dict[str, Any]]:
         """
         Finds nodes of `prunable_types` types that receive the output of a pruned cluster as input
         and collects all info specified in NextNode.
 
         :param graph: NNCFGraph.
         :param pruning_groups: `Clusterization` of pruning groups.
         :return Dictionary of next nodes by cluster {cluster_id: [node]}.
         """
         # 1. Propagate symbolic masks throught the net
         for pruned_layer_info in pruning_groups.get_all_nodes():
             node = graph.get_node_by_id(pruned_layer_info.nncf_node_id)
-            node.data['output_mask'] = SymbolicMask(get_output_channels(node), node.node_id)
+            node.data["output_mask"] = SymbolicMask(get_output_channels(node), node.node_id)
 
-        MaskPropagationAlgorithm(graph,
-                                 self._pruning_operations_metatype,
-                                 SymbolicMaskProcessor).mask_propagation()
+        MaskPropagationAlgorithm(graph, self._pruning_operations_metatype, SymbolicMaskProcessor).mask_propagation()
 
         # 2. Find next nodes and correspondent sparse multipliers
         next_nodes = {}
         for cluster in pruning_groups.get_all_clusters():
             next_nodes_cluster = set()
             cluster_nodes = set()
             for pruned_layer_info in cluster.elements:
@@ -201,15 +199,16 @@
                 curr_next_nodes = get_next_nodes_of_types(graph, nncf_cluster_node, self._prunable_types)
 
                 next_nodes_cluster = next_nodes_cluster.union(curr_next_nodes)
             next_nodes_cluster = next_nodes_cluster - cluster_nodes
             next_nodes[cluster.id] = []
             for next_node in next_nodes_cluster:
                 sparse_multiplier = self._get_next_node_sparse_multiplier(graph, next_node, cluster)
-                next_nodes[cluster.id].append({'node_name': next_node.node_name,
-                                               'sparse_multiplier': sparse_multiplier})
+                next_nodes[cluster.id].append(
+                    {"node_name": next_node.node_name, "sparse_multiplier": sparse_multiplier}
+                )
 
         # 3. Clean graph output shapes
         for node in graph.get_all_nodes():
-            node.data['output_shape'] = None
+            node.data["output_shape"] = None
 
         return next_nodes
```

### Comparing `nncf-2.4.0/nncf/common/pruning/statistics.py` & `nncf-2.5.0/nncf/common/pruning/statistics.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,36 +1,31 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 from nncf.api.statistics import Statistics
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.helpers import create_table
 
 
 class PrunedLayerSummary:
     """
     Contains information about the pruned layer.
     """
 
-    def __init__(self,
-                 name: str,
-                 weight_shape: List[int],
-                 mask_shape: List[int],
-                 filter_pruning_level: float):
+    def __init__(self, name: str, weight_shape: List[int], mask_shape: List[int], filter_pruning_level: float):
         """
         Initializes a summary about the pruned layer.
 
         :param name: Layer's name.
         :param weight_shape: Weight's shape.
         :param mask_shape: Mask's shape.
         :param filter_pruning_level: Filter's pruning level.
@@ -42,22 +37,24 @@
 
 
 class PrunedModelStatistics(Statistics):
     """
     Contains statistics of the pruned model.
     """
 
-    def __init__(self,
-                 full_flops: int,
-                 current_flops: int,
-                 full_params_num: int,
-                 current_params_num: int,
-                 full_filters_num: int,
-                 current_filters_num: int,
-                 pruned_layers_summary: List[PrunedLayerSummary]):
+    def __init__(
+        self,
+        full_flops: int,
+        current_flops: int,
+        full_params_num: int,
+        current_params_num: int,
+        full_filters_num: int,
+        current_filters_num: int,
+        pruned_layers_summary: List[PrunedLayerSummary],
+    ):
         """
         Initializes statistics of the pruned model.
 
         :param full_flops: The total amount of FLOPs in the model.
         :param current_flops: Current amount of FLOPs in the model.
         :param full_params_num: The total amount of weights in the model.
         :param current_params_num: Current amount of weights in the model.
@@ -77,98 +74,101 @@
         self.full_filters_num = full_filters_num
         self.current_filters_num = current_filters_num
         self.filter_pruning_level = 1 - self.current_filters_num / self.full_filters_num
         self.pruned_layers_summary = pruned_layers_summary
 
     def to_str(self) -> str:
         model_string = create_table(
-            header=['#', 'Full', 'Current', 'Pruning level'],
+            header=["#", "Full", "Current", "Pruning level"],
             rows=[
-                ['GFLOPs', f'{self.full_flops / self._giga:.3f}',
-                           f'{self.current_flops / self._giga:.3f}',
-                           self.flops_pruning_level],
-                ['MParams', f'{self.full_params_num / self._mega:.3f}',
-                            f'{self.current_params_num / self._mega:.3f}',
-                            self.params_pruning_level],
-                ['Filters', self.full_filters_num, self.current_filters_num, self.filter_pruning_level],
-            ]
+                [
+                    "GFLOPs",
+                    f"{self.full_flops / self._giga:.3f}",
+                    f"{self.current_flops / self._giga:.3f}",
+                    self.flops_pruning_level,
+                ],
+                [
+                    "MParams",
+                    f"{self.full_params_num / self._mega:.3f}",
+                    f"{self.current_params_num / self._mega:.3f}",
+                    self.params_pruning_level,
+                ],
+                ["Filters", self.full_filters_num, self.current_filters_num, self.filter_pruning_level],
+            ],
         )
 
-        header = ['Layer\'s name', 'Weight\'s shape', 'Mask\'s shape', 'Filter pruning level']
+        header = ["Layer's name", "Weight's shape", "Mask's shape", "Filter pruning level"]
         rows = []
         for s in self.pruned_layers_summary:
             rows.append([s.name, s.weight_shape, s.mask_shape, s.filter_pruning_level])
 
         layers_string = create_table(header, rows)
 
-        pruning_level_desc = 'Prompt: statistic pruning level = 1 - statistic current / statistic full.'
+        pruning_level_desc = "Prompt: statistic pruning level = 1 - statistic current / statistic full."
         pretty_string = (
-            f'Statistics by pruned layers:\n{layers_string}\n'
-            f'Statistics of the pruned model:\n{model_string}\n'
-            + pruning_level_desc
+            f"Statistics by pruned layers:\n{layers_string}\n"
+            f"Statistics of the pruned model:\n{model_string}\n" + pruning_level_desc
         )
 
         return pretty_string
 
 
+@api()
 class FilterPruningStatistics(Statistics):
     """
     Contains statistics of the filter pruning algorithm.
+
+    :param model_statistics: Statistics of the pruned model.
+    :param current_pruning_level: A current level of the pruning for the algorithm for the current epoch.
+    :param target_pruning_level: A target level of the pruning for the algorithm.
+    :param prune_flops: Is pruning algo sets flops pruning level or not (filter pruning level).
     """
 
-    def __init__(self,
-                 model_statistics: PrunedModelStatistics,
-                 current_pruning_level: float,
-                 target_pruning_level: float,
-                 prune_flops: bool):
-        """
-        Initializes statistics of the filter pruning algorithm.
-
-        :param model_statistics: Statistics of the pruned model.
-        :param current_pruning_level: A current level of the pruning
-            for the algorithm for the current epoch.
-        :param target_pruning_level: A target level of the pruning
-            for the algorithm.
-        :param prune_flops: Is pruning algo sets flops pruning level or
-            not (filter pruning level).
-        """
+    def __init__(
+        self,
+        model_statistics: PrunedModelStatistics,
+        current_pruning_level: float,
+        target_pruning_level: float,
+        prune_flops: bool,
+    ):
         self.model_statistics = model_statistics
         self.current_pruning_level = current_pruning_level
         self.target_pruning_level = target_pruning_level
         self.prune_flops = prune_flops
 
     def to_str(self) -> str:
-        pruning_mode = 'FLOPs' if self.prune_flops else 'filter'
+        pruning_mode = "FLOPs" if self.prune_flops else "filter"
         algorithm_string = create_table(
-            header=['Statistic\'s name', 'Value'],
+            header=["Statistic's name", "Value"],
             rows=[
-                [f'{pruning_mode.capitalize()} pruning level in current epoch', self.current_pruning_level],
-                [f'Target {pruning_mode} pruning level', self.target_pruning_level],
-            ]
+                [f"{pruning_mode.capitalize()} pruning level in current epoch", self.current_pruning_level],
+                [f"Target {pruning_mode} pruning level", self.target_pruning_level],
+            ],
         )
 
         pretty_string = (
-            f'{self.model_statistics.to_str()}\n'
-            f'Statistics of the filter pruning algorithm:\n{algorithm_string}'
+            f"{self.model_statistics.to_str()}\n" f"Statistics of the filter pruning algorithm:\n{algorithm_string}"
         )
         return pretty_string
 
 
 class PrunedModelTheoreticalBorderline(Statistics):
     """
     Contains theoretical borderline statistics of the filter pruning algorithm.
     """
 
-    def __init__(self,
-                 num_pruned_layers: int,
-                 num_prunable_layers: int,
-                 min_possible_flops: float,
-                 min_possible_params: float,
-                 total_flops: int,
-                 total_params: int):
+    def __init__(
+        self,
+        num_pruned_layers: int,
+        num_prunable_layers: int,
+        min_possible_flops: float,
+        min_possible_params: float,
+        total_flops: int,
+        total_params: int,
+    ):
         """
         Initializes statistics of the filter pruning theoretical borderline.
 
         :param num_pruned_layers: Number of layers which was actually pruned.
         :param num_prunable_layers: Number of layers which have prunable type.
         :param min_possible_flops: Number of flops for pruned model with pruning level = 1.
         :param min_possible_params: Number of weights for pruned model with pruning level = 1.
@@ -182,22 +182,28 @@
         self.min_possible_flops = min_possible_flops
         self.min_possible_params = min_possible_params
         self.total_flops = total_flops
         self.total_params = total_params
 
     def to_str(self) -> str:
         algorithm_string = create_table(
-            header=['Statistic\'s name', 'Value'],
+            header=["Statistic's name", "Value"],
             rows=[
-                ['Pruned layers count / prunable layers count', f'{self.pruned_layers_num} /'
-                                                                f' {self.prunable_layers_num}'],
-                ['GFLOPs minimum possible after pruning / total', f'{self.min_possible_flops / self._giga:.3f} /'
-                                                                  f' {self.total_flops / self._giga:.3f}'],
-                ['MParams minimum possible after pruning / total', f'{self.min_possible_params / self._mega:.3f} /'
-                                                                   f' {self.total_params / self._mega:.3f}'],
-            ]
+                [
+                    "Pruned layers count / prunable layers count",
+                    f"{self.pruned_layers_num} /" f" {self.prunable_layers_num}",
+                ],
+                [
+                    "GFLOPs minimum possible after pruning / total",
+                    f"{self.min_possible_flops / self._giga:.3f} /" f" {self.total_flops / self._giga:.3f}",
+                ],
+                [
+                    "MParams minimum possible after pruning / total",
+                    f"{self.min_possible_params / self._mega:.3f} /" f" {self.total_params / self._mega:.3f}",
+                ],
+            ],
         )
 
         pretty_string = (
-            f'Theoretical borderline of the filter pruning algorithm\nfor current model:\n{algorithm_string}'
+            f"Theoretical borderline of the filter pruning algorithm\nfor current model:\n{algorithm_string}"
         )
         return pretty_string
```

### Comparing `nncf-2.4.0/nncf/common/pruning/structs.py` & `nncf-2.5.0/nncf/data/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,20 +1,12 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from nncf.common.graph import NNCFNodeName
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-
-class PrunedLayerInfoBase:
-    def __init__(self, node_name: NNCFNodeName, node_id: int, is_depthwise: bool):
-        self.node_name = node_name
-        self.nncf_node_id = node_id
-        self.is_depthwise = is_depthwise
+from nncf.data.dataset import Dataset
```

### Comparing `nncf-2.4.0/nncf/common/pruning/symbolic_mask.py` & `nncf-2.5.0/nncf/common/pruning/symbolic_mask.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List, Union
 
-from nncf.common.tensor import NNCFTensor
 from nncf.common.pruning.tensor_processor import NNCFPruningBaseTensorProcessor
+from nncf.common.tensor import NNCFTensor
 
 
 class SymbolicMaskProducer:
     """
     Container of information about a NNCFNode which is produsing a symbolic mask.
     NNCFNode produced a (symbolic or not) mask means this mask was set as an output
     mask to this NNCFNode during (symbolic or not) mask propagation.
@@ -33,21 +31,22 @@
         return self._sparse_multiplier
 
     @property
     def id(self) -> int:
         return self._id
 
     @classmethod
-    def merge_producers(cls, masks: List['SymbolicMask']) -> List['SymbolicMaskProducer']:
+    def merge_producers(cls, masks: List["SymbolicMask"]) -> List["SymbolicMaskProducer"]:
         merged_producers = {}
         for mask in masks:
             for mask_producer in mask.mask_producers:
                 if mask_producer.id in merged_producers:
-                    assert mask_producer.sparse_multiplier == merged_producers[mask_producer.id].sparse_multiplier, \
-                        f"Inconsistent sparse multiplier for NNCF node with id={mask_producer.id}"
+                    assert (
+                        mask_producer.sparse_multiplier == merged_producers[mask_producer.id].sparse_multiplier
+                    ), f"Inconsistent sparse multiplier for NNCF node with id={mask_producer.id}"
             merged_producers.update({p.id: p for p in mask.mask_producers})
         return list(merged_producers.values())
 
 
 class SymbolicMask(NNCFTensor):
     """
     Framework agnostic 1D NNCFTensor representation which only uses given dimension and do not uses value
@@ -106,30 +105,31 @@
         producers = SymbolicMaskProducer.merge_producers(tensors)
         return SymbolicMask(ret_shape, producers)
 
     @classmethod
     def ones(cls, shape: Union[int, List[int]], device) -> SymbolicMask:
         if isinstance(shape, list):
             if len(shape) != 1:
-                raise RuntimeError(f'Unexpected shape = {shape} for 1D symbolic mask')
+                raise RuntimeError(f"Unexpected shape = {shape} for 1D symbolic mask")
             shape = shape[0]
 
         return SymbolicMask(shape)
 
     @classmethod
     def assert_allclose(cls, tensors: List[SymbolicMask]) -> None:
         for input_mask in tensors[1:]:
             assert tensors[0].shape == input_mask.shape
 
     @classmethod
     def repeat(cls, tensor: SymbolicMask, repeats: int) -> SymbolicMask:
         updated_mask_producers = []
         for mask_producer in tensor.mask_producers:
             updated_mask_producers.append(
-                SymbolicMaskProducer(mask_producer.id, mask_producer.sparse_multiplier * repeats))
+                SymbolicMaskProducer(mask_producer.id, mask_producer.sparse_multiplier * repeats)
+            )
         return SymbolicMask(tensor.shape[0] * repeats, updated_mask_producers)
 
     @classmethod
     def elementwise_mask_propagation(cls, input_masks: List[SymbolicMask]) -> SymbolicMask:
         """
         Assemble output mask for elementwise pruning operation from given input masks.
         In case input_masks have different shape don't propagate any masks.
@@ -143,12 +143,14 @@
                 return AmbiguousSymbolicMask(producers)
 
         return SymbolicMask(input_masks[0].shape[0], producers)
 
     @classmethod
     def split(cls, tensor: SymbolicMask, output_shapes: List[int]) -> List[SymbolicMask]:
         if any(shape <= 0 for shape in output_shapes) or tensor.shape[0] != sum(output_shapes):
-            raise AssertionError('Symbolic mask split was called with'\
-                f'invalid parammeters: input mask shape: {tensor.shape[0]}, output masks shapes: {output_shapes}')
+            raise AssertionError(
+                "Symbolic mask split was called with"
+                f"invalid parammeters: input mask shape: {tensor.shape[0]}, output masks shapes: {output_shapes}"
+            )
 
         producers = tensor.mask_producers
         return [SymbolicMask(output_shape, producers) for output_shape in output_shapes]
```

### Comparing `nncf-2.4.0/nncf/common/pruning/tensor_processor.py` & `nncf-2.5.0/nncf/common/pruning/tensor_processor.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,23 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import abstractmethod
 from typing import List, Union
 
-from nncf.common.tensor import NNCFTensor, DeviceType
+from nncf.common.tensor import DeviceType
+from nncf.common.tensor import NNCFTensor
 
 
 class NNCFPruningBaseTensorProcessor:
     """
     An interface of the processing methods for NNCFTensors.
     """
```

### Comparing `nncf-2.4.0/nncf/common/pruning/utils.py` & `nncf-2.5.0/nncf/common/pruning/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,49 +1,42 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import math
 from enum import Enum
 from functools import partial
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
-from typing import Union
+from typing import Dict, List, Optional, Tuple, Union
 
 import numpy as np
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
-from nncf.common.graph.layer_attributes import LinearLayerAttributes
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
+from nncf.common.graph.layer_attributes import LinearLayerAttributes
 from nncf.common.tensor import NNCFTensor
 from nncf.common.utils.registry import Registry
 
 
 def is_grouped_conv(node: NNCFNode) -> bool:
     """
     Returns `True` if a feeded node is a grouped convolution node.
 
     :param node: NNCFNode to check.
     :return: `True` if a feeded node a grouped convolution node.
     """
-    return isinstance(node.layer_attributes, ConvolutionLayerAttributes) \
-           and node.layer_attributes.groups != 1
+    return isinstance(node.layer_attributes, ConvolutionLayerAttributes) and node.layer_attributes.groups != 1
 
 
 def is_batched_linear(node: NNCFNode, graph: NNCFGraph) -> bool:
     """
     Returns `True` if a feeded linear node output tensor has no more than two dimensions.
     A linear layer has more than two output dimensions means, that this
     linear layer multiplies several input matrices feeded by batch dimensions
@@ -70,16 +63,15 @@
 
     :param sources_types: List of sources types.
     :param nncf_node: NNCFNode to get sources.
     :param graph: NNCF graph to work with.
     :return: List of all sources nodes.
     """
     visited = {node_id: False for node_id in graph.get_all_node_ids()}
-    partial_traverse_function = partial(traverse_function, type_check_fn=lambda x: x in sources_types,
-                                        visited=visited)
+    partial_traverse_function = partial(traverse_function, type_check_fn=lambda x: x in sources_types, visited=visited)
     nncf_nodes = [nncf_node]
     if nncf_node.node_type in sources_types:
         nncf_nodes = graph.get_previous_nodes(nncf_node)
 
     source_nodes = []
     for node in nncf_nodes:
         source_nodes.extend(graph.traverse_graph(node, partial_traverse_function, False))
@@ -96,16 +88,15 @@
 
     :param graph: Graph to work with.
     :param nncf_node: NNCFNode to start search.
     :param types: List of types.
     :return: List of next nodes for nncf_node of type not from types list.
     """
     visited = {node_id: False for node_id in graph.get_all_node_ids()}
-    partial_traverse_function = partial(traverse_function, type_check_fn=lambda x: x not in types,
-                                        visited=visited)
+    partial_traverse_function = partial(traverse_function, type_check_fn=lambda x: x not in types, visited=visited)
     nncf_nodes = [nncf_node]
     if nncf_node.node_type not in types:
         nncf_nodes = graph.get_next_nodes(nncf_node)
 
     next_nodes = []
     for node in nncf_nodes:
         next_nodes.extend(graph.traverse_graph(node, partial_traverse_function))
@@ -120,16 +111,15 @@
     :param graph: Graph to work with.
     :param nncf_node: NNCFNode to start search.
     :param types: List of types to find.
     :return: List of next nodes of nncf_node with type from types list.
     """
     sources_types = types
     visited = {node_id: False for node_id in graph.get_all_node_ids()}
-    partial_traverse_function = partial(traverse_function, type_check_fn=lambda x: x in sources_types,
-                                        visited=visited)
+    partial_traverse_function = partial(traverse_function, type_check_fn=lambda x: x in sources_types, visited=visited)
     nncf_nodes = [nncf_node]
     if nncf_node.node_type in sources_types:
         nncf_nodes = graph.get_next_nodes(nncf_node)
 
     next_nodes = []
     for node in nncf_nodes:
         next_nodes.extend(graph.traverse_graph(node, partial_traverse_function))
@@ -147,16 +137,15 @@
     :param multiple_of: Number of remaining elements must be a multiple of `multiple_of`.
     :return: Number of elements to be zeroed.
     """
     remaining_elems = math.ceil((total - total * sparsity_rate) / multiple_of) * multiple_of
     return max(total - remaining_elems, 0)
 
 
-def traverse_function(node: NNCFNode, output: List[NNCFNode], type_check_fn, visited) \
-        -> Tuple[bool, List[NNCFNode]]:
+def traverse_function(node: NNCFNode, output: List[NNCFNode], type_check_fn, visited) -> Tuple[bool, List[NNCFNode]]:
     if visited[node.node_id]:
         return True, output
     visited[node.node_id] = True
 
     if not type_check_fn(node.node_type):
         return False, output
 
@@ -173,26 +162,25 @@
     :param op_types: Types of modules to track.
     :param graph: Graph to work with.
     :return: List of all last pruned nodes.
     """
     graph_outputs = graph.get_output_nodes()  # NNCFNodes here
 
     visited = {node_id: False for node_id in graph.get_all_node_ids()}
-    partial_traverse_function = partial(traverse_function,
-                                        type_check_fn=lambda x: x in op_types,
-                                        visited=visited)
+    partial_traverse_function = partial(traverse_function, type_check_fn=lambda x: x in op_types, visited=visited)
     last_nodes_of_type = []
     for output in graph_outputs:
         last_nodes_of_type.extend(graph.traverse_graph(output, partial_traverse_function, False))
 
     return last_nodes_of_type
 
 
-def get_previous_convs(graph: NNCFGraph, nncf_node: NNCFNode,
-                       pruning_types: List[str], stop_propagation_ops: List[str]) -> List[NNCFNode]:
+def get_previous_convs(
+    graph: NNCFGraph, nncf_node: NNCFNode, pruning_types: List[str], stop_propagation_ops: List[str]
+) -> List[NNCFNode]:
     """
     Returns source convolutions of the node.
 
     :return: List of source convolutions of node.
     """
     sources = get_sources_of_node(nncf_node, graph, pruning_types + stop_propagation_ops)
     sources = [source for source in sources if source.node_type in pruning_types]
@@ -236,16 +224,17 @@
 
             super_register(obj, cls_name)
             op_names = obj.get_all_op_aliases()
             for name in op_names:
                 if name not in self._op_name_to_op_class:
                     self._op_name_to_op_class[name] = obj
                 else:
-                    assert self._op_name_to_op_class[name] == obj, \
-                        'Inconsistent operator type registry - single patched op name maps to multiple metatypes!'
+                    assert (
+                        self._op_name_to_op_class[name] == obj
+                    ), "Inconsistent operator type registry - single patched op name maps to multiple metatypes!"
             return obj
 
         return wrap
 
     def get_operator_metatype_by_op_name(self, op_name: str):
         if op_name in self._op_name_to_op_class:
             return self._op_name_to_op_class[op_name]
@@ -253,71 +242,74 @@
 
 
 class PruningAnalysisReason(Enum):
     """
     Enum of possible pruning analysis decisions reasons.
     """
 
-    IGNORED_SCOPE = 'node in ignored scope'
-    FIRST_CONV = 'this scope is one of the first convolutions'
-    LAST_CONV = 'this scope is convolution with output which directly affects model output dimensions'
-    GROUP_CONV = 'this scope is grouped convolution'
-    DOWNSAMPLE_CONV = 'this scope is convolution with downsample'
-    MODEL_ANALYSIS = 'of model analysis'
-    DIMENSION_MISMATCH = 'of dimension mismatch'
-    CLOSING_CONV_MISSING = 'closing convolution missing'
-    IN_GROUP_OF_UNPRUNABLE = 'is in the group with non prunable layers'
-    BATCHED_LINEAR = 'linear node has bathced dimension(s)'
-    INCOMPATIBLE_DIMS_IN_CLUSTER = 'channels in cluster nodes have different values'
+    IGNORED_SCOPE = "node in ignored scope"
+    FIRST_CONV = "this scope is one of the first convolutions"
+    LAST_CONV = "this scope is convolution with output which directly affects model output dimensions"
+    GROUP_CONV = "this scope is grouped convolution"
+    DOWNSAMPLE_CONV = "this scope is convolution with downsample"
+    MODEL_ANALYSIS = "of model analysis"
+    DIMENSION_MISMATCH = "of dimension mismatch"
+    CLOSING_CONV_MISSING = "closing convolution missing"
+    IN_GROUP_OF_UNPRUNABLE = "is in the group with non prunable layers"
+    BATCHED_LINEAR = "linear node has bathced dimension(s)"
+    INCOMPATIBLE_DIMS_IN_CLUSTER = "channels in cluster nodes have different values"
 
     @classmethod
-    def message(cls, node_name: str, decision: Optional['PruningAnalysisDecision']) -> str:
+    def message(cls, node_name: str, decision: Optional["PruningAnalysisDecision"]) -> str:
         """
         Returns the node pruning analysis decisions in a human-readable format.
 
         :param node_name: Name of given node.
         :param decision: Pruning analysis decision for given node.
         :return: Pruning analysis decision in a human-readable format.
         """
-        prefix = f'ignored adding Weight Pruner in: {node_name}'
+        prefix = f"ignored adding Weight Pruner in: {node_name}"
         reasons = decision.reasons
         if not reasons:
             return prefix
         # Filter messages
         if len(reasons) > 1 and cls.CLOSING_CONV_MISSING in reasons:
             reasons.remove(cls.CLOSING_CONV_MISSING)
         if len(reasons) == 1 and cls.IN_GROUP_OF_UNPRUNABLE in reasons:
-            return ''
-        return prefix + ' because ' + ' and '.join([reason.value for reason in reasons])
+            return ""
+        return prefix + " because " + " and ".join([reason.value for reason in reasons])
 
 
 class PruningAnalysisDecision:
     """
     Container for pruning analysis decisions. Contains decision which is boolean marker either
     node prunable or not (prunable if decision attribute is True) and
     pruning analysis reason in PruningAnalysisReason format. In case of positive
     decision (decision == True) possible reason will be ignored.
     """
 
-    def __init__(self,
-                 decision: bool,
-                 possible_reasons: Optional[Union[List[PruningAnalysisReason], PruningAnalysisReason]] = None):
+    def __init__(
+        self,
+        decision: bool,
+        possible_reasons: Optional[Union[List[PruningAnalysisReason], PruningAnalysisReason]] = None,
+    ):
         self.decision = decision
         if not isinstance(possible_reasons, list):
             possible_reasons = [possible_reasons]
-        self._reasons = possible_reasons if not decision and possible_reasons else None \
-            # type: Optional[List[PruningAnalysisReason]]
+        self._reasons = (
+            possible_reasons if not decision and possible_reasons else None
+        )  # type: Optional[List[PruningAnalysisReason]]
 
     def __repr__(self) -> str:
-        representation = f'Prunable: {self.decision}'
+        representation = f"Prunable: {self.decision}"
         if not self.decision:
-            representation += '; Reasons: ' + str(self._reasons)
+            representation += "; Reasons: " + str(self._reasons)
         return representation
 
-    def __eq__(self, other: 'PruningAnalysisDecision') -> bool:
+    def __eq__(self, other: "PruningAnalysisDecision") -> bool:
         eq = self.decision == other.decision
         if self._reasons is None:
             return eq and other._reasons is None
         if other._reasons is None:
             return False
         return eq and set(self._reasons) == set(other._reasons)
 
@@ -326,15 +318,15 @@
 
     @property
     def reasons(self) -> Optional[List[PruningAnalysisReason]]:
         if self._reasons:
             return self._reasons.copy()
         return None
 
-    def join(self, other: 'PruningAnalysisDecision') -> 'PruningAnalysisDecision':
+    def join(self, other: "PruningAnalysisDecision") -> "PruningAnalysisDecision":
         """
         Join two pruning analysis decisions about one NNCFNode.
 
         :param other: pruning analysis decision to join with.
         :return: Joint pruning analysis decision.
         """
         if self.decision and other.decision:
@@ -347,71 +339,72 @@
 
         return PruningAnalysisDecision(False, reasons)
 
 
 def is_prunable_depthwise_conv(node: NNCFNode) -> bool:
     # Only convolutions with in_channels == groups == out_channels are supported
     # by pruning algorithm. Depthwise convolutions support ticket: #68580
-    return isinstance(node.layer_attributes, ConvolutionLayerAttributes) \
-           and node.layer_attributes.groups == node.layer_attributes.in_channels \
-           and (node.layer_attributes.out_channels == node.layer_attributes.in_channels) \
-           and node.layer_attributes.in_channels > 1
+    return (
+        isinstance(node.layer_attributes, ConvolutionLayerAttributes)
+        and node.layer_attributes.groups == node.layer_attributes.in_channels
+        and (node.layer_attributes.out_channels == node.layer_attributes.in_channels)
+        and node.layer_attributes.in_channels > 1
+    )
 
 
 def is_conv_with_downsampling(node: NNCFNode) -> bool:
     layer_attrs = node.layer_attributes
     if isinstance(layer_attrs, ConvolutionLayerAttributes):
-        return not np.all(np.array(layer_attrs.stride) == 1) \
-               and not layer_attrs.transpose
+        return not np.all(np.array(layer_attrs.stride) == 1) and not layer_attrs.transpose
     return False
 
 
 def get_input_masks(node: NNCFNode, graph: NNCFGraph) -> List[Optional[NNCFTensor]]:
     """
     Returns input masks for all inputs of given NNCFNode.
 
     :param node: Given NNCFNode.
     :param graph: Graph to work with.
     :return: Input masks.
     """
     retval = []
-    input_masks = [input_node.data['output_mask'] for input_node in graph.get_previous_nodes(node)]
+    input_masks = [input_edge.from_node.data["output_mask"] for input_edge in graph.get_input_edges(node)]
     for input_mask in input_masks:
         retval.append(input_mask[node.node_name] if isinstance(input_mask, dict) else input_mask)
     return retval
 
 
 def get_input_channels(node: NNCFNode) -> int:
     """
     Returns count of input channels of an prunable node.
 
     :param node: Given prunable node.
     :return: Count of input channels of the given node.
     """
-    layer_attrs = node.layer_attributes # type: Union[ConvolutionLayerAttributes, LinearLayerAttributes]
+    layer_attrs = node.layer_attributes  # type: Union[ConvolutionLayerAttributes, LinearLayerAttributes]
     if isinstance(layer_attrs, ConvolutionLayerAttributes):
         return layer_attrs.in_channels
     if isinstance(layer_attrs, LinearLayerAttributes):
         return layer_attrs.in_features
-    raise RuntimeError(f'Can\'t get count of input channels from node {node}')
+    raise RuntimeError(f"Can't get count of input channels from node {node}")
 
 
 def get_output_channels(node: NNCFNode) -> int:
     """
     Returns count of output channels of an prunable node.
 
     :param node: Given prunable node.
     :return: Count of output channels of the given node.
     """
-    layer_attrs = node.layer_attributes # type: Union[ConvolutionLayerAttributes, LinearLayerAttributes]
+    layer_attrs = node.layer_attributes  # type: Union[ConvolutionLayerAttributes, LinearLayerAttributes]
     if isinstance(layer_attrs, ConvolutionLayerAttributes):
         return layer_attrs.out_channels
     if isinstance(layer_attrs, LinearLayerAttributes):
         return layer_attrs.out_features
-    raise RuntimeError(f'Can\'t get count of output channels from node {node}')
+    raise RuntimeError(f"Can't get count of output channels from node {node}")
 
 
 def identity_mask_propagation(node: NNCFNode, graph: NNCFGraph) -> None:
     """
     Propagates input mask through NNCFNode.
 
     :param node: Graph node to perform identity mask propagation on.
@@ -419,8 +412,8 @@
     """
     input_masks = get_input_masks(node, graph)
     if not input_masks:
         # In case for disconnected NNCFGraph
         input_masks = [None]
     assert len(input_masks) == 1
 
-    node.data['output_mask'] = input_masks[0]
+    node.data["output_mask"] = input_masks[0]
```

### Comparing `nncf-2.4.0/nncf/common/pruning/weights_flops_calculator.py` & `nncf-2.5.0/nncf/common/pruning/weights_flops_calculator.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,60 +1,55 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Dict
-from typing import List
-from typing import Tuple
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Dict, List, Tuple
 
 import numpy as np
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.operator_metatypes import OperatorMetatype
-from nncf.common.pruning.utils import is_prunable_depthwise_conv
 from nncf.common.pruning.utils import get_output_channels
+from nncf.common.pruning.utils import is_prunable_depthwise_conv
 
 
 class WeightsFlopsCalculator:
     """
     Collection of weight and flops calculation functions.
     Class instance keeps only parameters that are constant during
     compression algorithms execution.
     """
 
-    def __init__(self,
-                 conv_op_metatypes: List[OperatorMetatype],
-                 linear_op_metatypes: List[OperatorMetatype]):
+    def __init__(self, conv_op_metatypes: List[OperatorMetatype], linear_op_metatypes: List[OperatorMetatype]):
         """
         Constructor.
 
         :param conv_op_metatypes: List of metatypes defining convolution operations.
         :param linear_op_metatypes: List of metatypes defining linear/fully connected operations.
         """
         self._conv_op_metatypes = conv_op_metatypes
         self._linear_op_metatypes = linear_op_metatypes
 
-    def count_flops_and_weights(self,
-                                graph: NNCFGraph,
-                                output_shapes: Dict[NNCFNodeName, int],
-                                input_channels: Dict[NNCFNodeName, int] = None,
-                                output_channels: Dict[NNCFNodeName, int] = None,
-                                kernel_sizes: Dict[NNCFNodeName, Tuple[int, int]] = None,
-                                op_addresses_to_skip: List[str] = None
-                                ) -> Tuple[int, int]:
+    def count_flops_and_weights(
+        self,
+        graph: NNCFGraph,
+        output_shapes: Dict[NNCFNodeName, int],
+        input_channels: Dict[NNCFNodeName, int] = None,
+        output_channels: Dict[NNCFNodeName, int] = None,
+        kernel_sizes: Dict[NNCFNodeName, Tuple[int, int]] = None,
+        op_addresses_to_skip: List[str] = None,
+    ) -> Tuple[int, int]:
         """
         Counts the number of weights and FLOPs in the model for convolution and fully connected layers.
 
         :param graph: NNCFGraph.
         :param output_shapes: Dictionary of output dimension shapes for convolutions and
             fully connected layers. E.g {node_name: (height, width)}
         :param input_channels: Dictionary of input channels number in convolutions.
@@ -65,27 +60,28 @@
             If not specified, taken from the graph. {node_name: kernel_size}.
             It's only supposed to be used in NAS in case of Elastic Kernel enabled.
         :param op_addresses_to_skip: List of operation addresses of layers that should be skipped from calculation.
             It's only supposed to be used in NAS in case of Elastic Depth enabled.
         :return number of FLOPs for the model
                 number of weights (params) in the model
         """
-        flops_pers_node, weights_per_node = self.count_flops_and_weights_per_node(graph, output_shapes,
-                                                                                  input_channels, output_channels,
-                                                                                  kernel_sizes, op_addresses_to_skip)
+        flops_pers_node, weights_per_node = self.count_flops_and_weights_per_node(
+            graph, output_shapes, input_channels, output_channels, kernel_sizes, op_addresses_to_skip
+        )
         return sum(flops_pers_node.values()), sum(weights_per_node.values())
 
-    def count_flops_and_weights_per_node(self,
-                                         graph: NNCFGraph,
-                                         output_shapes: Dict[NNCFNodeName, int],
-                                         input_channels: Dict[NNCFNodeName, int] = None,
-                                         output_channels: Dict[NNCFNodeName, int] = None,
-                                         kernel_sizes: Dict[NNCFNodeName, Tuple[int, int]] = None,
-                                         op_addresses_to_skip: List[NNCFNodeName] = None) -> \
-        Tuple[Dict[NNCFNodeName, int], Dict[NNCFNodeName, int]]:
+    def count_flops_and_weights_per_node(
+        self,
+        graph: NNCFGraph,
+        output_shapes: Dict[NNCFNodeName, int],
+        input_channels: Dict[NNCFNodeName, int] = None,
+        output_channels: Dict[NNCFNodeName, int] = None,
+        kernel_sizes: Dict[NNCFNodeName, Tuple[int, int]] = None,
+        op_addresses_to_skip: List[NNCFNodeName] = None,
+    ) -> Tuple[Dict[NNCFNodeName, int], Dict[NNCFNodeName, int]]:
         """
         Counts the number of weights and FLOPs per node in the model for convolution and fully connected layers.
 
         :param graph: NNCFGraph.
         :param output_shapes: Dictionary of output dimension shapes for convolutions and
             fully connected layers. E.g {node_name: (height, width)}
         :param input_channels: Dictionary of input channels number in convolutions.
@@ -118,16 +114,17 @@
                 # because common way to calculate filters per
                 # channel for such layer leads to zero in case
                 # some of the output channels are pruned.
                 filters_per_channel = 1
             else:
                 filters_per_channel = num_out_channels // node.layer_attributes.groups
 
-            flops_numpy = 2 * np.prod(kernel_size) * num_in_channels * filters_per_channel *\
-                np.prod(output_shapes[name])
+            flops_numpy = (
+                2 * np.prod(kernel_size) * num_in_channels * filters_per_channel * np.prod(output_shapes[name])
+            )
             weights_numpy = np.prod(kernel_size) * num_in_channels * filters_per_channel
 
             flops[name] = flops_numpy.astype(int).item()
             weights[name] = weights_numpy.astype(int).item()
 
         for node in graph.get_nodes_by_metatypes(self._linear_op_metatypes):
             name = node.node_name
@@ -140,17 +137,15 @@
             flops_numpy = 2 * num_in_features * num_out_features * np.prod(output_shapes[name][:-1])
             weights_numpy = num_in_features * num_out_features
             flops[name] = flops_numpy
             weights[name] = weights_numpy
 
         return flops, weights
 
-    def count_filters_num(self,
-                          graph: NNCFGraph,
-                          output_channels: Dict[NNCFNodeName, int] = None) -> int:
+    def count_filters_num(self, graph: NNCFGraph, output_channels: Dict[NNCFNodeName, int] = None) -> int:
         """
         Counts filters of `op_metatypes` layers taking into account new output channels number.
 
         :param graph: NNCFGraph.
         :param output_channels:  A dictionary of output channels number in pruned model.
         :return: Current number of filters according to given graph and output channels.
         """
```

### Comparing `nncf-2.4.0/nncf/common/quantization/collectors.py` & `nncf-2.5.0/nncf/common/quantization/collectors.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,41 +1,41 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List, Tuple
-from collections import Counter
 from abc import abstractmethod
+from collections import Counter
+from typing import List, Tuple
 
 from nncf.common.collector import StatisticsCollector
-from nncf.common.quantization.statistics import QuantizersCounter
 from nncf.common.quantization.statistics import QuantizationStatistics
+from nncf.common.quantization.statistics import QuantizersCounter
 
 
 class QuantizerDescription:
     """
     Contains information about the quantizer.
     """
 
-    def __init__(self,
-                 num_bits: int,
-                 is_per_channel: bool,
-                 is_signed: bool,
-                 is_symmetric: bool,
-                 is_weight_quantizer: bool,
-                 is_enabled: bool):
+    def __init__(
+        self,
+        num_bits: int,
+        is_per_channel: bool,
+        is_signed: bool,
+        is_symmetric: bool,
+        is_weight_quantizer: bool,
+        is_enabled: bool,
+    ):
         """
         Initializes the description of the quantizer.
 
         :param num_bits: Bitwidth of the quantization.
         :param is_per_channel: `True` for per-channel quantization, `False` for per-tensor.
         :param is_signed: `True` for signed quantization, `False` for unsigned.
         :param is_symmetric: `True` for symmetric quantizer, `False` for asymmetric.
@@ -143,9 +143,10 @@
 
         num_wq_per_bitwidth = dict(Counter(wq_bitwidths))
         num_aq_per_bitwidth = dict(Counter(aq_bitwidths))
 
         total_count = wq_counter.total_count + aq_counter.total_count
         ratio_of_enabled_quantizations = 100 * (num_enabled_quantizers / max(total_count, 1))
 
-        return QuantizationStatistics(wq_counter, aq_counter, num_wq_per_bitwidth,
-                                      num_aq_per_bitwidth, ratio_of_enabled_quantizations)
+        return QuantizationStatistics(
+            wq_counter, aq_counter, num_wq_per_bitwidth, num_aq_per_bitwidth, ratio_of_enabled_quantizations
+        )
```

### Comparing `nncf-2.4.0/nncf/common/quantization/config_assignment.py` & `nncf-2.5.0/nncf/common/quantization/config_assignment.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,33 +1,30 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from copy import deepcopy
-from typing import Dict
-from typing import List
+from typing import Dict, List
 
 from nncf.common.graph import NNCFNode
 from nncf.common.hardware.config import HWConfig
 from nncf.common.quantization.structs import QuantizationConstraints
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.scopes import matches_any
 
 
-def get_scoped_quantizer_config(base_config: QuantizerConfig,
-                                scope_str: str,
-                                scope_overrides: Dict = None) -> QuantizerConfig:
+def get_scoped_quantizer_config(
+    base_config: QuantizerConfig, scope_str: str, scope_overrides: Dict = None
+) -> QuantizerConfig:
     """
     Returns a QuantizerConfig which is based on a given config, which will have overrides
     applied on top of it according to the dictionary of per-scope overrides.
 
     :param base_config: The base quantizer configuration - corresponding parameters will
       be used in the returned qconfig if no override for this parameter is given.
     :param scope_str: A string identifier of the spot in the model that will be associated with
@@ -49,20 +46,21 @@
             if config_overrides.get("per_channel") is not None:
                 qconfig.per_channel = config_overrides["per_channel"]
             if config_overrides.get("signed") is not None:
                 qconfig.signedness_to_force = config_overrides["signed"]
     return qconfig
 
 
-def assign_qconfig_lists_to_modules(nodes_with_weights: List[NNCFNode],
-                                    default_weight_qconfig: QuantizerConfig,
-                                    global_weight_constraints: QuantizationConstraints = None,
-                                    scope_overrides_dict: Dict = None,
-                                    hw_config: HWConfig = None) -> Dict[NNCFNode,
-                                                                        List[QuantizerConfig]]:
+def assign_qconfig_lists_to_modules(
+    nodes_with_weights: List[NNCFNode],
+    default_weight_qconfig: QuantizerConfig,
+    global_weight_constraints: QuantizationConstraints = None,
+    scope_overrides_dict: Dict = None,
+    hw_config: HWConfig = None,
+) -> Dict[NNCFNode, List[QuantizerConfig]]:
     """
     Assigns a list of possible quantizer configurations (as determined by HW config, defaults and overrides)
     to each weighted node that was passed.
 
     :param nodes_with_weights: The nodes in NNCFGraph that correspond to weighted operations.
     :param default_weight_qconfig: The default quantizer configuration for weights, to be used if
       no other information is given.
@@ -80,17 +78,17 @@
         default_qconfig = global_weight_constraints.apply_constraints_to(default_qconfig)
     if scope_overrides_dict is None:
         scope_overrides_dict = {}
     weight_scope_overrides_dict = scope_overrides_dict.get("weights")
     if hw_config is not None:
         meta_vs_qconfig_map = hw_config.get_metatype_vs_quantizer_configs_map(for_weights=True)
     for node in nodes_with_weights:
-        qconfig_for_current_scope = get_scoped_quantizer_config(default_qconfig,
-                                                                node.node_name,
-                                                                weight_scope_overrides_dict)
+        qconfig_for_current_scope = get_scoped_quantizer_config(
+            default_qconfig, node.node_name, weight_scope_overrides_dict
+        )
         if hw_config is None:
             qconfig_list = [qconfig_for_current_scope]
         else:
             metatype = node.metatype
             qconfig_list = meta_vs_qconfig_map[metatype]
             if HWConfig.is_wildcard_quantization(qconfig_list):  # Empty list = wildcard quantization
                 qconfig_list = [default_qconfig]
```

### Comparing `nncf-2.4.0/nncf/common/quantization/initialization/range.py` & `nncf-2.5.0/nncf/common/quantization/initialization/range.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,25 +1,23 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List, Dict, Optional
+from typing import Dict, List, Optional
 
 from nncf.common.initialization.dataloader import NNCFDataLoader
-from nncf.common.quantization.structs import QuantizerGroup
 from nncf.common.quantization.structs import QuantizationMode
+from nncf.common.quantization.structs import QuantizerGroup
 from nncf.config.schemata.defaults import NUM_INIT_SAMPLES
 
 
 class RangeInitConfig:
     """
     The `RangeInitConfig` class representing the quantization range initialization
     parameters.
@@ -42,89 +40,97 @@
         if self.init_type_specific_params is None:
             self.init_type_specific_params = {}
 
     def __eq__(self, other):
         return self.__dict__ == other.__dict__
 
     @classmethod
-    def from_dict(cls, dct: Dict) -> 'RangeInitConfig':
-        num_init_samples = dct.get('num_init_samples', NUM_INIT_SAMPLES)
+    def from_dict(cls, dct: Dict) -> "RangeInitConfig":
+        num_init_samples = dct.get("num_init_samples", NUM_INIT_SAMPLES)
         if num_init_samples < 0:
-            raise ValueError('Number of initialization samples must be >= 0')
-        return cls(dct.get('type', 'mixed_min_max'),
-                   num_init_samples,
-                   dct.get('params'))
+            raise ValueError("Number of initialization samples must be >= 0")
+        return cls(dct.get("type", "mixed_min_max"), num_init_samples, dct.get("params"))
 
 
 class PerLayerRangeInitConfig(RangeInitConfig):
     """
     The `PerLayerRangeInitConfig` class representing the quantization range
     initialization parameters for layers which are specified using the target
     and ignored scopes and the target group of quantizers.
     """
 
-    def __init__(self, range_init_config: RangeInitConfig,
-                 target_scopes: Optional[List[str]],
-                 ignored_scopes: Optional[List[str]],
-                 target_quantizer_group: QuantizerGroup = None):
+    def __init__(
+        self,
+        range_init_config: RangeInitConfig,
+        target_scopes: Optional[List[str]],
+        ignored_scopes: Optional[List[str]],
+        target_quantizer_group: QuantizerGroup = None,
+    ):
         """
         Initializes the quantization range initialization parameters.
 
         :param range_init_config: The quantization range initialization parameters.
         :param target_scopes: A list of model control flow graph node scopes
             to be considered for this operation - functions as a 'denylist'
         :param ignored_scopes: A list of model control flow graph node scopes
             to be ignored for this operation - functions as an 'allowlist'
         :param target_quantizer_group: The target group of quantizers for which
             specified type of range initialization will be applied. It can be
             quantizers group for activations or weights.
         """
 
-        super().__init__(range_init_config.init_type, range_init_config.num_init_samples,
-                         range_init_config.init_type_specific_params)
+        super().__init__(
+            range_init_config.init_type, range_init_config.num_init_samples, range_init_config.init_type_specific_params
+        )
         if target_scopes is None and ignored_scopes is None:
-            raise ValueError('At least one of the (target_scopes, ignored_scopes) should be specified'
-                             ' for a per-layer range init config!')
+            raise ValueError(
+                "At least one of the (target_scopes, ignored_scopes) should be specified"
+                " for a per-layer range init config!"
+            )
         self.target_scopes = target_scopes
         self.ignored_scopes = ignored_scopes
         self.target_group = target_quantizer_group
 
     @classmethod
-    def from_dict(cls, dct: Dict) -> 'PerLayerRangeInitConfig':
+    def from_dict(cls, dct: Dict) -> "PerLayerRangeInitConfig":
         base_config = RangeInitConfig.from_dict(dct)
 
         def get_list(dct: Dict, attr_name: str) -> Optional[List[str]]:
             str_or_list = dct.get(attr_name)
             if str_or_list is None:
                 return None
             if isinstance(str_or_list, str):
                 retval_list = [str_or_list]
             else:
                 retval_list = str_or_list
             return retval_list
-        target_scopes, ignored_scopes = get_list(dct, 'target_scopes'), get_list(dct, 'ignored_scopes')
 
-        target_group_str = dct.get('target_quantizer_group')
+        target_scopes, ignored_scopes = get_list(dct, "target_scopes"), get_list(dct, "ignored_scopes")
+
+        target_group_str = dct.get("target_quantizer_group")
         target_group = None
         if target_group_str is not None:
             target_group = QuantizerGroup(target_group_str)
 
         return cls(base_config, target_scopes, ignored_scopes, target_group)
 
 
 class RangeInitParams:
     """
     The `RangeInitParams` class representing the initialization dataset and the
     quantization range initialization parameters for all model layers.
     """
 
-    def __init__(self, init_range_data_loader: NNCFDataLoader,
-                 device: str,
-                 global_init_config: Optional[RangeInitConfig],
-                 per_layer_range_init_configs: List[PerLayerRangeInitConfig]):
+    def __init__(
+        self,
+        init_range_data_loader: NNCFDataLoader,
+        device: str,
+        global_init_config: Optional[RangeInitConfig],
+        per_layer_range_init_configs: List[PerLayerRangeInitConfig],
+    ):
         """
 
         :param init_range_data_loader: Provides an iterable over the given dataset.
         :param device: Device to perform initialization. If `device` is `None`
             then the device of the model parameters will be used.
         :param global_init_config: The quantization range initialization parameters
             for a model
@@ -167,12 +173,12 @@
     @property
     def use_abs_max(self) -> bool:
         """Applies abs(max) for symmetric quantization."""
         return self._mode == QuantizationMode.SYMMETRIC
 
     @property
     def use_means_of_mins(self) -> bool:
-        return not self._is_weights and not self._per_channel and self._mode == 'asymmetric'
+        return not self._is_weights and not self._per_channel and self._mode == "asymmetric"
 
     @property
     def use_means_of_maxs(self) -> bool:
         return not self._is_weights and not self._per_channel
```

### Comparing `nncf-2.4.0/nncf/common/quantization/quantizer_propagation/graph.py` & `nncf-2.5.0/nncf/common/quantization/quantizer_propagation/graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,32 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 # pylint:disable=too-many-lines
 from collections import deque
 from copy import copy
 from copy import deepcopy
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Set
-from typing import Tuple
-from typing import Type
-from typing import Union
+from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Type, Union
 
 import networkx as nx
 
 from nncf.common.graph import INPUT_NOOP_METATYPES
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph import OperatorMetatype
@@ -34,14 +24,15 @@
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.insertion_point_graph import InsertionPointGraph
 from nncf.common.insertion_point_graph import InsertionPointGraphNodeType
 from nncf.common.insertion_point_graph import PostHookInsertionPoint
 from nncf.common.insertion_point_graph import PreHookInsertionPoint
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.quantizer_propagation.grouping import UnifiedScalePropagatingQuantizerGroupManager
+from nncf.common.quantization.quantizer_propagation.structs import IgnoreReason
 from nncf.common.quantization.quantizer_propagation.structs import PropagatingQuantizer
 from nncf.common.quantization.quantizer_propagation.structs import PropagationPath
 from nncf.common.quantization.quantizer_propagation.structs import QuantizationTrait
 from nncf.common.quantization.quantizer_propagation.structs import QuantizerPropagationStateGraphNodeType
 from nncf.common.quantization.quantizer_propagation.structs import SharedAffectedOpsPropagatingQuantizerGroup
 from nncf.common.quantization.quantizer_setup import ActivationQuantizationInsertionPoint
 from nncf.common.quantization.quantizer_setup import MultiConfigQuantizationPoint
@@ -52,15 +43,15 @@
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import UnifiedScaleType
 from nncf.common.scopes import should_consider_scope
 
 
 class QuantizerPropagationStateGraph(nx.DiGraph):
-    #pylint:disable=too-many-public-methods
+    # pylint:disable=too-many-public-methods,too-many-return-statements
     """
     This class is based upon InsertionPointGraph and represents
     a"chessboard" for PropagatingQuantizer items.  It tracks the current state of
     quantizer propagation by associating the operator and insertion point nodes and
     edges to propagating quantizers, if any. It can move a propagating quantizer
     via own edges and mark its progress through the graph, which is required for
     resolving situations when multiple quantizers attempt to proceed via one and
@@ -76,48 +67,51 @@
     NODE_TYPE_NODE_ATTR = "node_type"
     IS_IN_IGNORED_SCOPES = "is_ignored"
     IS_MERGED_NODE_ATTR = "is_merged"
     MERGED_NNCF_NODE_LIST_NODE_ATTR = "merged_node_list"
     IS_INTEGER_PATH_EDGE_ATTR = "is_integer"
     BARRIER_NODE_KEY_POSTFIX = "BARRIER"
 
-    def __init__(self, ip_graph: InsertionPointGraph,
-                 ignored_scopes: List[str] = None,
-                 target_scopes: List[str] = None):
+    def __init__(
+        self, ip_graph: InsertionPointGraph, ignored_scopes: List[str] = None, target_scopes: List[str] = None
+    ):
         super().__init__()
         ip_graph = deepcopy(ip_graph)
         self._created_prop_quantizer_counter = 0
 
         self._ignored_scopes = deepcopy(ignored_scopes)
         self._target_scopes = deepcopy(target_scopes)
-        self.ignored_node_keys = []
+        self.ignored_node_keys = {}  # type: Dict[str, IgnoreReason]
 
         self._unified_scale_group_manager = UnifiedScalePropagatingQuantizerGroupManager()
         self._input_node_keys_vs_nncf_nodes = {}  # type: Dict[str, NNCFNode]
         self._pqs_after_weight_dependent_output_quantized_nodes = {}  # type: Dict[PropagatingQuantizer, str]
-        self.op_node_keys_to_underlying_nodes_mapping = {} # type: Dict[str, List[NNCFNode]]
+        self.op_node_keys_to_underlying_nodes_mapping = {}  # type: Dict[str, List[NNCFNode]]
 
         iteration_scope_node_keys = []
         for node_key, node in ip_graph.nodes.items():
             qpg_node = {
-                self.NODE_TYPE_NODE_ATTR: \
-                    self.ipg_node_type_to_qpsg_node_type(node[InsertionPointGraph.NODE_TYPE_NODE_ATTR])}
-            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] in [InsertionPointGraphNodeType.PRE_HOOK,
-                                                                 InsertionPointGraphNodeType.POST_HOOK]:
+                self.NODE_TYPE_NODE_ATTR: self.ipg_node_type_to_qpsg_node_type(
+                    node[InsertionPointGraph.NODE_TYPE_NODE_ATTR]
+                )
+            }
+            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] in [
+                InsertionPointGraphNodeType.PRE_HOOK,
+                InsertionPointGraphNodeType.POST_HOOK,
+            ]:
                 qpg_node[self.PROPAGATING_QUANTIZER_NODE_ATTR] = None
                 qpg_node[self.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] = []
 
                 ip = node[InsertionPointGraph.INSERTION_POINT_NODE_ATTR]
                 qip = self._insertion_point_to_quant_insertion_point(ip)
                 qpg_node[self.QUANT_INSERTION_POINT_DATA_NODE_ATTR] = qip
 
             elif node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:
                 qpg_node[self.ALLOWED_INPUT_QUANTIZATION_TYPES_NODE_ATTR] = set()
-                qpg_node[
-                    self.QUANTIZATION_TRAIT_NODE_ATTR] = QuantizationTrait.NON_QUANTIZABLE
+                qpg_node[self.QUANTIZATION_TRAIT_NODE_ATTR] = QuantizationTrait.NON_QUANTIZABLE
                 qpg_node[self.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] = []
                 qpg_node[self.IS_IN_IGNORED_SCOPES] = False
 
                 nncf_node_ref = node[InsertionPointGraph.REGULAR_NODE_REF_NODE_ATTR]  # type: NNCFNode
 
                 qpg_node[self.IS_MERGED_NODE_ATTR] = node[InsertionPointGraph.IS_MERGED_NODE_ATTR]
                 if node[InsertionPointGraph.IS_MERGED_NODE_ATTR]:
@@ -133,15 +127,15 @@
                 # underlying_nncf_nodes list.
                 primary_node = underlying_nncf_nodes[0]
                 if not should_consider_scope(primary_node.node_name, self._ignored_scopes, self._target_scopes):
                     ignored = True
 
                 if ignored:
                     qpg_node[self.IS_IN_IGNORED_SCOPES] = True
-                    self.ignored_node_keys.append(node_key)
+                    self.ignored_node_keys[node_key] = IgnoreReason.USER_REQUESTED
                     # TODO (vshampor): do we need here NoopMetatype
                     qpg_node[self.OPERATOR_METATYPE_NODE_ATTR] = NoopMetatype
                 else:
                     qpg_node[self.OPERATOR_METATYPE_NODE_ATTR] = nncf_node_ref.metatype
 
                 if nncf_node_ref.metatype in INPUT_NOOP_METATYPES:
                     self._input_node_keys_vs_nncf_nodes[node_key] = nncf_node_ref
@@ -153,15 +147,15 @@
 
         for from_node, to_node, edge_data in ip_graph.edges(data=True):
             edge_data[self.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] = []
             is_integer = edge_data.pop(InsertionPointGraph.IS_INTEGER_PATH_EDGE_ATTR)
             edge_data[self.IS_INTEGER_PATH_EDGE_ATTR] = is_integer
             self.add_edge(from_node, to_node, **edge_data)
 
-        for barred_node_key in self.ignored_node_keys + iteration_scope_node_keys:
+        for barred_node_key in list(self.ignored_node_keys.keys()) + iteration_scope_node_keys:
             self._add_barrier_after_node(barred_node_key)
 
     def get_node_keys_by_metatype(self, metatype: Type[OperatorMetatype]) -> List[str]:
         """
         Returns a list of node keys, whose metatype is corresponding to the 'metatype'.
 
         :param metatype: The metatype to look for.
@@ -169,80 +163,91 @@
         """
         output = []
         for node, node_metatype in self.nodes(self.OPERATOR_METATYPE_NODE_ATTR):
             if node_metatype == metatype:
                 output.append(node)
         return output
 
-    def _insertion_point_to_quant_insertion_point(self,
-                                                  ip: Union[PreHookInsertionPoint,
-                                                            PostHookInsertionPoint]) -> QuantizationInsertionPointBase:
+    def _insertion_point_to_quant_insertion_point(
+        self, ip: Union[PreHookInsertionPoint, PostHookInsertionPoint]
+    ) -> QuantizationInsertionPointBase:
         if isinstance(ip, PreHookInsertionPoint):
             return ActivationQuantizationInsertionPoint(ip.target_node_name, input_port_id=ip.input_port_id)
         assert isinstance(ip, PostHookInsertionPoint)
         return ActivationQuantizationInsertionPoint(ip.target_node_name, input_port_id=None)
 
     def _add_barrier_after_node(self, node_key: str):
         qpg_node_barrier = {
             self.NODE_TYPE_NODE_ATTR: QuantizerPropagationStateGraphNodeType.AUXILIARY_BARRIER,
-            'label': QuantizerPropagationStateGraph.BARRIER_NODE_KEY_POSTFIX}
+            "label": QuantizerPropagationStateGraph.BARRIER_NODE_KEY_POSTFIX,
+        }
         barrier_node_key = self.get_barrier_node_key(node_key)
         self.add_node(barrier_node_key, **qpg_node_barrier)
 
         next_node_keys = list(self.succ[node_key].keys())
         for next_node_key in next_node_keys:
             edge_attrs = self.edges[node_key, next_node_key]
             self.add_edge(node_key, barrier_node_key, **edge_attrs)
             self.add_edge(barrier_node_key, next_node_key, **edge_attrs)
             self.remove_edge(node_key, next_node_key)
 
     @staticmethod
-    def ipg_node_type_to_qpsg_node_type(ipg_node_type: InsertionPointGraphNodeType) \
-        -> QuantizerPropagationStateGraphNodeType:
+    def ipg_node_type_to_qpsg_node_type(
+        ipg_node_type: InsertionPointGraphNodeType,
+    ) -> QuantizerPropagationStateGraphNodeType:
         if ipg_node_type == InsertionPointGraphNodeType.PRE_HOOK:
             return QuantizerPropagationStateGraphNodeType.PRE_HOOK
         if ipg_node_type == InsertionPointGraphNodeType.POST_HOOK:
             return QuantizerPropagationStateGraphNodeType.POST_HOOK
         if ipg_node_type == InsertionPointGraphNodeType.OPERATOR:
             return QuantizerPropagationStateGraphNodeType.OPERATOR
         raise RuntimeError("Invalid insertion point graph node type.")
 
     @staticmethod
     def get_barrier_node_key(node_key: str) -> str:
         return f"{QuantizerPropagationStateGraph.BARRIER_NODE_KEY_POSTFIX} {node_key}"
 
-
     def mark_act_quantizer_as_dependent_on_weights(self, pq: PropagatingQuantizer, operator_node_key: str):
         """
         Marks a given propagating quantizer corresponding to input activation quantization
         of some downstream op as depenedent on weights of an operation that gives its weights directly
         as outputs (such as Embedding). The quantizer marked in this manner will be later considered
         for removal if the weights of the weight-as-outputs operation are quantized in a compatible
         way (i.e. with the same quantizer configuration) as is required by the propagating activation
         quantizer.
 
         :param: pq - the propagating quantizer corresponding to input quantization of some op
         :param: operator_node_key - a key of the node in QuantizerPropagationStateGraph that corresponds to
             a weights-as-outputs node.
         """
         op_node = self.nodes[operator_node_key]
-        assert op_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR] is \
-               QuantizerPropagationStateGraphNodeType.OPERATOR
-        assert op_node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR] is \
-               QuantizationTrait.OUTPUT_QUANTIZATION_AS_WEIGHTS
-        if pq in self._pqs_after_weight_dependent_output_quantized_nodes and \
-                self._pqs_after_weight_dependent_output_quantized_nodes[pq] != operator_node_key:
-            raise RuntimeError("Propagating quantizer {} is already marked as depending on node {} weight "
-                               "quantization!".format(pq.id, operator_node_key))
+        assert (
+            op_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
+            is QuantizerPropagationStateGraphNodeType.OPERATOR
+        )
+        assert (
+            op_node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]
+            is QuantizationTrait.OUTPUT_QUANTIZATION_AS_WEIGHTS
+        )
+        if (
+            pq in self._pqs_after_weight_dependent_output_quantized_nodes
+            and self._pqs_after_weight_dependent_output_quantized_nodes[pq] != operator_node_key
+        ):
+            raise RuntimeError(
+                "Propagating quantizer {} is already marked as depending on node {} weight "
+                "quantization!".format(pq.id, operator_node_key)
+            )
         self._pqs_after_weight_dependent_output_quantized_nodes[pq] = operator_node_key
 
     @staticmethod
     def is_insertion_point(qpsg_node_type: QuantizerPropagationStateGraphNodeType) -> bool:
-        return qpsg_node_type in [QuantizerPropagationStateGraphNodeType.PRE_HOOK,
-                                  QuantizerPropagationStateGraphNodeType.POST_HOOK]
+        return qpsg_node_type in [
+            QuantizerPropagationStateGraphNodeType.PRE_HOOK,
+            QuantizerPropagationStateGraphNodeType.POST_HOOK,
+        ]
 
     # pylint:disable=too-many-branches
     def merge_quantizer_into_path(self, prop_quantizer: PropagatingQuantizer, path: PropagationPath):
         curr_node = self.nodes[prop_quantizer.current_location_node_key]
         curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = None
         surviving_quantizers = []  # type: List[PropagatingQuantizer]
         for from_node_key, to_node_key in path:
@@ -273,39 +278,45 @@
                 pq.affected_ip_nodes.update(prop_quantizer.affected_ip_nodes)
                 pq.affected_edges.update(prop_quantizer.affected_edges)
                 if prop_quantizer in pq.downstream_propagating_quantizers:
                     pq.downstream_propagating_quantizers.remove(prop_quantizer)
                 for from_node_key, to_node_key in prop_quantizer.affected_edges:
                     to_node = self.nodes[to_node_key]
                     to_node_type = to_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
-                    if to_node_type in [QuantizerPropagationStateGraphNodeType.PRE_HOOK,
-                                        QuantizerPropagationStateGraphNodeType.POST_HOOK,
-                                        QuantizerPropagationStateGraphNodeType.OPERATOR]:
+                    if to_node_type in [
+                        QuantizerPropagationStateGraphNodeType.PRE_HOOK,
+                        QuantizerPropagationStateGraphNodeType.POST_HOOK,
+                        QuantizerPropagationStateGraphNodeType.OPERATOR,
+                    ]:
                         self.nodes[to_node_key][
-                            QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(pq)
+                            QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR
+                        ].append(pq)
 
             if prop_quantizer.unified_scale_type is not None:
                 gid = self._unified_scale_group_manager.get_group_id_by_propagating_quantizer_id(prop_quantizer.id)
                 for other_pq in surviving_quantizers:
                     if other_pq.unified_scale_type is not None:
                         other_gid = self._unified_scale_group_manager.get_group_id_by_propagating_quantizer_id(
-                            other_pq.id)
+                            other_pq.id
+                        )
                         self._unified_scale_group_manager.merge_groups(gid, other_gid)
                     else:
                         self._unified_scale_group_manager.add_to_group(gid, other_pq)
 
             for affected_edge_tuple in prop_quantizer.affected_edges:
                 edge = self.edges[affected_edge_tuple]
                 affecting_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
                 for pq in surviving_quantizers:
                     affecting_quantizers.append(pq)
             self.remove_propagating_quantizer(prop_quantizer)
         else:
-            raise RuntimeError("Surviving_quantizers not found !"
-                               " Nodes quantized with quantizer #{} will be lost".format(prop_quantizer.id))
+            raise RuntimeError(
+                "Surviving_quantizers not found !"
+                " Nodes quantized with quantizer #{} will be lost".format(prop_quantizer.id)
+            )
 
     @staticmethod
     def _get_major_unified_scale_type(type_list: List[Optional[UnifiedScaleType]]) -> Optional[UnifiedScaleType]:
         """
         Treats input list entries as unified scale types of merged quantizers, and outputs
         the unified scale type of the resulting merge-quantizer so that it is still compatible with the
         downstream ops.
@@ -314,18 +325,21 @@
         if UnifiedScaleType.UNIFY_ALWAYS in type_list:
             major_unified_scale_type = UnifiedScaleType.UNIFY_ALWAYS
         if UnifiedScaleType.UNIFY_ONLY_PER_TENSOR in type_list:
             major_unified_scale_type = UnifiedScaleType.UNIFY_ONLY_PER_TENSOR
         return major_unified_scale_type
 
     # pylint:disable=too-many-statements
-    def merge_quantizers_for_branching_node(self, quantizers_to_merge: List[PropagatingQuantizer],
-                                            merged_qconf_list: List[QuantizerConfig],
-                                            branch_qconf_lists: List[Optional[List[QuantizerConfig]]],
-                                            branching_node_key: str) -> List[PropagatingQuantizer]:
+    def merge_quantizers_for_branching_node(
+        self,
+        quantizers_to_merge: List[PropagatingQuantizer],
+        merged_qconf_list: List[QuantizerConfig],
+        branch_qconf_lists: List[Optional[List[QuantizerConfig]]],
+        branching_node_key: str,
+    ) -> List[PropagatingQuantizer]:
         # A branching node may currently be either a post-hook node, or an operator node if the
         # corresponding operator does not support post-hooking (such as torch.chunk)
         branching_node_type = self.nodes[branching_node_key][QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
 
         target_ip_node_keys = []
         if self.is_insertion_point(branching_node_type):
             target_ip_node_keys.append(branching_node_key)
@@ -346,34 +360,41 @@
             branch_qconf_list = branch_qconf_lists[idx]
             if branch_qconf_list is not None:
                 pq.potential_quant_configs = branch_qconf_list
 
         if merged_qconf_list is None:
             return []
 
-        unified_scale_types_of_merged_branches = [pq.unified_scale_type for idx, pq in enumerate(quantizers_to_merge)
-                                                  if branch_qconf_lists[idx] is None]
+        unified_scale_types_of_merged_branches = [
+            pq.unified_scale_type for idx, pq in enumerate(quantizers_to_merge) if branch_qconf_lists[idx] is None
+        ]
         merge_pq_unified_scale_type = self._get_major_unified_scale_type(unified_scale_types_of_merged_branches)
 
         merge_gid = None
         if merge_pq_unified_scale_type is not None:
             merge_gid = self._unified_scale_group_manager.register_group(set())
 
         merge_pqs = []
         for target_ip_node_key in target_ip_node_keys:
             target_ip_node = self.nodes[target_ip_node_key]
             target_type = target_ip_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
             if target_type is QuantizerPropagationStateGraphNodeType.PRE_HOOK:
-                merge_pq = self.add_propagating_quantizer(merged_qconf_list,
-                                                          target_ip_node_key,
-                                                          unified_scale_type=merge_pq_unified_scale_type,
-                                                          unified_scale_group_id_override=merge_gid)
+                merge_pq = self.add_propagating_quantizer(
+                    merged_qconf_list,
+                    target_ip_node_key,
+                    unified_scale_type=merge_pq_unified_scale_type,
+                    unified_scale_group_id_override=merge_gid,
+                )
             elif target_type is QuantizerPropagationStateGraphNodeType.POST_HOOK:
-                merge_pq = PropagatingQuantizer(self._get_next_prop_quantizer_id(), merged_qconf_list,
-                                                target_ip_node_key, unified_scale_type=merge_pq_unified_scale_type)
+                merge_pq = PropagatingQuantizer(
+                    self._get_next_prop_quantizer_id(),
+                    merged_qconf_list,
+                    target_ip_node_key,
+                    unified_scale_type=merge_pq_unified_scale_type,
+                )
                 merge_pq.last_accepting_location_node_key = target_ip_node_key
                 merge_pq.affected_ip_nodes.add(target_ip_node_key)
 
                 target_ip_node = self.nodes[target_ip_node_key]
                 assert target_ip_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] is None
                 target_ip_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = merge_pq
                 target_ip_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(merge_pq)
@@ -436,36 +457,40 @@
             pred_node_type = pred_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
             if pred_node_type is QuantizerPropagationStateGraphNodeType.OPERATOR:
                 pred_node_trait = pred_node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]
                 if pred_node_trait is QuantizationTrait.OUTPUT_QUANTIZATION_AS_WEIGHTS:
                     matches.append(pred_key)
         return matches
 
-    def backtrack_propagation_until_accepting_location(self, prop_quantizer: PropagatingQuantizer) -> \
-            Optional[PropagatingQuantizer]:
+    def backtrack_propagation_until_accepting_location(
+        self, prop_quantizer: PropagatingQuantizer
+    ) -> Optional[PropagatingQuantizer]:
         if prop_quantizer.last_accepting_location_node_key is None:
             # The quantizer was stillborn.
             # If there are quantizer-affected inbound edges, should transfer this quantizer's
             # affected edges and nodes to the inbound edge quantizers
             curr_node_key = prop_quantizer.current_location_node_key
             inbound_affecting_quantizers = set()
             for in_edge_key in self.in_edges(curr_node_key):
                 in_edge = self.edges[in_edge_key]
                 inbound_affecting_quantizers.update(
-                    in_edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR])
+                    in_edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
+                )
 
             for inbound_pq in inbound_affecting_quantizers:
                 inbound_pq.affected_edges.update(prop_quantizer.affected_edges)
                 inbound_pq.affected_ip_nodes.update(prop_quantizer.affected_ip_nodes)
             for edge in prop_quantizer.affected_edges:
                 self.edges[edge][QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] += list(
-                    inbound_affecting_quantizers)
+                    inbound_affecting_quantizers
+                )
             for ip_node_key in prop_quantizer.affected_ip_nodes:
                 self.nodes[ip_node_key][QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR] += list(
-                    inbound_affecting_quantizers)
+                    inbound_affecting_quantizers
+                )
 
             self.remove_propagating_quantizer(prop_quantizer)
             return None
 
         curr_node_key = prop_quantizer.current_location_node_key
         curr_node = self.nodes[curr_node_key]
         curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = None
@@ -487,45 +512,53 @@
                 prop_quantizer.current_location_node_key = to_node_key
 
         target_ip_node_key = prop_quantizer.current_location_node_key
         target_node = self.nodes[target_ip_node_key]
         target_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = prop_quantizer
         return prop_quantizer
 
-    def unify_pq_scales(self, primary_pq: PropagatingQuantizer, secondary_pq: PropagatingQuantizer,
-                        unified_scale_type: Optional[UnifiedScaleType] = None):
+    def unify_pq_scales(
+        self,
+        primary_pq: PropagatingQuantizer,
+        secondary_pq: PropagatingQuantizer,
+        unified_scale_type: Optional[UnifiedScaleType] = None,
+    ):
         if unified_scale_type is None:
             primary_pq.unified_scale_type = UnifiedScaleType.UNIFY_ALWAYS
         else:
             primary_pq.unified_scale_type = unified_scale_type
         secondary_pq.unified_scale_type = primary_pq.unified_scale_type
         primary_gid = self._unified_scale_group_manager.get_group_id_by_propagating_quantizer_id(primary_pq.id)
         if primary_gid is None:
             primary_gid = self._unified_scale_group_manager.register_group({primary_pq})
         self._unified_scale_group_manager.add_to_group(primary_gid, secondary_pq)
 
-    def add_propagating_quantizer(self, qconf_list: List[QuantizerConfig], ip_node_key: str,
-                                  unified_scale_type: Optional[UnifiedScaleType] = None,
-                                  unified_scale_group_id_override: Optional[int] = None) -> PropagatingQuantizer:
+    def add_propagating_quantizer(
+        self,
+        qconf_list: List[QuantizerConfig],
+        ip_node_key: str,
+        unified_scale_type: Optional[UnifiedScaleType] = None,
+        unified_scale_group_id_override: Optional[int] = None,
+    ) -> PropagatingQuantizer:
         ip_node = self.nodes[ip_node_key]
         ip_type = ip_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
         if ip_type != QuantizerPropagationStateGraphNodeType.PRE_HOOK:
             # The insertion point key should immediately precede a quantizable op,
             # otherwise it is hard to determine affected node here (although possible)
             raise RuntimeError("Can only add propagating quantizers into pre-hook spots!")
 
-        prop_quantizer = PropagatingQuantizer(self._get_next_prop_quantizer_id(), qconf_list, ip_node_key,
-                                              unified_scale_type)
+        prop_quantizer = PropagatingQuantizer(
+            self._get_next_prop_quantizer_id(), qconf_list, ip_node_key, unified_scale_type
+        )
 
         if unified_scale_type is not None:
             if unified_scale_group_id_override is None:
                 self._unified_scale_group_manager.register_group({prop_quantizer})
             else:
-                self._unified_scale_group_manager.add_to_group(unified_scale_group_id_override,
-                                                               prop_quantizer)
+                self._unified_scale_group_manager.add_to_group(unified_scale_group_id_override, prop_quantizer)
 
         ip_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = prop_quantizer
         ip_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(prop_quantizer)
 
         affected_op_node_key = next(self.successors(ip_node_key))
         affected_op_node = self.nodes[affected_op_node_key]
         affected_op_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(prop_quantizer)
@@ -536,59 +569,72 @@
         prop_quantizer.affected_edges.add(initial_edge_key)
         prop_quantizer.affected_ip_nodes.add(ip_node_key)
         prop_quantizer.affected_operator_nodes.add(affected_op_node_key)
         prop_quantizer.quantized_input_sink_operator_nodes.add(affected_op_node_key)
         return prop_quantizer
 
     def _verify_nodes_and_edges_for_pq(self, prop_quantizer: PropagatingQuantizer):
-        node_keys_to_verify = list(prop_quantizer.affected_operator_nodes) + \
-                              list(prop_quantizer.quantized_input_sink_operator_nodes) + \
-                              [prop_quantizer.current_location_node_key] + \
-                              list(prop_quantizer.affected_ip_nodes)
+        node_keys_to_verify = (
+            list(prop_quantizer.affected_operator_nodes)
+            + list(prop_quantizer.quantized_input_sink_operator_nodes)
+            + [prop_quantizer.current_location_node_key]
+            + list(prop_quantizer.affected_ip_nodes)
+        )
         if prop_quantizer.last_accepting_location_node_key is not None:
             node_keys_to_verify.append(prop_quantizer.last_accepting_location_node_key)
 
         for node_key in node_keys_to_verify:
             if node_key not in self.nodes:
-                raise RuntimeError("Unknown node referenced by propagating quantizer to be registered: {}".format(
-                    node_key
-                ))
+                raise RuntimeError(
+                    "Unknown node referenced by propagating quantizer to be registered: {}".format(node_key)
+                )
         edge_keys_to_verify = list(prop_quantizer.affected_edges) + list(prop_quantizer.propagation_path)
         for edge_key in edge_keys_to_verify:
             if edge_key not in self.edges:
-                raise RuntimeError("Unknown edge referenced by propagating quantizer to be registered: {}".format(
-                    edge_key
-                ))
+                raise RuntimeError(
+                    "Unknown edge referenced by propagating quantizer to be registered: {}".format(edge_key)
+                )
 
     @staticmethod
-    def _verify_qconfig_matching(prop_quantizer: PropagatingQuantizer,
-                                 existing_prop_quantizers: List[PropagatingQuantizer]):
+    def _verify_qconfig_matching(
+        prop_quantizer: PropagatingQuantizer, existing_prop_quantizers: List[PropagatingQuantizer]
+    ):
         for existing_pq in existing_prop_quantizers:  # type: PropagatingQuantizer
             if existing_pq.potential_quant_configs != prop_quantizer.potential_quant_configs:
-                raise RuntimeError("Configurations of the quantizer to be registered are conflicting with "
-                                   "existing quantizer {}".format(existing_pq.id))
+                raise RuntimeError(
+                    "Configurations of the quantizer to be registered are conflicting with "
+                    "existing quantizer {}".format(existing_pq.id)
+                )
 
     def register_propagating_quantizer(self, prop_quantizer: PropagatingQuantizer):
         """Will only succeed if the new quantizer information is consistent with the rest of the graph state."""
         all_pqs = self.collect_all_propagating_quantizers()
         for existing_pq_id in all_pqs:
             if prop_quantizer.id == existing_pq_id:
-                raise RuntimeError("The propagating quantizer to be registered has an ID that is already assigned to "
-                                   "an existing propagating quantizer!")
+                raise RuntimeError(
+                    "The propagating quantizer to be registered has an ID that is already assigned to "
+                    "an existing propagating quantizer!"
+                )
         target_node = self.nodes[prop_quantizer.current_location_node_key]
         pq_in_target_node = target_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]
         if pq_in_target_node is not None:
-            raise RuntimeError("The propagating quantizer to be registered is occupying the same position "
-                               "as an existing propagating quantizer {}!".format(pq_in_target_node.id))
+            raise RuntimeError(
+                "The propagating quantizer to be registered is occupying the same position "
+                "as an existing propagating quantizer {}!".format(pq_in_target_node.id)
+            )
         target_node_affecting_quantizers = target_node[
-            QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
+            QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR
+        ]
         if target_node_affecting_quantizers:
-            raise RuntimeError("Cannot register a propagating quantizer into a node that is already "
-                               "affected by existing propagating quantizers (ids: {})!".format(
-                                   [pq.id for pq in target_node_affecting_quantizers]))
+            raise RuntimeError(
+                "Cannot register a propagating quantizer into a node that is already "
+                "affected by existing propagating quantizers (ids: {})!".format(
+                    [pq.id for pq in target_node_affecting_quantizers]
+                )
+            )
 
         self._verify_nodes_and_edges_for_pq(prop_quantizer)
 
         for node_key in prop_quantizer.affected_operator_nodes:
             node = self.nodes[node_key]
             node_pqs = node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
             self._verify_qconfig_matching(prop_quantizer, node_pqs)
@@ -623,16 +669,17 @@
 
         if cloned_prop_quant.unified_scale_type is not None:
             gid = self._unified_scale_group_manager.get_group_id_by_propagating_quantizer_id(prop_quantizer.id)
             self._unified_scale_group_manager.add_to_group(gid, cloned_prop_quant)
 
         return cloned_prop_quant
 
-    def remove_propagating_quantizer(self, prop_quantizer: PropagatingQuantizer,
-                                     keep_propagating_quantizer_at_current_node=False):
+    def remove_propagating_quantizer(
+        self, prop_quantizer: PropagatingQuantizer, keep_propagating_quantizer_at_current_node=False
+    ):
         for edge_tuple in prop_quantizer.affected_edges:
             edge = self.edges[edge_tuple]
             affecting_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
             affecting_quantizers.remove(prop_quantizer)
         for node_key in prop_quantizer.affected_ip_nodes:
             node = self.nodes[node_key]
             affecting_quantizers = node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
@@ -650,16 +697,17 @@
             self.nodes[node_key][QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = None
         prop_quantizer.affected_ip_nodes.clear()
         prop_quantizer.affected_edges.clear()
         if prop_quantizer.unified_scale_type is not None:
             gid = self._unified_scale_group_manager.get_group_id_by_propagating_quantizer_id(prop_quantizer.id)
             self._unified_scale_group_manager.remove_from_group(gid, prop_quantizer)
 
-    def propagate_quantizer_via_path(self, prop_quantizer: PropagatingQuantizer,
-                                     path: PropagationPath) -> PropagatingQuantizer:
+    def propagate_quantizer_via_path(
+        self, prop_quantizer: PropagatingQuantizer, path: PropagationPath
+    ) -> PropagatingQuantizer:
         curr_node_key = prop_quantizer.current_location_node_key
         curr_node = self.nodes[curr_node_key]
         existing_quantizer = curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]
         if existing_quantizer is not None and existing_quantizer.id == prop_quantizer.id:
             curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] = None
         for edge_tuple in path:
             edge = self.edges[edge_tuple]
@@ -698,25 +746,25 @@
                         target_node_list.append(successor_key)
                         return
                 recursive_helper(successor_key, target_node_list)
 
         recursive_helper(node_key, ret_node_key_list)
         return ret_node_key_list
 
-    def get_paths_to_immediately_dominating_insertion_points(self, insertion_point_node_key: str) -> \
-            List[PropagationPath]:
+    def get_paths_to_immediately_dominating_insertion_points(
+        self, insertion_point_node_key: str
+    ) -> List[PropagationPath]:
         group_dict = self.get_paths_to_immediately_dominating_insertion_points_grouped_by_unified_scales(
-            insertion_point_node_key,
-            set())
+            insertion_point_node_key, set()
+        )
         return group_dict[None]
 
     def get_paths_to_immediately_dominating_insertion_points_grouped_by_unified_scales(
-            self,
-            insertion_point_node_key: str,
-            unified_scale_op_metatypes: Set[Type[OperatorMetatype]]) -> Dict[Optional[int], List[PropagationPath]]:
+        self, insertion_point_node_key: str, unified_scale_op_metatypes: Set[Type[OperatorMetatype]]
+    ) -> Dict[Optional[int], List[PropagationPath]]:
         """Paths are lists of edges."""
         next_group_idx = 0
         paths = {}
 
         def recursive_helper(curr_edge, curr_path, all_paths, curr_group):
             nonlocal next_group_idx
             curr_path.append(curr_edge)
@@ -728,37 +776,43 @@
                     all_paths[curr_group].append(curr_path)
                 else:
                     all_paths[curr_group] = [curr_path]
                 return
 
             if curr_node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:
                 metatype = curr_node[QuantizerPropagationStateGraph.OPERATOR_METATYPE_NODE_ATTR]
-                if metatype in unified_scale_op_metatypes and curr_group is None and \
-                        len(self.in_edges(curr_node_key)) > 1:
+                if (
+                    metatype in unified_scale_op_metatypes
+                    and curr_group is None
+                    and len(self.in_edges(curr_node_key)) > 1
+                ):
                     curr_group = next_group_idx
                     next_group_idx += 1
 
             for in_edge in self.in_edges(curr_node_key):
                 path_copy = deepcopy(curr_path)
                 recursive_helper(in_edge, path_copy, all_paths, curr_group)
 
         for in_edge in self.in_edges(insertion_point_node_key):
-            if self.nodes[in_edge[0]][QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR] == \
-                    QuantizerPropagationStateGraphNodeType.AUXILIARY_BARRIER:
+            if (
+                self.nodes[in_edge[0]][QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
+                == QuantizerPropagationStateGraphNodeType.AUXILIARY_BARRIER
+            ):
                 continue
             recursive_helper(in_edge, [], paths, curr_group=None)
         if not paths:
             paths[None] = []
         return paths
 
     def get_propagating_quantizers_immediately_dominated_by_node(self, node_key: str) -> Set[PropagatingQuantizer]:
         retval = set()  # type: Set[PropagatingQuantizer]
 
-        def traverse_fn(curr_node_key: str, all_pqs: Set[PropagatingQuantizer]) -> \
-                Tuple[bool, Set[PropagatingQuantizer]]:
+        def traverse_fn(
+            curr_node_key: str, all_pqs: Set[PropagatingQuantizer]
+        ) -> Tuple[bool, Set[PropagatingQuantizer]]:
             curr_node = self.nodes[curr_node_key]
             curr_node_type = curr_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
             if self.is_insertion_point(curr_node_type):
                 pq = curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]
                 if pq is not None:
                     all_pqs.add(pq)
                     return True, all_pqs
@@ -770,47 +824,52 @@
     def get_visualized_graph(self):
         out_graph = nx.DiGraph()
         unified_scale_group_vs_pq_node_id_dict = {}  # type: Dict[int, List[str]]
         for node_key, node in self.nodes.items():
             node_type = node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
             if self.is_insertion_point(node_type):
                 insertion_point_data = node[
-                    QuantizerPropagationStateGraph.QUANT_INSERTION_POINT_DATA_NODE_ATTR]  # type: TargetPoint
+                    QuantizerPropagationStateGraph.QUANT_INSERTION_POINT_DATA_NODE_ATTR
+                ]  # type: TargetPoint
                 label = "TP: {}".format(str(insertion_point_data))
                 out_graph.add_node(node_key, label=label, color="red")
                 if node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR] is not None:
                     prop_quantizer = node[
-                        QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]  # type: PropagatingQuantizer
+                        QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR
+                    ]  # type: PropagatingQuantizer
                     quant_node_key = "Quantizer #{}".format(prop_quantizer.id)
                     if prop_quantizer.potential_quant_configs:
                         quant_configs_str_list = [str(conf) for conf in prop_quantizer.potential_quant_configs]
                     else:
                         quant_configs_str_list = ["!!! NONE !!!]"]
-                    sub_label = '[' + ',\n'.join(quant_configs_str_list) + ']'
-                    quant_node_label = quant_node_key + '\n' + "T: {}\n".format(sub_label)
-                    quant_node_label += 'Q-input sink ops: {}'.format(
-                        "\n".join(prop_quantizer.quantized_input_sink_operator_nodes))
-                    pq_color = "blue" if prop_quantizer not in self._pqs_after_weight_dependent_output_quantized_nodes \
+                    sub_label = "[" + ",\n".join(quant_configs_str_list) + "]"
+                    quant_node_label = quant_node_key + "\n" + "T: {}\n".format(sub_label)
+                    quant_node_label += "Q-input sink ops: {}".format(
+                        "\n".join(prop_quantizer.quantized_input_sink_operator_nodes)
+                    )
+                    pq_color = (
+                        "blue"
+                        if prop_quantizer not in self._pqs_after_weight_dependent_output_quantized_nodes
                         else "yellow"
-                    out_graph.add_node(quant_node_key,
-                                       color=pq_color, label=quant_node_label)
-                    out_graph.add_edge(quant_node_key, node_key,
-                                       style="dashed")
+                    )
+                    out_graph.add_node(quant_node_key, color=pq_color, label=quant_node_label)
+                    out_graph.add_edge(quant_node_key, node_key, style="dashed")
                     if prop_quantizer.unified_scale_type is not None:
                         gid = self._unified_scale_group_manager.get_group_id_by_propagating_quantizer_id(
-                            prop_quantizer.id)
+                            prop_quantizer.id
+                        )
                         if gid in unified_scale_group_vs_pq_node_id_dict:
                             unified_scale_group_vs_pq_node_id_dict[gid].append(quant_node_key)
                         else:
                             unified_scale_group_vs_pq_node_id_dict[gid] = [quant_node_key]
 
             elif node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:
                 out_graph.add_node(node_key)
             elif node_type == QuantizerPropagationStateGraphNodeType.AUXILIARY_BARRIER:
-                out_graph.add_node(node_key, color='green', label=node['label'])
+                out_graph.add_node(node_key, color="green", label=node["label"])
             else:
                 raise RuntimeError("Invalid QuantizerPropagationStateGraph node!")
         for u, v in self.edges:
             edge = self.edges[u, v]
             attrs = {}
             affecting_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
             if affecting_quantizers:
@@ -831,25 +890,32 @@
             while not done:
                 curr_pq_node_key = next(curr_elt_iter)
                 try:
                     next_pq_node_key = next(next_elt_iter)
                 except StopIteration:
                     done = True
                     next_pq_node_key = group_pq_node_keys[0]  # back to the first elt
-                out_graph.add_edge(curr_pq_node_key, next_pq_node_key, arrowhead="none",
-                                   style="dotted",
-                                   label="Unified group {}".format(gid))
+                out_graph.add_edge(
+                    curr_pq_node_key,
+                    next_pq_node_key,
+                    arrowhead="none",
+                    style="dotted",
+                    label="Unified group {}".format(gid),
+                )
 
         return out_graph
 
-    def traverse_graph(self, curr_node_key: str,
-                       traverse_function: Callable[[str, Any], Tuple[bool, Any]],
-                       output: Any,
-                       traverse_forward: bool = True,
-                       dfs: bool = True) -> Any:
+    def traverse_graph(
+        self,
+        curr_node_key: str,
+        traverse_function: Callable[[str, Any], Tuple[bool, Any]],
+        output: Any,
+        traverse_forward: bool = True,
+        dfs: bool = True,
+    ) -> Any:
         visited_node_keys = set()  # type: Set[str]
         node_keys_to_visit = deque()  # type: Deque[Tuple[str, Any]]
         next_node_keys_indexer = self.succ if traverse_forward else self.pred
         # Storing the node-specific operation output is required so that this function
         # interface could generalize to situations where 'output' is not a global storage
         # for some sort of data to be gathered from the graph as a whole, but is a traversal history-
         # aware node-specific output, such as which quantizer affects the current node.
@@ -865,26 +931,32 @@
             if not is_finished:
                 for next_node_key in next_node_keys_indexer[node_key]:
                     if next_node_key not in visited_node_keys:
                         node_keys_to_visit.appendleft((next_node_key, new_output))
 
         return output
 
-    def _traverse_graph_recursive_helper(self, curr_node_key: str, visited_node_keys: Set[str],
-                                         traverse_function: Callable[[str, Any], Tuple[bool, Any]],
-                                         output: Any, traverse_forward: bool):
+    def _traverse_graph_recursive_helper(
+        self,
+        curr_node_key: str,
+        visited_node_keys: Set[str],
+        traverse_function: Callable[[str, Any], Tuple[bool, Any]],
+        output: Any,
+        traverse_forward: bool,
+    ):
         """This is DFS, and may fail with 'maximum recursion depth exceeded' for complex graphs."""
         is_finished, output = traverse_function(curr_node_key, output)
         visited_node_keys.add(curr_node_key)
         next_node_keys_indexer = self.succ if traverse_forward else self.pred
         if not is_finished:
             for node_key in next_node_keys_indexer[curr_node_key]:
                 if node_key not in visited_node_keys:
-                    self._traverse_graph_recursive_helper(node_key, visited_node_keys,
-                                                          traverse_function, output, traverse_forward)
+                    self._traverse_graph_recursive_helper(
+                        node_key, visited_node_keys, traverse_function, output, traverse_forward
+                    )
         return output
 
     def _get_next_prop_quantizer_id(self):
         self._created_prop_quantizer_counter += 1
         return self._created_prop_quantizer_counter
 
     def _is_position_accepting(self, ip_node_key: str):
@@ -919,43 +991,55 @@
             current_input_quantizer_ids = []
             recursive_helper(input_node_key, current_input_quantizer_ids)
             retval[input_nncf_node] = current_input_quantizer_ids
 
         return retval
 
     def merge_redundant_subsequent_quantizers_across_graph(self):
-        def is_downstream_quantizer_redundant(downstream_quantizer: PropagatingQuantizer,
-                                              upstream_quantizer: PropagatingQuantizer):
+        def is_downstream_quantizer_redundant(
+            downstream_quantizer: PropagatingQuantizer, upstream_quantizer: PropagatingQuantizer
+        ):
             ds_configs = downstream_quantizer.potential_quant_configs
             us_configs = upstream_quantizer.potential_quant_configs
             assert len(ds_configs) == 1
             assert len(us_configs) == 1
             ds_config = ds_configs[0]
             us_config = us_configs[0]
             is_redundant = True
             is_redundant = is_redundant and (ds_config.num_bits == us_config.num_bits)
 
             # Avoid asymmetric quantization if a symmetrically quantized tensor arrived
-            is_redundant = is_redundant and ((ds_config.mode == us_config.mode) or (
-                ds_config.mode == QuantizationMode.ASYMMETRIC and us_config.mode == QuantizationMode.SYMMETRIC))
+            is_redundant = is_redundant and (
+                (ds_config.mode == us_config.mode)
+                or (ds_config.mode == QuantizationMode.ASYMMETRIC and us_config.mode == QuantizationMode.SYMMETRIC)
+            )
 
             # Avoid per-channel quantization if a per-tensor-quantized tensor arrived
-            is_redundant = is_redundant and ((ds_config.per_channel == us_config.per_channel) or (
-                ds_config.per_channel is True and us_config.per_channel is False))
+            is_redundant = is_redundant and (
+                (ds_config.per_channel == us_config.per_channel)
+                or (ds_config.per_channel is True and us_config.per_channel is False)
+            )
             return is_redundant
 
-        def merge_traverse_fn(curr_node_key: str,
-                              affecting_pq_and_prev_node_key: Tuple[Optional[PropagatingQuantizer],
-                                                                    str]) -> Tuple[Optional[PropagatingQuantizer], str]:
+        def merge_traverse_fn(
+            curr_node_key: str, affecting_pq_and_prev_node_key: Tuple[Optional[PropagatingQuantizer], str]
+        ) -> Tuple[Optional[PropagatingQuantizer], str]:
             # For this to work, DFS must be used for graph traversal. Also, this only
             # works with the generic traverse_graph interface because of
             # Python's pass-by-value mechanism for tuples.
             affecting_pq, prev_node_key = affecting_pq_and_prev_node_key
             curr_node = self.nodes[curr_node_key]
             curr_node_type = curr_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
+
+            # Skipping traversing through the INTEGER path.
+            if curr_node_key != prev_node_key:
+                edge = self.edges[prev_node_key, curr_node_key]
+                if edge[QuantizerPropagationStateGraph.IS_INTEGER_PATH_EDGE_ATTR]:
+                    return False, (None, curr_node_key)
+
             if self.is_insertion_point(curr_node_type):
                 curr_pq = curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]
                 if curr_pq is not None:
                     if affecting_pq is None:
                         return False, (curr_pq, curr_node_key)
 
                     if is_downstream_quantizer_redundant(curr_pq, affecting_pq):
@@ -978,16 +1062,17 @@
 
         for graph_root_key in graph_roots:
             self.traverse_graph(graph_root_key, merge_traverse_fn, (None, graph_root_key))
 
     def collect_all_propagating_quantizers(self) -> Set[PropagatingQuantizer]:
         retval = set()  # type: Set[PropagatingQuantizer]
 
-        def traverse_fn(curr_node_key: str, all_pqs: Set[PropagatingQuantizer]) -> Tuple[
-                bool, Set[PropagatingQuantizer]]:
+        def traverse_fn(
+            curr_node_key: str, all_pqs: Set[PropagatingQuantizer]
+        ) -> Tuple[bool, Set[PropagatingQuantizer]]:
             curr_node = self.nodes[curr_node_key]
             curr_node_type = curr_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
             if self.is_insertion_point(curr_node_type):
                 pq = curr_node[QuantizerPropagationStateGraph.PROPAGATING_QUANTIZER_NODE_ATTR]
                 if pq is not None:
                     all_pqs.add(pq)
                     return False, all_pqs
@@ -999,20 +1084,20 @@
                 graph_roots.append(node_key)
 
         for graph_root_key in graph_roots:
             self.traverse_graph(graph_root_key, traverse_fn, retval)
 
         return retval
 
-    def get_quant_insertion_point_for_propagating_quantizer(self, prop_quant: PropagatingQuantizer) \
-            -> QuantizationInsertionPointBase:
+    def get_quant_insertion_point_for_propagating_quantizer(
+        self, prop_quant: PropagatingQuantizer
+    ) -> QuantizationInsertionPointBase:
         final_node_key = prop_quant.current_location_node_key
         final_node = self.nodes[final_node_key]
-        insertion_point = final_node[
-            QuantizerPropagationStateGraph.QUANT_INSERTION_POINT_DATA_NODE_ATTR]
+        insertion_point = final_node[QuantizerPropagationStateGraph.QUANT_INSERTION_POINT_DATA_NODE_ATTR]
         return insertion_point
 
     def _get_all_quantizers_grouped_by_affecting_op_set(self) -> List[SharedAffectedOpsPropagatingQuantizerGroup]:
         all_pqs = self.collect_all_propagating_quantizers()
 
         class Grouper:
             """
@@ -1033,16 +1118,17 @@
 
             def _merge_groups(self, gid_to: int, gid_from: int):
                 self._group_vs_node_keys_and_pqs[gid_to].update(self._group_vs_node_keys_and_pqs[gid_from])
                 self._group_vs_node_keys_and_pqs.pop(gid_from)
 
             def add_pq(self, pq: PropagatingQuantizer):
                 new_gid = self._get_next_gid()
-                self._group_vs_node_keys_and_pqs[new_gid] = \
-                    SharedAffectedOpsPropagatingQuantizerGroup({pq}, set(pq.quantized_input_sink_operator_nodes))
+                self._group_vs_node_keys_and_pqs[new_gid] = SharedAffectedOpsPropagatingQuantizerGroup(
+                    {pq}, set(pq.quantized_input_sink_operator_nodes)
+                )
                 new_group_data = self._group_vs_node_keys_and_pqs[new_gid]
                 gids_to_merge = set()  # type: Set[int]
                 for gid, group_data in self._group_vs_node_keys_and_pqs.items():
                     if gid == new_gid:
                         continue
                     for node_key in new_group_data.affected_op_node_keys:
                         if node_key in group_data.affected_op_node_keys:
@@ -1058,99 +1144,113 @@
         for pq in all_pqs:
             grouper.add_pq(pq)
 
         groups = grouper.get_groups()
         return list(groups.values())
 
     def get_num_input_activations(self, operator_node_key: str) -> int:
-        assert self.nodes[operator_node_key][QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR] == \
-               QuantizerPropagationStateGraphNodeType.OPERATOR
+        assert (
+            self.nodes[operator_node_key][QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
+            == QuantizerPropagationStateGraphNodeType.OPERATOR
+        )
         return len(list(self.predecessors(operator_node_key)))
 
-    def create_quantizer_setup(self, weight_quantizable_node_names_vs_configs: Dict[NNCFNodeName,
-                                                                                    List[QuantizerConfig]]) \
-            -> MultiConfigQuantizerSetup:
+    def create_quantizer_setup(
+        self, weight_quantizable_node_names_vs_configs: Dict[NNCFNodeName, List[QuantizerConfig]]
+    ) -> MultiConfigQuantizerSetup:
         same_op_groups = self._get_all_quantizers_grouped_by_affecting_op_set()
         setup = MultiConfigQuantizerSetup()
 
         pqid_vs_qpid = {}  # type: Dict[int, QuantizationPointId]
         qm_node_vs_same_op_gid = {}  # type: Dict[NNCFNodeName, int]
         for group in same_op_groups:
             grouped_ids = set()
             for pq in group.affecting_prop_quants:
                 directly_quantized_operator_node_names = [
                     next(iter(self.op_node_keys_to_underlying_nodes_mapping[key])).node_name
-                    for key in pq.quantized_input_sink_operator_nodes]
+                    for key in pq.quantized_input_sink_operator_nodes
+                ]
                 if pq.downstream_propagating_quantizers:
                     affected_operator_nodes = set()
                     for apq in pq.downstream_propagating_quantizers:
                         affected_operator_nodes.update(apq.quantized_input_sink_operator_nodes)
-                    directly_quantized_operator_node_names = \
-                        [next(iter(self.op_node_keys_to_underlying_nodes_mapping[key])).node_name
-                         for key in pq.quantized_input_sink_operator_nodes - affected_operator_nodes]
-                quant_point = MultiConfigQuantizationPoint(self.get_quant_insertion_point_for_propagating_quantizer(pq),
-                                                           pq.potential_quant_configs,
-                                                           directly_quantized_operator_node_names)
+                    directly_quantized_operator_node_names = [
+                        next(iter(self.op_node_keys_to_underlying_nodes_mapping[key])).node_name
+                        for key in pq.quantized_input_sink_operator_nodes - affected_operator_nodes
+                    ]
+                quant_point = MultiConfigQuantizationPoint(
+                    self.get_quant_insertion_point_for_propagating_quantizer(pq),
+                    pq.potential_quant_configs,
+                    directly_quantized_operator_node_names,
+                )
                 qp_id = pq.id
                 pqid_vs_qpid[pq.id] = qp_id
                 setup.quantization_points[qp_id] = quant_point
                 grouped_ids.add(qp_id)
 
             gid = setup.register_shared_inputs_group(list(grouped_ids))
             for weighted_node_name in weight_quantizable_node_names_vs_configs.keys():
                 for affected_node_key in group.affected_op_node_keys:
-                    underlying_node_names = [n.node_name
-                                             for n in self.op_node_keys_to_underlying_nodes_mapping[affected_node_key]]
+                    underlying_node_names = [
+                        n.node_name for n in self.op_node_keys_to_underlying_nodes_mapping[affected_node_key]
+                    ]
                     if weighted_node_name in underlying_node_names:
                         qm_node_vs_same_op_gid[weighted_node_name] = gid
 
         if setup.quantization_points.keys():
             max_aq_id = max(setup.quantization_points.keys()) + 1
         else:
             max_aq_id = 0
 
         next_wq_id = max_aq_id + 1
         wao_op_node_key_vs_wq_id = {}  # type: Dict[str, QuantizationPointId]
         for weighted_node_name, qconfig_list in weight_quantizable_node_names_vs_configs.items():
-            quant_point = MultiConfigQuantizationPoint(WeightQuantizationInsertionPoint(weighted_node_name),
-                                                       qconfig_list, [weighted_node_name])
+            quant_point = MultiConfigQuantizationPoint(
+                WeightQuantizationInsertionPoint(weighted_node_name), qconfig_list, [weighted_node_name]
+            )
             setup.quantization_points[next_wq_id] = quant_point
             if weighted_node_name not in qm_node_vs_same_op_gid:
                 # Happens for LSTM cells. The "hidden" Linear layer, as represented in NNCFGraph, has no
                 # input edges, since its input is not a regular network input, but a recurrent input
                 # from the previous execution step. TODO: extend recurrent operations handling so that NNCF graph
                 # has information on which operation accepts recurrent inputs.
-                nncf_logger.debug("Could not find an associated input activation quantizer "
-                                  "for a weighted node with quantizable weights: {}\n".format(weighted_node_name))
+                nncf_logger.debug(
+                    "Could not find an associated input activation quantizer "
+                    "for a weighted node with quantizable weights: {}\n".format(weighted_node_name)
+                )
             else:
                 associated_same_op_gid = qm_node_vs_same_op_gid[weighted_node_name]
                 setup.shared_input_operation_set_groups[associated_same_op_gid].add(next_wq_id)
 
             for wao_op_node_key in self._pqs_after_weight_dependent_output_quantized_nodes.values():
-                underlying_node_names = [n.node_name
-                                         for n in self.op_node_keys_to_underlying_nodes_mapping[wao_op_node_key]]
+                underlying_node_names = [
+                    n.node_name for n in self.op_node_keys_to_underlying_nodes_mapping[wao_op_node_key]
+                ]
                 if weighted_node_name in underlying_node_names:
                     wao_op_node_key_vs_wq_id[wao_op_node_key] = next_wq_id
             next_wq_id += 1
 
         pq_sets_grouped_by_unified_scale = list(
-            self._unified_scale_group_manager.get_group_vs_prop_quants_dict().values())
+            self._unified_scale_group_manager.get_group_vs_prop_quants_dict().values()
+        )
         for pq_set in pq_sets_grouped_by_unified_scale:
-            setup.register_unified_scale_group_with_types([pqid_vs_qpid[pq.id] for pq in pq_set],
-                                                          [pq.unified_scale_type for pq in pq_set])
+            setup.register_unified_scale_group_with_types(
+                [pqid_vs_qpid[pq.id] for pq in pq_set], [pq.unified_scale_type for pq in pq_set]
+            )
 
-        setup = self._handle_output_quantizers_for_weights_as_outputs_ops(setup, pqid_vs_qpid,
-                                                                          wao_op_node_key_vs_wq_id)
+        setup = self._handle_output_quantizers_for_weights_as_outputs_ops(setup, pqid_vs_qpid, wao_op_node_key_vs_wq_id)
 
         return setup
 
-    def _handle_output_quantizers_for_weights_as_outputs_ops(self, setup: MultiConfigQuantizerSetup,
-                                                             pqid_vs_qpid: Dict[int, QuantizationPointId],
-                                                             wao_op_node_key_vs_wq_id: Dict[str, QuantizationPointId]) \
-            -> MultiConfigQuantizerSetup:
+    def _handle_output_quantizers_for_weights_as_outputs_ops(
+        self,
+        setup: MultiConfigQuantizerSetup,
+        pqid_vs_qpid: Dict[int, QuantizationPointId],
+        wao_op_node_key_vs_wq_id: Dict[str, QuantizationPointId],
+    ) -> MultiConfigQuantizerSetup:
         """
         In case there are propagating quantizers dependent on the weights-as-outputs weighted operations
         (as marked by mark_act_quantizer_as_dependent_on_weights) in the current state of the quantizer setup,
         and if the quantizer configurations between the dependent activation quantizer and the weight output
         quantizer have at least one compatible configuration (checked across all AQ's in the unified
         scale group of the dependent AQ), then the activation quantizer will be removed and the weight quantizer's
         config options will be limited to the common configurations between the dependent quantizer and the
@@ -1189,40 +1289,44 @@
             unified_scale_gid = setup.get_unified_scale_group_id(qp_id_for_current_pq)
             if unified_scale_gid is not None:
                 all_qp_ids_in_unified_scale_group = deepcopy(setup.unified_scale_groups[unified_scale_gid])
             else:
                 all_qp_ids_in_unified_scale_group = {qp_id_for_current_pq}
             for act_qp_id in all_qp_ids_in_unified_scale_group:
                 curr_act_qconfigs = setup.quantization_points[act_qp_id].possible_qconfigs
-                curr_intersection_of_qconfigs = [qconf for qconf in curr_intersection_of_qconfigs
-                                                 if qconf in curr_act_qconfigs]
+                curr_intersection_of_qconfigs = [
+                    qconf for qconf in curr_intersection_of_qconfigs if qconf in curr_act_qconfigs
+                ]
 
             # Do further filtering for per-tensor quantizations only.
             # TODO: relax the requirement to allow the scale shape of the weight-as-output quantizer
             # matching the scale shape of the output quantizer (which may, in theory, end up being per-channel
-            curr_intersection_of_qconfigs = list(filter(lambda x: not x.per_channel,
-                                                        curr_intersection_of_qconfigs))
+            curr_intersection_of_qconfigs = list(filter(lambda x: not x.per_channel, curr_intersection_of_qconfigs))
 
             if not curr_intersection_of_qconfigs:
                 # Requantization is unavoidable
-                nncf_logger.debug(f"Attempted to use weight quantizer of {wao_op_node_key} "
-                                  f"to quantize input of {pq.affected_operator_nodes}, "
-                                  f"but no compatible configs were found.")
+                nncf_logger.debug(
+                    f"Attempted to use weight quantizer of {wao_op_node_key} "
+                    f"to quantize input of {pq.affected_operator_nodes}, "
+                    f"but no compatible configs were found."
+                )
                 continue
 
             setup.quantization_points[wao_qp_id].possible_qconfigs = curr_intersection_of_qconfigs
             for act_qp_id in all_qp_ids_in_unified_scale_group:
                 setup.quantization_points[act_qp_id].possible_qconfigs = curr_intersection_of_qconfigs
 
             if unified_scale_gid is not None:
                 setup.register_existing_qp_id_in_unified_scale_group(wao_qp_id, unified_scale_gid)
-                unified_scale_qp_printable_str = ", ".join([str(setup.quantization_points[qp_id]) for qp_id in
-                                                            all_qp_ids_in_unified_scale_group])
-                nncf_logger.debug(f"Unifying weight quantizer ranges of {wao_op_node_key} "
-                                  f"with {unified_scale_qp_printable_str}")
+                unified_scale_qp_printable_str = ", ".join(
+                    [str(setup.quantization_points[qp_id]) for qp_id in all_qp_ids_in_unified_scale_group]
+                )
+                nncf_logger.debug(
+                    f"Unifying weight quantizer ranges of {wao_op_node_key} " f"with {unified_scale_qp_printable_str}"
+                )
 
             # The activation quantizer is now unnecessary since we could find a matching weight quantization
             # for the op. Should discard it, but first transfer the knowledge on the operators it quantizes downstream
             # to the weights-as-outputs quantization point.
             dir_quant_ops = setup.quantization_points[qp_id_for_current_pq].directly_quantized_operator_node_names
             setup.quantization_points[wao_qp_id].directly_quantized_operator_node_names.extend(deepcopy(dir_quant_ops))
             setup.discard(qp_id_for_current_pq, keep_shared_input_qps=True)
@@ -1278,10 +1382,12 @@
                 assert pq in edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
             for affected_ip_node_key in pq.affected_ip_nodes:
                 ip_node = self.nodes[affected_ip_node_key]
                 assert self.is_insertion_point(ip_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR])
                 assert pq in ip_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
             for affected_op_node_key in pq.affected_operator_nodes:
                 op_node = self.nodes[affected_op_node_key]
-                assert op_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR] == \
-                       QuantizerPropagationStateGraphNodeType.OPERATOR
+                assert (
+                    op_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
+                    == QuantizerPropagationStateGraphNodeType.OPERATOR
+                )
                 assert pq in op_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
```

### Comparing `nncf-2.4.0/nncf/common/quantization/quantizer_propagation/grouping.py` & `nncf-2.5.0/nncf/common/quantization/quantizer_propagation/grouping.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,20 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from copy import copy
-from typing import Dict
-from typing import Optional
-from typing import Set
+from typing import Dict, Optional, Set
 
 from nncf.common.quantization.quantizer_propagation.structs import PropagatingQuantizer
 
 
 class UnifiedScalePropagatingQuantizerGroupManager:
     """
     Keeps track of the groups of quantizers that have to have their scales unified in the final
@@ -39,34 +35,35 @@
         Registers a set of propagating quantizers as a new group.
 
         :param prop_quants: A set of propagating quantizers to be registered.
         :return: The ID of the newly created group.
         """
         for pq in prop_quants:
             for gid, group in self._group_vs_prop_quants_dict.items():
-                assert pq not in group, 'Propagating quantizer #{} is already registered in a group {}!'.format(pq.id,
-                                                                                                                gid)
+                assert pq not in group, "Propagating quantizer #{} is already registered in a group {}!".format(
+                    pq.id, gid
+                )
         gid = self._get_next_gid()
         self._group_vs_prop_quants_dict[gid] = prop_quants
         return gid
 
     def add_to_group(self, target_gid: int, prop_quant: PropagatingQuantizer):
         """
         Adds a propagating quantizer to an already existing group.
 
         :param target_gid: The ID of the group to be extended.
         :param prop_quant: The propagating quantizer to be registered in the group. The quantizer
           must not be already registered in any group.
         """
         for gid, group in self._group_vs_prop_quants_dict.items():
             if target_gid != gid:
-                assert prop_quant not in group, 'Tried to add propagating quantizer #{} to group #{}, ' \
-                                                'but it is already registered in a group {}!'.format(prop_quant.id,
-                                                                                                     target_gid,
-                                                                                                     gid)
+                assert prop_quant not in group, (
+                    "Tried to add propagating quantizer #{} to group #{}, "
+                    "but it is already registered in a group {}!".format(prop_quant.id, target_gid, gid)
+                )
         self._group_vs_prop_quants_dict[target_gid].add(prop_quant)
 
     def remove_from_group(self, group: int, prop_quant: PropagatingQuantizer):
         """
         Removes a propagating quantizer from a group.
 
         :param group: The ID of the group from where a quantizer should be removed.
```

### Comparing `nncf-2.4.0/nncf/common/quantization/quantizer_propagation/solver.py` & `nncf-2.5.0/nncf/common/quantization/quantizer_propagation/solver.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,51 +1,42 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-#pylint:disable=too-many-lines
+# pylint:disable=too-many-lines
 from collections import Counter
 from collections import OrderedDict
 from collections import deque
 from copy import deepcopy
 from enum import Enum
 from functools import partial
-from typing import Deque
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Set
-from typing import Tuple
+from typing import Deque, Dict, List, Optional, Set, Tuple
 
 import networkx as nx
 
 from nncf.common.graph import INPUT_NOOP_METATYPES
 from nncf.common.graph import OUTPUT_NOOP_METATYPES
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph import OperatorMetatype
 from nncf.common.graph.graph import NNCFGraph
-from nncf.common.graph.operator_metatypes import InputNoopMetatype
-from nncf.common.graph.operator_metatypes import OutputNoopMetatype
 from nncf.common.graph.operator_metatypes import UnknownMetatype
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.hardware.config import HWConfig
-from nncf.common.insertion_point_graph import ConstantNodesFilter
 from nncf.common.insertion_point_graph import InsertionPointGraph
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.quantizer_propagation.graph import QuantizerPropagationStateGraph
 from nncf.common.quantization.quantizer_propagation.grouping import QuantizersWaitingForMergeManager
+from nncf.common.quantization.quantizer_propagation.structs import IgnoreReason
 from nncf.common.quantization.quantizer_propagation.structs import PropagatingQuantizer
 from nncf.common.quantization.quantizer_propagation.structs import PropagationPath
 from nncf.common.quantization.quantizer_propagation.structs import QuantizationTrait
 from nncf.common.quantization.quantizer_propagation.structs import QuantizerPropagationStateGraphNodeType
 from nncf.common.quantization.quantizer_setup import DEFAULT_QUANTIZER_CONFIG
 from nncf.common.quantization.quantizer_setup import MultiConfigQuantizerSetup
 from nncf.common.quantization.quantizer_setup import QuantizationPointId
@@ -53,14 +44,15 @@
 from nncf.common.quantization.structs import QuantizableWeightedLayerNode
 from nncf.common.quantization.structs import QuantizationConstraints
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import QuantizerGroup
 from nncf.common.quantization.structs import UnifiedScaleType
 from nncf.common.scopes import matches_any
+from nncf.common.scopes import should_consider_scope
 from nncf.common.utils.debug import DEBUG_LOG_DIR
 from nncf.common.utils.debug import is_debug
 from nncf.common.utils.dot_file_rw import write_dot_graph
 
 
 class TransitionStatus(Enum):
     SHOULD_TRANSITION = 0
@@ -91,16 +83,19 @@
 class FinalizedQuantizationProposal:
     """
     Describes a version of QuantizationProposal in which a single quantizer configuration has been chosen
     (using one or the other way of disambiguation) for each quantization point in the setup that was made available
     in the original QuantizationProposel
     """
 
-    def __init__(self, single_config_quantizer_setup: SingleConfigQuantizerSetup,
-                 quant_prop_graph: QuantizerPropagationStateGraph):
+    def __init__(
+        self,
+        single_config_quantizer_setup: SingleConfigQuantizerSetup,
+        quant_prop_graph: QuantizerPropagationStateGraph,
+    ):
         """
         :param single_config_quantizer_setup: The single-configuration quantizer setup.
         :param quant_prop_graph: The quantizer propagation state graph to which this quantizer setup is related.
         """
         self.single_config_quantizer_setup = single_config_quantizer_setup
         self._quant_prop_graph = quant_prop_graph
 
@@ -113,46 +108,49 @@
     """
     Describes an intermediate state in the quantizer setup creation, at which the quantizers have already been
     propagated until a standstill, and each quantizer still has more than one (in general) quantizer configurations
     to be chosen from. This object serves as an input to the external algorithm (such as HAWQ or AutoQ) that
     disambiguates the quantizer configurations at each location without changing the quantizer positions.
     """
 
-    def __init__(self, quantizer_setup: MultiConfigQuantizerSetup,
-                 quant_prop_graph: QuantizerPropagationStateGraph,
-                 quantization_point_id_vs_prop_quantizer: Dict[QuantizationPointId,
-                                                               PropagatingQuantizer]):
+    def __init__(
+        self,
+        quantizer_setup: MultiConfigQuantizerSetup,
+        quant_prop_graph: QuantizerPropagationStateGraph,
+        quantization_point_id_vs_prop_quantizer: Dict[QuantizationPointId, PropagatingQuantizer],
+    ):
         """
         :param quantizer_setup: The MultiConfigQuantizerSetup object obtained from a quantizer propagation solver.
         :param quant_prop_graph: The QuantizerPropagationStateGraph whose state correspoinds to the `quantizer_setup`,
           also obtained from the solver
         :param quantization_point_id_vs_prop_quantizer: A mapping of the quantization point IDs in `quantizer_setup` to
           propagating quantizers registered in `quant_prop_graph`.
         """
         self.quantizer_setup = quantizer_setup
         self._quant_prop_graph = quant_prop_graph
         self._quantization_point_id_vs_prop_quantizer = quantization_point_id_vs_prop_quantizer
         self._prop_quantizer_vs_quantization_point_id = {}  # type: Dict[PropagatingQuantizer, QuantizationPointId]
         for qp_id, pq in self._quantization_point_id_vs_prop_quantizer.items():
             self._prop_quantizer_vs_quantization_point_id[pq] = qp_id
 
-    def constrain_quantizer_config_list_for_insertion(self, quantization_point_id: QuantizationPointId,
-                                                      constrained_config_list: List[QuantizerConfig]):
+    def constrain_quantizer_config_list_for_insertion(
+        self, quantization_point_id: QuantizationPointId, constrained_config_list: List[QuantizerConfig]
+    ):
         """
         Constrains a set of available quantizer configurations for a quantization point with a given ID as
         defined by the list of quantizer configurations - in essence, performs a selection.
 
         :param quantization_point_id: The ID of the quantization point.
         :param constrained_config_list: The list of configs (of which every config is already present in the
           currently available in the quantization point's set of available config) that will replace the list
           of the quantizer configs for the quantization point defined by `quantization_point_id`.
         """
         prior_list = self.quantizer_setup.quantization_points[quantization_point_id].possible_qconfigs
         if not all(qc in prior_list for qc in constrained_config_list):
-            raise RuntimeError('Constrained config list is incompatible with the result of the quantizer propagation!')
+            raise RuntimeError("Constrained config list is incompatible with the result of the quantizer propagation!")
         # TODO (vshampor): only allow to constrain 'input-group'-wise?
         self.quantizer_setup.quantization_points[quantization_point_id].possible_qconfigs = constrained_config_list
 
         if quantization_point_id in self._quantization_point_id_vs_prop_quantizer:
             pq = self._quantization_point_id_vs_prop_quantizer[quantization_point_id]
             pq.potential_quant_configs = constrained_config_list
 
@@ -169,53 +167,66 @@
         """
         for pq, qp_id in self._prop_quantizer_vs_quantization_point_id.items():
             if qp_id not in final_quantizer_setup.quantization_points:
                 self._quant_prop_graph.remove_propagating_quantizer(pq)
             else:
                 final_qconfig = final_quantizer_setup.quantization_points[qp_id].qconfig
                 if strict:
+
                     def is_final_qconfig_compatible_to_initial(initial_qconfig: QuantizerConfig):
-                        return final_qconfig.per_channel == initial_qconfig.per_channel and \
-                               final_qconfig.mode == initial_qconfig.mode and \
-                               final_qconfig.num_bits == initial_qconfig.num_bits and \
-                               (final_qconfig.signedness_to_force == initial_qconfig.signedness_to_force or
-                                initial_qconfig.signedness_to_force is None or
-                                final_qconfig.signedness_to_force is None)
+                        return (
+                            final_qconfig.per_channel == initial_qconfig.per_channel
+                            and final_qconfig.mode == initial_qconfig.mode
+                            and final_qconfig.num_bits == initial_qconfig.num_bits
+                            and (
+                                final_qconfig.signedness_to_force == initial_qconfig.signedness_to_force
+                                or initial_qconfig.signedness_to_force is None
+                                or final_qconfig.signedness_to_force is None
+                            )
+                        )
 
                     compatible_initial_qconfs = list(
-                        filter(is_final_qconfig_compatible_to_initial,
-                               self.quantizer_setup.quantization_points[qp_id].possible_qconfigs))
+                        filter(
+                            is_final_qconfig_compatible_to_initial,
+                            self.quantizer_setup.quantization_points[qp_id].possible_qconfigs,
+                        )
+                    )
                     if not compatible_initial_qconfs:
-                        raise RuntimeError('The final quantizer setup has configurations that were not present in the '
-                                           'initial proposal!')
+                        raise RuntimeError(
+                            "The final quantizer setup has configurations that were not present in the "
+                            "initial proposal!"
+                        )
                     if final_qconfig.signedness_to_force is None:
                         initial_qconfs_signedness_values = {qc.signedness_to_force for qc in compatible_initial_qconfs}
                         if None not in initial_qconfs_signedness_values and len(initial_qconfs_signedness_values) == 1:
                             # The initial configs were either all forced-signed or all forced-unsigned - should set
                             # final qconfig's forced field appropriately
                             final_qconfig.signedness_to_force = initial_qconfs_signedness_values.pop()
 
                 pq.potential_quant_configs = [final_qconfig]
-        return FinalizedQuantizationProposal(final_quantizer_setup,
-                                             self._quant_prop_graph)
+        return FinalizedQuantizationProposal(final_quantizer_setup, self._quant_prop_graph)
 
 
 class PostprocessingNodeLocator:
     """
     Detects the nodes in the QuantizerPropagationStateGraph, which implement the post-processing logic in the model.
     Based on the special post-processing marker metatypes the nodes are placed in the ignored.
     """
 
-    def __init__(self, quant_prop_graph: QuantizerPropagationStateGraph,
-                 quantizable_layer_nodes: List[QuantizableWeightedLayerNode],
-                 post_processing_marker_metatypes: List[OperatorMetatype]):
+    def __init__(
+        self,
+        quant_prop_graph: QuantizerPropagationStateGraph,
+        quantizable_layer_nodes: List[QuantizableWeightedLayerNode],
+        post_processing_marker_metatypes: List[OperatorMetatype],
+    ):
         self._quant_prop_graph = quant_prop_graph
         self._post_processing_marker_metatypes = post_processing_marker_metatypes
-        self._quantizable_layer_node_keys = [q_nodes.node.data[NNCFGraph.KEY_NODE_ATTR] for q_nodes in
-                                             quantizable_layer_nodes]
+        self._quantizable_layer_node_keys = [
+            q_nodes.node.data[NNCFGraph.KEY_NODE_ATTR] for q_nodes in quantizable_layer_nodes
+        ]
         self._post_processing_marker_encountered = False
 
     def _is_node_has_underlying_weights(self, node_key: str) -> bool:
         underlying_nncf_nodes = self._quant_prop_graph.op_node_keys_to_underlying_nodes_mapping[node_key]
         for node in underlying_nncf_nodes:
             if node.data[NNCFGraph.KEY_NODE_ATTR] in self._quantizable_layer_node_keys:
                 return True
@@ -236,27 +247,28 @@
     def _get_ignored_node_keys(self, node_keys: List[str]) -> List[str]:
         output = []
         for node_key, node_metatype in zip(node_keys, map(self._get_node_metatype, node_keys)):
             if node_metatype not in self._post_processing_marker_metatypes:
                 output.append(node_key)
         return output
 
-    def get_post_processing_node_keys(self) -> List[str]:
+    def get_post_processing_node_keys(self) -> Set[str]:
         """
         Finds out the nodes of the QuantizerPropagationStateGraph, which are in post-processing part of the model.
         Starting from the output nodes all the nodes are added, until the quantizable nodes with weights are faced.
         If the path with the nodes has the post-processing marker node,
         all the nodes in this path will be added into ignored.
-        :return: The list of the node keys to be ignored.
+        :return: Set of the node keys to be ignored.
         """
 
         visited_nodes = set()
 
-        def backward_traverse_function(node_key: str, output: List[str],
-                                       visited_nodes: Set[str]) -> Tuple[bool, List[str]]:
+        def backward_traverse_function(
+            node_key: str, output: List[str], visited_nodes: Set[str]
+        ) -> Tuple[bool, List[str]]:
             """
             Realizes the search of the quantization ignored nodes in graph.
             Only QuantizerPropagationStateGraphNodeType.OPERATOR nodes are processed during the traversing.
             If the current node is in the list of the quantizable nodes with weights,
              the traversing is being stopped.
             The new forward traversing from the current node starts.
             If the quantizable nodes with weights is faced in forward traversing faced,
@@ -266,16 +278,17 @@
             and whether the node should be added to the traversed path.
             :param output: Path contains the list of the visited nodes.
             :param visited_nodes: Set stores whether the particular node was visited before or not.
             :return: The first value shows whether the traversing finished,
             the second one is traversing path containing the visited nodes.
             """
 
-            def forward_traverse_function(node_key: str, output: List[str],
-                                          visited_nodes: Set[str]) -> Tuple[bool, List[bool]]:
+            def forward_traverse_function(
+                node_key: str, output: List[str], visited_nodes: Set[str]
+            ) -> Tuple[bool, List[bool]]:
                 # If the node is not operator
                 if not self._is_node_operator(node_key):
                     return False, output
                 if node_key in visited_nodes:
                     return True, output
                 output.append(node_key)
                 if self._is_node_has_underlying_weights(node_key):
@@ -289,86 +302,99 @@
                 return True, output
 
             node_metatype = self._get_node_metatype(node_key)
             # If the node weight quantizable
             if self._is_node_has_underlying_weights(node_key):
                 visited_nodes.add(node_key)
                 return True, output
-            if node_metatype in [InputNoopMetatype, OutputNoopMetatype]:
+            if node_metatype in list(OUTPUT_NOOP_METATYPES.values()) + list(INPUT_NOOP_METATYPES.values()):
                 visited_nodes.add(node_key)
                 return False, output
             self._check_if_postprocessing(node_metatype)
-            partial_forward_traverse_function = partial(forward_traverse_function,
-                                                        visited_nodes=visited_nodes)
-            forward_visited_node_keys = self._quant_prop_graph.traverse_graph(node_key,
-                                                                           partial_forward_traverse_function,
-                                                                           output=[],
-                                                                           traverse_forward=True)
+            partial_forward_traverse_function = partial(forward_traverse_function, visited_nodes=visited_nodes)
+            forward_visited_node_keys = self._quant_prop_graph.traverse_graph(
+                node_key, partial_forward_traverse_function, output=[], traverse_forward=True
+            )
             # If in the path there are nodes with weights should stop the main backward traversing
             for forward_visited_node_key in forward_visited_node_keys:
                 if self._is_node_has_underlying_weights(forward_visited_node_key):
                     visited_nodes.add(node_key)
                     return True, output
             output.append(node_key)
             return False, output
 
         partial_backward_traverse_function = partial(backward_traverse_function, visited_nodes=visited_nodes)
-        output = []
-        for start_node_key in self._quant_prop_graph.get_node_keys_by_metatype(OutputNoopMetatype):
+        output = set()
+
+        output_nodes = []
+        for output_metatype in OUTPUT_NOOP_METATYPES.values():
+            output_nodes.extend(self._quant_prop_graph.get_node_keys_by_metatype(output_metatype))
+
+        for start_node_key in output_nodes:
             self._post_processing_marker_encountered = False
-            node_keys = self._quant_prop_graph.traverse_graph(start_node_key, partial_backward_traverse_function,
-                                                              output=[], traverse_forward=False)
+            node_keys = self._quant_prop_graph.traverse_graph(
+                start_node_key, partial_backward_traverse_function, output=[], traverse_forward=False
+            )
             if self._post_processing_marker_encountered:
                 ignored_node_keys = self._get_ignored_node_keys(node_keys)
-                output.extend(ignored_node_keys)
+                output.update(ignored_node_keys)
 
         return output
 
 
 class QuantizerPropagationSolver:
     """
     Analyzes a fresh QuantizerPropagationStateGraph object according to HW
     configuration supplied in the initializer and produces the list of insertion
     commands that correspond to the final state of the quantizer propagation graph
     when the model has the most contol flow graph edges quantized according to HW
     capabilities.
     """
 
-    DEFAULT_QUANTIZATION_TYPES = [QuantizerConfig(
-        num_bits=8,
-        mode=QuantizationMode.SYMMETRIC,
-        signedness_to_force=None,
-        per_channel=False)]
+    DEFAULT_QUANTIZATION_TYPES = [
+        QuantizerConfig(num_bits=8, mode=QuantizationMode.SYMMETRIC, signedness_to_force=None, per_channel=False)
+    ]
 
     DEFAULT_PROPAGATION_STRATEGY = PropagationStrategy.MERGE_WITH_SINGLE_FQ_RESULT
 
-    def __init__(self,
-                 ignored_scopes: List[str] = None,
-                 target_scopes: List[str] = None,
-                 hw_config: HWConfig = None,
-                 default_trait_to_metatype_map: Dict[QuantizationTrait, List[OperatorMetatype]] = None,
-                 propagation_strategy: PropagationStrategy = None,
-                 default_qconfig_list: List[QuantizerConfig] = None,
-                 quantizable_layer_nodes: List[QuantizableWeightedLayerNode] = None,
-                 scope_overrides: Dict = None,
-                 global_constraints: Dict[QuantizerGroup, QuantizationConstraints] = None,
-                 additional_unified_scale_op_scopes: List[List[str]] = None,
-                 run_consistency_checks: bool = False,
-                 quantize_outputs: bool = False,
-                 post_processing_marker_metatypes: List[OperatorMetatype] = None):
+    def __init__(
+        self,
+        activation_ignored_scopes: List[str] = None,
+        weight_ignored_scopes: List[str] = None,
+        activation_target_scopes: List[str] = None,
+        weight_target_scopes: List[str] = None,
+        hw_config: HWConfig = None,
+        default_trait_to_metatype_map: Dict[QuantizationTrait, List[OperatorMetatype]] = None,
+        propagation_strategy: PropagationStrategy = None,
+        default_qconfig_list: List[QuantizerConfig] = None,
+        quantizable_layer_nodes: List[QuantizableWeightedLayerNode] = None,
+        scope_overrides: Dict = None,
+        global_constraints: Dict[QuantizerGroup, QuantizationConstraints] = None,
+        additional_unified_scale_op_scopes: List[List[str]] = None,
+        run_consistency_checks: bool = False,
+        quantize_outputs: bool = False,
+        post_processing_marker_metatypes: List[OperatorMetatype] = None,
+    ):
         """
         Initializes the solver with parameters affecting the resulting quantizer setup.
 
-        :param ignored_scopes: A list of strings to match against NNCFGraph node names
-          and ignore matching nodes. Ignored nodes will not have quantizers applied to their inputs (even if
-          required by node's metatype and HW config), and the downstream quantizers will not propagate upwards
-          through the corresponding node.
-        :param target_scopes: A list of strings to match against NNCFGraph and define a set of nodes
-          to be considered during quantizer propagation. When `ignored_scopes` is a "denylist",
-          the `target_scopes` is an "allowlist"; otherwise, same effects apply as for `ignored_scopes`.
+        :param activation_ignored_scopes: A list of strings to match against NNCFGraph node names
+          and ignore matching nodes. Ignored nodes will not have quantizers applied to their activation inputs
+          (even if required by node's metatype and HW config), and the downstream quantizers will not propagate
+          upwards through the corresponding node.
+        :param weight_ignored_scopes: A list of strings to match against NNCFGraph node names
+          and ignore matching nodes. Ignored nodes will not have quantizers applied to their weight inputs
+          (even if required by node's metatype and HW config).
+        :param activation_target_scopes: A list of strings to match against NNCFGraph and define a set of nodes
+          to be considered during quantizer propagation. When `activation_ignored_scopes` is a "denylist",
+          the `activation_target_scopes` is an "allowlist";
+          otherwise, same effects apply as for `activation_ignored_scopes`.
+        :param weight_target_scopes: A list of strings to match against NNCFGraph and define a set of nodes
+          which weights should be quantized. When `weight_ignored_scopes` is a "denylist",
+          the `weight_target_scopes` is an "allowlist"; otherwise, same effects apply as for `weight_ignored_scopes`.
         :param hw_config: A hardware config to be used for determining the set of operations to be quantized with
         respect to their inputs and, for every such operation, the set of allowed quantizer configurations
         :param default_trait_to_metatype_map: The mapping of QuantizationTrait's to the metatypes to be associated with
         these by default. Used if no HW config is passed, or if an operation that is unknown to HW config is
         encountered.
         :param propagation_strategy: The strategy to be used while propagating and merging quantizers.
         :param default_qconfig_list: The list of quantizer configurations that should be applied for quantizing
@@ -400,28 +426,31 @@
             self._default_trait_to_metatype_map = {}
         else:
             self._default_trait_to_metatype_map = default_trait_to_metatype_map
         self.default_global_qconfig_list = default_qconfig_list
         self._hw_config = hw_config  # type: HWConfig
         self._visualizer = None
         if is_debug():
-            from nncf.common.quantization.quantizer_propagation.visualizer import \
-                QuantizerPropagationVisualizer  # pylint: disable=cyclic-import
+            from nncf.common.quantization.quantizer_propagation.visualizer import (
+                QuantizerPropagationVisualizer,  # pylint: disable=cyclic-import
+            )
+
             self._visualizer = QuantizerPropagationVisualizer(DEBUG_LOG_DIR + "/quant_prop")
-        self._propagation_strategy = propagation_strategy if propagation_strategy \
-            else QuantizerPropagationSolver.DEFAULT_PROPAGATION_STRATEGY  # TODO (vshampor): determine from config
+        self._propagation_strategy = (
+            propagation_strategy if propagation_strategy else QuantizerPropagationSolver.DEFAULT_PROPAGATION_STRATEGY
+        )  # TODO (vshampor): determine from config
         self._operator_quantization_trait_map = self.get_operator_quantization_traits_map()
         self._operator_allowed_qconfigs_map = self._get_operator_qconfigs_map()
         self._quantize_outputs = quantize_outputs
-        if quantizable_layer_nodes is not None:
-            self._weight_quantizable_node_names_vs_qconfigs = {
-                x.node.node_name: x.qconfig_list for x in quantizable_layer_nodes
-            }  # type: Dict[NNCFNodeName, List[QuantizerConfig]]
-        else:
-            self._weight_quantizable_node_names_vs_qconfigs = {}  # type: Dict[NNCFNodeName, List[QuantizerConfig]]
+        self._ignored_scopes = activation_ignored_scopes
+        self._target_scopes = activation_target_scopes
+        self._weight_quantizable_node_names_vs_qconfigs = self._filter_by_weight_ignored_target_scopes(
+            quantizable_layer_nodes, weight_ignored_scopes, weight_target_scopes
+        )
+
         if scope_overrides is None:
             self._scope_overrides = {}
         else:
             self._scope_overrides = scope_overrides  # type: Dict
         self._global_constraints = global_constraints  # type: Dict['QuantizerGroup', 'QuantizationConstraints']
         self._run_consistency_checks = run_consistency_checks
 
@@ -436,23 +465,41 @@
             for op_meta, qconf_list in self._operator_allowed_qconfigs_map.items():
                 trait = self._operator_quantization_trait_map.get(op_meta, QuantizationTrait.QUANTIZATION_AGNOSTIC)
                 if trait == QuantizationTrait.INPUTS_QUANTIZABLE:
                     if HWConfig.is_qconf_list_corresponding_to_unspecified_op(qconf_list):
                         self._operator_allowed_qconfigs_map[op_meta] = default_qconfig_list
         self._active_propagating_quantizers_queue = deque()
         self._finished_propagating_quantizers = []  # type: List[PropagatingQuantizer]
-        self._ignored_scopes = ignored_scopes
-        self._target_scopes = target_scopes
         self._quantizers_waiting_for_branch_merge = QuantizersWaitingForMergeManager()
 
         self._potential_quantizers = {}
         self._num_potential_quantized_activations = 0
         self._quantizable_layer_nodes = quantizable_layer_nodes
         self._post_processing_marker_metatypes = post_processing_marker_metatypes
 
+    def _filter_by_weight_ignored_target_scopes(
+        self,
+        quantizable_layer_nodes: List[QuantizableWeightedLayerNode],
+        weight_ignored_scopes: Dict[QuantizerGroup, List[str]],
+        weight_target_scopes: Dict[QuantizerGroup, List[str]],
+    ) -> Dict[NNCFNodeName, List[QuantizerConfig]]:
+        if quantizable_layer_nodes is None:
+            return {}
+
+        weight_quantizable_node_names_vs_qconfigs = {}
+        for x in quantizable_layer_nodes:
+            node_name = x.node.node_name
+            if should_consider_scope(
+                node_name, ignored_scopes=weight_ignored_scopes, target_scopes=weight_target_scopes
+            ):
+                weight_quantizable_node_names_vs_qconfigs[node_name] = x.qconfig_list
+            else:
+                nncf_logger.debug(f"Ignored adding weight quantizer for: {node_name}")
+        return weight_quantizable_node_names_vs_qconfigs
+
     def run_on_ip_graph(self, ip_graph: InsertionPointGraph) -> QuantizationProposal:
         """
         The main function to be used on an InsertionPointGraph to produce
         the list of insertion commands and configs corresponding to the desired quantized
         graph state. The result of the function is not final, as it will define multiple
         possible quantizer configuration for each weight and activation quantization locations;
         a single configuration for each location must be chosen using external means.
@@ -461,24 +508,19 @@
         original model graph. The propagating quantizers will travel along the pre- and post-
         hook nodes registered in this graph.
         :return: The intermediate propagation state in the form of QuantizationProposal, which
         defines unambiguously the locations of the propagating quantizers, but not the final
         configurations.
         """
         self._num_potential_quantized_activations = 0
-        quantizable_layer_node_keys = []
-        if self._quantizable_layer_nodes is not None:
-            quantizable_layer_node_keys = [node.node.data['key'] for node in self._quantizable_layer_nodes]
-        filtered_ip_graph = ConstantNodesFilter.filter(ip_graph, quantizable_layer_node_keys)
-        quant_prop_graph = QuantizerPropagationStateGraph(filtered_ip_graph,
-                                                          self._ignored_scopes,
-                                                          self._target_scopes)
+        quant_prop_graph = QuantizerPropagationStateGraph(ip_graph, self._ignored_scopes, self._target_scopes)
         if self._post_processing_marker_metatypes is not None:
-            post_processing_node_locator = PostprocessingNodeLocator(quant_prop_graph, self._quantizable_layer_nodes,
-                                                                     self._post_processing_marker_metatypes)
+            post_processing_node_locator = PostprocessingNodeLocator(
+                quant_prop_graph, self._quantizable_layer_nodes, self._post_processing_marker_metatypes
+            )
             post_processing_node_keys = post_processing_node_locator.get_post_processing_node_keys()
             for post_processing_node_key in post_processing_node_keys:
                 self._add_node_to_ignored(post_processing_node_key, quant_prop_graph)
         quant_prop_graph = self.set_allowed_quantization_types_for_operator_nodes(quant_prop_graph)
         quant_prop_graph = self.setup_initial_quantizers(quant_prop_graph)
 
         if self._run_consistency_checks:
@@ -498,44 +540,57 @@
 
         if self._visualizer is not None:
             self._visualizer.visualize_quantizer_propagation(self, quant_prop_graph, "proposed")
 
         if self._run_consistency_checks:
             quant_prop_graph.run_consistency_check()
 
+        for node_key in ip_graph:
+            node = ip_graph.nodes[node_key]
+            if node.get(InsertionPointGraph.IS_MERGED_NODE_ATTR, False):
+                merged_nncf_nodes = node[InsertionPointGraph.MERGED_NNCF_NODE_LIST_NODE_ATTR]
+                # If first op in fused pattern has weights, then they should be quantized
+                for node in merged_nncf_nodes[1:]:
+                    if node.node_name in self._weight_quantizable_node_names_vs_qconfigs:
+                        self._weight_quantizable_node_names_vs_qconfigs.pop(node.node_name)
+
         quantizer_setup = quant_prop_graph.create_quantizer_setup(self._weight_quantizable_node_names_vs_qconfigs)
         insertions_vs_associated_prop_quants = self._map_quantization_points_to_prop_quantizers(
-            self._finished_propagating_quantizers, quant_prop_graph, quantizer_setup)
+            self._finished_propagating_quantizers, quant_prop_graph, quantizer_setup
+        )
 
-        return QuantizationProposal(quantizer_setup=quantizer_setup,
-                                    quant_prop_graph=quant_prop_graph,
-                                    quantization_point_id_vs_prop_quantizer=insertions_vs_associated_prop_quants)
+        return QuantizationProposal(
+            quantizer_setup=quantizer_setup,
+            quant_prop_graph=quant_prop_graph,
+            quantization_point_id_vs_prop_quantizer=insertions_vs_associated_prop_quants,
+        )
 
     def _add_node_to_ignored(self, node_key: str, quant_prop_graph: QuantizerPropagationStateGraph) -> None:
-        quant_prop_graph.ignored_node_keys.append(node_key)
+        quant_prop_graph.ignored_node_keys[node_key] = IgnoreReason.AUTOGENERATED
         quant_prop_graph.nodes[node_key][quant_prop_graph.IS_IN_IGNORED_SCOPES] = True
 
-    def _map_quantization_points_to_prop_quantizers(self,
-                                                    prop_quant_list: List[PropagatingQuantizer],
-                                                    quant_prop_graph: QuantizerPropagationStateGraph,
-                                                    quantizer_setup: MultiConfigQuantizerSetup) -> \
-            Dict[QuantizationPointId, PropagatingQuantizer]:
+    def _map_quantization_points_to_prop_quantizers(
+        self,
+        prop_quant_list: List[PropagatingQuantizer],
+        quant_prop_graph: QuantizerPropagationStateGraph,
+        quantizer_setup: MultiConfigQuantizerSetup,
+    ) -> Dict[QuantizationPointId, PropagatingQuantizer]:
         qps_vs_associated_prop_quants_dict = {}  # type: Dict[QuantizationPointId, PropagatingQuantizer]
 
         for finished_prop_quantizer in prop_quant_list:
-            qip = quant_prop_graph.get_quant_insertion_point_for_propagating_quantizer(
-                finished_prop_quantizer)
+            qip = quant_prop_graph.get_quant_insertion_point_for_propagating_quantizer(finished_prop_quantizer)
             for qp_id, qp in quantizer_setup.quantization_points.items():
                 if qp.insertion_point == qip:
                     qps_vs_associated_prop_quants_dict[qp_id] = finished_prop_quantizer
 
         return qps_vs_associated_prop_quants_dict
 
-    def get_final_quantizer_setup(self, finalized_quantization_proposal: FinalizedQuantizationProposal) -> \
-            SingleConfigQuantizerSetup:
+    def get_final_quantizer_setup(
+        self, finalized_quantization_proposal: FinalizedQuantizationProposal
+    ) -> SingleConfigQuantizerSetup:
         """
         Merges consequent quantizers which ended up having the same quantization configuration.
         :param finalized_quantization_proposal:
         :return:
         """
         quant_prop_graph = finalized_quantization_proposal.quant_prop_graph
         quant_prop_graph.merge_redundant_subsequent_quantizers_across_graph()
@@ -545,32 +600,38 @@
 
         if self._run_consistency_checks:
             quant_prop_graph.run_consistency_check()
 
         final_weight_quantizable_node_names_vs_qconfig_dict = {}
         for qp in finalized_quantization_proposal.single_config_quantizer_setup.quantization_points.values():
             if qp.is_weight_quantization_point():
-                final_weight_quantizable_node_names_vs_qconfig_dict[qp.insertion_point.target_node_name] = \
-                    [qp.qconfig]  # sic!
-
-        if Counter(final_weight_quantizable_node_names_vs_qconfig_dict.keys()) != \
-                Counter(self._weight_quantizable_node_names_vs_qconfigs.keys()):
+                final_weight_quantizable_node_names_vs_qconfig_dict[qp.insertion_point.target_node_name] = [
+                    qp.qconfig
+                ]  # sic!
+
+        if Counter(final_weight_quantizable_node_names_vs_qconfig_dict.keys()) != Counter(
+            self._weight_quantizable_node_names_vs_qconfigs.keys()
+        ):
             raise RuntimeError("Final weight quantizer setup is inconsistent with initial solver assumptions!")
 
         multi_setup_with_one_config_per_point = quant_prop_graph.create_quantizer_setup(
-            final_weight_quantizable_node_names_vs_qconfig_dict)
+            final_weight_quantizable_node_names_vs_qconfig_dict
+        )
         final_setup = multi_setup_with_one_config_per_point.select_first_qconfig_for_each_point()
         return final_setup
 
     def get_num_potential_quantized_activations(self) -> int:
         return self._num_potential_quantized_activations
 
-    def _handle_quantizer_merge(self, waiting_pqs: Set[PropagatingQuantizer],
-                                quant_prop_graph: QuantizerPropagationStateGraph,
-                                branching_node_key: str):
+    def _handle_quantizer_merge(
+        self,
+        waiting_pqs: Set[PropagatingQuantizer],
+        quant_prop_graph: QuantizerPropagationStateGraph,
+        branching_node_key: str,
+    ):
         # pylint:disable=too-many-branches
         waiting_pqs_list = list(waiting_pqs)
         merged_pqs = []
         unmerged_pqs = []
         abort_merge = False
         for pq in waiting_pqs_list:
             # While the quantizers were waiting for the merge, one of the concat nodes
@@ -580,27 +641,29 @@
             if sts is TransitionStatus.SHOULD_NOT_TRANSITION:
                 abort_merge = True
         if not abort_merge:
             # All quantizers that are dominated by the current branching node are waiting
             # for the merge - should merge them now
             nncf_logger.debug(f"Merging PQs: {','.join([str(pq.id) for pq in waiting_pqs_list])}")
             qconfs_list = [pq.potential_quant_configs for pq in waiting_pqs_list]
-            merged_qconf_list, branch_qconf_lists = \
-                self.get_merged_qconfigs_for_downward_branching_case(qconfs_list)
+            merged_qconf_list, branch_qconf_lists = self.get_merged_qconfigs_for_downward_branching_case(qconfs_list)
 
-            if merged_qconf_list is None and \
-                    self._propagation_strategy == PropagationStrategy.MERGE_WITH_SINGLE_FQ_RESULT:
-                all_confs = '\n'.join(', '.join([f'[{str(qconf)}]' for qconf in qconfs]) for qconfs in qconfs_list)
-                nncf_logger.debug(f"Could not merge the quantizers at branching point {branching_node_key} - "
-                                  f"no common quantizer configurations found among the following: \n{all_confs}")
-
-            merge_pqs = quant_prop_graph.merge_quantizers_for_branching_node(waiting_pqs_list,
-                                                                             merged_qconf_list,
-                                                                             branch_qconf_lists,
-                                                                             branching_node_key)
+            if (
+                merged_qconf_list is None
+                and self._propagation_strategy == PropagationStrategy.MERGE_WITH_SINGLE_FQ_RESULT
+            ):
+                all_confs = "\n".join(", ".join([f"[{str(qconf)}]" for qconf in qconfs]) for qconfs in qconfs_list)
+                nncf_logger.debug(
+                    f"Could not merge the quantizers at branching point {branching_node_key} - "
+                    f"no common quantizer configurations found among the following: \n{all_confs}"
+                )
+
+            merge_pqs = quant_prop_graph.merge_quantizers_for_branching_node(
+                waiting_pqs_list, merged_qconf_list, branch_qconf_lists, branching_node_key
+            )
             for idx, qconf_list in enumerate(branch_qconf_lists):
                 if qconf_list is None:
                     merged_pqs.append(waiting_pqs_list[idx])
                 else:
                     unmerged_pqs.append(waiting_pqs_list[idx])
         else:
             nncf_logger.debug(f"Merge aborted for PQs {','.join([str(pq.id) for pq in waiting_pqs_list])}")
@@ -619,16 +682,17 @@
             else:
                 self._active_propagating_quantizers_queue.appendleft(pq_from_queue)
 
         if merge_pqs:
             self._active_propagating_quantizers_queue.extendleft(merge_pqs)
         self._quantizers_waiting_for_branch_merge.resolve_merged_node(branching_node_key)
 
-    def propagation_step(self, curr_prop_quantizer: PropagatingQuantizer,
-                         quant_prop_graph: QuantizerPropagationStateGraph) -> QuantizerPropagationStateGraph:
+    def propagation_step(
+        self, curr_prop_quantizer: PropagatingQuantizer, quant_prop_graph: QuantizerPropagationStateGraph
+    ) -> QuantizerPropagationStateGraph:
         """
         Returns an updated curr_prop_quantizer state if the quantizer is not
         yet in its final (accepting) position, and None if the quantizer is in its
         final location.  The location before and after the step should correspond to
         some insertion point.
 
         :param curr_prop_quantizer: The PropagatingQuantizer to currently be propagated.
@@ -643,17 +707,19 @@
         curr_node_type = curr_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
         assert QuantizerPropagationStateGraph.is_insertion_point(curr_node_type)
 
         if curr_prop_quantizer in self._quantizers_waiting_for_branch_merge:
             branching_node_key = self._quantizers_waiting_for_branch_merge.get_blocking_node(curr_prop_quantizer)
             dom_pqs = quant_prop_graph.get_propagating_quantizers_immediately_dominated_by_node(branching_node_key)
             active_dom_pqs = set(
-                filter(lambda x: x in self._active_propagating_quantizers_queue or x is curr_prop_quantizer, dom_pqs))
+                filter(lambda x: x in self._active_propagating_quantizers_queue or x is curr_prop_quantizer, dom_pqs)
+            )
             waiting_pqs = self._quantizers_waiting_for_branch_merge.get_waiting_quantizers_for_branching_node_key(
-                branching_node_key)
+                branching_node_key
+            )
             if waiting_pqs == active_dom_pqs:
                 self._active_propagating_quantizers_queue.append(curr_prop_quantizer)
                 self._handle_quantizer_merge(waiting_pqs, quant_prop_graph, branching_node_key)
             else:
                 # Not all of the dominated quantizers have reached the blocking node yet
                 self._active_propagating_quantizers_queue.appendleft(curr_prop_quantizer)
             return quant_prop_graph
@@ -679,17 +745,19 @@
         surviving_prop_quantizers = []
 
         prop_quantizers_to_process = []
         did_clone = False
 
         # TODO (vshampor): include information on unified scale type in grouping; for now assuming that
         # only concat unified scale groups appear here
-        unified_scale_grouped_paths = \
+        unified_scale_grouped_paths = (
             quant_prop_graph.get_paths_to_immediately_dominating_insertion_points_grouped_by_unified_scales(
-                curr_node_key, self._unified_scales_operation_set)
+                curr_node_key, self._unified_scales_operation_set
+            )
+        )
 
         unified_scale_path_groups_vs_pqs = {k: [] for k in unified_scale_grouped_paths.keys() if k is not None}
         existing_pq_assigned = False
         for gid, path_group in unified_scale_grouped_paths.items():
             for _ in path_group:
                 if existing_pq_assigned:
                     pq = quant_prop_graph.clone_propagating_quantizer(curr_prop_quantizer)
@@ -709,20 +777,20 @@
             for pq in pq_group[1:]:
                 quant_prop_graph.unify_pq_scales(primary_pq, pq)
 
         cloned_prop_quantizers = prop_quantizers_to_process if did_clone else None
 
         pqs_and_paths = zip(paths, prop_quantizers_to_process)
         for path, prop_quantizer in pqs_and_paths:
-            status = self.check_transition_via_path(prop_quantizer, path, quant_prop_graph,
-                                                    cloned_prop_quantizers)
+            status = self.check_transition_via_path(prop_quantizer, path, quant_prop_graph, cloned_prop_quantizers)
             if status == TransitionStatus.SHOULD_NOT_TRANSITION:
                 if did_clone and prop_quantizer is not curr_prop_quantizer:
-                    quant_prop_graph.remove_propagating_quantizer(prop_quantizer,
-                                                                  keep_propagating_quantizer_at_current_node=True)
+                    quant_prop_graph.remove_propagating_quantizer(
+                        prop_quantizer, keep_propagating_quantizer_at_current_node=True
+                    )
                 else:
                     prop_quantizer = quant_prop_graph.backtrack_propagation_until_accepting_location(prop_quantizer)
                     if prop_quantizer is not None:
                         self._finished_propagating_quantizers.append(prop_quantizer)
             elif status == TransitionStatus.SHOULD_TRANSITION:
                 prop_quantizer = quant_prop_graph.propagate_quantizer_via_path(prop_quantizer, path)
                 surviving_prop_quantizers.append(prop_quantizer)
@@ -736,17 +804,17 @@
             elif status == TransitionStatus.SHOULD_WAIT_FOR_MERGE:
                 branching_node_key = None
                 for from_node_key, _ in path:
                     if len(list(quant_prop_graph.successors(from_node_key))) > 1:
                         branching_node_key = path[0][0]
                         break
                 assert branching_node_key is not None
-                # pylint:disable=line-too-long
-                self._quantizers_waiting_for_branch_merge.add_propagating_quantizer_to_wait_on_node_key(prop_quantizer,
-                                                                                                        branching_node_key)
+                self._quantizers_waiting_for_branch_merge.add_propagating_quantizer_to_wait_on_node_key(
+                    prop_quantizer, branching_node_key
+                )
                 surviving_prop_quantizers.append(prop_quantizer)
 
         for prop_quantizer in surviving_prop_quantizers:
             self._active_propagating_quantizers_queue.appendleft(prop_quantizer)
         return quant_prop_graph
 
     def get_allowed_quantizer_configs_for_operator(self, quant_det_id: OperatorMetatype) -> List[QuantizerConfig]:
@@ -755,17 +823,17 @@
         given metatype by HW config or other means.
 
         :param quant_det_id: The metatype of the operation.
         :return: The list of allowed quantizer configurations.
         """
         return self._operator_allowed_qconfigs_map.get(quant_det_id, [])
 
-    def set_allowed_quantization_types_for_operator_nodes(self,
-                                                          quant_prop_graph: QuantizerPropagationStateGraph) -> \
-            QuantizerPropagationStateGraph:
+    def set_allowed_quantization_types_for_operator_nodes(
+        self, quant_prop_graph: QuantizerPropagationStateGraph
+    ) -> QuantizerPropagationStateGraph:
         """
         Marks the operator nodes in the quantizer propagation state graph with
         correct quantization types based on the type of operation, HW config and/or
         other considerations.
 
         :param quant_prop_graph: The quantizer propagation state graph.
         :return: The same quantizer propagation state graph where operations are marked with a corresponding
@@ -782,20 +850,22 @@
                 quant_det_id = node[QuantizerPropagationStateGraph.OPERATOR_METATYPE_NODE_ATTR]
                 if quant_det_id is None:
                     nncf_logger.debug(f"Unknown metatype for operator node: {node_key}")
                     trait = QuantizationTrait.QUANTIZATION_AGNOSTIC
                 elif quant_det_id is UnknownMetatype:
                     trait = QuantizationTrait.NON_QUANTIZABLE
                 else:
-                    trait = self._operator_quantization_trait_map.get(quant_det_id,
-                                                                      QuantizationTrait.QUANTIZATION_AGNOSTIC)
+                    trait = self._operator_quantization_trait_map.get(
+                        quant_det_id, QuantizationTrait.QUANTIZATION_AGNOSTIC
+                    )
                 node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR] = trait
                 if trait == QuantizationTrait.INPUTS_QUANTIZABLE:
-                    node[QuantizerPropagationStateGraph.ALLOWED_INPUT_QUANTIZATION_TYPES_NODE_ATTR] = \
-                        self.get_allowed_quantizer_configs_for_operator(quant_det_id)
+                    node[
+                        QuantizerPropagationStateGraph.ALLOWED_INPUT_QUANTIZATION_TYPES_NODE_ATTR
+                    ] = self.get_allowed_quantizer_configs_for_operator(quant_det_id)
         return quant_prop_graph
 
     def get_operator_quantization_traits_map(self) -> Dict[OperatorMetatype, QuantizationTrait]:
         """
         :return: A mapping of operator metatypes to the quantization traits to be assigned to such operations.
         """
         # TODO (vshampor): ensure that there are no name collisions between ops in different torch subpackages with
@@ -828,17 +898,19 @@
             # namespace (yet) - use default trait
             for default_trait, meta_list in self._default_trait_to_metatype_map.items():
                 if op_meta in meta_list:
                     trait = default_trait
                     break
             else:
                 trait = QuantizationTrait.QUANTIZATION_AGNOSTIC
-                nncf_logger.debug(f"Operation metatype {op_meta} encountered, but it has no default "
-                                  f"quantization trait and the HW config entry is not given for it - "
-                                  f"assuming quantization-agnostic.")
+                nncf_logger.debug(
+                    f"Operation metatype {op_meta} encountered, but it has no default "
+                    f"quantization trait and the HW config entry is not given for it - "
+                    f"assuming quantization-agnostic."
+                )
         else:
             # There IS a valid HW config name for the metatype, but it is deliberately not specified
             # in the config, which means that it should execute in FP32
             trait = QuantizationTrait.NON_QUANTIZABLE
 
         return trait
 
@@ -871,26 +943,26 @@
         """
         out_graph = quant_prop_graph.get_visualized_graph()
         active_ids_str = ", ".join([str(pq.id) for pq in self._active_propagating_quantizers_queue])
         finished_ids_str = ", ".join([str(pq.id) for pq in self._finished_propagating_quantizers])
         next_id_str = ""
         if self._active_propagating_quantizers_queue:
             next_id_str = str(self._active_propagating_quantizers_queue[-1].id)
-        out_graph.graph['graph'] = {
-            "label": "Propagating quantizers: {}\n" \
-                     "Next quantizer to be propagated: {}\n" \
-                     "Finished quantizers: {}".format(active_ids_str,
-                                                      next_id_str,
-                                                      finished_ids_str),
-            "labelloc": "t"}
+        out_graph.graph["graph"] = {
+            "label": "Propagating quantizers: {}\n"
+            "Next quantizer to be propagated: {}\n"
+            "Finished quantizers: {}".format(active_ids_str, next_id_str, finished_ids_str),
+            "labelloc": "t",
+        }
         pth = deepcopy(dump_path)
         write_dot_graph(out_graph, pth)
 
-    def setup_initial_quantizers(self,
-                                 quant_prop_graph: QuantizerPropagationStateGraph) -> QuantizerPropagationStateGraph:
+    def setup_initial_quantizers(
+        self, quant_prop_graph: QuantizerPropagationStateGraph
+    ) -> QuantizerPropagationStateGraph:
         """
         Determines the initial subset of the nodes that must be quantized
         and corresponding allowed quantization configs (possibly multiple) for each
         quantizable operation, and sets up propagating quantizers at initial locations.
 
         :param quant_prop_graph: The quantizer propagation state graph without any
           quantizers registered.
@@ -901,78 +973,95 @@
         for node_key in nx.lexicographical_topological_sort(quant_prop_graph):
             node = quant_prop_graph.nodes[node_key]
             node_type = node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
             if node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:
                 num_input_activations = quant_prop_graph.get_num_input_activations(node_key)
                 self._num_potential_quantized_activations += num_input_activations
                 if node_key in quant_prop_graph.ignored_node_keys:
-                    nncf_logger.info(f"Not adding activation input quantizer for operation: {node_key}")
+                    msg = f"Not adding activation input quantizer for operation: {node_key}"
+                    if quant_prop_graph.ignored_node_keys[node_key] == IgnoreReason.AUTOGENERATED:
+                        nncf_logger.debug(msg)
+                    else:
+                        nncf_logger.info(msg)
                     continue
                 self._setup_initial_quantizers_for_operator_node(node_key, quant_prop_graph)
 
         if self._additional_unified_scale_op_scopes is not None:
             # Link the prop quantizers according to specification in NNCF config
             occupied_insertion_points_vs_pqs = {}  # type: Dict[TargetPoint, PropagatingQuantizer]
             for pq in self._active_propagating_quantizers_queue:
                 ip_node_key = pq.current_location_node_key
                 ip_node = quant_prop_graph.nodes[ip_node_key]
                 assert QuantizerPropagationStateGraph.is_insertion_point(
-                    ip_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR])
+                    ip_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
+                )
                 ip = ip_node[QuantizerPropagationStateGraph.QUANT_INSERTION_POINT_DATA_NODE_ATTR]
                 occupied_insertion_points_vs_pqs[ip] = pq
-            coalesced_ips = self.coalesce_insertion_points(list(occupied_insertion_points_vs_pqs.keys()),
-                                                           self._additional_unified_scale_op_scopes)
+            coalesced_ips = self.coalesce_insertion_points(
+                list(occupied_insertion_points_vs_pqs.keys()), self._additional_unified_scale_op_scopes
+            )
             for linked_ip_group in coalesced_ips:
                 if len(linked_ip_group) <= 2:
                     continue
                 main_ip = linked_ip_group[0]
                 main_pq = occupied_insertion_points_vs_pqs[main_ip]
 
                 for ip in linked_ip_group[1:]:
                     pq = occupied_insertion_points_vs_pqs[ip]
                     quant_prop_graph.unify_pq_scales(main_pq, pq)
 
         return quant_prop_graph
 
     @staticmethod
-    def coalesce_insertion_points(target_insertion_points: List[TargetPoint],
-                                  linked_scopes_groups_list: List[List[str]]) -> List[List[TargetPoint]]:
+    def coalesce_insertion_points(
+        target_insertion_points: List[TargetPoint], linked_scopes_groups_list: List[List[str]]
+    ) -> List[List[TargetPoint]]:
         """
         Accepts a list of TargetPoints and groups these according to linked_scope_groups_list.
         The matching of a TargetPoint to the string entries in the lists is made based on the
         string representation of TargetPoint's target_node_name attribute.
 
         :param target_insertion_points: A list of TargetPoint objects to be grouped based on the
           `linked_scopes_groups_list`
         :param linked_scopes_groups_list: A list of string lists, where each list defines a desired grouping
           of TargetPoints, and each string of the list is matched against the string representation of
           corresponding TargetPoints.
         :return: A list of TargetPoint groups; each group is a list of TargetPoint's.
         """
         # pylint:disable=too-many-branches
         if linked_scopes_groups_list is None:
-            return [[ip, ] for ip in target_insertion_points]
+            return [
+                [
+                    ip,
+                ]
+                for ip in target_insertion_points
+            ]
         retval = []
         insertion_point_indices_vs_group_id = OrderedDict()
 
         for group_idx, group_list in enumerate(linked_scopes_groups_list):
             for group_member_node_name in group_list:
                 matching_indices = list(
-                    filter(lambda x: target_insertion_points[x].target_node_name == group_member_node_name,
-                           range(len(target_insertion_points))))
+                    filter(
+                        lambda x: target_insertion_points[x].target_node_name == group_member_node_name,
+                        range(len(target_insertion_points)),
+                    )
+                )
                 if len(matching_indices) == 0:
-                    raise RuntimeError("No match for linked quantizer entry {} among activation quantizers!".format(
-                        group_member_node_name))
+                    raise RuntimeError(
+                        "No match for linked quantizer entry {} among activation quantizers!".format(
+                            group_member_node_name
+                        )
+                    )
 
                 for target_idx in matching_indices:
                     if target_idx in insertion_point_indices_vs_group_id:
                         raise RuntimeError(
                             "Linked activation quantizer groups {} and {} "
-                            "overlap!".format(group_idx,
-                                              insertion_point_indices_vs_group_id[target_idx])
+                            "overlap!".format(group_idx, insertion_point_indices_vs_group_id[target_idx])
                         )
                 for target_idx in matching_indices:
                     insertion_point_indices_vs_group_id[target_idx] = group_idx
 
         for i in range(len(target_insertion_points)):
             if i not in insertion_point_indices_vs_group_id:
                 insertion_point_indices_vs_group_id[i] = None
@@ -981,27 +1070,34 @@
         for insertion_point_idx, group_idx in insertion_point_indices_vs_group_id.items():
             if group_idx is not None:
                 group_indices_list[group_idx].append(insertion_point_idx)
 
         for intra_group_indices in group_indices_list:
             main_ip_idx = intra_group_indices[0]
             main_ip = target_insertion_points[main_ip_idx]
-            grouped_list = [main_ip, ]
+            grouped_list = [
+                main_ip,
+            ]
             for linked_ip_idx in intra_group_indices[1:]:
                 grouped_list.append(target_insertion_points[linked_ip_idx])
             retval.append(grouped_list)
 
         for insertion_point_idx, group_idx in insertion_point_indices_vs_group_id.items():
             if group_idx is None:
-                retval.append([target_insertion_points[insertion_point_idx], ])
+                retval.append(
+                    [
+                        target_insertion_points[insertion_point_idx],
+                    ]
+                )
 
         return retval
 
-    def _filter_qconfigs_according_to_scope(self, qconf_list: List[QuantizerConfig],
-                                            nncf_node_name: NNCFNodeName) -> List[QuantizerConfig]:
+    def _filter_qconfigs_according_to_scope(
+        self, qconf_list: List[QuantizerConfig], nncf_node_name: NNCFNodeName
+    ) -> List[QuantizerConfig]:
         if self._global_constraints is not None:
             local_constraints = self._global_constraints[QuantizerGroup.ACTIVATIONS]
         else:
             local_constraints = QuantizationConstraints()
 
         act_scope_overrides = self._scope_overrides.get("activations", {})
         for overridden_scope, scoped_override_dict in act_scope_overrides.items():
@@ -1019,33 +1115,35 @@
                 err_msg += nncf_node_name
                 raise RuntimeError(err_msg) from e
         else:
             constrained_config_list = [local_constraints.apply_constraints_to(qconfig) for qconfig in qconf_list]
 
         return constrained_config_list
 
-    def _setup_initial_quantizers_for_operator_node(self, operator_node_key: str,
-                                                    quant_prop_graph: QuantizerPropagationStateGraph):
+    def _setup_initial_quantizers_for_operator_node(
+        self, operator_node_key: str, quant_prop_graph: QuantizerPropagationStateGraph
+    ):
         # pylint:disable=too-many-branches
         node = quant_prop_graph.nodes[operator_node_key]
 
         # preds are in sorted order for reproducibility
         preds = list(sorted(quant_prop_graph.predecessors(operator_node_key)))
 
         if not preds:
             return  # TODO (vshampor): remove this once module insertion points are included in the IP graph
 
         metatype = node[QuantizerPropagationStateGraph.OPERATOR_METATYPE_NODE_ATTR]
         if not self._quantize_outputs and metatype in OUTPUT_NOOP_METATYPES:
             return
         # No need to place quantizers for FP32-forced ops, naturally
-        if node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR] in \
-                [QuantizationTrait.NON_QUANTIZABLE,
-                 QuantizationTrait.QUANTIZATION_AGNOSTIC,
-                 QuantizationTrait.CONCAT] and metatype not in OUTPUT_NOOP_METATYPES:
+        if (
+            node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]
+            in [QuantizationTrait.NON_QUANTIZABLE, QuantizationTrait.QUANTIZATION_AGNOSTIC, QuantizationTrait.CONCAT]
+            and metatype not in OUTPUT_NOOP_METATYPES
+        ):
             return
         quant_det_id = node[QuantizerPropagationStateGraph.OPERATOR_METATYPE_NODE_ATTR]
         qconf_list = self.get_allowed_quantizer_configs_for_operator(quant_det_id)
         if quant_det_id in OUTPUT_NOOP_METATYPES:
             qconf_list = deepcopy(self.default_global_qconfig_list)
         assert qconf_list is not None
 
@@ -1070,66 +1168,84 @@
             per_tensor_qconf_list = list(filter(lambda x: x.per_channel is False, qconf_list))
             op_meta_name = quant_det_id.__class__.__name__
             if len(per_tensor_qconf_list) != len(qconf_list):
                 if not per_tensor_qconf_list:
                     raise RuntimeError(
                         "Unified scales currently do not support per-channel configuration - dropping"
                         "per-channel configuration options for {} resulted in no valid quantization "
-                        "configs!".format(op_meta_name))
-                nncf_logger.warning(f"Unified scales currently do not support per-channel configuration - dropping"
-                                    f"per-channel configuration options for {op_meta_name}")
+                        "configs!".format(op_meta_name)
+                    )
+                nncf_logger.warning(
+                    f"Unified scales currently do not support per-channel configuration - dropping"
+                    f"per-channel configuration options for {op_meta_name}"
+                )
                 qconf_list = per_tensor_qconf_list
 
         pred_ip_key_vs_qconf_dict = OrderedDict()
         # Should be immediately preceded by insertion points (pre-hook)
         for pred_ip_key in preds:
             pred_node = quant_prop_graph.nodes[pred_ip_key]
             pred_node_type = pred_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
-            assert QuantizerPropagationStateGraph.is_insertion_point(pred_node_type), \
-                "Invalid insertion point graph supplied for quantizer propagation!"
+            assert QuantizerPropagationStateGraph.is_insertion_point(
+                pred_node_type
+            ), "Invalid insertion point graph supplied for quantizer propagation!"
+
+            ip = pred_node[QuantizerPropagationStateGraph.QUANT_INSERTION_POINT_DATA_NODE_ATTR]
+            input_port_id = ip.input_port_id
+            if input_port_id in metatype.ignored_input_ports:
+                continue
 
             edge = quant_prop_graph.edges[pred_ip_key, operator_node_key]
             if not edge[QuantizerPropagationStateGraph.IS_INTEGER_PATH_EDGE_ATTR]:
                 pred_ip_key_vs_qconf_dict[pred_ip_key] = qconf_list
             else:
-                nncf_logger.debug(f"Detected integer input {pred_ip_key} - won't set up "
-                                  f"a propagating quantizer for it")
+                nncf_logger.debug(
+                    f"Detected integer input {pred_ip_key} - won't set up " f"a propagating quantizer for it"
+                )
 
         if not pred_ip_key_vs_qconf_dict:
             # All inputs to the operator were integer
             return
 
         # Cloning a single propagating quantizer onto all node inputs - revise if separate
         # quantizer configuration for different inputs is required
         pred_ip_key_vs_qconf_list = list(iter(pred_ip_key_vs_qconf_dict.items()))
         main_pq_ip_key, main_pq_qconf_list = pred_ip_key_vs_qconf_list[0]
         main_prop_quantizer = quant_prop_graph.add_propagating_quantizer(
-            main_pq_qconf_list, main_pq_ip_key,
-            unified_scale_type=UnifiedScaleType.UNIFY_ALWAYS if is_unified_scale else None)
+            main_pq_qconf_list,
+            main_pq_ip_key,
+            unified_scale_type=UnifiedScaleType.UNIFY_ALWAYS if is_unified_scale else None,
+        )
         main_prop_quantizer.last_accepting_location_node_key = main_pq_ip_key
         self._active_propagating_quantizers_queue.appendleft(main_prop_quantizer)
 
         main_pq_gid = None
 
         if is_unified_scale:
             main_pq_gid = quant_prop_graph.get_unified_scale_group_id_by_propagating_quantizer_id(
-                main_prop_quantizer.id)
+                main_prop_quantizer.id
+            )
 
         for additional_pq_ip_key, _ in pred_ip_key_vs_qconf_list[1:]:
             additional_pq = quant_prop_graph.add_propagating_quantizer(
-                main_pq_qconf_list, additional_pq_ip_key,
+                main_pq_qconf_list,
+                additional_pq_ip_key,
                 unified_scale_type=UnifiedScaleType.UNIFY_ALWAYS if is_unified_scale else None,
-                unified_scale_group_id_override=main_pq_gid)
+                unified_scale_group_id_override=main_pq_gid,
+            )
             additional_pq.last_accepting_location_node_key = additional_pq_ip_key
             self._active_propagating_quantizers_queue.appendleft(additional_pq)
 
     # pylint:disable=too-many-return-statements
-    def check_branching_transition(self, quant_prop_graph: QuantizerPropagationStateGraph,
-                                   prop_quant_to_transition: PropagatingQuantizer,
-                                   branching_node_key: str) -> TransitionStatus:
+    def check_branching_transition(
+        self,
+        quant_prop_graph: QuantizerPropagationStateGraph,
+        prop_quant_to_transition: PropagatingQuantizer,
+        branching_node_key: str,
+    ) -> TransitionStatus:
         """
         If a propagating quantizer advances through a node that branches
         downwards, the branches neighbouring to the one that the propagating quantizer
         had just propagated from will have the precision of the quantizer imposed upon
         them.  This is not always desirable - we might want to keep some branches in
         higher precision than the others. For this reason, this function checks whether
         the quantizer may safely advance through a branching node based on the possible
@@ -1139,81 +1255,88 @@
         :param prop_quant_to_transition: The propagating quantizer that is about to transition
           upwards through a branching node.
         :param branching_node_key: The node key in `quant_prop_graph` corresponding to the node
           that branches downwards.
         :return: The TransitionStatus indicating in which fashion the transition should occur.
         """
         dom_op_node_keys = quant_prop_graph.get_non_quant_agnostic_op_nodes_immediately_dominated_by_node(
-            branching_node_key)
+            branching_node_key
+        )
         dom_op_quantizers = set()
         for op_node_key in dom_op_node_keys:
             op_node = quant_prop_graph.nodes[op_node_key]
             trait = op_node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]
-            affecting_prop_quantizers = op_node[
-                QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
+            affecting_prop_quantizers = op_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
             if affecting_prop_quantizers:
                 for aff_pq in affecting_prop_quantizers:
                     dom_op_quantizers.add(aff_pq)
             else:
                 if trait is not QuantizationTrait.CONCAT:
                     # The branch op is forced to be FP32 - should not proceed through the branch node.
                     return TransitionStatus.SHOULD_NOT_TRANSITION
 
                 # Have to determine if the concat node will potentially have input quantization applied
                 # as a result of further propagation.
                 pqs_dominated_by_cat = quant_prop_graph.get_propagating_quantizers_immediately_dominated_by_node(
-                    op_node_key)
+                    op_node_key
+                )
                 active_pqs_dominated_by_cat = set(
-                    filter(lambda x: x in self._active_propagating_quantizers_queue,
-                           pqs_dominated_by_cat))
+                    filter(lambda x: x in self._active_propagating_quantizers_queue, pqs_dominated_by_cat)
+                )
                 if not active_pqs_dominated_by_cat:
                     # There is no chance for this concat node to be quantized later,
                     # should not attempt merge.
                     return TransitionStatus.SHOULD_NOT_TRANSITION
                 # There are still some quantizers that may propagate upwards through this concat node
                 # and ultimately lead to the concat node having quantized inputs
                 dom_op_quantizers.update(active_pqs_dominated_by_cat)
 
         dom_op_quantizers.discard(prop_quant_to_transition)
         if dom_op_quantizers:
             return TransitionStatus.SHOULD_WAIT_FOR_MERGE
 
         return TransitionStatus.SHOULD_TRANSITION
 
-    def _check_affecting_quantizers_in_common_path(self,
-                                                   affecting_quantizers: List[PropagatingQuantizer],
-                                                   cloned_prop_quantizers: List[PropagatingQuantizer]):
+    def _check_affecting_quantizers_in_common_path(
+        self, affecting_quantizers: List[PropagatingQuantizer], cloned_prop_quantizers: List[PropagatingQuantizer]
+    ):
         # Handling the case where multiple freshly cloned quantizers have to follow paths that are different,
         # but have a common edge or node
         safe_affecting_quantizers = [pq for pq in affecting_quantizers if pq in cloned_prop_quantizers]
         assert safe_affecting_quantizers == affecting_quantizers
 
-    def _check_for_affecting_quantizer_conflicts(self,
-                                                 curr_prop_quantizer: PropagatingQuantizer,
-                                                 affecting_quantizers: List[PropagatingQuantizer],
-                                                 cloned_prop_quantizers: Optional[List[PropagatingQuantizer]]
-                                                 ) -> Optional[TransitionStatus]:
+    def _check_for_affecting_quantizer_conflicts(
+        self,
+        curr_prop_quantizer: PropagatingQuantizer,
+        affecting_quantizers: List[PropagatingQuantizer],
+        cloned_prop_quantizers: Optional[List[PropagatingQuantizer]],
+    ) -> Optional[TransitionStatus]:
         if cloned_prop_quantizers is not None:
             self._check_affecting_quantizers_in_common_path(affecting_quantizers, cloned_prop_quantizers)
             return None
 
         # Affecting quantizers should have the same configs by construction, so we only
         # check the first
         curr_pq_configs = curr_prop_quantizer.potential_quant_configs
         target_pq_configs = affecting_quantizers[0].potential_quant_configs
-        if curr_pq_configs == target_pq_configs or \
-                HWConfig.is_wildcard_quantization(curr_pq_configs) or \
-                HWConfig.is_wildcard_quantization(target_pq_configs):
+        if (
+            curr_pq_configs == target_pq_configs
+            or HWConfig.is_wildcard_quantization(curr_pq_configs)
+            or HWConfig.is_wildcard_quantization(target_pq_configs)
+        ):
             return TransitionStatus.SHOULD_MERGE
         return TransitionStatus.SHOULD_NOT_TRANSITION
 
-    def check_transition_via_path(self, prop_quantizer: PropagatingQuantizer, path: PropagationPath,
-                                  quant_prop_graph: QuantizerPropagationStateGraph,
-                                  cloned_prop_quantizers: Optional[
-                                      List[PropagatingQuantizer]] = None) -> TransitionStatus:
+    def check_transition_via_path(
+        self,
+        prop_quantizer: PropagatingQuantizer,
+        path: PropagationPath,
+        quant_prop_graph: QuantizerPropagationStateGraph,
+        cloned_prop_quantizers: Optional[List[PropagatingQuantizer]] = None,
+    ) -> TransitionStatus:
         """
         Determines which action should be taken regarding the
         prop_quantizer's propagation via path, which may be one of many possible
         propagation paths.
 
         :param prop_quantizer: The propagating quantizer to be currently considered.
         :param path: The path, defined in terms of `quant_prop_graph` edges, along which the
@@ -1229,43 +1352,41 @@
 
             from_node_type = from_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
             if from_node_type == QuantizerPropagationStateGraphNodeType.OPERATOR:
                 trait = from_node[QuantizerPropagationStateGraph.QUANTIZATION_TRAIT_NODE_ATTR]
                 if trait is QuantizationTrait.OUTPUT_QUANTIZATION_AS_WEIGHTS:
                     quant_prop_graph.mark_act_quantizer_as_dependent_on_weights(prop_quantizer, from_node_key)
                     return TransitionStatus.SHOULD_NOT_TRANSITION
-                if trait in [QuantizationTrait.NON_QUANTIZABLE,
-                             QuantizationTrait.INPUTS_QUANTIZABLE]:
+                if trait in [QuantizationTrait.NON_QUANTIZABLE, QuantizationTrait.INPUTS_QUANTIZABLE]:
                     return TransitionStatus.SHOULD_NOT_TRANSITION
 
             edge = quant_prop_graph.edges[from_node_key, to_node_key]
             # Check if current edge to traverse corresponds to integer-valued tensors such as indices
             if edge[QuantizerPropagationStateGraph.IS_INTEGER_PATH_EDGE_ATTR]:
                 return TransitionStatus.SHOULD_NOT_TRANSITION
 
             # Check if current edge to traverse is affected by any of the quantizers
             potential_quantizers = edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
             if potential_quantizers:
-                sts = self._check_for_affecting_quantizer_conflicts(prop_quantizer,
-                                                                    potential_quantizers,
-                                                                    cloned_prop_quantizers)
+                sts = self._check_for_affecting_quantizer_conflicts(
+                    prop_quantizer, potential_quantizers, cloned_prop_quantizers
+                )
 
                 if sts is not None:
                     return sts
 
             # Check if the target node is affected by any of the quantizers
             from_node_type = from_node[QuantizerPropagationStateGraph.NODE_TYPE_NODE_ATTR]
             if QuantizerPropagationStateGraph.is_insertion_point(from_node_type):
                 potential_quantizers = from_node[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR]
                 if potential_quantizers:
-                    sts = self._check_for_affecting_quantizer_conflicts(prop_quantizer,
-                                                                        potential_quantizers,
-                                                                        cloned_prop_quantizers)
+                    sts = self._check_for_affecting_quantizer_conflicts(
+                        prop_quantizer, potential_quantizers, cloned_prop_quantizers
+                    )
                     if sts == TransitionStatus.SHOULD_NOT_TRANSITION:
-
                         # Did not merge - the edge will remain untraversed, but the quantizers at the next node will
                         # still be affecting it
                         for pq in potential_quantizers:
                             pq.affected_edges.add((from_node_key, to_node_key))
                             edge[QuantizerPropagationStateGraph.AFFECTING_PROPAGATING_QUANTIZERS_ATTR].append(pq)
 
                         return sts
@@ -1276,26 +1397,23 @@
             if len(list(quant_prop_graph.successors(from_node_key))) > 1:
                 # If a quantizer simply passes up through a downward-branching node, it may spoil the
                 # precision for operations on neighbouring branches. Consider a 4-bit quantizer rising
                 # through a branch node and an 8-bit quantizer arriving at the same node later. Therefore,
                 # prior to allowing the quantizer to pass through a branching node we need to ensure that
                 # the precision of the quantizer is a superset of precisions of the first non-quantization agnostic
                 # operations on each branch.
-                status = self.check_branching_transition(quant_prop_graph, prop_quantizer,
-                                                         from_node_key)
+                status = self.check_branching_transition(quant_prop_graph, prop_quantizer, from_node_key)
                 if status is TransitionStatus.SHOULD_NOT_TRANSITION or status is TransitionStatus.SHOULD_WAIT_FOR_MERGE:
                     return status
 
         return TransitionStatus.SHOULD_TRANSITION
 
-    def get_merged_qconfigs_for_downward_branching_case(self,
-                                                        potential_qconfigs_for_each_branch: List[
-                                                            List[Optional[QuantizerConfig]]]) -> \
-            Tuple[Optional[List[QuantizerConfig]],
-                  List[Optional[List[QuantizerConfig]]]]:
+    def get_merged_qconfigs_for_downward_branching_case(
+        self, potential_qconfigs_for_each_branch: List[List[Optional[QuantizerConfig]]]
+    ) -> Tuple[Optional[List[QuantizerConfig]], List[Optional[List[QuantizerConfig]]]]:
         """
         Returns a tuple, of which the first node is the qconfig list for the quantizer to be placed
         above the branching node (i.e. that will affect all of the downward branches), and a list
         of nodes which are either None (which means that the corresponding branch quantizer has been successfully
         merged, or qconfigs list to be set for the corresponding branch quantizer if it cannot be merged (e.g. if
         requantization to a lower bitwidth has to be done for this branch)
 
@@ -1325,25 +1443,23 @@
         qconfigs_union = set()
         for branch_qconfig_list in potential_qconfigs_for_each_branch:
             qconfigs_union.update(set(branch_qconfig_list))
         merged_qconfig_list = []
 
         nncf_logger.debug(f"Union of configs: {';'.join([str(qc) for qc in qconfigs_union])}")
 
-        def compatible_with_requant(qconf: QuantizerConfig,
-                                    other_qconf_list: List[QuantizerConfig]) -> bool:
+        def compatible_with_requant(qconf: QuantizerConfig, other_qconf_list: List[QuantizerConfig]) -> bool:
             if qconf in other_qconf_list:
                 return True
             for other_qconf in other_qconf_list:
                 if not other_qconf.is_valid_requantization_for(qconf):
                     return False
             return True
 
-        def compatible_wo_requant(qconf: QuantizerConfig,
-                                  other_qconf_list: List[QuantizerConfig]) -> bool:
+        def compatible_wo_requant(qconf: QuantizerConfig, other_qconf_list: List[QuantizerConfig]) -> bool:
             if qconf in other_qconf_list:
                 return True
             return False
 
         if self._propagation_strategy == PropagationStrategy.MERGE_WITH_POTENTIAL_REQUANTIZATION:
             compatible_fn = compatible_with_requant
         elif self._propagation_strategy == PropagationStrategy.MERGE_WITH_SINGLE_FQ_RESULT:
@@ -1359,20 +1475,21 @@
 
         if not merged_qconfig_list:
             # Impossible to produce a merged configuration space of any kind, won't merge
             return None, potential_qconfigs_for_each_branch
 
         # Sort the merged list according to an ad-hoc-calculated priority
         qconfig_and_priority_list = self.__assign_priorities_to_configs_in_merged_list(
-            merged_qconfig_list,
-            potential_qconfigs_for_each_branch)
+            merged_qconfig_list, potential_qconfigs_for_each_branch
+        )
 
         qconfig_and_priority_list_sorted_by_priority = sorted(qconfig_and_priority_list, key=lambda x: x[1])
-        config_list_to_print = ";".join([str(qc_tup[1]) + ':' + str(qc_tup[0]) for qc_tup in
-                                         qconfig_and_priority_list_sorted_by_priority])
+        config_list_to_print = ";".join(
+            [str(qc_tup[1]) + ":" + str(qc_tup[0]) for qc_tup in qconfig_and_priority_list_sorted_by_priority]
+        )
         nncf_logger.debug(f"Priority-sorted merge qconfigs: {config_list_to_print}")
 
         merged_qconfig_list = self.__disambiguate_config_list(qconfig_and_priority_list_sorted_by_priority)
         nncf_logger.debug(f"Disambiguated merge qconfig list: {';'.join([str(qc) for qc in merged_qconfig_list])}")
 
         merged_qconfig_list_counter = Counter(merged_qconfig_list)
         resulting_branch_qconfig_lists = [None for _ in potential_qconfigs_for_each_branch]
@@ -1381,17 +1498,19 @@
             for idx, branch_qconfig_list in enumerate(potential_qconfigs_for_each_branch):
                 if Counter(branch_qconfig_list) == merged_qconfig_list_counter:
                     continue  # This branch will have the branch quantizer removed
                 resulting_branch_qconfig_lists[idx] = branch_qconfig_list
 
         return merged_qconfig_list, resulting_branch_qconfig_lists
 
-    def __assign_priorities_to_configs_in_merged_list(self, merged_qconfig_list: List[QuantizerConfig],
-                                                      potential_qconfigs_for_each_branch: List[
-                                                          List[QuantizerConfig]]) -> List[Tuple[QuantizerConfig, int]]:
+    def __assign_priorities_to_configs_in_merged_list(
+        self,
+        merged_qconfig_list: List[QuantizerConfig],
+        potential_qconfigs_for_each_branch: List[List[QuantizerConfig]],
+    ) -> List[Tuple[QuantizerConfig, int]]:
         # Basically, the original branches vote on a priority of a config in the merged
         # qconfig list based on the position of said qconfig in their own qconfig list.
         # This still does not properly disambiguate configs in all situations. Downstream code
         # takes 0-th config in the list as the final config file. Without an external, unambiguous
         # priority mechanism or manual config selection there is no way to do a consistent, branch order-independent
         # merge.
         qconfig_and_priority_list = []  # type: List[Tuple[QuantizerConfig, int]]
@@ -1404,41 +1523,46 @@
                 except ValueError:
                     # Move the configs that inevitably lead to requantization closer to the end of the list
                     idx = max_original_list_len + 1
                 priority += idx
             qconfig_and_priority_list.append((merged_qconfig, priority))
         return qconfig_and_priority_list
 
-    def __disambiguate_config_list(self, qconfig_list_with_priority: List[Tuple[QuantizerConfig, int]]) -> \
-            List[QuantizerConfig]:
+    def __disambiguate_config_list(
+        self, qconfig_list_with_priority: List[Tuple[QuantizerConfig, int]]
+    ) -> List[QuantizerConfig]:
         """
         The input list should be sorted in descending order of priority. In case some qconfigs in the list have the
         same priority, this function will resolve the ambiguity in ordering these qconfigs in the final returned
         list.
         """
 
         class QConfigComparator:
             def __init__(self, qconfig: QuantizerConfig):
                 self.qconfig = qconfig
 
-            def __lt__(self, other: 'QConfigComparator'):
+            def __lt__(self, other: "QConfigComparator"):
                 # Prefer higher bitwidths, per-tensor, symmetrical
                 if self.qconfig.num_bits > other.qconfig.num_bits:
                     return True
                 if self.qconfig.num_bits < other.qconfig.num_bits:
                     return False
                 if self.qconfig.per_channel is False and other.qconfig.per_channel is True:
                     return True
                 if self.qconfig.per_channel is True and other.qconfig.per_channel is False:
                     return False
-                if self.qconfig.mode is QuantizationMode.SYMMETRIC and other.qconfig.mode is \
-                        QuantizationMode.ASYMMETRIC:
+                if (
+                    self.qconfig.mode is QuantizationMode.SYMMETRIC
+                    and other.qconfig.mode is QuantizationMode.ASYMMETRIC
+                ):
                     return True
-                if self.qconfig.mode is QuantizationMode.ASYMMETRIC and other.qconfig.mode is \
-                        QuantizationMode.SYMMETRIC:
+                if (
+                    self.qconfig.mode is QuantizationMode.ASYMMETRIC
+                    and other.qconfig.mode is QuantizationMode.SYMMETRIC
+                ):
                     return False
                 return False
 
         slices_to_sort = []
 
         if len(qconfig_list_with_priority) > 1:
             curr_priority_start_idx = 0
@@ -1472,27 +1596,29 @@
         :return: The queue of propagating quantizers that are still propagating.
         """
         return self._active_propagating_quantizers_queue
 
     def get_total_quantizer_count(self):
         return len(self.get_finished_propagating_quantizers()) + len(self.get_active_propagating_quantizers_queue())
 
-    def _filter_integer_input_quantizers(self,
-                                         quant_prop_graph: QuantizerPropagationStateGraph) -> \
-            QuantizerPropagationStateGraph:
+    def _filter_integer_input_quantizers(
+        self, quant_prop_graph: QuantizerPropagationStateGraph
+    ) -> QuantizerPropagationStateGraph:
         input_node_vs_qid_dict = quant_prop_graph.get_quantizers_at_input_nncf_nodes()
         integer_input_quantizer_ids = set()
 
         for input_node, input_quantizer_ids in input_node_vs_qid_dict.items():
             assert input_node.metatype in INPUT_NOOP_METATYPES
             if input_node.is_integer_input():
                 integer_input_quantizer_ids.update(set(input_quantizer_ids))
 
-        filtered_finished_pqs = list(filter(lambda pq: pq.id not in integer_input_quantizer_ids,
-                                            self._finished_propagating_quantizers))
-        integer_input_pqs = list(filter(lambda pq: pq.id in integer_input_quantizer_ids,
-                                        self._finished_propagating_quantizers))
+        filtered_finished_pqs = list(
+            filter(lambda pq: pq.id not in integer_input_quantizer_ids, self._finished_propagating_quantizers)
+        )
+        integer_input_pqs = list(
+            filter(lambda pq: pq.id in integer_input_quantizer_ids, self._finished_propagating_quantizers)
+        )
         self._finished_propagating_quantizers = filtered_finished_pqs
         for integer_input_pq in integer_input_pqs:
             quant_prop_graph.remove_propagating_quantizer(integer_input_pq)
 
         return quant_prop_graph
```

### Comparing `nncf-2.4.0/nncf/common/quantization/quantizer_propagation/structs.py` & `nncf-2.5.0/nncf/common/quantization/quantizer_propagation/structs.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,25 +1,20 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from enum import Enum
-from typing import List
-from typing import Optional
-from typing import Set
-from typing import Tuple
+from typing import List, Optional, Set, Tuple
 
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import UnifiedScaleType
 
 
 class QuantizationTrait(Enum):
     """
@@ -50,16 +45,21 @@
     graph that this quantizer affects. It should be moved against the data flow of
     the model, tracking the affected nodes and edges of
     QuantizerPropagationStateGraph. No actual quantization modules are used here,
     only the associated configs (such as bitwidths, modes, signed/unsigned
     attributes etc.)
     """
 
-    def __init__(self, id_: int, quant_configs: List[QuantizerConfig], init_location_node_key: str,
-                 unified_scale_type: Optional[UnifiedScaleType] = None):
+    def __init__(
+        self,
+        id_: int,
+        quant_configs: List[QuantizerConfig],
+        init_location_node_key: str,
+        unified_scale_type: Optional[UnifiedScaleType] = None,
+    ):
         """
         :param id_: The unique identifier of the new propagating quantizer.
         :param quant_configs: The quantizer configurations that this quantizer currently allows.
         :param init_location_node_key: The node key in QuantizerPropagationStateGraph that this
           quantizer is being inserted into.
         :param unified_scale_type: The type of unified scales for this quantizer - if unspecified,
           this quantizer won't require unified scales.
@@ -87,18 +87,24 @@
     PRE_HOOK = 0
     POST_HOOK = 1
     OPERATOR = 2
     AUXILIARY_BARRIER = 3
 
 
 class SharedAffectedOpsPropagatingQuantizerGroup:
-    """ Combines propagating quantizers that share affected operations """
+    """Combines propagating quantizers that share affected operations"""
+
     def __init__(self, affecting_prop_quants: Set[PropagatingQuantizer], affected_op_node_keys: Set[str]):
         self.affecting_prop_quants = affecting_prop_quants  # type: Set[PropagatingQuantizer]
         self.affected_op_node_keys = affected_op_node_keys  # type: Set[str]
 
-    def update(self, other: 'SharedAffectedOpsPropagatingQuantizerGroup'):
+    def update(self, other: "SharedAffectedOpsPropagatingQuantizerGroup"):
         self.affected_op_node_keys.update(other.affected_op_node_keys)
         self.affecting_prop_quants.update(other.affecting_prop_quants)
 
 
+class IgnoreReason(Enum):
+    USER_REQUESTED = 0
+    AUTOGENERATED = 1
+
+
 PropagationPath = List[Tuple[str, str]]
```

### Comparing `nncf-2.4.0/nncf/common/quantization/quantizer_propagation/visualizer.py` & `nncf-2.5.0/nncf/common/quantization/quantizer_propagation/visualizer.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,33 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import shutil
 from pathlib import Path
 
 from nncf.common.quantization.quantizer_propagation.graph import QuantizerPropagationStateGraph
 from nncf.common.quantization.quantizer_propagation.solver import QuantizerPropagationSolver
 
 
 class QuantizerPropagationVisualizer:
     """
     An object performing visualization of the quantizer propagation algorithm's state into a chosen directory.
     """
+
     def __init__(self, dump_dir: str = None):
         self.dump_dir = Path(dump_dir)
         if self.dump_dir.exists():
             shutil.rmtree(str(self.dump_dir))
 
-    def visualize_quantizer_propagation(self,
-                                        prop_solver: QuantizerPropagationSolver,
-                                        prop_graph: QuantizerPropagationStateGraph,
-                                        iteration: str):
+    def visualize_quantizer_propagation(
+        self, prop_solver: QuantizerPropagationSolver, prop_graph: QuantizerPropagationStateGraph, iteration: str
+    ):
         self.dump_dir.mkdir(parents=True, exist_ok=True)
         fname = "quant_prop_iter_{}.dot".format(iteration)
-        prop_solver.debug_visualize(prop_graph,
-                                    str(self.dump_dir / Path(fname)))
+        prop_solver.debug_visualize(prop_graph, str(self.dump_dir / Path(fname)))
```

### Comparing `nncf-2.4.0/nncf/common/quantization/quantizer_setup.py` & `nncf-2.5.0/nncf/common/quantization/quantizer_setup.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,53 +1,47 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import ABC
 from collections import Counter
 from copy import deepcopy
 from enum import Enum
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Set
+from typing import Any, Dict, List, Optional, Set
 
 from nncf.common.graph import NNCFNodeName
+from nncf.common.logging import nncf_logger
 from nncf.common.quantization.structs import NonWeightQuantizerId
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import UnifiedScaleType
 from nncf.common.quantization.structs import WeightQuantizerId
 from nncf.common.stateful_classes_registry import CommonStatefulClassesRegistry
-from nncf.common.logging import nncf_logger
 
 QuantizationPointId = int
 
-DEFAULT_QUANTIZER_CONFIG = QuantizerConfig(num_bits=8,
-                                           mode=QuantizationMode.SYMMETRIC,
-                                           signedness_to_force=None,
-                                           per_channel=False)
+DEFAULT_QUANTIZER_CONFIG = QuantizerConfig(
+    num_bits=8, mode=QuantizationMode.SYMMETRIC, signedness_to_force=None, per_channel=False
+)
+
 
 class QuantizationPointType(Enum):
     WEIGHT_QUANTIZATION = 0
     ACTIVATION_QUANTIZATION = 1
 
 
 class QIPointStateNames:
-    TARGET_NODE_NAME = 'target_node_name'
+    TARGET_NODE_NAME = "target_node_name"
 
 
 class QuantizationInsertionPointBase(ABC):
     _state_names = QIPointStateNames
 
     def __init__(self, target_node_name: NNCFNodeName):
         self.target_node_name = target_node_name
@@ -58,53 +52,54 @@
         represents state of the object.
 
         :return: state of the object
         """
         return {self._state_names.TARGET_NODE_NAME: self.target_node_name}
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'QuantizationInsertionPointBase':
+    def from_state(cls, state: Dict[str, Any]) -> "QuantizationInsertionPointBase":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
 
 @CommonStatefulClassesRegistry.register()
 class WeightQuantizationInsertionPoint(QuantizationInsertionPointBase):
-    def __eq__(self, other: 'WeightQuantizationInsertionPoint'):
-        return isinstance(other, WeightQuantizationInsertionPoint) and \
-               self.target_node_name == other.target_node_name
+    def __eq__(self, other: "WeightQuantizationInsertionPoint"):
+        return isinstance(other, WeightQuantizationInsertionPoint) and self.target_node_name == other.target_node_name
 
     def __str__(self):
         return str(WeightQuantizerId(self.target_node_name))
 
     def __hash__(self):
         return hash(str(self))
 
 
 class AQIPointStateNames:
-    INPUT_PORT_ID = 'input_port_id'
-    TARGET_NODE_NAME = 'target_node_name'
+    INPUT_PORT_ID = "input_port_id"
+    TARGET_NODE_NAME = "target_node_name"
 
 
 @CommonStatefulClassesRegistry.register()
 class ActivationQuantizationInsertionPoint(QuantizationInsertionPointBase):
     _state_names = AQIPointStateNames
 
     def __init__(self, target_node_name: NNCFNodeName, input_port_id: Optional[int] = None):
         super().__init__(target_node_name)
         self.input_port_id = input_port_id
 
-    def __eq__(self, other: 'ActivationQuantizationInsertionPoint'):
-        return isinstance(other, ActivationQuantizationInsertionPoint) and \
-               self.target_node_name == other.target_node_name and \
-               self.input_port_id == other.input_port_id
+    def __eq__(self, other: "ActivationQuantizationInsertionPoint"):
+        return (
+            isinstance(other, ActivationQuantizationInsertionPoint)
+            and self.target_node_name == other.target_node_name
+            and self.input_port_id == other.input_port_id
+        )
 
     def __str__(self):
         return str(NonWeightQuantizerId(self.target_node_name, self.input_port_id))
 
     def __hash__(self):
         return hash(str(self))
 
@@ -113,21 +108,24 @@
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
         return {
             self._state_names.TARGET_NODE_NAME: self.target_node_name,
-            self._state_names.INPUT_PORT_ID: self.input_port_id
+            self._state_names.INPUT_PORT_ID: self.input_port_id,
         }
 
 
 class QuantizationPointBase:
-    def __init__(self, quant_insertion_point: QuantizationInsertionPointBase,
-                 directly_quantized_operator_node_names: List[NNCFNodeName]):
+    def __init__(
+        self,
+        quant_insertion_point: QuantizationInsertionPointBase,
+        directly_quantized_operator_node_names: List[NNCFNodeName],
+    ):
         self.insertion_point = quant_insertion_point
         self.directly_quantized_operator_node_names = directly_quantized_operator_node_names
 
     def is_activation_quantization_point(self) -> bool:
         return not self.is_weight_quantization_point()
 
     def is_weight_quantization_point(self) -> bool:
@@ -137,30 +135,34 @@
         raise NotImplementedError
 
     def __eq__(self, other):
         return self.__dict__ == other.__dict__
 
 
 class SCQPointStateNames:
-    QCONFIG = 'qconfig'
-    INSERTION_POINT = 'qip'
-    INSERTION_POINT_CLASS_NAME = 'qip_class'
-    NAMES_OF_QUANTIZED_OPS = 'directly_quantized_operator_node_names'
+    QCONFIG = "qconfig"
+    INSERTION_POINT = "qip"
+    INSERTION_POINT_CLASS_NAME = "qip_class"
+    NAMES_OF_QUANTIZED_OPS = "directly_quantized_operator_node_names"
 
 
 class SingleConfigQuantizationPoint(QuantizationPointBase):
     _state_names = SCQPointStateNames
 
-    def __init__(self, qip: QuantizationInsertionPointBase, qconfig: QuantizerConfig,
-                 directly_quantized_operator_node_names: List[NNCFNodeName]):
+    def __init__(
+        self,
+        qip: QuantizationInsertionPointBase,
+        qconfig: QuantizerConfig,
+        directly_quantized_operator_node_names: List[NNCFNodeName],
+    ):
         super().__init__(qip, directly_quantized_operator_node_names)
         self.qconfig = deepcopy(qconfig)
 
     def __str__(self):
-        return str(self.insertion_point) + ' ' + str(self.qconfig)
+        return str(self.insertion_point) + " " + str(self.qconfig)
 
     def get_all_configs_list(self) -> List[QuantizerConfig]:
         return [self.qconfig]
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
@@ -168,38 +170,42 @@
 
         :return: state of the object
         """
         return {
             self._state_names.INSERTION_POINT: self.insertion_point.get_state(),
             self._state_names.INSERTION_POINT_CLASS_NAME: self.insertion_point.__class__.__name__,
             self._state_names.QCONFIG: self.qconfig.get_state(),
-            self._state_names.NAMES_OF_QUANTIZED_OPS: self.directly_quantized_operator_node_names
+            self._state_names.NAMES_OF_QUANTIZED_OPS: self.directly_quantized_operator_node_names,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'SingleConfigQuantizationPoint':
+    def from_state(cls, state: Dict[str, Any]) -> "SingleConfigQuantizationPoint":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         insertion_point_cls_name = state[cls._state_names.INSERTION_POINT_CLASS_NAME]
         insertion_point_cls = CommonStatefulClassesRegistry.get_registered_class(insertion_point_cls_name)
         insertion_point = insertion_point_cls.from_state(state[cls._state_names.INSERTION_POINT])
         kwargs = {
             cls._state_names.INSERTION_POINT: insertion_point,
             cls._state_names.QCONFIG: QuantizerConfig.from_state(state[cls._state_names.QCONFIG]),
-            cls._state_names.NAMES_OF_QUANTIZED_OPS: state[cls._state_names.NAMES_OF_QUANTIZED_OPS]
+            cls._state_names.NAMES_OF_QUANTIZED_OPS: state[cls._state_names.NAMES_OF_QUANTIZED_OPS],
         }
         return cls(**kwargs)
 
 
 class MultiConfigQuantizationPoint(QuantizationPointBase):
-    def __init__(self, qip: QuantizationInsertionPointBase, possible_qconfigs: List[QuantizerConfig],
-                 directly_quantized_operator_node_names: List[NNCFNodeName]):
+    def __init__(
+        self,
+        qip: QuantizationInsertionPointBase,
+        possible_qconfigs: List[QuantizerConfig],
+        directly_quantized_operator_node_names: List[NNCFNodeName],
+    ):
         super().__init__(qip, directly_quantized_operator_node_names)
         self.possible_qconfigs = possible_qconfigs
 
     @property
     def possible_qconfigs(self):
         return deepcopy(self._possible_qconfigs)
 
@@ -209,27 +215,28 @@
 
     def select_qconfig(self, qconfig: QuantizerConfig) -> SingleConfigQuantizationPoint:
         if qconfig not in self.possible_qconfigs:
             # Allow selecting an "unsigned" or "signed" version if "any-signed" version is present
             qconfig_any = deepcopy(qconfig)
             qconfig_any.signedness_to_force = None
             if qconfig_any not in self.possible_qconfigs:
-                raise ValueError("Invalid selection for a quantizer config - "
-                                 "tried to select {} among [{}]".format(qconfig,
-                                                                        ",".join(
-                                                                            [str(q) for q in self.possible_qconfigs])))
+                raise ValueError(
+                    "Invalid selection for a quantizer config - "
+                    "tried to select {} among [{}]".format(qconfig, ",".join([str(q) for q in self.possible_qconfigs]))
+                )
             qconfig = qconfig_any
         return SingleConfigQuantizationPoint(self.insertion_point, qconfig, self.directly_quantized_operator_node_names)
 
     def __str__(self):
-        return str(self.insertion_point) + ' ' + ';'.join([str(qc) for qc in self.possible_qconfigs])
+        return str(self.insertion_point) + " " + ";".join([str(qc) for qc in self.possible_qconfigs])
 
     def get_all_configs_list(self) -> List[QuantizerConfig]:
         return self.possible_qconfigs
 
+
 class QuantizerSetupBase:
     def __init__(self):
         self.quantization_points = {}  # type: Dict[QuantizationPointId, QuantizationPointBase]
         self.unified_scale_groups = {}  # type: Dict[int, Set[QuantizationPointId]]
         self.shared_input_operation_set_groups = {}  # type: Dict[int, Set[QuantizationPointId]]
         self._next_unified_scale_gid = 0
         self._next_shared_inputs_gid = 0
@@ -282,23 +289,21 @@
 
             if not keep_shared_input_qps:
                 for idx in sorted(indices_to_delete, reverse=True):
                     for additional_id in self.shared_input_operation_set_groups[idx]:
                         self.__discard_independent(additional_id)
                     del self.shared_input_operation_set_groups[idx]
 
-    def get_unified_scale_group_id(self,
-                                   qp_id: QuantizationPointId) -> Optional[int]:
+    def get_unified_scale_group_id(self, qp_id: QuantizationPointId) -> Optional[int]:
         for gid, unified_scale_group in self.unified_scale_groups.items():
             if qp_id in unified_scale_group:
                 return gid
         return None
 
-    def get_shared_inputs_group_id(self,
-                                   qp_id: QuantizationPointId) -> Optional[int]:
+    def get_shared_inputs_group_id(self, qp_id: QuantizationPointId) -> Optional[int]:
         for gid, shared_inputs_group in self.shared_input_operation_set_groups.items():
             if qp_id in shared_inputs_group:
                 return gid
         return None
 
     def register_existing_qp_id_in_unified_scale_group(self, qp_id: QuantizationPointId, unified_scale_gid: int):
         gid = self.get_unified_scale_group_id(qp_id)
@@ -311,73 +316,82 @@
         if gid is not None:
             raise RuntimeError("QP id {} is already in shared inputs group {}".format(qp_id, gid))
         self.shared_input_operation_set_groups[shared_inputs_gid].add(qp_id)
 
     def remove_unified_scale_from_point(self, qp_id: QuantizationPointId):
         gid = self.get_unified_scale_group_id(qp_id)
         if gid is None:
-            nncf_logger.debug(f"Attempted to remove QP id {qp_id} from associated unified scale group, but the QP"
-                              f"is not in any unified scale group - ignoring.")
+            nncf_logger.debug(
+                f"Attempted to remove QP id {qp_id} from associated unified scale group, but the QP"
+                f"is not in any unified scale group - ignoring."
+            )
             return
         self.unified_scale_groups[gid].discard(qp_id)
         if not self.unified_scale_groups[gid]:
             nncf_logger.debug(f"Removed last entry from a unified scale group {gid} - removing group itself")
             self.unified_scale_groups.pop(gid)
 
-    def equivalent_to(self, other: 'QuantizerSetupBase') -> bool:
+    def equivalent_to(self, other: "QuantizerSetupBase") -> bool:
         this_qp_id_to_other_qp_id_dict = {}  # type: Dict[QuantizationPointId, QuantizationPointId]
 
-        def _compare_qps(first: 'QuantizerSetupBase', second: 'QuantizerSetupBase') -> bool:
+        def _compare_qps(first: "QuantizerSetupBase", second: "QuantizerSetupBase") -> bool:
             for this_qp_id, this_qp in first.quantization_points.items():
                 matches = []  # type: List[QuantizationPointId]
                 for other_qp_id, other_qp in second.quantization_points.items():
                     if this_qp == other_qp:
                         matches.append(other_qp_id)
                 if len(matches) == 0:
                     return False
                 assert len(matches) == 1  # separate quantization points should not compare equal to each other
                 this_qp_id_to_other_qp_id_dict[this_qp_id] = matches[0]
             return True
 
-        def _compare_shared_input_groups(first: 'QuantizerSetupBase', second: 'QuantizerSetupBase') -> bool:
+        def _compare_shared_input_groups(first: "QuantizerSetupBase", second: "QuantizerSetupBase") -> bool:
             for this_same_input_group_set in first.shared_input_operation_set_groups.values():
-                translated_id_set = set(this_qp_id_to_other_qp_id_dict[this_qp_id]
-                                        for this_qp_id in this_same_input_group_set)
+                translated_id_set = set(
+                    this_qp_id_to_other_qp_id_dict[this_qp_id] for this_qp_id in this_same_input_group_set
+                )
                 matches = []
 
                 for other_shared_inputs_group in second.shared_input_operation_set_groups.values():
                     if translated_id_set == other_shared_inputs_group:
                         matches.append(other_shared_inputs_group)
                 if not matches:
                     return False
                 assert len(matches) == 1  # shared inputs group entries should be present in only one group
             return True
 
-        def _compare_unified_scale_groups(first: 'QuantizerSetupBase', second: 'QuantizerSetupBase') -> bool:
+        def _compare_unified_scale_groups(first: "QuantizerSetupBase", second: "QuantizerSetupBase") -> bool:
             for this_unified_scales_group in first.unified_scale_groups.values():
-                translated_id_set = set(this_qp_id_to_other_qp_id_dict[this_qp_id]
-                                        for this_qp_id in this_unified_scales_group)
+                translated_id_set = set(
+                    this_qp_id_to_other_qp_id_dict[this_qp_id] for this_qp_id in this_unified_scales_group
+                )
                 matches = []
                 for other_unified_scales_group in second.unified_scale_groups.values():
                     if translated_id_set == other_unified_scales_group:
                         matches.append(other_unified_scales_group)
                 if not matches:
                     return False
                 assert len(matches) == 1  # unified scale group entries should be present in only one group
             return True
 
-        return _compare_qps(self, other) and _compare_qps(other, self) and \
-               _compare_shared_input_groups(self, other) and _compare_shared_input_groups(self, other) and \
-               _compare_unified_scale_groups(self, other) and _compare_unified_scale_groups(self, other)
+        return (
+            _compare_qps(self, other)
+            and _compare_qps(other, self)
+            and _compare_shared_input_groups(self, other)
+            and _compare_shared_input_groups(self, other)
+            and _compare_unified_scale_groups(self, other)
+            and _compare_unified_scale_groups(self, other)
+        )
 
 
 class SCQSetupStateNames:
-    SHARED_INPUT_OPERATION_SET_GROUPS = 'shared_input_operation_set_groups'
-    UNIFIED_SCALE_GROUPS = 'unified_scale_groups'
-    QUANTIZATION_POINTS = 'quantization_points'
+    SHARED_INPUT_OPERATION_SET_GROUPS = "shared_input_operation_set_groups"
+    UNIFIED_SCALE_GROUPS = "unified_scale_groups"
+    QUANTIZATION_POINTS = "quantization_points"
 
 
 class SingleConfigQuantizerSetup(QuantizerSetupBase):
     _state_names = SCQSetupStateNames
 
     def __init__(self):
         super().__init__()
@@ -401,15 +415,15 @@
         return {
             self._state_names.QUANTIZATION_POINTS: quantization_points_state,
             self._state_names.UNIFIED_SCALE_GROUPS: unified_scale_groups_state,
             self._state_names.SHARED_INPUT_OPERATION_SET_GROUPS: shared_input_operation_set_groups_state,
         }
 
     @classmethod
-    def from_state(cls, state: Dict) -> 'SingleConfigQuantizerSetup':
+    def from_state(cls, state: Dict) -> "SingleConfigQuantizerSetup":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         setup = SingleConfigQuantizerSetup()
 
@@ -430,43 +444,44 @@
 
 class MultiConfigQuantizerSetup(QuantizerSetupBase):
     def __init__(self):
         super().__init__()
         self.quantization_points = {}  # type: Dict[QuantizationPointId, MultiConfigQuantizationPoint]
         self._unified_scale_qpid_vs_type = {}  # type: Dict[QuantizationPointId, UnifiedScaleType]
 
-    def register_unified_scale_group_with_types(self, qp_group: List[QuantizationPointId],
-                                                us_types: List[UnifiedScaleType]) -> int:
+    def register_unified_scale_group_with_types(
+        self, qp_group: List[QuantizationPointId], us_types: List[UnifiedScaleType]
+    ) -> int:
         assert len(qp_group) == len(us_types)
         gid = super().register_unified_scale_group(qp_group)
         for qp_id, us_type in zip(qp_group, us_types):
             self._unified_scale_qpid_vs_type[qp_id] = us_type
         return gid
 
-    def select_qconfigs(self, qp_id_vs_selected_qconfig_dict: Dict[QuantizationPointId, QuantizerConfig],
-                        strict: bool =True) -> \
-            SingleConfigQuantizerSetup:
+    def select_qconfigs(
+        self, qp_id_vs_selected_qconfig_dict: Dict[QuantizationPointId, QuantizerConfig], strict: bool = True
+    ) -> SingleConfigQuantizerSetup:
         retval = SingleConfigQuantizerSetup()
         retval.unified_scale_groups = deepcopy(self.unified_scale_groups)
         retval.shared_input_operation_set_groups = deepcopy(self.shared_input_operation_set_groups)
 
         if Counter(qp_id_vs_selected_qconfig_dict.keys()) != Counter(self.quantization_points.keys()):
-            raise ValueError("The set of quantization points for a selection is inconsistent with quantization"
-                             "points in the quantizer setup!")
+            raise ValueError(
+                "The set of quantization points for a selection is inconsistent with quantization"
+                "points in the quantizer setup!"
+            )
         for qp_id, qp in self.quantization_points.items():
             if strict:
-                retval.quantization_points[qp_id] = qp.select_qconfig(
-                    qp_id_vs_selected_qconfig_dict[qp_id]
-                )
+                retval.quantization_points[qp_id] = qp.select_qconfig(qp_id_vs_selected_qconfig_dict[qp_id])
             else:
                 multi_qp = qp
                 qconfig = qp_id_vs_selected_qconfig_dict[qp_id]
                 retval.quantization_points[qp_id] = SingleConfigQuantizationPoint(
-                    multi_qp.insertion_point, qconfig,
-                    multi_qp.directly_quantized_operator_node_names)
+                    multi_qp.insertion_point, qconfig, multi_qp.directly_quantized_operator_node_names
+                )
 
         # Segregate the unified scale groups into sub-groups based on what exact config was chosen.
         for us_group in self.unified_scale_groups.values():
             per_channel_qids = set()
             per_tensor_qids = set()
             for us_qid in us_group:
                 final_qconfig = retval.quantization_points[us_qid].qconfig
@@ -480,38 +495,40 @@
                     retval.remove_unified_scale_from_point(qid)
 
                 retval.register_unified_scale_group(list(per_tensor_qids))
 
             for per_channel_qid in per_channel_qids:
                 us_type = self._unified_scale_qpid_vs_type[per_channel_qid]
                 if us_type is UnifiedScaleType.UNIFY_ONLY_PER_TENSOR:
-                    nncf_logger.debug("Per-channel quantizer config selected in a MultiConfigQuantizerSetup for a "
-                                      "unified scale point that only supports per-tensor scale unification, disabling "
-                                      "unified scales for this point.")
+                    nncf_logger.debug(
+                        "Per-channel quantizer config selected in a MultiConfigQuantizerSetup for a "
+                        "unified scale point that only supports per-tensor scale unification, disabling "
+                        "unified scales for this point."
+                    )
                 retval.remove_unified_scale_from_point(per_channel_qid)
 
         return retval
 
     def select_first_qconfig_for_each_point(self) -> SingleConfigQuantizerSetup:
         qp_id_vs_qconfig_dict = {}  # type: Dict[QuantizationPointId, QuantizerConfig]
         for qp_id, qp in self.quantization_points.items():
             qp_id_vs_qconfig_dict[qp_id] = qp.possible_qconfigs[0]
         return self.select_qconfigs(qp_id_vs_qconfig_dict)
 
     @classmethod
-    def from_single_config_setup(cls, single_conf_setup: SingleConfigQuantizerSetup) -> 'MultiConfigQuantizerSetup':
+    def from_single_config_setup(cls, single_conf_setup: SingleConfigQuantizerSetup) -> "MultiConfigQuantizerSetup":
         retval = cls()
         for qp_id, qp in single_conf_setup.quantization_points.items():
             multi_pt = MultiConfigQuantizationPoint(
                 qip=qp.insertion_point,
                 possible_qconfigs=[deepcopy(qp.qconfig)],
-                directly_quantized_operator_node_names=qp.directly_quantized_operator_node_names)
+                directly_quantized_operator_node_names=qp.directly_quantized_operator_node_names,
+            )
             retval.quantization_points[qp_id] = multi_pt
         for qp_set in single_conf_setup.unified_scale_groups.values():
             qp_list = list(qp_set)
             qp_types = [UnifiedScaleType.UNIFY_ALWAYS for _ in qp_list]
-            retval.register_unified_scale_group_with_types(qp_list,
-                                                           qp_types)
+            retval.register_unified_scale_group_with_types(qp_list, qp_types)
         for qp_set in single_conf_setup.shared_input_operation_set_groups.values():
             qp_list = list(qp_set)
             retval.register_shared_inputs_group(qp_list)
         return retval
```

### Comparing `nncf-2.4.0/nncf/common/quantization/quantizers.py` & `nncf-2.5.0/nncf/common/quantization/quantizers.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,74 +1,70 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Tuple
 
 
-def calculate_symmetric_level_ranges(
-        num_bits: int,
-        signed: bool,
-        narrow_range: bool = False) -> Tuple[int, int, int]:
+def calculate_symmetric_level_ranges(num_bits: int, signed: bool, narrow_range: bool = False) -> Tuple[int, int]:
     """
     Calculates the numbers of the low and high quant and the number of
     quantization levels for the symmetric quantization scheme.
 
     :param num_bits: The bitwidth of the quantization.
     :param signed: The flag specifying type of the symmetric quantization scheme
         if it is True then the symmetric quantization scheme is the signed and
         the un-signed otherwise.
     :param narrow_range: The flag specifying quantization range if it is True
         then [1; 2^num_bits - 1] and [0; 2^num_bits - 1] otherwise.
     :return: A Tuple
         level_low - the low quant number
         level_high - the high quant number
-        levels - the number of quantization levels
     """
-    levels = 2 ** num_bits
+    levels = 2**num_bits
 
     if signed:
         level_high = (levels // 2) - 1
         level_low = -(levels // 2)
     else:
         level_high = levels - 1
         level_low = 0
 
     if narrow_range:
-        level_low = level_low + 1
-        levels = levels - 1
+        if level_low < 0:
+            level_low += 1
+        else:
+            level_high -= 1
 
-    return level_low, level_high, levels
+    return level_low, level_high
 
 
-def calculate_asymmetric_level_ranges(
-        num_bits: int,
-        narrow_range: bool = False) -> Tuple[int, int, int]:
+def calculate_asymmetric_level_ranges(num_bits: int, narrow_range: bool = False) -> Tuple[int, int]:
     """
     Calculates the numbers of the low and high quant and the number of
     quantization levels for the asymmetric quantization scheme.
 
     :param num_bits: The bitwidth of the quantization
     :param narrow_range: The flag specifying quantization range if it is True
         then [1; 2^num_bits - 1] and [0; 2^num_bits - 1] otherwise
     :return: A Tuple
         level_low - the low quant number
         level_high - the high quant number
-        levels - the number of quantization levels
     """
-    levels = 2 ** num_bits
+    levels = 2**num_bits
     level_high = levels - 1
     level_low = 0
 
     if narrow_range:
         level_low = level_low + 1
-        levels = levels - 1
 
-    return level_low, level_high, levels
+    return level_low, level_high
+
+
+def get_num_levels(level_low: int, level_high: int):
+    return level_high - level_low + 1
```

### Comparing `nncf-2.4.0/nncf/common/quantization/statistics.py` & `nncf-2.5.0/nncf/common/quantization/statistics.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,41 +1,42 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict, Optional, List
+from typing import Dict, List, Optional
 
 from nncf.api.statistics import Statistics
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.helpers import create_table
 
 
 def _proportion_str(num: int, total_count: int):
     percentage = 100 * (num / max(total_count, 1))
-    return f'{percentage:.2f} % ({num} / {total_count})'
+    return f"{percentage:.2f} % ({num} / {total_count})"
 
 
 class QuantizersCounter:
-    def __init__(self,
-                 num_symmetric: int = 0,
-                 num_asymmetric: int = 0,
-                 num_signed: int = 0,
-                 num_unsigned: int = 0,
-                 num_per_tensor: int = 0,
-                 num_per_channel: int = 0,
-                 total_count: int = 0,
-                 potential_count: Optional[int] = None):
+    def __init__(
+        self,
+        num_symmetric: int = 0,
+        num_asymmetric: int = 0,
+        num_signed: int = 0,
+        num_unsigned: int = 0,
+        num_per_tensor: int = 0,
+        num_per_channel: int = 0,
+        total_count: int = 0,
+        potential_count: Optional[int] = None,
+    ):
         """
         Initializes quantizers counter.
 
         :param num_symmetric: Number of symmetric quantizers.
         :param num_asymmetric: Number of asymmetric quantizers.
         :param num_signed: Number of signed quantizers.
         :param num_unsigned: Number of unsigned quantizers.
@@ -62,135 +63,132 @@
     :param qt: Type of the counter. Takes one of the following values:
         - `WQ` - for the counter of the quantizers for weight.
         - `AQ` - for the counter of the quantizers for activation.
     :return: List of rows.
     """
     rows = [
         [
-            f'Symmetric {qt}s / All placed {qt}s',
+            f"Symmetric {qt}s / All placed {qt}s",
             _proportion_str(counter.num_symmetric, counter.total_count),
         ],
         [
-            f'Asymmetric {qt}s / All placed {qt}s',
+            f"Asymmetric {qt}s / All placed {qt}s",
             _proportion_str(counter.num_asymmetric, counter.total_count),
         ],
         [
-            f'Signed {qt}s / All placed {qt}s',
+            f"Signed {qt}s / All placed {qt}s",
             _proportion_str(counter.num_signed, counter.total_count),
         ],
         [
-            f'Unsigned {qt}s / All placed {qt}s',
+            f"Unsigned {qt}s / All placed {qt}s",
             _proportion_str(counter.num_unsigned, counter.total_count),
         ],
         [
-            f'Per-tensor {qt}s / All placed {qt}s',
+            f"Per-tensor {qt}s / All placed {qt}s",
             _proportion_str(counter.num_per_tensor, counter.total_count),
         ],
         [
-            f'Per-channel {qt}s / All placed {qt}s',
+            f"Per-channel {qt}s / All placed {qt}s",
             _proportion_str(counter.num_per_channel, counter.total_count),
         ],
     ]
 
     if counter.potential_count:
-        rows.append(
-            [
-                f'Placed {qt}s / Potential {qt}s',
-                _proportion_str(counter.total_count, counter.potential_count)
-            ]
-        )
+        rows.append([f"Placed {qt}s / Potential {qt}s", _proportion_str(counter.total_count, counter.potential_count)])
 
     return rows
 
 
+@api()
 class QuantizationStatistics(Statistics):
     """
-    Contains statistics of the quantization algorithm.
+    Contains statistics of the quantization algorithm. These statistics include:
 
-    These statistics include:
-        - Information about the share of the quantization. It includes following:
-            - Percentage of symmetric/asymmetric/per-channel/per-tensor weight
-            quantizers relative to the number of placed weight quantizers.
-            - Percentage of symmetric/asymmetric/per-channel/per-tensor non-weight
-            quantizers relative to the number of placed non weight quantizers.
-            - Percentage of weight quantizers and non-weight quantizers for each
-            precision relative to the number potential* quantizers/placed quantizers.
-
-            * The maximum possible number of potential quantizers depends on the presence
-            of ignored scopes and the mode of quantizer setup that is used at the time of
-            collecting the metric.
-        - Information about the distribution of the bitwidth of the quantizers.
-        - Ratio of enabled quantization.
-    """
+    * Information about the share of the quantization, such as:
 
-    def __init__(self,
-                 wq_counter: QuantizersCounter,
-                 aq_counter: QuantizersCounter,
-                 num_wq_per_bitwidth: Dict[int, int],
-                 num_aq_per_bitwidth: Dict[int, int],
-                 ratio_of_enabled_quantizations: float):
-        """
-        Initializes statistics of the quantization algorithm.
+      * Percentage of symmetric/asymmetric/per-channel/per-tensor weight quantizers relative to the number of placed
+        weight quantizers.
+      * Percentage of symmetric/asymmetric/per-channel/per-tensor non-weight quantizers relative to the number of
+        placed non weight quantizers.
+      * Percentage of weight quantizers and non-weight quantizers for each precision relative to the number
+        of potential quantizers/placed quantizers.
+
+    * Information about the distribution of the bitwidth of the quantizers.
+    * Ratio of enabled quantization.
+
+    .. note:: The maximum possible number of potential quantizers depends on the presence of ignored scopes and the
+      mode of quantizer setup that is used at the time of collecting the metric.
+
+    :param wq_counter: Weight quantizers counter.
+    :param aq_counter: Activation quantizers counter.
+    :param num_wq_per_bitwidth: Number of weight quantizers per bit width.
+    :param num_aq_per_bitwidth: Number of activation quantizers per bit width.
+    :param ratio_of_enabled_quantizations: Ratio of enabled quantizations.
+    """
 
-        :param wq_counter: Weight quantizers counter.
-        :param aq_counter: Activation quantizers counter.
-        :param num_wq_per_bitwidth: Number of weight quantizers per bit width.
-        :param num_aq_per_bitwidth: Number of activation quantizers per bit width.
-        :param ratio_of_enabled_quantizations: Ratio of enabled quantizations.
-        """
+    def __init__(
+        self,
+        wq_counter: QuantizersCounter,
+        aq_counter: QuantizersCounter,
+        num_wq_per_bitwidth: Dict[int, int],
+        num_aq_per_bitwidth: Dict[int, int],
+        ratio_of_enabled_quantizations: float,
+    ):
         self.wq_counter = wq_counter
         self.aq_counter = aq_counter
         self.num_wq_per_bitwidth = num_wq_per_bitwidth
         self.num_aq_per_bitwidth = num_aq_per_bitwidth
         self.ratio_of_enabled_quantizations = ratio_of_enabled_quantizations
 
     def to_str(self) -> str:
         pretty_strings = []
 
         table = create_table(
-            header=['Statistic\'s name', 'Value'],
-            rows=[['Ratio of enabled quantizations', self.ratio_of_enabled_quantizations]]
+            header=["Statistic's name", "Value"],
+            rows=[["Ratio of enabled quantizations", self.ratio_of_enabled_quantizations]],
         )
 
-        pretty_strings.append(f'Statistics of the quantization algorithm:\n{table}')
+        pretty_strings.append(f"Statistics of the quantization algorithm:\n{table}")
         pretty_strings.append(self._get_quantization_share_str())
         pretty_strings.append(self._get_bitwidth_distribution_str())
-        pretty_string = '\n\n'.join(pretty_strings)
+        pretty_string = "\n\n".join(pretty_strings)
         return pretty_string
 
     def _get_quantization_share_str(self) -> str:
-        header = ['Statistic\'s name', 'Value']
+        header = ["Statistic's name", "Value"]
 
         rows = []
-        rows.extend(_quantizers_counter_to_rows(self.wq_counter, 'WQ'))
-        rows.extend(_quantizers_counter_to_rows(self.aq_counter, 'AQ'))
+        rows.extend(_quantizers_counter_to_rows(self.wq_counter, "WQ"))
+        rows.extend(_quantizers_counter_to_rows(self.aq_counter, "AQ"))
 
         table = create_table(header, rows)
-        pretty_string = f'Statistics of the quantization share:\n{table}'
+        pretty_string = f"Statistics of the quantization share:\n{table}"
         return pretty_string
 
     def _get_bitwidth_distribution_str(self) -> str:
         wq_total_num = sum(self.num_wq_per_bitwidth.values())
         aq_total_num = sum(self.num_aq_per_bitwidth.values())
         q_total_num = wq_total_num + aq_total_num
 
         bitwidths = self.num_wq_per_bitwidth.keys() | self.num_aq_per_bitwidth.keys()  # union of all bitwidths
         bitwidths = sorted(bitwidths, reverse=True)
 
         # Table creation
-        header = ['Num bits (N)', 'N-bits WQs / Placed WQs', 'N-bits AQs / Placed AQs', 'N-bits Qs / Placed Qs']
+        header = ["Num bits (N)", "N-bits WQs / Placed WQs", "N-bits AQs / Placed AQs", "N-bits Qs / Placed Qs"]
         rows = []
         for bitwidth in bitwidths:
             wq_num = self.num_wq_per_bitwidth.get(bitwidth, 0)  # for current bitwidth
             aq_num = self.num_aq_per_bitwidth.get(bitwidth, 0)  # for current bitwidth
             q_num = wq_num + aq_num  # for current bitwidth
 
-            rows.append([
-                bitwidth,
-                _proportion_str(wq_num, wq_total_num),
-                _proportion_str(aq_num, aq_total_num),
-                _proportion_str(q_num, q_total_num)
-            ])
+            rows.append(
+                [
+                    bitwidth,
+                    _proportion_str(wq_num, wq_total_num),
+                    _proportion_str(aq_num, aq_total_num),
+                    _proportion_str(q_num, q_total_num),
+                ]
+            )
 
         table = create_table(header, rows)
-        pretty_string = f'Statistics of the bitwidth distribution:\n{table}'
+        pretty_string = f"Statistics of the bitwidth distribution:\n{table}"
         return pretty_string
```

### Comparing `nncf-2.4.0/nncf/common/quantization/structs.py` & `nncf-2.5.0/nncf/common/quantization/structs.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,45 +1,55 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from copy import deepcopy
 from enum import Enum
-from typing import Dict, List, Optional, Any
+from typing import Any, Dict, List, Optional
 
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
+from nncf.common.utils.api_marker import api
 from nncf.config.schemata.defaults import QUANTIZATION_BITS
 from nncf.config.schemata.defaults import QUANTIZATION_PER_CHANNEL
 
 
+@api()
 class QuantizationMode:
-    SYMMETRIC = 'symmetric'
-    ASYMMETRIC = 'asymmetric'
+    """
+    Basic enumeration for quantization mode specification.
+
+    :param SYMMETRIC:
+    :param ASYMMETRIC:
+    """
+
+    SYMMETRIC = "symmetric"
+    ASYMMETRIC = "asymmetric"
 
 
 class QuantizerConfig:
     """
     A generic, framework-agnostic information on a configuration of a quantizer for abstract reasoning
     and determination of a quantizer setup scheme for a given model.
     """
 
-    def __init__(self, num_bits: int = QUANTIZATION_BITS,
-                 mode: QuantizationMode = QuantizationMode.SYMMETRIC,
-                 signedness_to_force: Optional[bool] = None,
-                 per_channel: bool = QUANTIZATION_PER_CHANNEL):
+    def __init__(
+        self,
+        num_bits: int = QUANTIZATION_BITS,
+        mode: QuantizationMode = QuantizationMode.SYMMETRIC,
+        signedness_to_force: Optional[bool] = None,
+        per_channel: bool = QUANTIZATION_PER_CHANNEL,
+    ):
         """
         :param num_bits: Bitwidth of the quantization.
         :param mode: The mode of quantization (symmetric or asymmetric).
         :param signedness_to_force: True if the quantizer *must* be signed, False if *must* be unsigned,
             None if the signed/unsigned attribute should be determined based on the incoming activation
             statistics during range initialization.
         :param per_channel: True for per-channel quantization, False for per-tensor.
@@ -49,24 +59,25 @@
         self.signedness_to_force = signedness_to_force
         self.per_channel = per_channel
 
     def __eq__(self, other):
         return self.__dict__ == other.__dict__
 
     def __str__(self):
-        return 'B:{bits} M:{mode} SGN:{signedness} PC:{per_channel}'.format(
+        return "B:{bits} M:{mode} SGN:{signedness} PC:{per_channel}".format(
             bits=self.num_bits,
-            mode='S' if self.mode == QuantizationMode.SYMMETRIC else 'A',
-            signedness='ANY' if self.signedness_to_force is None else ('S' if self.signedness_to_force else 'U'),
-            per_channel='Y' if self.per_channel else 'N')
+            mode="S" if self.mode == QuantizationMode.SYMMETRIC else "A",
+            signedness="ANY" if self.signedness_to_force is None else ("S" if self.signedness_to_force else "U"),
+            per_channel="Y" if self.per_channel else "N",
+        )
 
     def __hash__(self):
         return hash(str(self))
 
-    def is_valid_requantization_for(self, other: 'QuantizerConfig') -> bool:
+    def is_valid_requantization_for(self, other: "QuantizerConfig") -> bool:
         """
         Quantizer config A is a valid requantization for quantizer config B if A is more strict -
         specifically, it might be reasonable to put quantizer A after quantizer B in tensor data control flow, so that
         the requantization will further constrain the input tensor data w.r.t. values it can take, but
         putting quantizer A after quantizer B would be unreasonable.
 
         :param other: The "primary" QuantizerConfig, i.e. the one that defines an already present quantization.
@@ -78,71 +89,75 @@
             self.signedness_to_force is None and other.signedness_to_force is not None,
             self.signedness_to_force is True and other.signedness_to_force is False,
         ]
         if any(fail_conditions):
             return False
         return True
 
-    def compatible_with_a_unified_scale_linked_qconfig(self, linked_qconfig: 'QuantizerConfig'):
+    def compatible_with_a_unified_scale_linked_qconfig(self, linked_qconfig: "QuantizerConfig"):
         """
         For two configs to be compatible in a unified scale scenario, all of their fundamental parameters
         must be aligned.
 
         :param linked_qconfig: A QuantizerConfig that is compared against the current config.
         :return: A boolean value specifying whether `linked_qconfig` is compatible with the current config in terms
             of scale unification.
         """
-        return self.num_bits == linked_qconfig.num_bits and \
-               self.mode == linked_qconfig.mode and \
-               self.signedness_to_force == linked_qconfig.signedness_to_force and \
-               self.per_channel == linked_qconfig.per_channel
+        return (
+            self.num_bits == linked_qconfig.num_bits
+            and self.mode == linked_qconfig.mode
+            and self.signedness_to_force == linked_qconfig.signedness_to_force
+            and self.per_channel == linked_qconfig.per_channel
+        )
 
-    def is_a_bitwidth_variant(self, other_qconfig: 'QuantizerConfig') -> bool:
+    def is_a_bitwidth_variant(self, other_qconfig: "QuantizerConfig") -> bool:
         """
         :param other_qconfig: A QuantizerConfig to be compared against the current config.
         :return: A boolean value specifying whether `other_config` is identical to the current config
             in everything except the bitwidth.
         """
-        return self.per_channel == other_qconfig.per_channel and \
-               self.signedness_to_force == other_qconfig.signedness_to_force and \
-               self.mode == other_qconfig.mode
+        return (
+            self.per_channel == other_qconfig.per_channel
+            and self.signedness_to_force == other_qconfig.signedness_to_force
+            and self.mode == other_qconfig.mode
+        )
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
-        return {'num_bits': self.num_bits,
-                'mode': self.mode,
-                'signedness_to_force': self.signedness_to_force,
-                'per_channel': self.per_channel}
+        return {
+            "num_bits": self.num_bits,
+            "mode": self.mode,
+            "signedness_to_force": self.signedness_to_force,
+            "per_channel": self.per_channel,
+        }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'QuantizerConfig':
+    def from_state(cls, state: Dict[str, Any]) -> "QuantizerConfig":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
 
 class QuantizerSpec:
     """
     A specific (potentially framework-aware) parameter struct required to initialize a
     given object that performs quantization of an input tensor.
     """
 
-    def __init__(self, num_bits: int,
-                 mode: QuantizationMode,
-                 signedness_to_force: bool,
-                 narrow_range: bool,
-                 half_range: bool):
+    def __init__(
+        self, num_bits: int, mode: QuantizationMode, signedness_to_force: bool, narrow_range: bool, half_range: bool
+    ):
         """
         :param num_bits: Bitwidth of the quantization.
         :param mode: The mode of quantization (symmetric or asymmetric).
         :param signedness_to_force: True if the quantizer *must* be signed, False if *must* be unsigned,
             None if the signed/unsigned attribute should be determined based on the incoming activation
             statistics during range initialization.
         :param narrow_range: True if the range of quantized values should be narrowed as compared to the
@@ -152,24 +167,20 @@
         """
         self.num_bits = num_bits
         self.mode = mode
         self.signedness_to_force = signedness_to_force
         self.narrow_range = narrow_range
         self.half_range = half_range
 
-    def __eq__(self, other: 'QuantizerSpec'):
+    def __eq__(self, other: "QuantizerSpec"):
         return self.__dict__ == other.__dict__
 
     @classmethod
-    def from_config(cls, qconfig: QuantizerConfig, narrow_range: bool, half_range: bool) -> 'QuantizerSpec':
-        return cls(qconfig.num_bits,
-                   qconfig.mode,
-                   qconfig.signedness_to_force,
-                   narrow_range,
-                   half_range)
+    def from_config(cls, qconfig: QuantizerConfig, narrow_range: bool, half_range: bool) -> "QuantizerSpec":
+        return cls(qconfig.num_bits, qconfig.mode, qconfig.signedness_to_force, narrow_range, half_range)
 
 
 class QuantizationConstraints:
     REF_QCONF_OBJ = QuantizerConfig()
 
     def __init__(self, **kwargs):
         """
@@ -177,15 +188,15 @@
         to set up constraints.
         E.g. QuantizationConstraint(bits=8, per_channel=True) will set up
         a constraint that corresponds to all 8-bit per-channel quantizers, either
         symmetric or asymmetric, either signed or unsigned.
         """
         for attr_name in kwargs:
             if not hasattr(QuantizationConstraints.REF_QCONF_OBJ, attr_name):
-                raise RuntimeError('Invalid constraint - QuantizerConfig has no attribute \'{}\''.format(attr_name))
+                raise RuntimeError("Invalid constraint - QuantizerConfig has no attribute '{}'".format(attr_name))
         self.qconf_attr_vs_constraint_dict = kwargs
 
     def apply_constraints_to(self, qconfig: QuantizerConfig) -> QuantizerConfig:
         for attr_name, constraint in self.qconf_attr_vs_constraint_dict.items():
             if constraint is not None:
                 setattr(qconfig, attr_name, constraint)
         return qconfig
@@ -195,46 +206,45 @@
         for attr_name, constraint in self.qconf_attr_vs_constraint_dict.items():
             if constraint is not None:
                 qconf_attr_value = getattr(qconfig, attr_name)
                 if qconf_attr_value != constraint:
                     is_compatible = False
         return is_compatible
 
-    def get_updated_constraints(self, overriding_constraints: 'QuantizationConstraints') -> 'QuantizationConstraints':
+    def get_updated_constraints(self, overriding_constraints: "QuantizationConstraints") -> "QuantizationConstraints":
         new_dict = deepcopy(self.qconf_attr_vs_constraint_dict)
         new_dict.update(overriding_constraints.qconf_attr_vs_constraint_dict)
         return QuantizationConstraints(**new_dict)
 
     @classmethod
-    def from_config_dict(cls, config_dict: Dict) -> 'QuantizationConstraints':
-        return cls(num_bits=config_dict.get('bits'),
-                   mode=config_dict.get('mode'),
-                   per_channel=config_dict.get('per_channel'),
-                   signedness_to_force=config_dict.get('signed'))
+    def from_config_dict(cls, config_dict: Dict) -> "QuantizationConstraints":
+        return cls(
+            num_bits=config_dict.get("bits"),
+            mode=config_dict.get("mode"),
+            per_channel=config_dict.get("per_channel"),
+            signedness_to_force=config_dict.get("signed"),
+        )
 
     def constrain_qconfig_list(self, quantizer_config_list: List[QuantizerConfig]) -> List[QuantizerConfig]:
         assert quantizer_config_list is not None
 
-        constrained_quantizer_config_list = list(filter(
-            self.is_config_compatible,
-            quantizer_config_list
-        ))
+        constrained_quantizer_config_list = list(filter(self.is_config_compatible, quantizer_config_list))
 
         # TODO: Make the logic more flexible when the flag "warning as error" is implemented.
         # It means that the qconfig from overrides must be selected as final config
         # even if it is not valid in hw-config.
         if not constrained_quantizer_config_list:
             raise RuntimeError()
 
         return constrained_quantizer_config_list
 
 
 class QuantizerGroup(Enum):
-    ACTIVATIONS = 'activations'
-    WEIGHTS = 'weights'
+    ACTIVATIONS = "activations"
+    WEIGHTS = "weights"
 
 
 class QuantizableWeightedLayerNode:
     def __init__(self, node: NNCFNode, qconfig_list: List[QuantizerConfig]):
         self.node = node
         self.qconfig_list = qconfig_list
 
@@ -253,47 +263,46 @@
 
     def __str__(self):
         return str(self.get_base()) + self.get_suffix()
 
     def __hash__(self):
         return hash((self.get_base(), self.get_suffix()))
 
-    def __eq__(self, other: 'QuantizerId'):
+    def __eq__(self, other: "QuantizerId"):
         return (self.get_base() == other.get_base()) and (self.get_suffix() == other.get_suffix())
 
 
 class WeightQuantizerId(QuantizerId):
-    """ Unique identifier of a quantizer for weights."""
+    """Unique identifier of a quantizer for weights."""
 
     def __init__(self, target_node_name: NNCFNodeName):
         self.target_node_name = target_node_name
 
     def get_base(self) -> str:
         return self.target_node_name
 
     def get_suffix(self) -> str:
-        return '|WEIGHT'
+        return "|WEIGHT"
 
 
 class NonWeightQuantizerId(QuantizerId):
     """
     Unique identifier of a quantizer, which corresponds to non-weight operations, such as
     ordinary activation, function and input
     """
 
-    def __init__(self, target_node_name: NNCFNodeName,
-                 input_port_id=None):
+    def __init__(self, target_node_name: NNCFNodeName, input_port_id=None):
         self.target_node_name = target_node_name
         self.input_port_id = input_port_id
 
     def get_base(self) -> str:
         return self.target_node_name
 
     def get_suffix(self) -> str:
-        return '|OUTPUT' if self.input_port_id is None else '|INPUT{}'.format(self.input_port_id)
+        return "|OUTPUT" if self.input_port_id is None else "|INPUT{}".format(self.input_port_id)
 
 
 class UnifiedScaleType(Enum):
     """
     UNIFY_ONLY_PER_TENSOR - only results in scale unification if per-tensor quantization is ultimately applied.
     This is the target scenario for concat unified scales since the channel count between the concatenated tensors
     may be mismatching and, more importantly, the concatenation might occur on exactly the channel dimension which
@@ -304,15 +313,20 @@
     tensor shapes and therefore the quantization channel count is the same.
     """
 
     UNIFY_ONLY_PER_TENSOR = 0
     UNIFY_ALWAYS = 1
 
 
+@api(canonical_alias="nncf.QuantizationPreset")
 class QuantizationPreset(Enum):
-    PERFORMANCE = 'performance'
-    MIXED = 'mixed'
+    """
+    An enum with values corresponding to the available quantization presets.
+    """
+
+    PERFORMANCE = "performance"
+    MIXED = "mixed"
 
     def get_params_configured_by_preset(self, quant_group: QuantizerGroup) -> Dict:
         if quant_group == QuantizerGroup.ACTIVATIONS and self == QuantizationPreset.MIXED:
-            return  {'mode': QuantizationMode.ASYMMETRIC}
-        return {'mode' : QuantizationMode.SYMMETRIC}
+            return {"mode": QuantizationMode.ASYMMETRIC}
+        return {"mode": QuantizationMode.SYMMETRIC}
```

### Comparing `nncf-2.4.0/nncf/common/schedulers.py` & `nncf-2.5.0/nncf/common/schedulers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,37 +1,34 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List, Optional, Dict, Any
 from bisect import bisect_right
+from typing import Any, Dict, List, Optional
 
 import numpy as np
 
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 
 
 class PolynomialDecaySchedule:
     """
     This schedule applies a polynomial decay function to an epoch index.
     For more details about polynomial decay see the [paper](https://arxiv.org/abs/1710.01878).
     """
 
-    def __init__(self, initial_value: float, target_value: float, target_epoch: int,
-                 power: float, concave: bool):
+    def __init__(self, initial_value: float, target_value: float, target_epoch: int, power: float, concave: bool):
         """
          Initializes a schedule with a polynomial decay function.
 
         :param initial_value: The initial value at which the schedule begins.
         :param target_value: The final value at which the schedule ends.
         :param target_epoch: Zero-based index of the epoch from which
             the function value will be equal to the `target_value` value.
@@ -87,15 +84,15 @@
         :param boundaries: List of zero-based epoch indices. Must be increasing.
         :param values: List of floats that specifies the values for the intervals
             defined by `boundaries`. It should have one more element than `boundaries`.
         :raises ValueError: If the number of elements in the `values` list does not
             equal to the number of elements in the `boundaries` list plus one.
         """
         if len(boundaries) + 1 != len(values):
-            raise ValueError('The length of `values` should be 1 more than the length of `boundaries`')
+            raise ValueError("The length of `values` should be 1 more than the length of `boundaries`")
 
         self.boundaries = boundaries
         self.values = values
 
     def __call__(self, epoch: int) -> float:
         """
         Calculates the value of the piecewise constant function for a given epoch index.
@@ -230,31 +227,27 @@
     def load_state(self, state: Dict[str, Any]) -> None:
         """
         Loads the compression scheduler state, but does not update the state of the
         compression method.
 
         :param state: Output of `get_state()` method.
         """
-        self._current_step = state['current_step']
-        self._current_epoch = state['current_epoch']
+        self._current_step = state["current_step"]
+        self._current_epoch = state["current_epoch"]
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns the compression scheduler state.
 
         :return: The compression scheduler state.
         """
-        return {
-            'current_step': self._current_step,
-            'current_epoch': self._current_epoch
-        }
+        return {"current_step": self._current_step, "current_epoch": self._current_epoch}
 
 
 class StubCompressionScheduler(CompressionScheduler):
-
     def step(self, next_step: Optional[int] = None) -> None:
         pass
 
     def epoch_step(self, next_epoch: Optional[int] = None) -> None:
         pass
 
     def load_state(self, state: Dict[str, Any]) -> None:
```

### Comparing `nncf-2.4.0/nncf/common/scopes.py` & `nncf-2.5.0/nncf/common/scopes.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,31 +1,27 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import re
-from typing import List
-from typing import Optional
-from typing import Union
+from typing import List, Optional, Union
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.quantization.structs import QuantizerId
-from nncf.parameters import IgnoredScope
-from nncf.parameters import convert_ignored_scope_to_list
+from nncf.scopes import IgnoredScope
+from nncf.scopes import convert_ignored_scope_to_list
 
 
 def matches_any(tested_str: str, str_or_list_to_match_to: Union[List[str], str]) -> bool:
     """
     Return True if tested_str matches at least one element in str_or_list_to_match_to.
 
     :param tested_str: One of the supported entity types to be matched - currently possible to pass either
@@ -66,16 +62,17 @@
         may be prefixed with `{re}` to enable regex matching.
     :param target_scopes: A list of strings specifying an allowlist for the serializable_id. Entries of the list
         may be prefixed with `{re}` to enable regex matching.
 
     :return: A boolean value specifying whether a serializable_id should be considered (i.e. "not ignored", "targeted")
     """
     string_id = str(serializable_id)
-    return (target_scopes is None or matches_any(string_id, target_scopes)) \
-               and not matches_any(string_id, ignored_scopes)
+    return (target_scopes is None or matches_any(string_id, target_scopes)) and not matches_any(
+        string_id, ignored_scopes
+    )
 
 
 def get_not_matched_scopes(scope: Union[List[str], str, IgnoredScope], nodes: List[NNCFNode]) -> List[str]:
     """
     Return list of scope that do not match node list.
 
     :param scope: List of ignored/target scope or instance of IgnoredScope.
```

### Comparing `nncf-2.4.0/nncf/common/sparsity/collector.py` & `nncf-2.5.0/nncf/common/sparsity/collector.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,28 +1,26 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List
 from abc import abstractmethod
+from typing import List
 
 import numpy as np
 
 from nncf.common.collector import StatisticsCollector
-from nncf.common.sparsity.statistics import SparsifiedModelStatistics
 from nncf.common.sparsity.statistics import SparsifiedLayerSummary
+from nncf.common.sparsity.statistics import SparsifiedModelStatistics
 
 
 class WeightDescription:
     """
     Contains information about the weight of the model.
     """
 
@@ -116,18 +114,14 @@
 
         sparse_layers_summary = []
         for w in weights_descriptions:
             if not w.is_sparse:
                 continue
 
             weight_percentage = 100 * (w.num_params / total_params)
-            sparse_layers_summary.append(
-                SparsifiedLayerSummary(w.name, w.shape, w.sparsity_level, weight_percentage)
-            )
+            sparse_layers_summary.append(SparsifiedLayerSummary(w.name, w.shape, w.sparsity_level, weight_percentage))
 
         sparse_model_stats = SparsifiedModelStatistics(
-            sparsity_level_for_model,
-            sparsity_level_for_sparse_layers,
-            sparse_layers_summary
+            sparsity_level_for_model, sparsity_level_for_sparse_layers, sparse_layers_summary
         )
 
         return sparse_model_stats
```

### Comparing `nncf-2.4.0/nncf/common/sparsity/controller.py` & `nncf-2.5.0/nncf/common/sparsity/controller.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.api.compression import CompressionAlgorithmController
 
 
 class SparsityController(CompressionAlgorithmController):
     """
     This is the class from which all sparsity controllers inherit.
```

### Comparing `nncf-2.4.0/nncf/common/sparsity/schedulers.py` & `nncf-2.5.0/nncf/common/sparsity/schedulers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,40 +1,38 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Optional, Dict, Any
+from typing import Any, Dict, Optional
 
 from nncf.common.logging import nncf_logger
-from nncf.common.utils.registry import Registry
-from nncf.common.schedulers import PolynomialDecaySchedule
+from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.schedulers import ExponentialDecaySchedule
 from nncf.common.schedulers import MultiStepSchedule
+from nncf.common.schedulers import PolynomialDecaySchedule
 from nncf.common.sparsity.controller import SparsityController
-from nncf.common.schedulers import BaseCompressionScheduler
+from nncf.common.utils.registry import Registry
 from nncf.config.schemata.defaults import SPARSITY_FREEZE_EPOCH
 from nncf.config.schemata.defaults import SPARSITY_MULTISTEP_SPARSITY_LEVELS
 from nncf.config.schemata.defaults import SPARSITY_MULTISTEP_STEPS
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_CONCAVE
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_PATIENCE
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_POWER
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_UPDATE_PER_OPTIMIZER_STEP
 from nncf.config.schemata.defaults import SPARSITY_TARGET
 from nncf.config.schemata.defaults import SPARSITY_TARGET_EPOCH
 
-SPARSITY_SCHEDULERS = Registry('sparsity_schedulers')
+SPARSITY_SCHEDULERS = Registry("sparsity_schedulers")
 
 
 class SparsityScheduler(BaseCompressionScheduler):
     """
     This is the class from which all sparsity schedulers inherit.
 
     A sparsity scheduler is an object which specifies the sparsity
@@ -57,29 +55,28 @@
         Initializes the internal state of the sparsity scheduler.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__()
         self._controller = controller
-        self.initial_level = params.get('sparsity_init')
-        self.target_level = params.get('sparsity_target', SPARSITY_TARGET)
-        self.target_epoch = params.get('sparsity_target_epoch', SPARSITY_TARGET_EPOCH)
-        self.freeze_epoch = params.get('sparsity_freeze_epoch', SPARSITY_FREEZE_EPOCH)
+        self.initial_level = params.get("sparsity_init")
+        self.target_level = params.get("sparsity_target", SPARSITY_TARGET)
+        self.target_epoch = params.get("sparsity_target_epoch", SPARSITY_TARGET_EPOCH)
+        self.freeze_epoch = params.get("sparsity_freeze_epoch", SPARSITY_FREEZE_EPOCH)
 
     def _calculate_sparsity_level(self) -> float:
         """
         Calculates a sparsity level that should be applied to the weights
         for the `current_epoch` or for step in the `current_epoch`.
 
         :return: Sparsity level that should be applied to the weights
             for the `current_epoch` or for step in the `current_epoch`.
         """
-        raise NotImplementedError(
-            'SparsityScheduler implementation must override _calculate_sparsity_level method.')
+        raise NotImplementedError("SparsityScheduler implementation must override _calculate_sparsity_level method.")
 
     def _update_sparsity_level(self) -> None:
         """
         Calculates the current sparsity level and updates the internal
         state of the `controller`.
         """
         if self.current_epoch >= self.freeze_epoch:
@@ -95,15 +92,15 @@
         :return: Current sparsity level.
         """
         if self._current_epoch == -1:
             return self.initial_level
         return self._calculate_sparsity_level()
 
 
-@SPARSITY_SCHEDULERS.register('polynomial')
+@SPARSITY_SCHEDULERS.register("polynomial")
 class PolynomialSparsityScheduler(SparsityScheduler):
     """
     Sparsity scheduler with a polynomial decay schedule.
 
     Two ways are available for calculations of the sparsity:
         - per epoch
         - per step
@@ -120,21 +117,26 @@
         """
         Initializes a sparsity scheduler with a polynomial decay schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
-        self.schedule = PolynomialDecaySchedule(self.initial_level, self.target_level, self.target_epoch,
-                                                params.get('power', SPARSITY_SCHEDULER_POWER),
-                                                params.get('concave', SPARSITY_SCHEDULER_CONCAVE))
+        self.schedule = PolynomialDecaySchedule(
+            self.initial_level,
+            self.target_level,
+            self.target_epoch,
+            params.get("power", SPARSITY_SCHEDULER_POWER),
+            params.get("concave", SPARSITY_SCHEDULER_CONCAVE),
+        )
         self._steps_in_current_epoch = 0
-        self._update_per_optimizer_step = params.get('update_per_optimizer_step',
-                                                     SPARSITY_SCHEDULER_UPDATE_PER_OPTIMIZER_STEP)
-        self._steps_per_epoch = params.get('steps_per_epoch', None)
+        self._update_per_optimizer_step = params.get(
+            "update_per_optimizer_step", SPARSITY_SCHEDULER_UPDATE_PER_OPTIMIZER_STEP
+        )
+        self._steps_per_epoch = params.get("steps_per_epoch", None)
         self._should_skip = False
 
     def step(self, next_step: Optional[int] = None) -> None:
         self._steps_in_current_epoch += 1
         if self._should_skip:
             return
 
@@ -156,20 +158,20 @@
     def _calculate_sparsity_level(self) -> float:
         local_step = max(self._steps_in_current_epoch - 1, 0)
         return self.schedule(self.current_epoch, local_step, self._steps_per_epoch)
 
     def load_state(self, state: Dict[str, Any]) -> None:
         super().load_state(state)
         if self._update_per_optimizer_step:
-            self._steps_per_epoch = state['_steps_per_epoch']
+            self._steps_per_epoch = state["_steps_per_epoch"]
 
     def get_state(self) -> Dict[str, Any]:
         state = super().get_state()
         if self._update_per_optimizer_step:
-            state['_steps_per_epoch'] = self._steps_per_epoch
+            state["_steps_per_epoch"] = self._steps_per_epoch
         return state
 
     def _maybe_should_skip(self) -> None:
         """
         Checks if the first epoch (with index 0) should be skipped to calculate
         the steps per epoch. If the skip is needed, then the internal state
         of the scheduler object will not be changed.
@@ -177,26 +179,30 @@
         self._should_skip = False
         if self._update_per_optimizer_step:
             if self._steps_per_epoch is None and self._steps_in_current_epoch > 0:
                 self._steps_per_epoch = self._steps_in_current_epoch
 
             if self._steps_per_epoch is not None and self._steps_in_current_epoch > 0:
                 if self._steps_per_epoch != self._steps_in_current_epoch:
-                    raise Exception('Actual steps per epoch and steps per epoch from the scheduler '
-                                    'parameters are different. Scheduling may be incorrect.')
+                    raise Exception(
+                        "Actual steps per epoch and steps per epoch from the scheduler "
+                        "parameters are different. Scheduling may be incorrect."
+                    )
 
             if self._steps_per_epoch is None:
                 self._should_skip = True
-                nncf_logger.warning('Scheduler set to update sparsity level per optimizer step, '
-                                    'but steps_per_epoch was not set in config. Will only start updating '
-                                    'sparsity level after measuring the actual steps per epoch as signaled '
-                                    'by a .epoch_step() call.')
+                nncf_logger.warning(
+                    "Scheduler set to update sparsity level per optimizer step, "
+                    "but steps_per_epoch was not set in config. Will only start updating "
+                    "sparsity level after measuring the actual steps per epoch as signaled "
+                    "by a .epoch_step() call."
+                )
 
 
-@SPARSITY_SCHEDULERS.register('exponential')
+@SPARSITY_SCHEDULERS.register("exponential")
 class ExponentialSparsityScheduler(SparsityScheduler):
     """
     Sparsity scheduler with an exponential decay schedule.
 
     This scheduler applies exponential decay to the density level
     to calculate the sparsity level for the `current_epoch`.
     The density level for the `current_epoch` is calculated as
@@ -222,30 +228,31 @@
 
     def _calculate_sparsity_level(self) -> float:
         current_density = self.schedule(self.current_epoch)
         current_level = 1.0 - current_density
         return min(current_level, self.target_level)
 
 
-@SPARSITY_SCHEDULERS.register('adaptive')
+@SPARSITY_SCHEDULERS.register("adaptive")
 class AdaptiveSparsityScheduler(SparsityScheduler):
     """
     Sparsity scheduler with an adaptive schedule.
     """
+
     def __init__(self, controller: SparsityController, params: dict):
         """
         Initializes a sparsity scheduler with an adaptive schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
-        self.decay_step = params.get('step', 0.05)
-        self.eps = params.get('eps', 0.03)
-        self.patience = params.get('patience', SPARSITY_SCHEDULER_PATIENCE)
+        self.decay_step = params.get("step", 0.05)
+        self.eps = params.get("eps", 0.03)
+        self.patience = params.get("patience", SPARSITY_SCHEDULER_PATIENCE)
         self.num_bad_epochs = 0
         self._current_level = self.initial_level
 
     @property
     def current_sparsity_level(self) -> float:
         """
         Returns sparsity level for the `current_epoch` or for step
@@ -270,41 +277,42 @@
 
         self._current_level = min(current_level, self.target_level)
 
         return self._current_level
 
     def load_state(self, state: Dict[str, Any]) -> None:
         super().load_state(state)
-        self.num_bad_epochs = state['num_bad_epochs']
-        self._current_level = state['current_sparsity_level']
+        self.num_bad_epochs = state["num_bad_epochs"]
+        self._current_level = state["current_sparsity_level"]
 
     def get_state(self) -> Dict[str, Any]:
         state = super().get_state()
-        state['num_bad_epochs'] = self.num_bad_epochs
-        state['current_sparsity_level'] = self._current_level
+        state["num_bad_epochs"] = self.num_bad_epochs
+        state["current_sparsity_level"] = self._current_level
         return state
 
 
-@SPARSITY_SCHEDULERS.register('multistep')
+@SPARSITY_SCHEDULERS.register("multistep")
 class MultiStepSparsityScheduler(SparsityScheduler):
     """
     Sparsity scheduler with a piecewise constant schedule.
     """
 
     def __init__(self, controller: SparsityController, params: dict):
         """
         Initializes a sparsity scheduler with a piecewise constant schedule.
 
         :param controller: Sparsity algorithm controller.
         :param params: Parameters of the scheduler.
         """
         super().__init__(controller, params)
         self.schedule = MultiStepSchedule(
-            sorted(params.get('multistep_steps', SPARSITY_MULTISTEP_STEPS)),
-            params.get('multistep_sparsity_levels', SPARSITY_MULTISTEP_SPARSITY_LEVELS))
+            sorted(params.get("multistep_steps", SPARSITY_MULTISTEP_STEPS)),
+            params.get("multistep_sparsity_levels", SPARSITY_MULTISTEP_SPARSITY_LEVELS),
+        )
         self.target_level = self.schedule.values[-1]
 
     @property
     def current_sparsity_level(self) -> float:
         """
         Returns sparsity level for the `current_epoch` or for step
         in the `current_epoch`.
```

### Comparing `nncf-2.4.0/nncf/common/sparsity/statistics.py` & `nncf-2.5.0/nncf/common/sparsity/statistics.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,225 +1,207 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 from nncf.api.statistics import Statistics
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.helpers import create_table
 
 
+@api()
 class SparsifiedLayerSummary:
     """
     Contains information about the sparsified layer.
+
+    :param name: Layer's name.
+    :param weight_shape: Weight's shape.
+    :param sparsity_level: Sparsity level of the sparsified layer.
+    :param weight_percentage: Proportion of the layer's weights in the whole model.
     """
 
-    def __init__(self,
-                 name: str,
-                 weight_shape: List[int],
-                 sparsity_level: float,
-                 weight_percentage: float):
-        """
-        Initializes a summary about the sparsified layer.
-
-        :param name: Layer's name.
-        :param weight_shape: Weight's shape.
-        :param sparsity_level: Sparsity level of the sparsified layer.
-        :param weight_percentage: Proportion of the layer's weights in the whole model.
-        """
+    def __init__(self, name: str, weight_shape: List[int], sparsity_level: float, weight_percentage: float):
         self.name = name
         self.weight_shape = weight_shape
         self.sparsity_level = sparsity_level
         self.weight_percentage = weight_percentage
 
 
+@api()
 class SparsifiedModelStatistics(Statistics):
     """
     Contains statistics of the sparsified model.
+
+    :param sparsity_level: Sparsity level of the whole model.
+    :param sparsity_level_for_layers: Sparsity level of all sparsified layers
+      (i.e. layers for which the algorithm was applied).
+    :param sparsified_layers_summary: Detailed summary for the sparsified layers.
     """
 
-    def __init__(self,
-                 sparsity_level: float,
-                 sparsity_level_for_layers: float,
-                 sparsified_layers_summary: List[SparsifiedLayerSummary]):
-        """
-        Initializes statistics of the sparsified model.
-
-        :param sparsity_level: Sparsity level of the whole model.
-        :param sparsity_level_for_layers: Sparsity level of all
-            sparsified layers (i.e. layers for which the algorithm was applied).
-        :param sparsified_layers_summary: Detailed summary for the
-            sparsified layers.
-        """
+    def __init__(
+        self,
+        sparsity_level: float,
+        sparsity_level_for_layers: float,
+        sparsified_layers_summary: List[SparsifiedLayerSummary],
+    ):
         self.sparsity_level = sparsity_level
         self.sparsity_level_for_layers = sparsity_level_for_layers
         self.sparsified_layers_summary = sparsified_layers_summary
 
     def to_str(self) -> str:
         model_string = create_table(
-            header=['Statistic\'s name', 'Value'],
+            header=["Statistic's name", "Value"],
             rows=[
-                ['Sparsity level of the whole model', self.sparsity_level],
-                ['Sparsity level of all sparsified layers', self.sparsity_level_for_layers],
-            ]
+                ["Sparsity level of the whole model", self.sparsity_level],
+                ["Sparsity level of all sparsified layers", self.sparsity_level_for_layers],
+            ],
         )
 
         layers_string = create_table(
-            header=['Layer\'s name', 'Weight\'s shape', 'Sparsity level', 'Weight\'s percentage'],
+            header=["Layer's name", "Weight's shape", "Sparsity level", "Weight's percentage"],
             rows=[
                 [s.name, s.weight_shape, s.sparsity_level, s.weight_percentage] for s in self.sparsified_layers_summary
-            ]
+            ],
         )
 
         pretty_string = (
-            f'Statistics of the sparsified model:\n{model_string}\n\n'
-            f'Statistics by sparsified layers:\n{layers_string}'
+            f"Statistics of the sparsified model:\n{model_string}\n\n"
+            f"Statistics by sparsified layers:\n{layers_string}"
         )
         return pretty_string
 
 
 class LayerThreshold:
     def __init__(self, name: str, threshold: float):
         self.name = name
         self.threshold = threshold
 
 
+@api()
 class MagnitudeSparsityStatistics(Statistics):
     """
     Contains statistics of the magnitude sparsity algorithm.
+
+    :param model_statistics: Statistics of the sparsified model.
+    :param thresholds: List of the sparsity thresholds.
+    :param target_sparsity_level: A target level of the sparsity for the algorithm for the current epoch.
     """
 
-    def __init__(self,
-                 model_statistics: SparsifiedModelStatistics,
-                 thresholds: List[LayerThreshold],
-                 target_sparsity_level: float):
-        """
-        Initializes statistics of the magnitude sparsity algorithm.
-
-        :param model_statistics: Statistics of the sparsified model.
-        :param thresholds: List of the sparsity thresholds.
-        :param target_sparsity_level: A target level of the sparsity
-            for the algorithm for the current epoch.
-        """
+    def __init__(
+        self,
+        model_statistics: SparsifiedModelStatistics,
+        thresholds: List[LayerThreshold],
+        target_sparsity_level: float,
+    ):
         self.model_statistics = model_statistics
         self.thresholds = thresholds
         self.target_sparsity_level = target_sparsity_level
 
     def to_str(self) -> str:
         thresholds_string = create_table(
-            ['Layer\'s name', 'Sparsity threshold'],
-            [[s.name, s.threshold] for s in self.thresholds]
+            ["Layer's name", "Sparsity threshold"], [[s.name, s.threshold] for s in self.thresholds]
         )
 
         algorithm_string = create_table(
-            header=['Statistic\'s name', 'Value'],
+            header=["Statistic's name", "Value"],
             rows=[
-                ['A target level of the sparsity for the algorithm for the current epoch', self.target_sparsity_level],
-            ]
+                ["A target level of the sparsity for the algorithm for the current epoch", self.target_sparsity_level],
+            ],
         )
 
         pretty_string = (
-            f'{self.model_statistics.to_str()}\n\n'
-            f'Statistics of the magnitude sparsity algorithm:\n{algorithm_string}\n{thresholds_string}'
+            f"{self.model_statistics.to_str()}\n\n"
+            f"Statistics of the magnitude sparsity algorithm:\n{algorithm_string}\n{thresholds_string}"
         )
         return pretty_string
 
 
+@api()
 class ConstSparsityStatistics(Statistics):
     """
     Contains statistics of the const sparsity algorithm.
+
+    :param model_statistics: Statistics of the sparsified model.
     """
 
     def __init__(self, model_statistics: SparsifiedModelStatistics):
-        """
-        Initializes statistics of the const sparsity algorithm.
-
-        :param model_statistics: Statistics of the sparsified model.
-        """
         self.model_statistics = model_statistics
 
     def to_str(self) -> str:
         pretty_string = self.model_statistics.to_str()
         return pretty_string
 
 
+@api()
 class RBSparsityStatistics(Statistics):
     """
     Contains statistics of the RB-sparsity algorithm.
+
+    :param model_statistics: Statistics of the sparsified model.
+    :param target_sparsity_level: A target level of the sparsity for the algorithm for the current epoch.
+    :param mean_sparse_prob: The probability that one weight will be zeroed.
     """
 
-    def __init__(self,
-                 model_statistics: SparsifiedModelStatistics,
-                 target_sparsity_level: float,
-                 mean_sparse_prob: float):
-        """
-        Initializes statistics of the RB-sparsity algorithm.
-
-        :param model_statistics: Statistics of the sparsified model.
-        :param target_sparsity_level: A target level of the sparsity
-            for the algorithm for the current epoch.
-        :param mean_sparse_prob: The probability that one weight
-            will be zeroed.
-        """
+    def __init__(
+        self, model_statistics: SparsifiedModelStatistics, target_sparsity_level: float, mean_sparse_prob: float
+    ):
         self.model_statistics = model_statistics
         self.target_sparsity_level = target_sparsity_level
         self.mean_sparse_prob = mean_sparse_prob
 
     def to_str(self) -> str:
         algorithm_string = create_table(
-            header=['Statistic\'s name', 'Value'],
+            header=["Statistic's name", "Value"],
             rows=[
-                ['A target level of the sparsity for the algorithm for the current epoch', self.target_sparsity_level],
-                ['The probability that one weight will be zeroed', self.mean_sparse_prob],
-            ]
+                ["A target level of the sparsity for the algorithm for the current epoch", self.target_sparsity_level],
+                ["The probability that one weight will be zeroed", self.mean_sparse_prob],
+            ],
         )
 
         pretty_string = (
-            f'{self.model_statistics.to_str()}\n\n'
-            f'Statistics of the RB-sparsity algorithm:\n{algorithm_string}'
+            f"{self.model_statistics.to_str()}\n\n" f"Statistics of the RB-sparsity algorithm:\n{algorithm_string}"
         )
         return pretty_string
 
 
+@api()
 class MovementSparsityStatistics(Statistics):
     """
     Contains statistics of the movement-sparsity algorithm.
+
+    :param model_statistics: Statistics of the sparsified model.
+    :param importance_threshold: Importance threshold for sparsity binary mask.
+    :param importance_regularization_factor: Penalty factor of importance score.
     """
 
-    def __init__(self,
-                 model_statistics: SparsifiedModelStatistics,
-                 importance_threshold: float,
-                 importance_regularization_factor: float):
-        """
-        Initializes statistics of the movement-sparsity algorithm.
-
-        :param model_statistics: Statistics of the sparsified model.
-        :param importance_threshold: Importance threshold for sparsity binary mask.
-        :param importance_regularization_factor: Penalty factor of importance score.
-        """
+    def __init__(
+        self,
+        model_statistics: SparsifiedModelStatistics,
+        importance_threshold: float,
+        importance_regularization_factor: float,
+    ):
         self.model_statistics = model_statistics
         self.importance_threshold = importance_threshold
         self.importance_regularization_factor = importance_regularization_factor
 
     def to_str(self) -> str:
         algorithm_string = create_table(
-            header=['Statistic\'s name', 'Value'],
+            header=["Statistic's name", "Value"],
             rows=[
-                ['Mask Importance Threshold', self.importance_threshold],
-                ['Importance Regularization Factor', self.importance_regularization_factor],
-            ]
+                ["Mask Importance Threshold", self.importance_threshold],
+                ["Importance Regularization Factor", self.importance_regularization_factor],
+            ],
         )
 
         pretty_string = (
-            f'{self.model_statistics.to_str()}\n\n'
-            f'Statistics of the movement-sparsity algorithm:\n{algorithm_string}'
+            f"{self.model_statistics.to_str()}\n\n"
+            f"Statistics of the movement-sparsity algorithm:\n{algorithm_string}"
         )
         return pretty_string
```

### Comparing `nncf-2.4.0/nncf/common/stateful_classes_registry.py` & `nncf-2.5.0/nncf/common/stateful_classes_registry.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,30 +1,28 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import inspect
-from typing import Callable
-from typing import Dict
+from typing import Callable, Dict
 
 
 class StatefulClassesRegistry:
     """
     Registry for the stateful classes  - classes that can be restored from their state by `from_state` method.
     """
-    REQUIRED_METHOD_NAME = 'from_state'
+
+    REQUIRED_METHOD_NAME = "from_state"
 
     def __init__(self):
         self._name_vs_class_map = {}  # type: Dict[str, object]
         self._class_vs_name_map = {}  # type: Dict[object, str]
 
     def register(self, name: str = None) -> Callable:
         """
@@ -34,24 +32,29 @@
         :return: The inner function for registration.
         """
 
         def decorator(cls):
             registered_name = name if name is not None else cls.__name__
 
             if registered_name in self._name_vs_class_map:
-                raise ValueError('{} has already been registered to {}'.format(
-                    registered_name, self._name_vs_class_map[registered_name]))
+                raise ValueError(
+                    "{} has already been registered to {}".format(
+                        registered_name, self._name_vs_class_map[registered_name]
+                    )
+                )
 
             if cls in self._class_vs_name_map:
-                raise ValueError('{} has already been registered to {}'.format(
-                    cls, self._class_vs_name_map[cls]))
+                raise ValueError("{} has already been registered to {}".format(cls, self._class_vs_name_map[cls]))
 
             if inspect.isclass(cls) and not hasattr(cls, self.REQUIRED_METHOD_NAME):
-                raise ValueError('Cannot register a class ({}) that does not have {}() method.'.format(
-                    registered_name, self.REQUIRED_METHOD_NAME))
+                raise ValueError(
+                    "Cannot register a class ({}) that does not have {}() method.".format(
+                        registered_name, self.REQUIRED_METHOD_NAME
+                    )
+                )
 
             self._class_vs_name_map[cls] = registered_name
             self._name_vs_class_map[registered_name] = cls
 
             return cls
 
         return decorator
@@ -61,26 +64,26 @@
         Provides a class that was registered with the given name.
 
         :param registered_name: name
         :return: class that was registered with the given name
         """
         if registered_name in self._name_vs_class_map:
             return self._name_vs_class_map[registered_name]
-        raise KeyError('No registered stateful classes with {} name'.format(registered_name))
+        raise KeyError("No registered stateful classes with {} name".format(registered_name))
 
     def get_registered_name(self, stateful_cls: object) -> str:
         """
         Provides a name that was used to register the given stateful class.
 
         :param stateful_cls: class
         :return: name that was used on registration of the given class
         """
         if stateful_cls in self._class_vs_name_map:
             return self._class_vs_name_map[stateful_cls]
-        raise KeyError('The class {} was not registered.'.format(stateful_cls.__name__))
+        raise KeyError("The class {} was not registered.".format(stateful_cls.__name__))
 
 
 class CommonStatefulClassesRegistry:
     """
     Common for TF and PT registry for the stateful classes.
     """
```

### Comparing `nncf-2.4.0/nncf/common/statistics.py` & `nncf-2.5.0/nncf/common/statistics.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Optional
 
 from nncf.api.statistics import Statistics
+from nncf.common.pruning.statistics import FilterPruningStatistics
+from nncf.common.quantization.statistics import QuantizationStatistics
+from nncf.common.sparsity.statistics import ConstSparsityStatistics
 from nncf.common.sparsity.statistics import MagnitudeSparsityStatistics
-from nncf.common.sparsity.statistics import RBSparsityStatistics
 from nncf.common.sparsity.statistics import MovementSparsityStatistics
-from nncf.common.sparsity.statistics import ConstSparsityStatistics
-from nncf.common.quantization.statistics import QuantizationStatistics
-from nncf.common.pruning.statistics import FilterPruningStatistics
+from nncf.common.sparsity.statistics import RBSparsityStatistics
+from nncf.common.utils.api_marker import api
 
 
+@api()
 class NNCFStatistics(Statistics):
     """
     Groups statistics for all available NNCF compression algorithms.
     Statistics are present only if the algorithm has been started.
     """
 
     def __init__(self):
@@ -38,105 +38,106 @@
     def magnitude_sparsity(self) -> Optional[MagnitudeSparsityStatistics]:
         """
         Returns statistics of the magnitude sparsity algorithm. If statistics
         have not been collected, `None` will be returned.
 
         :return: Instance of the `MagnitudeSparsityStatistics` class.
         """
-        return self._storage.get('magnitude_sparsity')
+        return self._storage.get("magnitude_sparsity")
 
     @property
     def rb_sparsity(self) -> Optional[RBSparsityStatistics]:
         """
         Returns statistics of the RB-sparsity algorithm. If statistics
         have not been collected, `None` will be returned.
 
         :return: Instance of the `RBSparsityStatistics` class.
         """
-        return self._storage.get('rb_sparsity')
+        return self._storage.get("rb_sparsity")
 
     @property
     def movement_sparsity(self) -> Optional[MovementSparsityStatistics]:
         """
         Returns statistics of the movement sparsity algorithm. If statistics
         have not been collected, `None` will be returned.
 
         :return: Instance of the `MovementSparsityStatistics` class.
         """
-        return self._storage.get('movement_sparsity')
+        return self._storage.get("movement_sparsity")
 
     @property
     def const_sparsity(self) -> Optional[ConstSparsityStatistics]:
         """
         Returns statistics of the const sparsity algorithm. If statistics
         have not been collected, `None` will be returned.
 
         :return: Instance of the `ConstSparsityStatistics` class.
         """
-        return self._storage.get('const_sparsity')
+        return self._storage.get("const_sparsity")
 
     @property
     def quantization(self) -> Optional[QuantizationStatistics]:
         """
         Returns statistics of the quantization algorithm. If statistics
         have not been collected, `None` will be returned.
 
         :return: Instance of the `QuantizationStatistics` class.
         """
-        return self._storage.get('quantization')
+        return self._storage.get("quantization")
 
     @property
     def filter_pruning(self) -> Optional[FilterPruningStatistics]:
         """
         Returns statistics of the filter pruning algorithm. If statistics
         have not been collected, `None` will be returned.
 
         :return: Instance of the `FilterPruningStatistics` class.
         """
-        return self._storage.get('filter_pruning')
+        return self._storage.get("filter_pruning")
 
     @property
     def binarization(self) -> None:
-        """
-        Returns statistics of the binarization algorithm. If statistics
-        have not been collected, `None` will be returned.
-
-        :return: `None`.
-        """
-        return self._storage.get('binarization')
+        raise NotImplementedError
 
     def register(self, algorithm_name: str, stats: Statistics):
         """
         Registers statistics for the algorithm.
 
         :param algorithm_name: Name of the algorithm. Should be one of the following
-            - magnitude_sparsity
-            - rb_sparsity
-            - const_sparsity
-            - quantization
-            - filter_pruning
-            - binarization
+            * magnitude_sparsity
+            * rb_sparsity
+            * const_sparsity
+            * quantization
+            * filter_pruning
+            * binarization
+
         :param stats: Statistics of the algorithm.
         """
 
         available_algorithms = [
-            'magnitude_sparsity', 'rb_sparsity', 'movement_sparsity', 'const_sparsity',
-            'quantization', 'filter_pruning', 'binarization'
+            "magnitude_sparsity",
+            "rb_sparsity",
+            "movement_sparsity",
+            "const_sparsity",
+            "quantization",
+            "filter_pruning",
+            "binarization",
         ]
         if algorithm_name not in available_algorithms:
-            raise ValueError('Can not register statistics for the algorithm. '
-                             f'Unknown name of the algorithm: {algorithm_name}.')
+            raise ValueError(
+                "Can not register statistics for the algorithm. " f"Unknown name of the algorithm: {algorithm_name}."
+            )
 
         self._storage[algorithm_name] = stats
 
     def to_str(self) -> str:
         """
         Calls `to_str()` method for all registered statistics of the algorithm and returns
         a sum-up string.
 
         :return: A representation of the NNCF statistics as a human-readable string.
         """
-        pretty_string = '\n\n'.join([stats.to_str() for stats in self._storage.values()])
+        pretty_string = "\n\n".join([stats.to_str() for stats in self._storage.values()])
         return pretty_string
 
     def __iter__(self):
         return iter(self._storage.items())
```

### Comparing `nncf-2.4.0/nncf/common/tensor.py` & `nncf-2.5.0/nncf/torch/tensor.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,46 +1,32 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from abc import abstractmethod
-from typing import TypeVar, List, Optional
-
-TensorType = TypeVar('TensorType')
-DeviceType = TypeVar('DeviceType')
-TensorElementsType = TypeVar('TensorElementsType')
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+import torch
 
-class NNCFTensor:
-    """
-    An interface of framework specific tensors for common NNCF algorithms.
-    """
+from nncf.common.tensor import NNCFTensor
 
-    def __init__(self, tensor: Optional[TensorType]):
-        self._tensor = tensor
 
-    def __eq__(self, other: "NNCFTensor") -> bool:
-        return self._tensor == other.tensor
+class PTNNCFTensor(NNCFTensor):
+    """
+    A realisation of torch tensors wrapper for common NNCF algorithms.
+    """
 
-    @property
-    def tensor(self) -> TensorType:
-        return self._tensor
+    def __init__(self, tensor: torch.tensor):
+        # In case somebody attempts to wrap
+        # tensor twice
+        if isinstance(tensor, self.__class__):
+            tensor = tensor.tensor
 
-    @property
-    def shape(self) -> List[int]:
-        if self._tensor is None:
-            raise RuntimeError('Attempt to get shape of empty NNCFTensor')
-        return self._tensor.shape
+        super().__init__(tensor)
 
     @property
-    @abstractmethod
-    def device(self) -> DeviceType:
-        pass
+    def device(self) -> torch.device:
+        return self._tensor.device
```

### Comparing `nncf-2.4.0/nncf/common/tensor_statistics/aggregator.py` & `nncf-2.5.0/nncf/common/tensor_statistics/aggregator.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,35 +1,33 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from abc import ABC
 from abc import abstractmethod
 from itertools import islice
-from typing import Dict, TypeVar, Any
+from typing import Any, Dict, TypeVar
 
 from tqdm import tqdm
 
-from nncf.common.factory import ModelTransformerFactory
 from nncf.common.factory import EngineFactory
+from nncf.common.factory import ModelTransformerFactory
 from nncf.common.graph.transformations.layout import TransformationLayout
 from nncf.common.tensor import NNCFTensor
 from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
 from nncf.data.dataset import Dataset
 
-TensorType = TypeVar('TensorType')
-TModel = TypeVar('TModel')
+TensorType = TypeVar("TensorType")
+TModel = TypeVar("TModel")
 
 
 class StatisticsAggregator(ABC):
     """
     Base class for statistics collection.
     """
 
@@ -43,65 +41,83 @@
         Collects statistics for registered StatisticPoints.
         The statistics are stored in self.statistic_points.
 
         :param model: backend-specific model instance
         """
         model_transformer = ModelTransformerFactory.create(model)
 
-        transformation_layout = self._get_transformation_layout_extra_outputs(self.statistic_points)
+        merged_statistics = self._get_merged_statistic_points(self.statistic_points, model)
+        transformation_layout = self._get_transformation_layout_extra_outputs(merged_statistics)
         model_with_outputs = model_transformer.transform(transformation_layout)
         engine = EngineFactory.create(model_with_outputs)
 
-        for input_data in tqdm(islice(self.dataset.get_inference_data(), self.stat_subset_size),
-                               total=self.stat_subset_size):
+        for input_data in tqdm(
+            islice(self.dataset.get_inference_data(), self.stat_subset_size),
+            total=self.stat_subset_size,
+            desc="Statistics collection",
+        ):
             outputs = engine.infer(input_data)
             processed_outputs = self._process_outputs(outputs)
-            self._register_statistics(processed_outputs, self.statistic_points)
+            self._register_statistics(processed_outputs, merged_statistics)
 
-    def register_stastistic_points(self, statistic_points: StatisticPointsContainer) -> None:
+    def register_statistic_points(self, statistic_points: StatisticPointsContainer) -> None:
         """
         Register statistic points for statistics collection and recalculates the maximum number samples
         for collecting statistics, based on the maximum value from the all algorithms.
 
         :param statistic_points: StatisticPointsContainer instance with the statistic points
         """
         for _, _statistic_points in statistic_points.items():
             for _statistic_point in _statistic_points:
                 self.statistic_points.add_statistic_point(_statistic_point)
 
         for _, _statistic_points in self.statistic_points.items():
             for _statistic_point in _statistic_points:
                 for _, tensor_collectors in _statistic_point.algorithm_to_tensor_collectors.items():
                     for tensor_collector in tensor_collectors:
-                        self.stat_subset_size = max(
-                            self.stat_subset_size, tensor_collector.num_samples)
+                        self.stat_subset_size = max(self.stat_subset_size, tensor_collector.num_samples)
 
     @abstractmethod
-    def _register_statistics(self,
-                             outputs: Dict[str, NNCFTensor],
-                             statistic_points: StatisticPointsContainer) -> None:
+    def _register_statistics(self, outputs: Dict[str, NNCFTensor], statistic_points: StatisticPointsContainer) -> None:
         """
         Process prepared raw model outputs and statistic points for the further usage.
 
         :param outputs: prepared raw model outputs
         :param statistic_points: StatisticPointsContainer instance with the statistic points
         """
 
-    @staticmethod
     @abstractmethod
-    def _get_transformation_layout_extra_outputs(statistic_points: StatisticPointsContainer) -> TransformationLayout:
+    def _get_transformation_layout_extra_outputs(
+        self, statistic_points: StatisticPointsContainer
+    ) -> TransformationLayout:
         """
-        Create backend-specific transformation layout for the further statistics collection
+        Creates backend-specific transformation layout for the further statistics collection.
 
         :param statistic_points: StatisticPointsContainer to add outputs
         :return: TransformationLayout with the corresponding transformations
         """
 
     @staticmethod
     @abstractmethod
+    def _get_merged_statistic_points(
+        statistic_points: StatisticPointsContainer, model: TModel
+    ) -> StatisticPointsContainer:
+        """
+        Creates a new StatisticPointContainer that has no duplicated tensor collectors for one
+        unique statistic point. Alters statistic collectors in the given statistic point container so statistics
+        collected by merged statistic collectors will be available in all corresponding statistic collectors
+        from the given statistic point container.
+
+        :param statistic_points: Registered statistic points with possible tensor collectors duplicates.
+        :param model: Backend-specific target model.
+        :return: Merged statistic points container bounded with given statistic point container.
+        """
+
+    @staticmethod
+    @abstractmethod
     def _process_outputs(outputs: Any) -> Dict[str, NNCFTensor]:
         """
         Post-process model outputs for the further statistics collection.
 
         :param outputs: raw model outputs
         :return: processed model outputs in Dict[str, NNCFTensor] format
         """
```

### Comparing `nncf-2.4.0/nncf/common/tensor_statistics/collectors.py` & `nncf-2.5.0/nncf/common/tensor_statistics/collectors.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
 from collections import deque
-from typing import Tuple, Optional, List, Union
+from typing import Callable, List, Optional, Tuple, Union
 
 import numpy as np
+
 from nncf.common.tensor import NNCFTensor
-from nncf.common.tensor import TensorType
 from nncf.common.tensor import TensorElementsType
+from nncf.common.tensor import TensorType
 from nncf.common.tensor_statistics.reduction import get_per_channel_history
 
 ReductionShape = Tuple[int]
+MaskedReduceFN = Callable[[NNCFTensor, Union[int, tuple, list], NNCFTensor, bool], NNCFTensor]
 
 
 class TensorStatisticCollectorBase(ABC):
     """Collector estimate statistics at the quantization point based on the provided reduction shape."""
 
     def __init__(self, reduction_shape: Optional[ReductionShape] = None, num_samples: Optional[int] = None):
         """
@@ -96,47 +96,52 @@
 class OnlineTensorStatisticCollector(TensorStatisticCollectorBase):
     """Base class for collectors that collects statistics in online regime, without storing the data."""
 
 
 class OfflineTensorStatisticCollector(TensorStatisticCollectorBase):
     """Collects statistics in offline regime by storing the data and aggregating it afterwards."""
 
-    def __init__(self, reduction_shape: Optional[ReductionShape] = None,
-                 num_samples: int = None, window_size: int = None):
+    def __init__(
+        self, reduction_shape: Optional[ReductionShape] = None, num_samples: int = None, window_size: int = None
+    ):
         super().__init__(reduction_shape, num_samples)
         self._samples = deque(maxlen=window_size)
 
     def _reset(self):
         self._samples.clear()
 
 
 class NNCFCollectorTensorProcessor(ABC):
     """
     An interface of the processing methods for NNCFTensors.
     """
 
     @staticmethod
     @abstractmethod
-    def reduce_min(x: NNCFTensor, axis: Union[int, tuple, list]) -> NNCFTensor:
+    def reduce_min(x: NNCFTensor, axis: Union[int, tuple, list], keepdims: bool = False) -> NNCFTensor:
         """
-         Computes minimum of elements across dimensions of NNCFTensor.
+        Computes minimum of elements across dimensions of NNCFTensor.
 
-         :param x: NNCFTensor to reduce
-         :param axis: The dimensions to reduce.
-         :return: Reduced NNCFTensor.
-         """
+        :param x: NNCFTensor to reduce
+        :param axis: The dimensions to reduce.
+        :param keepdims: If this is set to True, the axes which are reduced are left
+           in the result as dimensions with size one.
+        :return: Reduced NNCFTensor.
+        """
 
     @staticmethod
     @abstractmethod
-    def reduce_max(x: NNCFTensor, axis: Union[int, tuple, list]) -> NNCFTensor:
+    def reduce_max(x: NNCFTensor, axis: Union[int, tuple, list], keepdims: bool = False) -> NNCFTensor:
         """
         Computes maximum of elements across dimensions of NNCFTensor.
 
         :param x: NNCFTensor to reduce
         :param axis: The dimensions to reduce.
+        :param keepdims: If this is set to True, the axes which are reduced are left
+            in the result as dimensions with size one.
         :return: Reduced NNCFTensor.
         """
 
     @staticmethod
     @abstractmethod
     def abs(x: NNCFTensor) -> NNCFTensor:
         """
@@ -166,37 +171,82 @@
         :param x1: NNCFTensor to compare.
         :param x2: NNCFTensor to compare.
         :return: Compared NNCFTensor.
         """
 
     @staticmethod
     @abstractmethod
-    def mean(x: NNCFTensor, axis: Union[int, tuple, list]) -> NNCFTensor:
+    def mean(x: NNCFTensor, axis: Union[int, tuple, list], keepdims=False) -> NNCFTensor:
         """
         Computes the mean of elements across given dimensions of NNCFTensor.
 
         :param x: NNCFTensor to reduce.
         :param axis: The dimensions to reduce.
+        :param keepdims: If this is set to True, the axes which are reduced are left
+            in the result as dimensions with size one.
+        :return: Reduced NNCFTensor.
+        """
+
+    @staticmethod
+    @abstractmethod
+    def median(x: NNCFTensor, axis: Union[int, tuple, list], keepdims=False) -> NNCFTensor:
+        """
+        Computes the median of elements across given dimensions of NNCFTensor.
+
+        :param x: NNCFTensor to reduce.
+        :param axis: The dimensions to reduce.
+        :param keepdims: If this is set to True, the axes which are reduced are left
+            in the result as dimensions with size one.
+        :return: Reduced NNCFTensor.
+        """
+
+    @staticmethod
+    @abstractmethod
+    def masked_mean(x: NNCFTensor, axis: Union[int, tuple, list], mask: NNCFTensor, keepdims=False) -> NNCFTensor:
+        """
+        Computes the masked mean of elements across given dimensions of NNCFTensor.
+
+        :param x: NNCFTensor to reduce.
+        :param axis: The dimensions to reduce.
+        :param maks: Boolean tensor that have the same shape as x. If an element in mask is True -
+            it is skipped during the aggregation.
+        :param keepdims: If True, the axes which are reduced are left in the result
+            as dimensions with size one.
         :return: Reduced NNCFTensor.
         """
 
     @staticmethod
     @abstractmethod
-    def stack(x: NNCFTensor) -> NNCFTensor:
+    def masked_median(x: NNCFTensor, axis: Union[int, tuple, list], mask: NNCFTensor, keepdims=False) -> NNCFTensor:
+        """
+        Computes the masked median of elements across given dimensions of NNCFTensor.
+
+        :param x: NNCFTensor to reduce.
+        :param axis: The dimensions to reduce.
+        :param maks: Boolean tensor that have the same shape as x. If an element in mask is True -
+            it is skipped during the aggregation.
+        :param keepdims: If True, the axes which are reduced are left in the result
+            as dimensions with size one.
+        :return: Reduced NNCFTensor.
+        """
+
+    @staticmethod
+    @abstractmethod
+    def stack(x: NNCFTensor, axis: int = 0) -> NNCFTensor:
         """
         Stacks a list or deque of NNCFTensors rank-R tensors into one NNCFTensor rank-(R+1) tensor.
 
         :param x: List or deque of NNCFTensors.
         :param axis: The axis to stack along.
         :return: Stacked NNCFTensor.
         """
 
     @staticmethod
     @abstractmethod
-    def unstack(x: NNCFTensor) -> List[NNCFTensor]:
+    def unstack(x: NNCFTensor, axis: int = 0) -> List[NNCFTensor]:
         """
         Unstack a NNCFTensor into list.
 
         :param x: NNCFTensor to unstack.
         :param axis: The axis to unstack along.
         :return: List of NNCFTensor.
         """
@@ -207,14 +257,58 @@
         """
         Returns a sum of each elements in a given NNCFTensor.
 
         :param tensor: Given NNCFTensor.
         :returns: Sum of each elements of the given NNCFTensor.
         """
 
+    @staticmethod
+    @abstractmethod
+    def quantile(
+        tensor: NNCFTensor, quantile: Union[float, List[float]], axis: Union[int, tuple, list], keepdims: bool = False
+    ) -> List[TensorElementsType]:
+        """
+        Compute the quantile-th percentile(s) of the data along the specified axis.
+
+        :param tensor: Given NNCFTensor.
+        :params quantile: Percentile or sequence of percentiles to compute, which must be between
+            0 and 1 inclusive.
+        :param axis: Axis or axes along which the percentiles are computed.
+        :param keepdims: If True, the axes which are reduced are left in the result
+            as dimensions with size one.
+        :returns: List of the quantile-th percentile(s) of the tensor elements.
+        """
+
+    @staticmethod
+    @abstractmethod
+    def mean_per_channel(x: NNCFTensor, axis: int) -> NNCFTensor:
+        """
+        Computes the mean of elements across given channel dimension of NNCFTensor.
+
+        :param x: NNCFTensor to reduce.
+        :param axis: The channel dimensions to reduce.
+        :return: Reduced NNCFTensor.
+        """
+
+    @classmethod
+    @abstractmethod
+    def no_outliers_map(cls, x: NNCFTensor, fn: MaskedReduceFN, axis: int = 0, alpha: float = 0.01) -> NNCFTensor:
+        """
+        Computes quantiles [alpha, 1 - alpha] on given tensor, masks all elements that
+        are smaller that alpha and bigger than 1 - alpha quantile and applies
+        given masked reduction function fn.
+
+        :param tensor: Given NNCFTensor.
+        :param fn: Masked reduce operation from the same NNCFCollectorTensorProcessor class.
+        :param axis: Axis along which the reduction function is computed.
+        :params alpha: Minimal percentile to filter outliers outside the range
+            [quantile(alpha), quantile(1 - alpha)]. Must be between 0 and 1. inclusive.
+        :returns: Result of given masked reduction function on filtered from outliers NNCFTensor.
+        """
+
 
 class MinMaxStatisticCollector(OnlineTensorStatisticCollector):
     """Collector estimates min of minimum values and max of maximum values."""
 
     def __init__(self, use_abs_max: bool, reduction_shape: ReductionShape, num_samples: int = None):
         super().__init__(reduction_shape, num_samples)
         self._use_abs_max = use_abs_max
@@ -251,20 +345,22 @@
 
 class MinMaxOfflineStatisticCollectorBase(OfflineTensorStatisticCollector):
     """
     Base class for collectors that aggregate statistics
     from minimum and maximum values of tensors.
     """
 
-    def __init__(self,
-                 use_per_sample_stats: bool,
-                 use_abs_max: bool,
-                 reduction_shape: ReductionShape,
-                 num_samples: int = None,
-                 window_size: int = None):
+    def __init__(
+        self,
+        use_per_sample_stats: bool,
+        use_abs_max: bool,
+        reduction_shape: ReductionShape,
+        num_samples: int = None,
+        window_size: int = None,
+    ):
         super().__init__(reduction_shape, num_samples)
         self._use_per_sample_stats = use_per_sample_stats
         self._use_abs_max = use_abs_max
         self._tensor_processor = self._get_processor()
 
         self._all_min_values = deque(maxlen=window_size)
         self._all_max_values = deque(maxlen=window_size)
@@ -301,22 +397,24 @@
 
 
 class MixedMinMaxStatisticCollector(MinMaxOfflineStatisticCollectorBase):
     """
     Collector aggregates (min or mean) of minimum values and (max or mean) of maximum values.
     """
 
-    def __init__(self,
-                 use_per_sample_stats: bool,
-                 use_abs_max: bool,
-                 use_means_of_mins: bool,
-                 use_means_of_maxs: bool,
-                 reduction_shape: ReductionShape,
-                 num_samples: int = None,
-                 window_size: int = None):
+    def __init__(
+        self,
+        use_per_sample_stats: bool,
+        use_abs_max: bool,
+        use_means_of_mins: bool,
+        use_means_of_maxs: bool,
+        reduction_shape: ReductionShape,
+        num_samples: int = None,
+        window_size: int = None,
+    ):
         super().__init__(use_per_sample_stats, use_abs_max, reduction_shape, num_samples, window_size)
         self._use_means_of_mins = use_means_of_mins
         self._use_means_of_maxs = use_means_of_maxs
 
     def _min_aggregate(self):
         stacked_min = self._tensor_processor.stack(self._all_min_values)
         if self._use_means_of_mins:
@@ -345,18 +443,17 @@
 
 
 class MeanStatisticCollector(OfflineTensorStatisticCollector):
     """
     Collector that aggregates statistics as mean along a pre-assigned axis.
     """
 
-    def __init__(self,
-                 reduction_shape: ReductionShape,
-                 num_samples: Optional[int] = None,
-                 window_size: Optional[int] = None) -> None:
+    def __init__(
+        self, reduction_shape: ReductionShape, num_samples: Optional[int] = None, window_size: Optional[int] = None
+    ) -> None:
         """
         :param reduction_shape: The shape for the reduction while statistics collection.
             For the MeanStatisticCollector this parameter contains the main axis.
         :param num_samples: Optional parameter for statistic collection that regulates
             the number of samples that will be processed.
         :param window_size: Optional maximum length for the statistic collection
         """
@@ -391,16 +488,15 @@
 
 class BatchStatisticCollector(OfflineTensorStatisticCollector):
     """
     Collects tensor samples, where each tensor is averaged along the batch axis (and only that axis).
     Each sample stays available for usage in further stages of the algorithm.
     """
 
-    def __init__(self,
-                 num_samples: Optional[int] = None) -> None:
+    def __init__(self, num_samples: Optional[int] = None) -> None:
         """
         :param num_samples: Optional parameter for statistic collection that regulates
             the number of samples that will be processed.
         """
         super().__init__(num_samples=num_samples)
         self._tensor_processor = self._get_processor()
         self._all_values = []
@@ -419,35 +515,36 @@
 
 class MedianMADStatisticCollector(OfflineTensorStatisticCollector):
     """
     Collector estimates median and median absolute deviation (MAD).
     """
 
     def _prepare_statistics(self):
-        per_channel_history = get_per_channel_history(self._samples, list(self._reduction_shape),
-                                                      discard_zeros=True)
+        per_channel_history = get_per_channel_history(self._samples, list(self._reduction_shape), discard_zeros=True)
         per_channel_median = [np.median(channel_hist) for channel_hist in per_channel_history]
         per_channel_mad = []
         for idx, median in enumerate(per_channel_median):
             per_channel_mad.append(np.median(abs(per_channel_history[idx] - median)))
         numpy_median = np.asarray(per_channel_median)
         numpy_mad = np.asarray(per_channel_mad)
         return numpy_median, numpy_mad
 
 
 class PercentileStatisticCollector(OfflineTensorStatisticCollector):
     """
     Collector estimates percentile values of all data history.
     """
 
-    def __init__(self,
-                 percentiles_to_collect: List[float],
-                 reduction_shape: Optional[ReductionShape] = None,
-                 num_samples: int = None,
-                 window_size: int = None):
+    def __init__(
+        self,
+        percentiles_to_collect: List[float],
+        reduction_shape: Optional[ReductionShape] = None,
+        num_samples: int = None,
+        window_size: int = None,
+    ):
         super().__init__(reduction_shape, num_samples, window_size)
         self._percentiles_to_collect = percentiles_to_collect
 
     def _prepare_statistics(self):
         per_channel_history = get_per_channel_history(self._samples, list(self._reduction_shape))
         percentile_vs_values_dict = {}
         for pc in self._percentiles_to_collect:
@@ -458,19 +555,21 @@
 
 
 class MeanPercentileStatisticCollector(OfflineTensorStatisticCollector):
     """
     Collector estimates percentile values per step and then averages the results.
     """
 
-    def __init__(self,
-                 percentiles_to_collect: List[float],
-                 reduction_shape: Optional[ReductionShape] = None,
-                 num_samples: int = None,
-                 window_size: int = None):
+    def __init__(
+        self,
+        percentiles_to_collect: List[float],
+        reduction_shape: Optional[ReductionShape] = None,
+        num_samples: int = None,
+        window_size: int = None,
+    ):
         super().__init__(reduction_shape, num_samples, window_size)
         self._all_pct_values = {}
         for pc in percentiles_to_collect:
             self._all_pct_values[pc] = deque(maxlen=window_size)
 
     def _reset(self):
         for _, val in self._all_pct_values.items():
```

### Comparing `nncf-2.4.0/nncf/common/tensor_statistics/reduction.py` & `nncf-2.5.0/nncf/common/tensor_statistics/reduction.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from collections import deque
 from typing import List, Tuple
 
 import numpy as np
```

### Comparing `nncf-2.4.0/nncf/common/tensor_statistics/statistics.py` & `nncf-2.5.0/nncf/common/tensor_statistics/statistics.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
 from collections import Counter
 from typing import TypeVar
 
-TensorType = TypeVar('TensorType')
+TensorType = TypeVar("TensorType")
 
 
 class TensorStatistic(ABC):
     """Base class that stores statistic data"""
 
     @staticmethod
     @abstractmethod
@@ -28,68 +27,77 @@
 
     @abstractmethod
     def __eq__(self, other):
         pass
 
 
 class MinMaxTensorStatistic(TensorStatistic):
+    MIN_STAT = "min_values"
+    MAX_STAT = "max_values"
+
     def __init__(self, min_values, max_values):
         self.min_values = min_values
         self.max_values = max_values
 
-    def __eq__(self, other: 'MinMaxTensorStatistic') -> bool:
-        return self.tensor_eq(self.min_values, other.min_values) and \
-               self.tensor_eq(self.max_values, other.max_values)
+    def __eq__(self, other: "MinMaxTensorStatistic") -> bool:
+        return self.tensor_eq(self.min_values, other.min_values) and self.tensor_eq(self.max_values, other.max_values)
 
 
 class MeanTensorStatistic(TensorStatistic):
+    MEAN_STAT = "mean_values"
+    SHAPE_STAT = "shape"
+
     """
     Base class for the statistics that collects as mean per-axis
     """
+
     def __init__(self, mean_values, shape):
         """
         :param mean_values: ollected mean per-axis values.
         :param shape: The shape of the collected statistics.
         """
         self.mean_values = mean_values
         self.shape = shape
 
-    def __eq__(self, other: 'MeanTensorStatistic') -> bool:
-        return self.tensor_eq(self.mean_values, other.mean_values) and \
-            self.tensor_eq(self.shape, other.shape)
+    def __eq__(self, other: "MeanTensorStatistic") -> bool:
+        return self.tensor_eq(self.mean_values, other.mean_values) and self.tensor_eq(self.shape, other.shape)
 
 
 class MedianMADTensorStatistic(TensorStatistic):
     def __init__(self, median_values, mad_values):
         self.median_values = median_values
         self.mad_values = mad_values
 
-    def __eq__(self, other: 'MedianMADTensorStatistic') -> bool:
-        return self.tensor_eq(self.median_values, other.median_values) and \
-               self.tensor_eq(self.mad_values, other.mad_values)
+    def __eq__(self, other: "MedianMADTensorStatistic") -> bool:
+        return self.tensor_eq(self.median_values, other.median_values) and self.tensor_eq(
+            self.mad_values, other.mad_values
+        )
 
 
 class PercentileTensorStatistic(TensorStatistic):
     def __init__(self, percentile_vs_values_dict):
         self.percentile_vs_values_dict = percentile_vs_values_dict
 
-    def __eq__(self, other: 'PercentileTensorStatistic', rtol=1e-9) -> bool:
+    def __eq__(self, other: "PercentileTensorStatistic", rtol=1e-9) -> bool:
         if Counter(self.percentile_vs_values_dict.keys()) != Counter(other.percentile_vs_values_dict.keys()):
             return False
         for pct in self.percentile_vs_values_dict.keys():
-            if not self.tensor_eq(self.percentile_vs_values_dict[pct],
-                                        other.percentile_vs_values_dict[pct]):
+            if not self.tensor_eq(self.percentile_vs_values_dict[pct], other.percentile_vs_values_dict[pct]):
                 return False
         return True
 
+
 class BatchTensorStatistic(TensorStatistic):
+    VALUES_STATS = "values"
+
     """
     Base class for the statistics that collects as mean per-batch
     """
+
     def __init__(self, values):
         """
         :param values: ollected per-batch values.
         """
         self.values = values
 
-    def __eq__(self, other: 'BatchTensorStatistic') -> bool:
+    def __eq__(self, other: "BatchTensorStatistic") -> bool:
         return self.tensor_eq(self.values, other.values)
```

### Comparing `nncf-2.4.0/nncf/common/utils/backend.py` & `nncf-2.5.0/nncf/common/utils/backend.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,60 +1,64 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from copy import deepcopy
 from enum import Enum
+from typing import TypeVar
 
-from nncf.api.compression import CompressionAlgorithmController
+TModel = TypeVar("TModel")
 
 
 class BackendType(Enum):
-    TORCH = 'Torch'
-    TENSORFLOW = 'Tensorflow'
-    ONNX = 'ONNX'
-    OPENVINO = 'OpenVINO'
+    TORCH = "Torch"
+    TENSORFLOW = "Tensorflow"
+    ONNX = "ONNX"
+    OPENVINO = "OpenVINO"
 
 
 def get_backend(model) -> BackendType:
     """
     Returns the NNCF backend name string inferred from the type of the model object passed into this function.
 
     :param model: The framework-specific object representing the trainable model.
     :return: A BackendType representing the correct NNCF backend to be used when working with the framework.
     """
     available_frameworks = []
     try:
         import torch
-        available_frameworks.append('PyTorch')
+
+        available_frameworks.append("PyTorch")
     except ImportError:
         torch = None
 
     try:
         import tensorflow
-        available_frameworks.append('Tensorflow')
+
+        available_frameworks.append("Tensorflow")
     except ImportError:
         tensorflow = None
 
     try:
         import onnx
-        available_frameworks.append('ONNX')
+
+        available_frameworks.append("ONNX")
     except ImportError:
         onnx = None
 
     try:
         import openvino.runtime as ov
-        available_frameworks.append('OpenVINO')
+
+        available_frameworks.append("OpenVINO")
     except ImportError:
         ov = None
 
     if torch is not None and isinstance(model, torch.nn.Module):
         return BackendType.TORCH
 
     if tensorflow is not None and isinstance(model, tensorflow.Module):
@@ -62,22 +66,33 @@
 
     if onnx is not None and isinstance(model, onnx.ModelProto):
         return BackendType.ONNX
 
     if ov is not None and isinstance(model, ov.Model):
         return BackendType.OPENVINO
 
-    raise RuntimeError('Could not infer the backend framework from the model type because '
-                       'the framework is not available or the model type is unsupported. '
-                       'The available frameworks found: {}.'.format(', '.join(available_frameworks)))
+    raise RuntimeError(
+        "Could not infer the backend framework from the model type because "
+        "the framework is not available or the model type is unsupported. "
+        "The available frameworks found: {}.".format(", ".join(available_frameworks))
+    )
 
 
-def infer_backend_from_compression_controller(compression_controller: CompressionAlgorithmController) -> BackendType:
+def copy_model(model: TModel) -> TModel:
     """
-    Returns the NNCF backend name string inferred from the type of the model
-    stored in the passed compression controller.
+    Function to create copy of the backend-specific model.
 
-    :param compression_controller: Passed compression controller
-    (of CompressionAlgorithmController type).
-    :return: A BackendType representing the NNCF backend.
+    :param model: the backend-specific model instance
+    :return: Copy of the backend-specific model instance
     """
-    return get_backend(compression_controller.model)
+    model_backend = get_backend(model)
+    if model_backend == BackendType.OPENVINO:
+        # TODO(l-bat): Remove after fixing ticket: 100919
+        return model.clone()
+    if model_backend == BackendType.TENSORFLOW:
+        # deepcopy and tensorflow.keras.models.clone_model does not work correctly on 2.8.4 version
+        from nncf.tensorflow.graph.model_transformer import TFModelTransformer
+        from nncf.tensorflow.graph.transformations.layout import TFTransformationLayout
+
+        model = TFModelTransformer(model).transform(TFTransformationLayout())
+        return model
+    return deepcopy(model)
```

### Comparing `nncf-2.4.0/nncf/common/utils/debug.py` & `nncf-2.5.0/nncf/common/utils/debug.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import logging
 from contextlib import contextmanager
 
 from nncf.common.logging import nncf_logger
 
 DEBUG_LOG_DIR = "./nncf_debug"
@@ -26,11 +24,12 @@
 def set_debug_log_dir(dir_: str):
     global DEBUG_LOG_DIR
     DEBUG_LOG_DIR = dir_
 
 
 @contextmanager
 def nncf_debug():
-    from nncf.common.logging.logger import set_log_level #pylint: disable=cyclic-import
+    from nncf.common.logging.logger import set_log_level  # pylint: disable=cyclic-import
+
     set_log_level(logging.DEBUG)
     yield
     set_log_level(logging.INFO)
```

### Comparing `nncf-2.4.0/nncf/common/utils/decorators.py` & `nncf-2.5.0/nncf/common/utils/decorators.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from importlib import import_module
 from typing import Callable, List
 
 from nncf.common.logging import nncf_logger
 
 IMPORTED_DEPENDENCIES = {}
@@ -22,30 +20,34 @@
 def skip_if_dependency_unavailable(dependencies: List[str]) -> Callable:
     """
     Decorator factory to skip a noreturn function if dependencies are not met.
 
     :param dependencies: A list of dependencies
     :return: A decorator
     """
+
     def wrap(func: Callable[..., None]) -> Callable[..., None]:
         def wrapped_f(*args, **kwargs):
             for libname in dependencies:
                 if libname in IMPORTED_DEPENDENCIES:
                     if IMPORTED_DEPENDENCIES[libname]:
                         continue
                     break
                 try:
                     _ = import_module(libname)
                     IMPORTED_DEPENDENCIES[libname] = True
                 except ImportError as ex:
                     nncf_logger.warning(
-                        f'{ex.msg} Please install NNCF package with dev '
-                        'extra. Use one of the following commands '
+                        f"{ex.msg} Please install NNCF package with dev "
+                        "extra. Use one of the following commands "
                         '"pip install .[dev]" running from the repository '
-                        'root directory or "pip install nncf[dev]"')
+                        'root directory or "pip install nncf[dev]"'
+                    )
                     IMPORTED_DEPENDENCIES[libname] = False
                     break
             else:
                 return func(*args, **kwargs)
             return None
+
         return wrapped_f
+
     return wrap
```

### Comparing `nncf-2.4.0/nncf/common/utils/dot_file_rw.py` & `nncf-2.5.0/nncf/common/utils/dot_file_rw.py`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/common/utils/helpers.py` & `nncf-2.5.0/nncf/common/utils/helpers.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,27 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import datetime
 import itertools
 import os
 import os.path as osp
-import datetime
-
-from typing import Dict
-from typing import Hashable
-from typing import List, Any
+from typing import Any, Dict, Hashable, List
 
 from texttable import Texttable
 
 
 def create_table(header: List[str], rows: List[List[Any]]) -> str:
     """
     Returns a string which represents a table with a header and rows.
@@ -38,17 +33,16 @@
     Create a subdirectory inside of the passed log directory
     to save checkpoints from the accuracy-aware training loop to.
 
     :param log_dir: Path to the main log directory.
     :return: Path to the accuracy-aware training subdirectory.
     """
     d = datetime.datetime.now()
-    run_id = '{:%Y-%m-%d__%H-%M-%S}'.format(d)
-    acc_aware_log_dir = osp.join(log_dir,
-                                 'accuracy_aware_training/{run_id}'.format(run_id=run_id))
+    run_id = "{:%Y-%m-%d__%H-%M-%S}".format(d)
+    acc_aware_log_dir = osp.join(log_dir, "accuracy_aware_training/{run_id}".format(run_id=run_id))
     os.makedirs(acc_aware_log_dir, exist_ok=True)
     return acc_aware_log_dir
 
 
 def product_dict(d: Dict[Hashable, List]) -> Dict:
     """
     Generates dicts which enumerate the options for keys given in the input dict;
```

### Comparing `nncf-2.4.0/nncf/common/utils/logger/__init__.py` & `nncf-2.5.0/nncf/common/utils/logger/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,26 +1,25 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from nncf.common.deprecation import warning_deprecated
 
-#pylint:disable=wrong-import-position
-from nncf.common.logging.logger import warning_deprecated
+# pylint:disable=wrong-import-position
 
 warning_deprecated(
     "Importing from nncf.common.utils.logger is deprecated. "
     "Import `from nncf` directly instead, i.e.: \n"
     "`from nncf import set_log_level` instead of `from nncf.common.utils.logger import set_log_level`, and:\n"
-    "`from nncf import nncf_logger` instead of `from nncf.common.utils.logger import logger as nncf_logger`")
+    "`from nncf import nncf_logger` instead of `from nncf.common.utils.logger import logger as nncf_logger`"
+)
 
-#pylint:disable=unused-import
+# pylint:disable=unused-import
+from nncf.common.logging.logger import disable_logging
 from nncf.common.logging.logger import nncf_logger as logger
 from nncf.common.logging.logger import set_log_level
-from nncf.common.logging.logger import disable_logging
```

### Comparing `nncf-2.4.0/nncf/common/utils/os.py` & `nncf-2.5.0/nncf/common/utils/os.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,39 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import sys
 from contextlib import contextmanager
 from pathlib import Path
 
+
 # pylint: disable=W1514
 @contextmanager
 def safe_open(file: Path, *args, **kwargs):
     """
     Safe function to open file and return a stream.
 
     For security reasons, should not follow symlinks. Use .resolve() on any Path
     objects before passing them here.
 
     :param file: The path to the file.
     :return: A file object.
     """
     if file.is_symlink():
-        raise RuntimeError('File {} is a symbolic link, aborting.'.format(str(file)))
+        raise RuntimeError("File {} is a symbolic link, aborting.".format(str(file)))
     with open(str(file), *args, **kwargs) as f:
         yield f
+
+
+def is_windows():
+    return "win32" in sys.platform
+
+
+def is_linux():
+    return "linux" in sys.platform
```

### Comparing `nncf-2.4.0/nncf/common/utils/registry.py` & `nncf-2.5.0/nncf/common/utils/registry.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,23 +1,21 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 
 class Registry:
-    REGISTERED_NAME_ATTR = '_registered_name'
+    REGISTERED_NAME_ATTR = "_registered_name"
 
     def __init__(self, name, add_name_as_attr=False):
         self._name = name
         self._registry_dict = {}
         self._add_name_as_attr = add_name_as_attr
 
     @property
@@ -25,15 +23,15 @@
         return self._registry_dict
 
     def values(self):
         return self._registry_dict.values()
 
     def _register(self, obj, name):
         if name in self._registry_dict:
-            raise KeyError('{} is already registered in {}'.format(name, self._name))
+            raise KeyError("{} is already registered in {}".format(name, self._name))
         self._registry_dict[name] = obj
 
     def register(self, name=None):
         def wrap(obj):
             cls_name = name
             if cls_name is None:
                 cls_name = obj.__name__
@@ -46,11 +44,11 @@
 
     def get(self, name):
         if name not in self._registry_dict:
             self._key_not_found(name)
         return self._registry_dict[name]
 
     def _key_not_found(self, name):
-        raise KeyError('{} is unknown type of {} '.format(name, self._name))
+        raise KeyError("{} is unknown type of {} ".format(name, self._name))
 
     def __contains__(self, item):
         return item in self._registry_dict.values()
```

### Comparing `nncf-2.4.0/nncf/common/utils/tensorboard.py` & `nncf-2.5.0/nncf/common/utils/tensorboard.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,29 +1,27 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict
 from functools import singledispatch
+from typing import Dict
 
-from nncf.common.statistics import NNCFStatistics
 from nncf.common.pruning.statistics import FilterPruningStatistics
+from nncf.common.sparsity.statistics import ConstSparsityStatistics
 from nncf.common.sparsity.statistics import MagnitudeSparsityStatistics
-from nncf.common.sparsity.statistics import RBSparsityStatistics
 from nncf.common.sparsity.statistics import MovementSparsityStatistics
-from nncf.common.sparsity.statistics import ConstSparsityStatistics
+from nncf.common.sparsity.statistics import RBSparsityStatistics
+from nncf.common.statistics import NNCFStatistics
 
 
 def prepare_for_tensorboard(nncf_stats: NNCFStatistics) -> Dict[str, float]:
     """
     Extracts scalar values from NNCF statistics for its reporting to the TensorBoard.
 
     :param nncf_stats: NNCF Statistics.
@@ -40,40 +38,40 @@
 def convert_to_dict(stats, algorithm_name: str):
     return {}
 
 
 @convert_to_dict.register(FilterPruningStatistics)
 def _(stats, algorithm_name):
     tensorboard_stats = {
-        f'{algorithm_name}/algo_current_pruning_level': stats.current_pruning_level,
-        f'{algorithm_name}/model_FLOPS_pruning_level': stats.model_statistics.flops_pruning_level,
-        f'{algorithm_name}/model_params_pruning_level': stats.model_statistics.params_pruning_level,
-        f'{algorithm_name}/model_filters_pruning_level': stats.model_statistics.filter_pruning_level,
+        f"{algorithm_name}/algo_current_pruning_level": stats.current_pruning_level,
+        f"{algorithm_name}/model_FLOPS_pruning_level": stats.model_statistics.flops_pruning_level,
+        f"{algorithm_name}/model_params_pruning_level": stats.model_statistics.params_pruning_level,
+        f"{algorithm_name}/model_filters_pruning_level": stats.model_statistics.filter_pruning_level,
     }
     return tensorboard_stats
 
 
 @convert_to_dict.register(MagnitudeSparsityStatistics)
 @convert_to_dict.register(RBSparsityStatistics)
 @convert_to_dict.register(ConstSparsityStatistics)
 def _(stats, algorithm_name):
     tensorboard_stats = {
-        f'{algorithm_name}/sparsity_level_for_model': stats.model_statistics.sparsity_level,
-        f'{algorithm_name}/sparsity_level_for_sparsified_layers': stats.model_statistics.sparsity_level_for_layers,
+        f"{algorithm_name}/sparsity_level_for_model": stats.model_statistics.sparsity_level,
+        f"{algorithm_name}/sparsity_level_for_sparsified_layers": stats.model_statistics.sparsity_level_for_layers,
     }
 
-    target_sparsity_level = getattr(stats, 'target_sparsity_level', None)
+    target_sparsity_level = getattr(stats, "target_sparsity_level", None)
     if target_sparsity_level is not None:
-        tensorboard_stats[f'{algorithm_name}/target_sparsity_level'] = target_sparsity_level
+        tensorboard_stats[f"{algorithm_name}/target_sparsity_level"] = target_sparsity_level
 
     return tensorboard_stats
 
 
 @convert_to_dict.register(MovementSparsityStatistics)
 def _(stats, algorithm_name):
     tensorboard_stats = {
-        f'{algorithm_name}/model_sparsity': stats.model_statistics.sparsity_level,
-        f'{algorithm_name}/linear_layer_sparsity': stats.model_statistics.sparsity_level_for_layers,
-        f'{algorithm_name}/importance_threshold': stats.importance_threshold,
-        f'{algorithm_name}/importance_regularization_factor': stats.importance_regularization_factor,
+        f"{algorithm_name}/model_sparsity": stats.model_statistics.sparsity_level,
+        f"{algorithm_name}/linear_layer_sparsity": stats.model_statistics.sparsity_level_for_layers,
+        f"{algorithm_name}/importance_threshold": stats.importance_threshold,
+        f"{algorithm_name}/importance_regularization_factor": stats.importance_regularization_factor,
     }
     return tensorboard_stats
```

### Comparing `nncf-2.4.0/nncf/config/__init__.py` & `nncf-2.5.0/nncf/onnx/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,13 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Base subpackage for NNCF ONNX functionality.
 """
-
-from nncf.config.config import NNCFConfig
```

### Comparing `nncf-2.4.0/nncf/config/config.py` & `nncf-2.5.0/nncf/config/config.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,67 +1,81 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from copy import deepcopy
 from pathlib import Path
-from typing import List
-from typing import Optional
-from typing import Type
+from typing import Dict, List, Optional, Type
 
 import jsonschema
 import jstyleson as json
 
 from nncf.common.logging import nncf_logger
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.os import safe_open
 from nncf.config.definitions import SCHEMA_VISUALIZATION_URL
-from nncf.config.schema import REF_VS_ALGO_SCHEMA
 from nncf.config.schema import NNCF_CONFIG_SCHEMA
+from nncf.config.schema import REF_VS_ALGO_SCHEMA
 from nncf.config.schema import validate_single_compression_algo_schema
 from nncf.config.structures import NNCFExtraConfigStruct
 
 
+@api(canonical_alias="nncf.NNCFConfig")
 class NNCFConfig(dict):
-    """A regular dictionary object extended with some utility functions."""
+    """Contains the configuration parameters required for NNCF to apply the selected algorithms.
+
+    This is a regular dictionary object extended with some utility functions, such as the ability to attach well-defined
+    structures to pass non-serializable objects as parameters. It is primarily built from a .json file, or from a
+    Python JSON-like dictionary - both data types will be checked against a JSONSchema. See the definition of the
+    schema at https://openvinotoolkit.github.io/nncf/schema/, or by calling NNCFConfig.schema()."""
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.__nncf_extra_structs = {}  # type: dict[str, NNCFExtraConfigStruct]
 
     @classmethod
-    def from_dict(cls, nncf_dict):
+    def from_dict(cls, nncf_dict: Dict) -> "NNCFConfig":
         """
-        Load NNCF config from dict;
-        The dict must contain only json supported primitives.
+        Load NNCF config from a Python dictionary. The dict must contain only JSON-supported primitives.
+
+        :param nncf_dict: A Python dict with the JSON-style configuration for NNCF.
         """
 
         NNCFConfig.validate(nncf_dict)
         return cls(deepcopy(nncf_dict))
 
     @classmethod
-    def from_json(cls, path) -> 'NNCFConfig':
+    def from_json(cls, path: str) -> "NNCFConfig":
+        """
+        Load NNCF config from a JSON file at `path`.
+
+        :param path: Path to the .json file containing the NNCF configuration.
+        """
         file_path = Path(path).resolve()
         with safe_open(file_path) as f:
             loaded_json = json.load(f)
         return cls.from_dict(loaded_json)
 
     def register_extra_structs(self, struct_list: List[NNCFExtraConfigStruct]):
+        """
+        Attach the supplied list of extra configuration structures to this configuration object.
+
+        :param struct_list: List of extra configuration structures.
+        """
         for struct in struct_list:
             struct_id = struct.get_id()
             if struct_id in self.__nncf_extra_structs:
-                raise RuntimeError(f'{struct_id} is already registered as extra struct in NNCFConfig!')
+                raise RuntimeError(f"{struct_id} is already registered as extra struct in NNCFConfig!")
             self.__nncf_extra_structs[struct_id] = struct
 
     def get_extra_struct(self, struct_cls: Type[NNCFExtraConfigStruct]) -> NNCFExtraConfigStruct:
         return self.__nncf_extra_structs[struct_cls.get_id()]
 
     def has_extra_struct(self, struct_cls: Type[NNCFExtraConfigStruct]) -> NNCFExtraConfigStruct:
         return struct_cls.get_id() in self.__nncf_extra_structs
@@ -79,42 +93,48 @@
         :param param_name: The name of a parameter in the .json specification of the NNCFConfig, that may
           be present either at the top-most level of the .json, or at the top level of the algorithm-specific
           subdict.
         :param algo_name: The name of the algorithm (among the allowed algorithm names in the .json) for which
           the resolution of the redefinable parameter should occur.
         :return: The value of the parameter that should be applied for the algo specified by `algo_name`.
         """
-        from nncf.config.extractors import extract_algo_specific_config #pylint: disable=cyclic-import
+        from nncf.config.extractors import extract_algo_specific_config  # pylint: disable=cyclic-import
+
         algo_config = extract_algo_specific_config(self, algo_name)
         param = self.get(param_name)
         algo_specific_param = algo_config.get(param_name)
         if algo_specific_param is not None:
             param = algo_specific_param
         return param
 
     @staticmethod
-    def schema():
+    def schema() -> Dict:
+        """
+        Returns the JSONSchema against which the input data formats (.json or Python dict) are validated.
+        """
         return NNCF_CONFIG_SCHEMA
 
     @staticmethod
     def _is_path_to_algorithm_name(path_parts: List[str]) -> bool:
-        return (len(path_parts) == 2 and path_parts[0] == "compression" and path_parts[1] == "algorithm") or \
-               (len(path_parts) == 3 and path_parts[0] == "compression" and path_parts[1].isnumeric()
-                and path_parts[2] == "algorithm")
+        return (len(path_parts) == 2 and path_parts[0] == "compression" and path_parts[1] == "algorithm") or (
+            len(path_parts) == 3
+            and path_parts[0] == "compression"
+            and path_parts[1].isnumeric()
+            and path_parts[2] == "algorithm"
+        )
 
     @staticmethod
     def validate(loaded_json):
         try:
             jsonschema.validate(loaded_json, NNCFConfig.schema())
         except jsonschema.ValidationError as e:
-            nncf_logger.error('Invalid NNCF config supplied!')
+            nncf_logger.error("Invalid NNCF config supplied!")
             absolute_path_parts = [str(x) for x in e.absolute_path]
             if not NNCFConfig._is_path_to_algorithm_name(absolute_path_parts):
-                e.message += f"\nRefer to the NNCF config schema documentation at " \
-                             f"{SCHEMA_VISUALIZATION_URL}"
+                e.message += f"\nRefer to the NNCF config schema documentation at " f"{SCHEMA_VISUALIZATION_URL}"
                 e.schema = "*schema too long for stdout display*"
                 raise e
 
             # Need to make the error more algo-specific in case the config was so bad that no
             # scheme could be matched
             # If error is in the algo section, will revalidate the algo sections separately to
             # make the error message more targeted instead of displaying the entire huge schema.
```

### Comparing `nncf-2.4.0/nncf/config/definitions.py` & `nncf-2.5.0/nncf/config/definitions.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,38 +1,36 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-ONLINE_DOCS_ROOT = 'https://github.com/openvinotoolkit/nncf/tree/develop/'
-SCHEMA_VISUALIZATION_URL = 'https://openvinotoolkit.github.io/nncf/'
+ONLINE_DOCS_ROOT = "https://github.com/openvinotoolkit/nncf/tree/develop/"
+SCHEMA_VISUALIZATION_URL = "https://openvinotoolkit.github.io/nncf/"
 
 ADAPTIVE_COMPRESSION_LEVEL_TRAINING_MODE_NAME_IN_CONFIG = "adaptive_compression_level"
 EARLY_EXIT_TRAINING_MODE_NAME_IN_CONFIG = "early_exit"
-EXPERIMENTAL_QUANTIZATION_ALGO_NAME_IN_CONFIG = 'experimental_quantization'
-BOOTSTRAP_NAS_ALGO_NAME_IN_CONFIG = 'bootstrapNAS'
+EXPERIMENTAL_QUANTIZATION_ALGO_NAME_IN_CONFIG = "experimental_quantization"
+BOOTSTRAP_NAS_ALGO_NAME_IN_CONFIG = "bootstrapNAS"
 BINARIZATION_ALGO_NAME_IN_CONFIG = "binarization"
 CONST_SPARSITY_ALGO_NAME_IN_CONFIG = "const_sparsity"
-FILTER_PRUNING_ALGO_NAME_IN_CONFIG = 'filter_pruning'
-KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG = 'knowledge_distillation'
+FILTER_PRUNING_ALGO_NAME_IN_CONFIG = "filter_pruning"
+KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG = "knowledge_distillation"
 MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG = "magnitude_sparsity"
 MOVEMENT_SPARSITY_ALGO_NAME_IN_CONFIG = "movement_sparsity"
 QUANTIZATION_ALGO_NAME_IN_CONFIG = "quantization"
 RB_SPARSITY_ALGO_NAME_IN_CONFIG = "rb_sparsity"
 
 ALGO_NAME_VS_README_URL = {
-    QUANTIZATION_ALGO_NAME_IN_CONFIG: 'docs/compression_algorithms/Quantization.md',
-    FILTER_PRUNING_ALGO_NAME_IN_CONFIG: 'docs/compression_algorithms/Pruning.md',
-    MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG: 'docs/compression_algorithms/Sparsity.md',
-    RB_SPARSITY_ALGO_NAME_IN_CONFIG: 'docs/compression_algorithms/Sparsity.md',
-    CONST_SPARSITY_ALGO_NAME_IN_CONFIG: 'docs/compression_algorithms/Sparsity.md',
-    KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG: 'docs/compression_algorithms/KnowledgeDistillation.md',
-    BINARIZATION_ALGO_NAME_IN_CONFIG: 'docs/compression_algorithms/Binarization.md'
+    QUANTIZATION_ALGO_NAME_IN_CONFIG: "docs/compression_algorithms/Quantization.md",
+    FILTER_PRUNING_ALGO_NAME_IN_CONFIG: "docs/compression_algorithms/Pruning.md",
+    MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG: "docs/compression_algorithms/Sparsity.md",
+    RB_SPARSITY_ALGO_NAME_IN_CONFIG: "docs/compression_algorithms/Sparsity.md",
+    CONST_SPARSITY_ALGO_NAME_IN_CONFIG: "docs/compression_algorithms/Sparsity.md",
+    KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG: "docs/compression_algorithms/KnowledgeDistillation.md",
+    BINARIZATION_ALGO_NAME_IN_CONFIG: "docs/compression_algorithms/Binarization.md",
 }
```

### Comparing `nncf-2.4.0/nncf/config/extractors.py` & `nncf-2.5.0/nncf/config/extractors.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,111 +1,113 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Dict
-from typing import List
-from typing import Optional
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Any
+from typing import Any, Dict, List, Optional
 
+from nncf.common.logging import nncf_logger
 from nncf.common.quantization.initialization.range import PerLayerRangeInitConfig
 from nncf.common.quantization.initialization.range import RangeInitConfig
-from nncf.common.logging import nncf_logger
 from nncf.config.config import NNCFConfig
 from nncf.config.schemata.defaults import NUM_BN_ADAPTATION_SAMPLES
 from nncf.config.structures import BNAdaptationInitArgs
 from nncf.config.structures import QuantizationRangeInitArgs
 
 
 def extract_algorithm_names(config: NNCFConfig) -> List[str]:
     retval = []
-    compression_config_json_section = config.get('compression', [])
+    compression_config_json_section = config.get("compression", [])
     if isinstance(compression_config_json_section, dict):
         compression_config_json_section = [compression_config_json_section]
     for algo_config in compression_config_json_section:
-        retval.append(algo_config['algorithm'])
+        retval.append(algo_config["algorithm"])
     return retval
 
 
 def extract_algo_specific_config(config: NNCFConfig, algo_name_to_match: str) -> Dict:
     """
     Extracts a .json sub-dictionary for a given compression algorithm from the
     common NNCFConfig.
 
     :param config: An instance of the NNCFConfig.
     :param algo_name_to_match: The name of the algorithm for which the algorithm-specific section
       should be extracted.
     :return: The sub-dictionary, exactly as it is specified in the NNCF configuration of the .json file,
     that corresponds to the algorithm-specific data (i.e. {"algorithm": "quantization", ... })
     """
-    compression_section = config.get('compression', [])
+    compression_section = config.get("compression", [])
     if isinstance(compression_section, list):
         algo_list = compression_section
     else:
         assert isinstance(compression_section, dict)
         algo_list = [compression_section]
 
-    from nncf.common.compression import NO_COMPRESSION_ALGORITHM_NAME #pylint: disable=cyclic-import
+    from nncf.common.compression import NO_COMPRESSION_ALGORITHM_NAME  # pylint: disable=cyclic-import
+
     if algo_name_to_match == NO_COMPRESSION_ALGORITHM_NAME:
         if len(algo_list) > 0:
-            raise RuntimeError(f'No algorithm configuration should be specified '
-                               f'when you try to extract {algo_name_to_match} from the NNCF config!')
+            raise RuntimeError(
+                f"No algorithm configuration should be specified "
+                f"when you try to extract {algo_name_to_match} from the NNCF config!"
+            )
         return {}
 
     matches = []
     for compression_algo_dict in algo_list:
-        algo_name = compression_algo_dict['algorithm']
+        algo_name = compression_algo_dict["algorithm"]
         if algo_name == algo_name_to_match:
             matches.append(compression_algo_dict)
 
     if len(matches) > 1:
-        raise RuntimeError(f'Multiple algorithm configurations specified for the same '
-                           f'algo {algo_name_to_match} in the NNCF config!')
+        raise RuntimeError(
+            f"Multiple algorithm configurations specified for the same "
+            f"algo {algo_name_to_match} in the NNCF config!"
+        )
     if not matches:
-        raise RuntimeError(f'Did not find an algorithm configuration for '
-                           f'algo {algo_name_to_match} in the NNCF config!')
+        raise RuntimeError(
+            f"Did not find an algorithm configuration for " f"algo {algo_name_to_match} in the NNCF config!"
+        )
     return next(iter(matches))
 
 
-def extract_range_init_params(config: NNCFConfig, algorithm_name: str = 'quantization') -> Optional[Dict[str, object]]:
+def extract_range_init_params(config: NNCFConfig, algorithm_name: str = "quantization") -> Optional[Dict[str, object]]:
     """
     Extracts parameters of the quantization range initialization algorithm from the
     compression algorithm NNCFconfig.
 
     :param config: An instance of the NNCFConfig.
     :param algorithm_name: Name of the compression algorithm. Should be
         one of the following: `quantization`, `experimental_quantization`.
     :return: Parameters of the quantization range initialization algorithm.
     """
     algo_config = extract_algo_specific_config(config, algorithm_name)
-    init_range_config_dict_or_list = algo_config.get('initializer', {}).get('range', {})
+    init_range_config_dict_or_list = algo_config.get("initializer", {}).get("range", {})
 
     range_init_args = None
     try:
         range_init_args = config.get_extra_struct(QuantizationRangeInitArgs)
     except KeyError:
         if not init_range_config_dict_or_list:
-            nncf_logger.warning('Initializer section not specified for quantization algorithm in NNCF config and '
-                                'quantization init args not supplied - the quantizer range initialization algorithm '
-                                'cannot proceed.')
+            nncf_logger.warning(
+                "Initializer section not specified for quantization algorithm in NNCF config and "
+                "quantization init args not supplied - the quantizer range initialization algorithm "
+                "cannot proceed."
+            )
             return None
 
     if not init_range_config_dict_or_list:
-        nncf_logger.warning('Enabling quantization range initialization with default parameters.')
-        init_range_config_dict_or_list = {'num_init_samples': 256}
+        nncf_logger.warning("Enabling quantization range initialization with default parameters.")
+        init_range_config_dict_or_list = {"num_init_samples": 256}
 
     max_num_init_samples = 0
     global_range_init_config = None
     scope_overrides = []  # type: List[PerLayerRangeInitConfig]
     if isinstance(init_range_config_dict_or_list, dict):
         global_range_init_config = RangeInitConfig.from_dict(init_range_config_dict_or_list)
         max_num_init_samples = global_range_init_config.num_init_samples
@@ -115,23 +117,24 @@
             max_num_init_samples_config = max(scope_overrides, key=lambda x: x.num_init_samples)
             max_num_init_samples = max_num_init_samples_config.num_init_samples
 
     if max_num_init_samples == 0:
         return None
     if range_init_args is None:
         raise ValueError(
-            'Should run range initialization as specified via config,'
-            'but the initializing data loader is not provided as an extra struct. '
-            'Refer to `NNCFConfig.register_extra_structs` and the `QuantizationRangeInitArgs` class')
+            "Should run range initialization as specified via config,"
+            "but the initializing data loader is not provided as an extra struct. "
+            "Refer to `NNCFConfig.register_extra_structs` and the `QuantizationRangeInitArgs` class"
+        )
 
     params = {
-        'init_range_data_loader': range_init_args.data_loader,
-        'device': range_init_args.device,
-        'global_init_config': global_range_init_config,
-        'per_layer_range_init_configs': scope_overrides
+        "init_range_data_loader": range_init_args.data_loader,
+        "device": range_init_args.device,
+        "global_init_config": global_range_init_config,
+        "per_layer_range_init_configs": scope_overrides,
     }
 
     return params
 
 
 def extract_bn_adaptation_init_params(config: NNCFConfig, algo_name: str) -> Optional[Dict[str, object]]:
     """
@@ -140,62 +143,77 @@
 
     :param config: An instance of the NNCFConfig.
     :param algo_name: The name of the algorithm for which the params have to be extracted.
     :return: Parameters for initialization of an object of the class `BatchnormAdaptationAlgorithm` specific
       to the supplied algorithm, or None if the config specified not to perform any batchnorm adaptation.
     """
     algo_config = extract_algo_specific_config(config, algo_name)
-    params = algo_config.get('initializer', {}).get('batchnorm_adaptation', {})
+    params = algo_config.get("initializer", {}).get("batchnorm_adaptation", {})
     return get_bn_adapt_algo_kwargs(config, params)
 
 
+def has_bn_section(config: NNCFConfig, algo_name: str) -> bool:
+    algo_config = extract_algo_specific_config(config, algo_name)
+    return algo_config.get("initializer", {}).get("batchnorm_adaptation") is not None
+
+
+class BNAdaptDataLoaderNotFoundError(RuntimeError):
+    pass
+
+
 def get_bn_adapt_algo_kwargs(nncf_config: NNCFConfig, params: Dict[str, Any]) -> Dict[str, Any]:
-    num_bn_adaptation_samples = params.get('num_bn_adaptation_samples', NUM_BN_ADAPTATION_SAMPLES)
+    num_bn_adaptation_samples = params.get("num_bn_adaptation_samples", NUM_BN_ADAPTATION_SAMPLES)
 
     if num_bn_adaptation_samples == 0:
         return None
 
     try:
         args = nncf_config.get_extra_struct(BNAdaptationInitArgs)
     except KeyError:
-        raise RuntimeError(
-            'Unable to create the batch-norm statistics adaptation algorithm '
-            'because the data loader is not provided as an extra struct. Refer to the '
-            '`NNCFConfig.register_extra_structs` method and the `BNAdaptationInitArgs` class.') from None
+        raise BNAdaptDataLoaderNotFoundError(
+            "Unable to create the batch-norm statistics adaptation algorithm "
+            "because the data loader is not provided as an extra struct. Refer to the "
+            "`NNCFConfig.register_extra_structs` method and the `BNAdaptationInitArgs` class."
+        ) from None
     params = {
-        'num_bn_adaptation_samples': num_bn_adaptation_samples,
-        'data_loader': args.data_loader,
-        'device': args.device
+        "num_bn_adaptation_samples": num_bn_adaptation_samples,
+        "data_loader": args.data_loader,
+        "device": args.device,
     }
     return params
 
 
 def extract_accuracy_aware_training_params(config: NNCFConfig) -> Dict[str, object]:
     """
     Extracts accuracy aware training parameters from NNCFConfig.
 
     :param: config: An instance of the NNCFConfig.
     :return: Accuracy aware training parameters.
     """
+
     class NNCFAlgorithmNames:
-        QUANTIZATION = 'quantization'
-        FILTER_PRUNING = 'filter_pruning'
-        SPARSITY = ['rb_sparsity', 'magnitude_sparsity', 'const_sparsity']
+        QUANTIZATION = "quantization"
+        FILTER_PRUNING = "filter_pruning"
+        SPARSITY = ["rb_sparsity", "magnitude_sparsity", "const_sparsity"]
 
     def validate_accuracy_aware_schema(config: NNCFConfig, params: Dict[str, object]):
-        from nncf.common.accuracy_aware_training import AccuracyAwareTrainingMode #pylint: disable=cyclic-import
+        from nncf.common.accuracy_aware_training import AccuracyAwareTrainingMode  # pylint: disable=cyclic-import
+
         if params["mode"] == AccuracyAwareTrainingMode.EARLY_EXIT:
             return
         if params["mode"] == AccuracyAwareTrainingMode.ADAPTIVE_COMPRESSION_LEVEL:
             algorithms = extract_algorithm_names(config)
-            if NNCFAlgorithmNames.FILTER_PRUNING in algorithms and \
-                    any(algo in NNCFAlgorithmNames.SPARSITY for algo in algorithms):
-                raise RuntimeError("adaptive_compression_level mode supports filter_pruning or sparsity algorithms"
-                                   "separately. Please, choose only one algorithm with adaptive compression level. "
-                                   "Take a note that you still can use it combined with quantization.")
+            if NNCFAlgorithmNames.FILTER_PRUNING in algorithms and any(
+                algo in NNCFAlgorithmNames.SPARSITY for algo in algorithms
+            ):
+                raise RuntimeError(
+                    "adaptive_compression_level mode supports filter_pruning or sparsity algorithms"
+                    "separately. Please, choose only one algorithm with adaptive compression level. "
+                    "Take a note that you still can use it combined with quantization."
+                )
             if len(algorithms) == 1 and algorithms[0] == NNCFAlgorithmNames.QUANTIZATION:
                 raise RuntimeError("adaptive_compression_level mode doesn't support quantization")
 
     accuracy_aware_training_config = config.get("accuracy_aware_training", None)
 
     mode = accuracy_aware_training_config.get("mode")
     params = {"mode": mode}
```

### Comparing `nncf-2.4.0/nncf/config/schema.py` & `nncf-2.5.0/nncf/config/schema.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,187 +1,195 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import logging
 from typing import Dict
 
 import jsonschema
 
 from nncf.config.definitions import ALGO_NAME_VS_README_URL
+from nncf.config.definitions import BINARIZATION_ALGO_NAME_IN_CONFIG
+from nncf.config.definitions import CONST_SPARSITY_ALGO_NAME_IN_CONFIG
+from nncf.config.definitions import FILTER_PRUNING_ALGO_NAME_IN_CONFIG
+from nncf.config.definitions import KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG
+from nncf.config.definitions import MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG
+from nncf.config.definitions import QUANTIZATION_ALGO_NAME_IN_CONFIG
+from nncf.config.definitions import RB_SPARSITY_ALGO_NAME_IN_CONFIG
 from nncf.config.definitions import SCHEMA_VISUALIZATION_URL
-from nncf.config.schemata.defaults import TARGET_DEVICE
-from nncf.config.schemata.experimental_schema import EXPERIMENTAL_REF_VS_ALGO_SCHEMA
 from nncf.config.schemata.accuracy_aware import ACCURACY_AWARE_MODES_VS_SCHEMA
 from nncf.config.schemata.accuracy_aware import ACCURACY_AWARE_TRAINING_SCHEMA
-from nncf.config.definitions import BINARIZATION_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.algo.binarization import BINARIZATION_SCHEMA
-from nncf.config.definitions import CONST_SPARSITY_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.algo.const_sparsity import CONST_SPARSITY_SCHEMA
-from nncf.config.definitions import FILTER_PRUNING_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.algo.filter_pruning import FILTER_PRUNING_SCHEMA
-from nncf.config.definitions import KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.algo.knowledge_distillation import KNOWLEDGE_DISTILLATION_SCHEMA
-from nncf.config.definitions import MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.algo.magnitude_sparsity import MAGNITUDE_SPARSITY_SCHEMA
-from nncf.config.definitions import QUANTIZATION_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.algo.quantization import QUANTIZATION_SCHEMA
-from nncf.config.definitions import RB_SPARSITY_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.algo.rb_sparsity import RB_SPARSITY_SCHEMA
 from nncf.config.schemata.basic import ARRAY_OF_NUMBERS
 from nncf.config.schemata.basic import BOOLEAN
 from nncf.config.schemata.basic import STRING
 from nncf.config.schemata.basic import make_object_or_array_of_objects_schema
 from nncf.config.schemata.basic import with_attributes
 from nncf.config.schemata.common.compression import COMPRESSION_LR_MULTIPLIER_PROPERTY
+from nncf.config.schemata.defaults import TARGET_DEVICE
+from nncf.config.schemata.experimental_schema import EXPERIMENTAL_REF_VS_ALGO_SCHEMA
 
-logger = logging.getLogger('nncf')
+logger = logging.getLogger("nncf")
 
 REF_VS_ALGO_SCHEMA = {
     QUANTIZATION_ALGO_NAME_IN_CONFIG: QUANTIZATION_SCHEMA,
     FILTER_PRUNING_ALGO_NAME_IN_CONFIG: FILTER_PRUNING_SCHEMA,
     MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG: MAGNITUDE_SPARSITY_SCHEMA,
     RB_SPARSITY_ALGO_NAME_IN_CONFIG: RB_SPARSITY_SCHEMA,
     KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG: KNOWLEDGE_DISTILLATION_SCHEMA,
     CONST_SPARSITY_ALGO_NAME_IN_CONFIG: CONST_SPARSITY_SCHEMA,
     BINARIZATION_ALGO_NAME_IN_CONFIG: BINARIZATION_SCHEMA,
-    **EXPERIMENTAL_REF_VS_ALGO_SCHEMA
+    **EXPERIMENTAL_REF_VS_ALGO_SCHEMA,
 }
 
 
 SINGLE_INPUT_INFO_SCHEMA = {
     "type": "object",
     "properties": {
-        "sample_size": with_attributes(ARRAY_OF_NUMBERS,
-                                       description="Shape of the tensor expected as input to the model.",
-                                       examples=[[1, 3, 224, 224]]),
-        "type": with_attributes(STRING,
-                                description="Data type of the model input tensor."),
-        "filler": with_attributes(STRING,
-                                  description="Determines what the tensor will be filled with when passed to the model"
-                                              " during tracing and exporting."),
-        "keyword": with_attributes(STRING,
-                                   description="Keyword to be used when passing the tensor to the model's "
-                                               "'forward' method.")
+        "sample_size": with_attributes(
+            ARRAY_OF_NUMBERS,
+            description="Shape of the tensor expected as input to the model.",
+            examples=[[1, 3, 224, 224]],
+        ),
+        "type": with_attributes(STRING, description="Data type of the model input tensor."),
+        "filler": with_attributes(
+            STRING,
+            description="Determines what the tensor will be filled with when passed to the model"
+            " during tracing and exporting.",
+        ),
+        "keyword": with_attributes(
+            STRING, description="Keyword to be used when passing the tensor to the model's 'forward' method."
+        ),
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
-TARGET_DEVICE_SCHEMA = {
-    "type": "string",
-    "enum": ["ANY", "CPU", "GPU", "VPU", "TRIAL", "CPU_SPR"]
-}
+TARGET_DEVICE_SCHEMA = {"type": "string", "enum": ["ANY", "CPU", "GPU", "VPU", "TRIAL", "CPU_SPR"]}
 
 
 NNCF_CONFIG_SCHEMA = {
     "$schema": "http://json-schema.org/draft-07/schema",
     "title": "NNCF configuration file schema",
     "description": "The NNCF configuration file follows the JSON format and is the primary way to configure "
-                   "the result of NNCF application to a given user model. This configuration file is "
-                   "loaded into the `NNCFConfig` object by the user at runtime, after which the `NNCFConfig` "
-                   "is passed to the NNCF functions that perform actual compression or "
-                   "preparations for compression-aware training. \n\n"
-                   "The NNCF JSON configuration file is usually set up on a per-model, per-compression use case "
-                   "basis to contain:\n"
-                   "- a description of one or more compression algorithms to be applied to the model\n"
-                   "- the configuration parameters for each of the chosen algorithms\n"
-                   "- additional settings depending on the NNCF use case or integration scenario, e.g. specifying "
-                   "parameters for accuracy-aware training, or specifying model input shape for frameworks "
-                   "that do not have this data encapsulated in the model object in general such as PyTorch)\n"
-                   "and other parameters, the list of which may extend with the ongoing development of NNCF.\n\n"
-                   "This schema serves as a reference for users to write correct NNCF configuration files and each "
-                   "loaded NNCF configuration file into an `NNCFConfig` object is validated against it.",
+    "the result of NNCF application to a given user model. This configuration file is "
+    "loaded into the `NNCFConfig` object by the user at runtime, after which the `NNCFConfig` "
+    "is passed to the NNCF functions that perform actual compression or "
+    "preparations for compression-aware training. \n\n"
+    "The NNCF JSON configuration file is usually set up on a per-model, per-compression use case "
+    "basis to contain:\n"
+    "- a description of one or more compression algorithms to be applied to the model\n"
+    "- the configuration parameters for each of the chosen algorithms\n"
+    "- additional settings depending on the NNCF use case or integration scenario, e.g. specifying "
+    "parameters for accuracy-aware training, or specifying model input shape for frameworks "
+    "that do not have this data encapsulated in the model object in general such as PyTorch)\n"
+    "and other parameters, the list of which may extend with the ongoing development of NNCF.\n\n"
+    "This schema serves as a reference for users to write correct NNCF configuration files and each "
+    "loaded NNCF configuration file into an `NNCFConfig` object is validated against it.",
     "type": "object",
     "properties": {
         "input_info": with_attributes(
             make_object_or_array_of_objects_schema(SINGLE_INPUT_INFO_SCHEMA),
             description="Describe the specifics of your model inputs here. "
-                        "This information is used to build the internal graph representation "
-                        "that is leveraged for proper compression functioning, and for "
-                        "exporting the compressed model to an executable format.\n"
-                        "For instance, in PyTorch a dummy tensor with a "
-                        "corresponding shape and filler will be generated for each entry "
-                        "and passed as a corresponding argument into the model's forward "
-                        "method. Keywords can be specified for each entry - if left "
-                        "unspecified, the dummy tensor will be passed as a positional arg."),
+            "This information is used to build the internal graph representation "
+            "that is leveraged for proper compression functioning, and for "
+            "exporting the compressed model to an executable format.\n"
+            "For instance, in PyTorch a dummy tensor with a "
+            "corresponding shape and filler will be generated for each entry "
+            "and passed as a corresponding argument into the model's forward "
+            "method. Keywords can be specified for each entry - if left "
+            "unspecified, the dummy tensor will be passed as a positional arg.",
+        ),
         "target_device": with_attributes(
             TARGET_DEVICE_SCHEMA,
             description="The target device, the specificity of which will be taken into "
-                        "account while compressing in order to obtain the best "
-                        "performance for this type of device. The default 'ANY' means "
-                        "compatible quantization supported by any HW. Set "
-                        "this value to 'TRIAL' if you are going to use a custom "
-                        "quantization schema.",
-            default=TARGET_DEVICE),
+            "account while compressing in order to obtain the best "
+            "performance for this type of device. The default 'ANY' means "
+            "compatible quantization supported by any HW. Set "
+            "this value to 'TRIAL' if you are going to use a custom "
+            "quantization schema.",
+            default=TARGET_DEVICE,
+        ),
         "compression": make_object_or_array_of_objects_schema(
-            {"oneOf": [
-                    {"$ref": f"#/$defs/{algo_name}"} for algo_name in REF_VS_ALGO_SCHEMA
-                ]}),
-
-        "accuracy_aware_training": with_attributes(ACCURACY_AWARE_TRAINING_SCHEMA,
-                                                   description="Options for the execution of the NNCF-powered "
-                                                               "'Accuracy Aware' training pipeline. The 'mode' "
-                                                               "property determines the mode of the accuracy-aware "
-                                                               "training execution and further available parameters."),
+            {"oneOf": [{"$ref": f"#/$defs/{algo_name}"} for algo_name in REF_VS_ALGO_SCHEMA]}
+        ),
+        "accuracy_aware_training": with_attributes(
+            ACCURACY_AWARE_TRAINING_SCHEMA,
+            description="Options for the execution of the NNCF-powered "
+            "'Accuracy Aware' training pipeline. The 'mode' "
+            "property determines the mode of the accuracy-aware "
+            "training execution and further available parameters.",
+        ),
         # Validation of each separate compression description schema occurs in a separate step.
         # This is required for better user feedback, since holistic schema validation is uninformative
         # if there is an error in one of the compression configs.
         **COMPRESSION_LR_MULTIPLIER_PROPERTY,
         "disable_shape_matching": with_attributes(
             BOOLEAN,
             description="[Deprecated] Whether to enable strict input tensor "
-                        "shape matching when building the internal graph "
-                        "representation of the model. Set this to false if your "
-                        "model inputs have any variable dimension other than "
-                        "the 0-th (batch) dimension, or if any non-batch "
-                        "dimension of the intermediate tensors in your model "
-                        "execution flow depends on the input dimension, "
-                        "otherwise the compression will most likely fail."),
-        "log_dir": with_attributes(STRING,
-                                   description="Log directory for NNCF-specific logging outputs."),
+            "shape matching when building the internal graph "
+            "representation of the model. Set this to false if your "
+            "model inputs have any variable dimension other than "
+            "the 0-th (batch) dimension, or if any non-batch "
+            "dimension of the intermediate tensors in your model "
+            "execution flow depends on the input dimension, "
+            "otherwise the compression will most likely fail.",
+        ),
+        "log_dir": with_attributes(STRING, description="Log directory for NNCF-specific logging outputs."),
     },
     "required": ["input_info"],
     "$defs": REF_VS_ALGO_SCHEMA,
 }
 
 
 def validate_single_compression_algo_schema(single_compression_algo_dict: Dict, ref_vs_algo_schema: Dict):
     """single_compression_algo_dict must conform to BASIC_COMPRESSION_ALGO_SCHEMA (and possibly has other
     algo-specific properties"""
     algo_name = single_compression_algo_dict["algorithm"]
     if algo_name not in ref_vs_algo_schema:
         raise jsonschema.ValidationError(
-            f"Incorrect algorithm name - must be one of {str(list(ref_vs_algo_schema.keys()))}")
+            f"Incorrect algorithm name - must be one of {str(list(ref_vs_algo_schema.keys()))}"
+        )
     try:
         jsonschema.validate(single_compression_algo_dict, schema=ref_vs_algo_schema[algo_name])
     except jsonschema.ValidationError as e:
-        e.message = f"While validating the config for algorithm '{algo_name}' , got:\n" + e.message + \
-                    f"\nRefer to the algorithm subschema definition at {SCHEMA_VISUALIZATION_URL}\n"
+        e.message = (
+            f"While validating the config for algorithm '{algo_name}' , got:\n"
+            + e.message
+            + f"\nRefer to the algorithm subschema definition at {SCHEMA_VISUALIZATION_URL}\n"
+        )
         if algo_name in ALGO_NAME_VS_README_URL:
-            e.message += f"or to the algorithm documentation for examples of the configs: " \
-                         f"{ALGO_NAME_VS_README_URL[algo_name]}"
+            e.message += (
+                f"or to the algorithm documentation for examples of the configs: "
+                f"{ALGO_NAME_VS_README_URL[algo_name]}"
+            )
         raise e
 
 
 def validate_accuracy_aware_training_schema(single_compression_algo_dict: Dict):
     """
     Checks accuracy_aware_training section.
     """
     jsonschema.validate(single_compression_algo_dict, schema=ACCURACY_AWARE_TRAINING_SCHEMA)
-    accuracy_aware_mode = single_compression_algo_dict.get('mode')
+    accuracy_aware_mode = single_compression_algo_dict.get("mode")
     if accuracy_aware_mode not in ACCURACY_AWARE_MODES_VS_SCHEMA:
         raise jsonschema.ValidationError(
             "Incorrect Accuracy Aware mode - must be one of ({})".format(
-                ", ".join(ACCURACY_AWARE_MODES_VS_SCHEMA.keys())))
+                ", ".join(ACCURACY_AWARE_MODES_VS_SCHEMA.keys())
+            )
+        )
     try:
         jsonschema.validate(single_compression_algo_dict, schema=ACCURACY_AWARE_MODES_VS_SCHEMA[accuracy_aware_mode])
     except Exception as e:
         raise e
```

### Comparing `nncf-2.4.0/nncf/config/schemata/accuracy_aware.py` & `nncf-2.5.0/nncf/config/schemata/accuracy_aware.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,136 +1,145 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import ADAPTIVE_COMPRESSION_LEVEL_TRAINING_MODE_NAME_IN_CONFIG
 from nncf.config.definitions import EARLY_EXIT_TRAINING_MODE_NAME_IN_CONFIG
 from nncf.config.definitions import ONLINE_DOCS_ROOT
 from nncf.config.schemata.basic import NUMBER
 from nncf.config.schemata.basic import with_attributes
 from nncf.config.schemata.defaults import AA_COMPRESSION_RATE_STEP_REDUCTION_FACTOR
 from nncf.config.schemata.defaults import AA_INITIAL_COMPRESSION_RATE_STEP
 from nncf.config.schemata.defaults import AA_INITIAL_TRAINING_PHASE_EPOCHS
 from nncf.config.schemata.defaults import AA_LR_REDUCTION_FACTOR
 from nncf.config.schemata.defaults import AA_MAXIMAL_TOTAL_EPOCHS
 from nncf.config.schemata.defaults import AA_MINIMAL_COMPRESSION_RATE_STEP
 from nncf.config.schemata.defaults import AA_PATIENCE_EPOCHS
 
 COMMON_AA_PROPERTIES = {
-    "maximal_relative_accuracy_degradation":
-        with_attributes(NUMBER,
-                        description="Maximally allowed accuracy degradation of the model "
-                                    "in percent relative to the original model accuracy."),
-    "maximal_absolute_accuracy_degradation":
-        with_attributes(NUMBER,
-                        description="Maximally allowed accuracy degradation of the model "
-                                    "in units of absolute metric of the original model."),
+    "maximal_relative_accuracy_degradation": with_attributes(
+        NUMBER,
+        description="Maximally allowed accuracy degradation of the model "
+        "in percent relative to the original model accuracy.",
+    ),
+    "maximal_absolute_accuracy_degradation": with_attributes(
+        NUMBER,
+        description="Maximally allowed accuracy degradation of the model "
+        "in units of absolute metric of the original model.",
+    ),
 }
 
 ADAPTIVE_COMPRESSION_LEVEL_TRAINING_SCHEMA = {
     "type": "object",
     "title": ADAPTIVE_COMPRESSION_LEVEL_TRAINING_MODE_NAME_IN_CONFIG,
     "description": f"Adaptive compression level training mode schema. See "
-                   f"[AdaptiveCompressionLevelTraining.md]"
-                   f"({ONLINE_DOCS_ROOT}docs/accuracy_aware_model_training/AdaptiveCompressionLevelTraining.md) "
-                   f"for more general info on this mode.",
+    f"[AdaptiveCompressionLevelTraining.md]"
+    f"({ONLINE_DOCS_ROOT}docs/accuracy_aware_model_training/AdaptiveCompressionLevelTraining.md) "
+    f"for more general info on this mode.",
     "properties": {
         "mode": {"const": ADAPTIVE_COMPRESSION_LEVEL_TRAINING_MODE_NAME_IN_CONFIG},
         "params": {
             "type": "object",
             "properties": {
                 **COMMON_AA_PROPERTIES,
-                "initial_training_phase_epochs":
-                    with_attributes(NUMBER,
-                                    description="Number of epochs to fine-tune during the initial "
-                                                "training phase of the adaptive compression training loop.",
-                                    default=AA_INITIAL_TRAINING_PHASE_EPOCHS),
-                "initial_compression_rate_step":
-                    with_attributes(NUMBER,
-                                    description="Initial value for the compression rate increase/decrease "
-                                                "training phase of the compression training loop.",
-                                    default=AA_INITIAL_COMPRESSION_RATE_STEP),
-                "compression_rate_step_reduction_factor":
-                    with_attributes(NUMBER,
-                                    description="Factor used to reduce the compression rate change step "
-                                                "in the adaptive compression training loop.",
-                                    default=AA_COMPRESSION_RATE_STEP_REDUCTION_FACTOR),
-                "lr_reduction_factor":
-                    with_attributes(NUMBER,
-                                    description="Factor used to reduce the learning rate after compression rate step "
-                                                "is reduced",
-                                    default=AA_LR_REDUCTION_FACTOR),
-                "minimal_compression_rate_step":
-                    with_attributes(NUMBER,
-                                    description="The minimal compression rate change step value "
-                                                "after which the training loop is terminated.",
-                                    default=AA_MINIMAL_COMPRESSION_RATE_STEP),
-                "patience_epochs":
-                    with_attributes(NUMBER,
-                                    description="The number of epochs to fine-tune the model "
-                                                "for a given compression rate after the initial "
-                                                "training phase of the training loop.",
-                                    default=AA_PATIENCE_EPOCHS),
-                "maximal_total_epochs":
-                    with_attributes(NUMBER,
-                                    description="The maximal total fine-tuning epoch count. If the epoch "
-                                                "counter reaches this number, the fine-tuning process will "
-                                                "stop and the model with the largest compression rate "
-                                                "will be returned.",
-                                    default=AA_MAXIMAL_TOTAL_EPOCHS),
+                "initial_training_phase_epochs": with_attributes(
+                    NUMBER,
+                    description="Number of epochs to fine-tune during the initial "
+                    "training phase of the adaptive compression training loop.",
+                    default=AA_INITIAL_TRAINING_PHASE_EPOCHS,
+                ),
+                "initial_compression_rate_step": with_attributes(
+                    NUMBER,
+                    description="Initial value for the compression rate increase/decrease "
+                    "training phase of the compression training loop.",
+                    default=AA_INITIAL_COMPRESSION_RATE_STEP,
+                ),
+                "compression_rate_step_reduction_factor": with_attributes(
+                    NUMBER,
+                    description="Factor used to reduce the compression rate change step "
+                    "in the adaptive compression training loop.",
+                    default=AA_COMPRESSION_RATE_STEP_REDUCTION_FACTOR,
+                ),
+                "lr_reduction_factor": with_attributes(
+                    NUMBER,
+                    description="Factor used to reduce the learning rate after compression rate step is reduced",
+                    default=AA_LR_REDUCTION_FACTOR,
+                ),
+                "minimal_compression_rate_step": with_attributes(
+                    NUMBER,
+                    description="The minimal compression rate change step value "
+                    "after which the training loop is terminated.",
+                    default=AA_MINIMAL_COMPRESSION_RATE_STEP,
+                ),
+                "patience_epochs": with_attributes(
+                    NUMBER,
+                    description="The number of epochs to fine-tune the model "
+                    "for a given compression rate after the initial "
+                    "training phase of the training loop.",
+                    default=AA_PATIENCE_EPOCHS,
+                ),
+                "maximal_total_epochs": with_attributes(
+                    NUMBER,
+                    description="The maximal total fine-tuning epoch count. If the epoch "
+                    "counter reaches this number, the fine-tuning process will "
+                    "stop and the model with the largest compression rate "
+                    "will be returned.",
+                    default=AA_MAXIMAL_TOTAL_EPOCHS,
+                ),
             },
-            "oneOf": [{"required": ["maximal_relative_accuracy_degradation"]},
-                      {"required": ["maximal_absolute_accuracy_degradation"]}],
+            "oneOf": [
+                {"required": ["maximal_relative_accuracy_degradation"]},
+                {"required": ["maximal_absolute_accuracy_degradation"]},
+            ],
             "required": ["initial_training_phase_epochs", "patience_epochs"],
-            "additionalProperties": False
+            "additionalProperties": False,
         },
-
     },
     "required": ["mode", "params"],
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 EARLY_EXIT_TRAINING_SCHEMA = {
     "type": "object",
     "title": EARLY_EXIT_TRAINING_MODE_NAME_IN_CONFIG,
     "description": f"Early exit mode schema. See "
-                   f"[EarlyExitTraining.md]"
-                   f"({ONLINE_DOCS_ROOT}docs/accuracy_aware_model_training/EarlyExitTraining.md) for "
-                   f"more general info on this mode.",
+    f"[EarlyExitTraining.md]"
+    f"({ONLINE_DOCS_ROOT}docs/accuracy_aware_model_training/EarlyExitTraining.md) for "
+    f"more general info on this mode.",
     "properties": {
         "mode": {"const": EARLY_EXIT_TRAINING_MODE_NAME_IN_CONFIG},
         "params": {
             "type": "object",
             "properties": {
                 **COMMON_AA_PROPERTIES,
-                "maximal_total_epochs":
-                    with_attributes(NUMBER,
-                                    description="The maximal total fine-tuning epoch count. If the accuracy criteria "
-                                                "wouldn't reach during fine-tuning, the most accurate model "
-                                                "will be returned.",
-                                    default=AA_MAXIMAL_TOTAL_EPOCHS),
+                "maximal_total_epochs": with_attributes(
+                    NUMBER,
+                    description="The maximal total fine-tuning epoch count. If the accuracy criteria "
+                    "wouldn't reach during fine-tuning, the most accurate model "
+                    "will be returned.",
+                    default=AA_MAXIMAL_TOTAL_EPOCHS,
+                ),
             },
-            "oneOf": [{"required": ["maximal_relative_accuracy_degradation"]},
-                      {"required": ["maximal_absolute_accuracy_degradation"]}],
+            "oneOf": [
+                {"required": ["maximal_relative_accuracy_degradation"]},
+                {"required": ["maximal_absolute_accuracy_degradation"]},
+            ],
             "required": ["maximal_total_epochs"],
-            "additionalProperties": False
+            "additionalProperties": False,
         },
     },
     "required": ["mode", "params"],
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 ACCURACY_AWARE_TRAINING_SCHEMA = {
     "type": "object",
-    "oneOf": [EARLY_EXIT_TRAINING_SCHEMA,
-              ADAPTIVE_COMPRESSION_LEVEL_TRAINING_SCHEMA],
+    "oneOf": [EARLY_EXIT_TRAINING_SCHEMA, ADAPTIVE_COMPRESSION_LEVEL_TRAINING_SCHEMA],
 }
 ACCURACY_AWARE_MODES_VS_SCHEMA = {
     ADAPTIVE_COMPRESSION_LEVEL_TRAINING_MODE_NAME_IN_CONFIG: ADAPTIVE_COMPRESSION_LEVEL_TRAINING_SCHEMA,
-    EARLY_EXIT_TRAINING_MODE_NAME_IN_CONFIG: EARLY_EXIT_TRAINING_SCHEMA
+    EARLY_EXIT_TRAINING_MODE_NAME_IN_CONFIG: EARLY_EXIT_TRAINING_SCHEMA,
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/algo/binarization.py` & `nncf-2.5.0/nncf/config/schemata/algo/binarization.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,47 +1,45 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import BINARIZATION_ALGO_NAME_IN_CONFIG
 from nncf.config.definitions import ONLINE_DOCS_ROOT
-from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
-from nncf.config.schemata.common.compression import COMPRESSION_LR_MULTIPLIER_PROPERTY
 from nncf.config.schemata.algo.quantization import QUANTIZATION_INITIALIZER_SCHEMA
 from nncf.config.schemata.algo.quantization import STAGED_QUANTIZATION_PARAMS
 from nncf.config.schemata.basic import STRING
 from nncf.config.schemata.basic import with_attributes
+from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
+from nncf.config.schemata.common.compression import COMPRESSION_LR_MULTIPLIER_PROPERTY
 from nncf.config.schemata.common.targeting import SCOPING_PROPERTIES
 from nncf.config.schemata.defaults import BINARIZATION_MODE
 
-BINARIZATION_MODE_OPTIONS = ['xnor', 'dorefa']
+BINARIZATION_MODE_OPTIONS = ["xnor", "dorefa"]
 BINARIZATION_SCHEMA = {
     **BASIC_COMPRESSION_ALGO_SCHEMA,
     "description": f"Binarization is a specific particular case of the more general quantization algorithm."
-                   f"\nSee [Binarization.md]"
-                   f"({ONLINE_DOCS_ROOT}"
-                   f"/docs/compression_algorithms/Binarization.md) and the rest of this schema for "
-                   f"more details and parameters.",
+    f"\nSee [Binarization.md]"
+    f"({ONLINE_DOCS_ROOT}"
+    f"/docs/compression_algorithms/Binarization.md) and the rest of this schema for "
+    f"more details and parameters.",
     "properties": {
-        "algorithm": {
-            "const": BINARIZATION_ALGO_NAME_IN_CONFIG
-        },
-        "mode": with_attributes(STRING,
-                                description="Selects the mode of binarization - either 'xnor' for XNOR binarization,"
-                                            "or 'dorefa' for DoReFa binarization.",
-                                enum=BINARIZATION_MODE_OPTIONS,
-                                default=BINARIZATION_MODE),
+        "algorithm": {"const": BINARIZATION_ALGO_NAME_IN_CONFIG},
+        "mode": with_attributes(
+            STRING,
+            description="Selects the mode of binarization - either 'xnor' for XNOR binarization,"
+            "or 'dorefa' for DoReFa binarization.",
+            enum=BINARIZATION_MODE_OPTIONS,
+            default=BINARIZATION_MODE,
+        ),
         "initializer": QUANTIZATION_INITIALIZER_SCHEMA,
         **STAGED_QUANTIZATION_PARAMS,
         **SCOPING_PROPERTIES,
         **COMPRESSION_LR_MULTIPLIER_PROPERTY,
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/algo/const_sparsity.py` & `nncf-2.5.0/nncf/config/schemata/algo/const_sparsity.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,29 +1,25 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import CONST_SPARSITY_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
 from nncf.config.schemata.common.targeting import SCOPING_PROPERTIES
 
 CONST_SPARSITY_SCHEMA = {
     **BASIC_COMPRESSION_ALGO_SCHEMA,
     "properties": {
-        "algorithm": {
-            "const": CONST_SPARSITY_ALGO_NAME_IN_CONFIG
-        },
+        "algorithm": {"const": CONST_SPARSITY_ALGO_NAME_IN_CONFIG},
         **SCOPING_PROPERTIES,
     },
     "additionalProperties": False,
     "description": "This algorithm takes no additional parameters and is used when you want to load "
-                   "a checkpoint trained with another sparsity algorithm and do other compression without "
-                   "changing the sparsity mask."
+    "a checkpoint trained with another sparsity algorithm and do other compression without "
+    "changing the sparsity mask.",
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/algo/filter_pruning.py` & `nncf-2.5.0/nncf/config/schemata/algo/filter_pruning.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import FILTER_PRUNING_ALGO_NAME_IN_CONFIG
 from nncf.config.definitions import ONLINE_DOCS_ROOT
-from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
 from nncf.config.schemata.basic import BOOLEAN
 from nncf.config.schemata.basic import NUMBER
 from nncf.config.schemata.basic import STRING
 from nncf.config.schemata.basic import with_attributes
+from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
 from nncf.config.schemata.common.targeting import GENERIC_INITIALIZER_SCHEMA
 from nncf.config.schemata.common.targeting import SCOPING_PROPERTIES
 from nncf.config.schemata.defaults import PRUNE_BATCH_NORMS
 from nncf.config.schemata.defaults import PRUNE_DOWNSAMPLE_CONVS
 from nncf.config.schemata.defaults import PRUNE_FIRST_CONV
 from nncf.config.schemata.defaults import PRUNING_ALL_WEIGHTS
 from nncf.config.schemata.defaults import PRUNING_FILTER_IMPORTANCE
@@ -31,134 +29,147 @@
 from nncf.config.schemata.defaults import PRUNING_LEGR_RANDOM_SEED
 from nncf.config.schemata.defaults import PRUNING_LEGR_TRAIN_STEPS
 from nncf.config.schemata.defaults import PRUNING_NUM_INIT_STEPS
 from nncf.config.schemata.defaults import PRUNING_SCHEDULE
 from nncf.config.schemata.defaults import PRUNING_STEPS
 from nncf.config.schemata.defaults import PRUNING_TARGET
 
-FILTER_PRUNING_SCHEDULE_OPTIONS = ['exponential', 'exponential_with_bias', 'baseline']
-FILTER_IMPORTANCE_OPTIONS = ['L2', 'L1', 'geometric_median']
-INTERLAYER_RANKING_TYPE_OPTIONS = ['unweighted_ranking', 'learned_ranking']
+FILTER_PRUNING_SCHEDULE_OPTIONS = ["exponential", "exponential_with_bias", "baseline"]
+FILTER_IMPORTANCE_OPTIONS = ["L2", "L1", "geometric_median"]
+INTERLAYER_RANKING_TYPE_OPTIONS = ["unweighted_ranking", "learned_ranking"]
 
 FILTER_PRUNING_SCHEMA = {
     **BASIC_COMPRESSION_ALGO_SCHEMA,
     "description": f"Applies filter pruning during training of the model to effectively remove entire "
-                   f"sub-dimensions of tensors in the original model from computation and therefore increase "
-                   f"performance.\n"
-                   f"See [Pruning.md]"
-                   f"({ONLINE_DOCS_ROOT}"
-                   f"/docs/compression_algorithms/Pruning.md) and the rest of this schema for "
-                   f"more details and parameters."
-    ,
+    f"sub-dimensions of tensors in the original model from computation and therefore increase "
+    f"performance.\n"
+    f"See [Pruning.md]"
+    f"({ONLINE_DOCS_ROOT}"
+    f"/docs/compression_algorithms/Pruning.md) and the rest of this schema for "
+    f"more details and parameters.",
     "properties": {
-        "algorithm": {
-            "const": FILTER_PRUNING_ALGO_NAME_IN_CONFIG
-        },
+        "algorithm": {"const": FILTER_PRUNING_ALGO_NAME_IN_CONFIG},
         "initializer": GENERIC_INITIALIZER_SCHEMA,
-        "pruning_init": with_attributes(NUMBER,
-                                        description="Initial value of the pruning level applied to the "
-                                                    "prunable operations.",
-                                        default=PRUNING_INIT),
-        "params":
-            {
-                "type": "object",
-                "properties": {
-                    "filter_importance": with_attributes(STRING,
-                                                         description="The type of filter importance metric.",
-                                                         enum=FILTER_IMPORTANCE_OPTIONS,
-                                                         default=PRUNING_FILTER_IMPORTANCE),
-                    "pruning_target": with_attributes(NUMBER,
-                                                      description="Target value of the pruning level for "
-                                                                  "the operations that can be pruned. "
-                                                                  "The operations are determined by analysis of the "
-                                                                  "model architecture during the pruning algorithm "
-                                                                  "initialization stage.",
-                                                      default=PRUNING_TARGET),
-                    "pruning_steps": with_attributes(NUMBER,
-                                                     description="Number of epochs during which the pruning level is "
-                                                                 "increased from `pruning_init` to `pruning_target`.",
-                                                     default=PRUNING_STEPS),
-                    "pruning_flops_target": with_attributes(NUMBER,
-                                                            description="Target value of the pruning level for model "
-                                                                        "FLOPs."),
-                    "schedule": with_attributes(STRING,
-                                                description="The type of scheduling to use for adjusting the target "
-                                                            "pruning level.",
-                                                enum=FILTER_PRUNING_SCHEDULE_OPTIONS,
-                                                default=PRUNING_SCHEDULE),
-
-                    "num_init_steps": with_attributes(NUMBER,
-                                                      description="Number of epochs for model pretraining before "
-                                                                  "starting filter pruning.",
-                                                      default=PRUNING_NUM_INIT_STEPS),
-
-                    "interlayer_ranking_type": with_attributes(STRING,
-                                                               description="The type of filter ranking across the "
-                                                                           "layers.",
-                                                               enum=INTERLAYER_RANKING_TYPE_OPTIONS,
-                                                               default=PRUNING_INTERLAYER_RANKING_TYPE),
-                    "all_weights": with_attributes(BOOLEAN,
-                                                   description="Whether to prune layers independently (choose filters "
-                                                               "with the smallest importance in each layer separately) "
-                                                               "or not.",
-                                                   default=PRUNING_ALL_WEIGHTS),
-                    "prune_first_conv": with_attributes(BOOLEAN,
-                                                        description="Whether to prune first convolutional layers or "
-                                                                    "not. A 'first' convolutional layer is such a "
-                                                                    "layer that the path from model input to "
-                                                                    "this layer has no other "
-                                                                    "convolution operations on it.",
-                                                        default=PRUNE_FIRST_CONV),
-                    "prune_downsample_convs": with_attributes(BOOLEAN,
-                                                              description="Whether to prune downsampling "
-                                                                          "convolutional layers (with stride > 1) "
-                                                                          "or not.",
-                                                              default=PRUNE_DOWNSAMPLE_CONVS),
-                    "prune_batch_norms": with_attributes(BOOLEAN,
-                                                         description="Whether to prune parameters of the Batch Norm "
-                                                                     "layer that corresponds to pruned filters of the "
-                                                                     "convolutional layer which feeds its output to "
-                                                                     "this Batch Norm.",
-                                                         default=PRUNE_BATCH_NORMS),
-                    "legr_params":
-                        {
-                            "type": "object",
-                            "description": f"Describes parameters specific to the LeGR pruning algorithm."
-                                           f"See [Pruning.md]"
-                                           f"({ONLINE_DOCS_ROOT}"
-                                           f"/docs/compression_algorithms/Pruning.md#interlayer-ranking-types) "
-                                           f"for more details.",
-                            "properties": {
-                                "generations": with_attributes(NUMBER,
-                                                               description="Number of generations for the evolution "
-                                                                           "algorithm.",
-                                                               default=PRUNING_LEGR_GENERATIONS),
-                                "train_steps": with_attributes(NUMBER,
-                                                               description="Number of training steps to estimate "
-                                                                           "pruned model accuracy.",
-                                                               default=PRUNING_LEGR_TRAIN_STEPS),
-                                "max_pruning": with_attributes(NUMBER,
-                                                               description="Pruning level for the model to train "
-                                                                           "LeGR algorithm on it. If learned ranking "
-                                                                           "will be used for multiple pruning "
-                                                                           "levels, the highest should be used as "
-                                                                           "`max_pruning`. If model will be pruned "
-                                                                           "with one pruning level, this target should "
-                                                                           "be used.",
-                                                               default=PRUNING_LEGR_MAX_PRUNING),
-                                "random_seed": with_attributes(NUMBER,
-                                                               description="Random seed for LeGR coefficients "
-                                                                           "generation.",
-                                                               default=PRUNING_LEGR_RANDOM_SEED)
-                            },
-                            "additionalProperties": False
-                        },
-                    "save_ranking_coeffs_path": with_attributes(STRING),  # TODO(vshampor): is this important?
-                    "load_ranking_coeffs_path": with_attributes(STRING),  # TODO(vshampor): is this important?
-
+        "pruning_init": with_attributes(
+            NUMBER,
+            description="Initial value of the pruning level applied to the prunable operations.",
+            default=PRUNING_INIT,
+        ),
+        "params": {
+            "type": "object",
+            "properties": {
+                "filter_importance": with_attributes(
+                    STRING,
+                    description="The type of filter importance metric.",
+                    enum=FILTER_IMPORTANCE_OPTIONS,
+                    default=PRUNING_FILTER_IMPORTANCE,
+                ),
+                "pruning_target": with_attributes(
+                    NUMBER,
+                    description="Target value of the pruning level for "
+                    "the operations that can be pruned. "
+                    "The operations are determined by analysis of the "
+                    "model architecture during the pruning algorithm "
+                    "initialization stage.",
+                    default=PRUNING_TARGET,
+                ),
+                "pruning_steps": with_attributes(
+                    NUMBER,
+                    description="Number of epochs during which the pruning level is "
+                    "increased from `pruning_init` to `pruning_target`.",
+                    default=PRUNING_STEPS,
+                ),
+                "pruning_flops_target": with_attributes(
+                    NUMBER, description="Target value of the pruning level for model FLOPs."
+                ),
+                "schedule": with_attributes(
+                    STRING,
+                    description="The type of scheduling to use for adjusting the target pruning level.",
+                    enum=FILTER_PRUNING_SCHEDULE_OPTIONS,
+                    default=PRUNING_SCHEDULE,
+                ),
+                "num_init_steps": with_attributes(
+                    NUMBER,
+                    description="Number of epochs for model pretraining before starting filter pruning.",
+                    default=PRUNING_NUM_INIT_STEPS,
+                ),
+                "interlayer_ranking_type": with_attributes(
+                    STRING,
+                    description="The type of filter ranking across the layers.",
+                    enum=INTERLAYER_RANKING_TYPE_OPTIONS,
+                    default=PRUNING_INTERLAYER_RANKING_TYPE,
+                ),
+                "all_weights": with_attributes(
+                    BOOLEAN,
+                    description="Whether to prune layers independently (choose filters "
+                    "with the smallest importance in each layer separately) "
+                    "or not.",
+                    default=PRUNING_ALL_WEIGHTS,
+                ),
+                "prune_first_conv": with_attributes(
+                    BOOLEAN,
+                    description="Whether to prune first convolutional layers or "
+                    "not. A 'first' convolutional layer is such a "
+                    "layer that the path from model input to "
+                    "this layer has no other "
+                    "convolution operations on it.",
+                    default=PRUNE_FIRST_CONV,
+                ),
+                "prune_downsample_convs": with_attributes(
+                    BOOLEAN,
+                    description="Whether to prune downsampling convolutional layers (with stride > 1) or not.",
+                    default=PRUNE_DOWNSAMPLE_CONVS,
+                ),
+                "prune_batch_norms": with_attributes(
+                    BOOLEAN,
+                    description="Whether to prune parameters of the Batch Norm "
+                    "layer that corresponds to pruned filters of the "
+                    "convolutional layer which feeds its output to "
+                    "this Batch Norm.",
+                    default=PRUNE_BATCH_NORMS,
+                ),
+                "legr_params": {
+                    "type": "object",
+                    "description": f"Describes parameters specific to the LeGR pruning algorithm."
+                    f"See [Pruning.md]"
+                    f"({ONLINE_DOCS_ROOT}"
+                    f"/docs/compression_algorithms/Pruning.md#interlayer-ranking-types) "
+                    f"for more details.",
+                    "properties": {
+                        "generations": with_attributes(
+                            NUMBER,
+                            description="Number of generations for the evolution algorithm.",
+                            default=PRUNING_LEGR_GENERATIONS,
+                        ),
+                        "train_steps": with_attributes(
+                            NUMBER,
+                            description="Number of training steps to estimate pruned model accuracy.",
+                            default=PRUNING_LEGR_TRAIN_STEPS,
+                        ),
+                        "max_pruning": with_attributes(
+                            NUMBER,
+                            description="Pruning level for the model to train "
+                            "LeGR algorithm on it. If learned ranking "
+                            "will be used for multiple pruning "
+                            "levels, the highest should be used as "
+                            "`max_pruning`. If model will be pruned "
+                            "with one pruning level, this target should "
+                            "be used.",
+                            default=PRUNING_LEGR_MAX_PRUNING,
+                        ),
+                        "random_seed": with_attributes(
+                            NUMBER,
+                            description="Random seed for LeGR coefficients generation.",
+                            default=PRUNING_LEGR_RANDOM_SEED,
+                        ),
+                    },
+                    "additionalProperties": False,
                 },
-                "additionalProperties": False,
+                "save_ranking_coeffs_path": with_attributes(STRING),  # TODO(vshampor): is this important?
+                "load_ranking_coeffs_path": with_attributes(STRING),  # TODO(vshampor): is this important?
             },
-        **SCOPING_PROPERTIES
+            "additionalProperties": False,
+        },
+        **SCOPING_PROPERTIES,
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/algo/knowledge_distillation.py` & `nncf-2.5.0/nncf/config/schemata/algo/knowledge_distillation.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,49 +1,47 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG
 from nncf.config.definitions import ONLINE_DOCS_ROOT
-from nncf.config.schemata.basic import STRING
-from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
 from nncf.config.schemata.basic import NUMBER
+from nncf.config.schemata.basic import STRING
 from nncf.config.schemata.basic import with_attributes
+from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
 from nncf.config.schemata.defaults import KNOWLEDGE_DISTILLATION_SCALE
 from nncf.config.schemata.defaults import KNOWLEDGE_DISTILLATION_TEMPERATURE
 
 KNOWLEDGE_DISTILLATION_TYPE_OPTIONS = ["mse", "softmax"]
 
 KNOWLEDGE_DISTILLATION_SCHEMA = {
     **BASIC_COMPRESSION_ALGO_SCHEMA,
     "description": f"This algorithm is only useful in combination with other compression algorithms and improves the"
-                   f"end accuracy result of the corresponding algorithm by calculating knowledge distillation loss "
-                   f"between the compressed model currently in training and its original, uncompressed counterpart. "
-                   f"See [KnowledgeDistillation.md]"
-                   f"({ONLINE_DOCS_ROOT}"
-                   f"/docs/compression_algorithms/KnowledgeDistillation.md) and the rest of this schema for "
-                   f"more details and parameters.",
+    f"end accuracy result of the corresponding algorithm by calculating knowledge distillation loss "
+    f"between the compressed model currently in training and its original, uncompressed counterpart. "
+    f"See [KnowledgeDistillation.md]"
+    f"({ONLINE_DOCS_ROOT}"
+    f"/docs/compression_algorithms/KnowledgeDistillation.md) and the rest of this schema for "
+    f"more details and parameters.",
     "properties": {
-        "algorithm": {
-            "const": KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG
-        },
-        "type": with_attributes(STRING,
-                                description="Type of Knowledge Distillation Loss.",
-                                enum=KNOWLEDGE_DISTILLATION_TYPE_OPTIONS),
-        "scale": with_attributes(NUMBER,
-                                 description="Knowledge Distillation loss value multiplier",
-                                 default=KNOWLEDGE_DISTILLATION_SCALE),
-        "temperature": with_attributes(NUMBER,
-                                       description="`softmax` type only - Temperature for logits softening.",
-                                       default=KNOWLEDGE_DISTILLATION_TEMPERATURE)
+        "algorithm": {"const": KNOWLEDGE_DISTILLATION_ALGO_NAME_IN_CONFIG},
+        "type": with_attributes(
+            STRING, description="Type of Knowledge Distillation Loss.", enum=KNOWLEDGE_DISTILLATION_TYPE_OPTIONS
+        ),
+        "scale": with_attributes(
+            NUMBER, description="Knowledge Distillation loss value multiplier", default=KNOWLEDGE_DISTILLATION_SCALE
+        ),
+        "temperature": with_attributes(
+            NUMBER,
+            description="`softmax` type only - Temperature for logits softening.",
+            default=KNOWLEDGE_DISTILLATION_TEMPERATURE,
+        ),
     },
     "additionalProperties": False,
-    "required": ["type"]
+    "required": ["type"],
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/algo/magnitude_sparsity.py` & `nncf-2.5.0/nncf/config/schemata/algo/magnitude_sparsity.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,62 +1,58 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG
 from nncf.config.definitions import ONLINE_DOCS_ROOT
-from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
 from nncf.config.schemata.basic import NUMBER
 from nncf.config.schemata.basic import STRING
 from nncf.config.schemata.basic import with_attributes
+from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
 from nncf.config.schemata.common.sparsity import COMMON_SPARSITY_PARAM_PROPERTIES
 from nncf.config.schemata.common.targeting import GENERIC_INITIALIZER_SCHEMA
 from nncf.config.schemata.common.targeting import SCOPING_PROPERTIES
 from nncf.config.schemata.defaults import SPARSITY_INIT
 
-WEIGHT_IMPORTANCE_OPTIONS = ['abs', 'normed_abs']
+WEIGHT_IMPORTANCE_OPTIONS = ["abs", "normed_abs"]
 
 MAGNITUDE_SPARSITY_SCHEMA = {
     **BASIC_COMPRESSION_ALGO_SCHEMA,
     "description": f"Applies sparsity on top of the current model. Each weight tensor value will be either kept as-is, "
-                   f"or set to 0 based on its magnitude. For large sparsity levels, this will improve performance on "
-                   f"hardware that can profit from it. "
-                   f"See [Sparsity.md]"
-                   f"({ONLINE_DOCS_ROOT}"
-                   f"/docs/compression_algorithms/Sparsity.md#magnitude-sparsity) and the rest of this schema for "
-                   f"more details and parameters.",
+    f"or set to 0 based on its magnitude. For large sparsity levels, this will improve performance on "
+    f"hardware that can profit from it. "
+    f"See [Sparsity.md]"
+    f"({ONLINE_DOCS_ROOT}"
+    f"/docs/compression_algorithms/Sparsity.md#magnitude-sparsity) and the rest of this schema for "
+    f"more details and parameters.",
     "properties": {
-        "algorithm": {
-            "const": MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG
-        },
-        "sparsity_init": with_attributes(NUMBER,
-                                         description="Initial value of the sparsity level applied to the "
-                                                     "model.",
-                                         default=SPARSITY_INIT),
+        "algorithm": {"const": MAGNITUDE_SPARSITY_ALGO_NAME_IN_CONFIG},
+        "sparsity_init": with_attributes(
+            NUMBER, description="Initial value of the sparsity level applied to the model.", default=SPARSITY_INIT
+        ),
         "initializer": GENERIC_INITIALIZER_SCHEMA,
-        "params":
-            {
-                "type": "object",
-                "properties": {
-                    **COMMON_SPARSITY_PARAM_PROPERTIES,
-                    "weight_importance": with_attributes(STRING,
-                                                         description="Determines the way in which the weight values "
-                                                                     "will be sorted after being aggregated in order "
-                                                                     "to determine the sparsity threshold "
-                                                                     "corresponding to a specific sparsity level.",
-                                                         enum=WEIGHT_IMPORTANCE_OPTIONS,
-                                                         default="normed_abs")
-                },
-                "additionalProperties": False
+        "params": {
+            "type": "object",
+            "properties": {
+                **COMMON_SPARSITY_PARAM_PROPERTIES,
+                "weight_importance": with_attributes(
+                    STRING,
+                    description="Determines the way in which the weight values "
+                    "will be sorted after being aggregated in order "
+                    "to determine the sparsity threshold "
+                    "corresponding to a specific sparsity level.",
+                    enum=WEIGHT_IMPORTANCE_OPTIONS,
+                    default="normed_abs",
+                ),
             },
-        **SCOPING_PROPERTIES
+            "additionalProperties": False,
+        },
+        **SCOPING_PROPERTIES,
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/algo/quantization.py` & `nncf-2.5.0/nncf/config/schemata/algo/quantization.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import ONLINE_DOCS_ROOT
 from nncf.config.definitions import QUANTIZATION_ALGO_NAME_IN_CONFIG
 from nncf.config.schemata.basic import ARRAY_OF_NUMBERS
 from nncf.config.schemata.basic import ARRAY_OF_STRINGS
 from nncf.config.schemata.basic import BOOLEAN
 from nncf.config.schemata.basic import NUMBER
 from nncf.config.schemata.basic import STRING
@@ -43,479 +41,515 @@
 from nncf.config.schemata.defaults import QUANTIZATION_PRESET
 from nncf.config.schemata.defaults import QUANTIZE_INPUTS
 from nncf.config.schemata.defaults import QUANTIZE_OUTPUTS
 from nncf.config.schemata.defaults import STAGED_QUANTIZATION_BASE_LR
 from nncf.config.schemata.defaults import STAGED_QUANTIZATION_BASE_WD
 from nncf.config.schemata.defaults import WEIGHTS_QUANT_START_EPOCH
 
-QUANTIZER_MODES = ['symmetric', 'asymmetric']
+QUANTIZER_MODES = ["symmetric", "asymmetric"]
 
 QUANTIZER_CONFIG_PROPERTIES = {
-    "mode":
-        with_attributes(STRING,
-                        description=f"Mode of quantization. "
-                                    f"See [Quantization.md]"
-                                    f"({ONLINE_DOCS_ROOT}"
-                                    f"/docs/compression_algorithms/Quantization.md#symmetric-quantization) for "
-                                    f"more details.",
-                        enum=QUANTIZER_MODES),
-    "bits": with_attributes(NUMBER,
-                            description="Bitwidth to quantize to. It is intended for manual bitwidth setting. Can be "
-                                        "overridden by the `bits` parameter from the `precision` initializer section. "
-                                        "An error occurs if it doesn't match a corresponding bitwidth constraints "
-                                        "from the hardware configuration.",
-                            default=QUANTIZATION_BITS),
-    "signed": with_attributes(BOOLEAN,
-                              description="Whether to use signed or unsigned input/output values for quantization. "
-                                          "`true` will force the quantization to support signed values, `false` will "
-                                          "force the quantization to only support input values with one and the same "
-                                          "sign, and leaving this value unspecified (default) means relying "
-                                          "on the initialization statistics to determine best approach. \n"
-                                          "Note: If set to `false`, but the input "
-                                          "values have differing signs during initialization, signed quantization "
-                                          "will be performed instead."),
-    "per_channel": with_attributes(BOOLEAN,
-                                   description="Whether to quantize inputs of this quantizer "
-                                               "per each channel of input tensor (per 0-th dimension for "
-                                               "weight quantization, and per 1-st dimension for activation "
-                                               "quantization).",
-                                   default=QUANTIZATION_PER_CHANNEL)
-}
-
-UNIFIED_SCALE_OP_SCOPES_SPECIFIER_SCHEMA = {
-    "type": "array",
-    "items": ARRAY_OF_STRINGS
+    "mode": with_attributes(
+        STRING,
+        description=f"Mode of quantization. "
+        f"See [Quantization.md]"
+        f"({ONLINE_DOCS_ROOT}"
+        f"/docs/compression_algorithms/Quantization.md#symmetric-quantization) for "
+        f"more details.",
+        enum=QUANTIZER_MODES,
+    ),
+    "bits": with_attributes(
+        NUMBER,
+        description="Bitwidth to quantize to. It is intended for manual bitwidth setting. Can be "
+        "overridden by the `bits` parameter from the `precision` initializer section. "
+        "An error occurs if it doesn't match a corresponding bitwidth constraints "
+        "from the hardware configuration.",
+        default=QUANTIZATION_BITS,
+    ),
+    "signed": with_attributes(
+        BOOLEAN,
+        description="Whether to use signed or unsigned input/output values for quantization. "
+        "`true` will force the quantization to support signed values, `false` will "
+        "force the quantization to only support input values with one and the same "
+        "sign, and leaving this value unspecified (default) means relying "
+        "on the initialization statistics to determine best approach. \n"
+        "Note: If set to `false`, but the input "
+        "values have differing signs during initialization, signed quantization "
+        "will be performed instead.",
+    ),
+    "per_channel": with_attributes(
+        BOOLEAN,
+        description="Whether to quantize inputs of this quantizer "
+        "per each channel of input tensor (per 0-th dimension for "
+        "weight quantization, and per 1-st dimension for activation "
+        "quantization).",
+        default=QUANTIZATION_PER_CHANNEL,
+    ),
 }
 
+UNIFIED_SCALE_OP_SCOPES_SPECIFIER_SCHEMA = {"type": "array", "items": ARRAY_OF_STRINGS}
+
 RANGE_INIT_TYPES_VS_DESCRIPTIONS = {
     "mixed_min_max": "Minimum quantizer range initialized using minima of per-channel minima of "
-                     "the tensor to be quantized, maximum quantizer range initialized using "
-                     "maxima of per-channel maxima of the tensor to be quantized. Offline.",
+    "the tensor to be quantized, maximum quantizer range initialized using "
+    "maxima of per-channel maxima of the tensor to be quantized. Offline.",
     "min_max": "Minimum quantizer range initialized using global minimum of values in "
-               "the tensor to be quantized, maximum quantizer range initialized using global maxima of the same"
-               "values. Online.",
+    "the tensor to be quantized, maximum quantizer range initialized using global maxima of the same"
+    "values. Online.",
     "mean_min_max": "Minimum quantizer range initialized using averages (across every single initialization sample) "
-                    "of minima of values in "
-                    "the tensor to be quantized, maximum quantizer range initialized using maxima respectively. "
-                    "Offline.",
+    "of minima of values in "
+    "the tensor to be quantized, maximum quantizer range initialized using maxima respectively. "
+    "Offline.",
     "threesigma": "Quantizer minimum and maximum ranges set to be equal to +- 3 median absolute deviation from the "
-                  "median of the observed values in the tensor to be quantized. Offline.",
+    "median of the observed values in the tensor to be quantized. Offline.",
     "percentile": "Quantizer minimum and maximum ranges set to be equal to specified percentiles of the the observed "
-                  "values (across the entire initialization sample set) in the tensor to be quantized. Offline.",
+    "values (across the entire initialization sample set) in the tensor to be quantized. Offline.",
     "mean_percentile": "Quantizer minimum and maximum ranges set to be equal to averaged (across every single "
-                       "initialization sample) specified percentiles of the the observed values in the tensor to be "
-                       "quantized. Offline."
+    "initialization sample) specified percentiles of the the observed values in the tensor to be "
+    "quantized. Offline.",
 }
 
 BASIC_RANGE_INIT_CONFIG_PROPERTIES = {
     "type": "object",
     "title": "global_range_init_configuration",
     "properties": {
-        "num_init_samples": with_attributes(NUMBER,
-                                            description="Number of samples from the training dataset to "
-                                                        "consume as sample model inputs for purposes of "
-                                                        "setting initial minimum and maximum quantization "
-                                                        "ranges.",
-                                            default=NUM_INIT_SAMPLES),
+        "num_init_samples": with_attributes(
+            NUMBER,
+            description="Number of samples from the training dataset to "
+            "consume as sample model inputs for purposes of "
+            "setting initial minimum and maximum quantization "
+            "ranges.",
+            default=NUM_INIT_SAMPLES,
+        ),
         "type": with_attributes(
             annotated_enum(RANGE_INIT_TYPES_VS_DESCRIPTIONS),
             description="Type of the initializer - determines which "
-                        "statistics gathered during initialization will be "
-                        "used to initialize the quantization ranges.\n"
-                        "'Online' initializers do not have to "
-                        "store intermediate statistics in memory, while 'offline' do. Increasing the number "
-                        "of initialization samples for 'offline' initialization types will increase RAM "
-                        "overhead of applying NNCF to the model.\n"
-                        "Depending on whether the quantizer is configured to be per-tensor or per-channel, the "
-                        "statistics will be collected either on the basis of the set of the entire tensor values, or "
-                        "these will be collected and applied separately for each channel value subset.",
-            default="mixed_min_max"),
+            "statistics gathered during initialization will be "
+            "used to initialize the quantization ranges.\n"
+            "'Online' initializers do not have to "
+            "store intermediate statistics in memory, while 'offline' do. Increasing the number "
+            "of initialization samples for 'offline' initialization types will increase RAM "
+            "overhead of applying NNCF to the model.\n"
+            "Depending on whether the quantizer is configured to be per-tensor or per-channel, the "
+            "statistics will be collected either on the basis of the set of the entire tensor values, or "
+            "these will be collected and applied separately for each channel value subset.",
+            default="mixed_min_max",
+        ),
         "params": {
             "type": "object",
             "description": "Type-specific parameters of the initializer.",
             "properties": {
-                "min_percentile":
-                    with_attributes(NUMBER,
-                                    description="For 'percentile' and 'mean_percentile' types - specify the percentile "
-                                                "of input value histograms to be set as the initial "
-                                                "value for the quantizer input minimum.",
-                                    default=MIN_PERCENTILE),
-                "max_percentile":
-                    with_attributes(NUMBER,
-                                    description="For 'percentile' and 'mean_percentile' types - specify the percentile "
-                                                "of input value histograms to be set as the initial "
-                                                "value for the quantizer input maximum.",
-                                    default=MAX_PERCENTILE),
-            }
-        }
+                "min_percentile": with_attributes(
+                    NUMBER,
+                    description="For 'percentile' and 'mean_percentile' types - specify the percentile "
+                    "of input value histograms to be set as the initial "
+                    "value for the quantizer input minimum.",
+                    default=MIN_PERCENTILE,
+                ),
+                "max_percentile": with_attributes(
+                    NUMBER,
+                    description="For 'percentile' and 'mean_percentile' types - specify the percentile "
+                    "of input value histograms to be set as the initial "
+                    "value for the quantizer input maximum.",
+                    default=MAX_PERCENTILE,
+                ),
+            },
+        },
     },
     "additionalProperties": False,
 }
 PER_LAYER_RANGE_INIT_CONFIG_PROPERTIES = {
     "type": "object",
     "properties": {
         **BASIC_RANGE_INIT_CONFIG_PROPERTIES["properties"],
         **SCOPING_PROPERTIES,
-        "target_quantizer_group":
-            with_attributes(STRING, description="The target group of quantizers for which "
-                                                "the specified type of range initialization will "
-                                                "be applied. If unspecified, then the range "
-                                                "initialization of the given type will be applied to all quantizers.",
-                            enum=['activations', 'weights'])
-    }
+        "target_quantizer_group": with_attributes(
+            STRING,
+            description="The target group of quantizers for which "
+            "the specified type of range initialization will "
+            "be applied. If unspecified, then the range "
+            "initialization of the given type will be applied to all quantizers.",
+            enum=["activations", "weights"],
+        ),
+    },
 }
 RANGE_INIT_CONFIG_PROPERTIES = {
     "initializer": {
         "type": "object",
         "properties": {
             "range": {
                 "description": "This initializer performs forward runs of the model to be quantized using "
-                               "samples from a user-supplied data loader to gather activation and weight "
-                               "tensor statistics within the network and use these to set up initial range parameters "
-                               "for quantizers.",
+                "samples from a user-supplied data loader to gather activation and weight "
+                "tensor statistics within the network and use these to set up initial range parameters "
+                "for quantizers.",
                 "oneOf": [
                     BASIC_RANGE_INIT_CONFIG_PROPERTIES,
                     {
                         "type": "array",
                         "title": "per_layer_range_init_configuration",
-                        "items": PER_LAYER_RANGE_INIT_CONFIG_PROPERTIES
-                    }
+                        "items": PER_LAYER_RANGE_INIT_CONFIG_PROPERTIES,
+                    },
                 ],
             },
         },
         "additionalProperties": False,
     },
 }
 BITWIDTH_ASSIGNMENT_MODE_SCHEMA = {
     "type": "string",
-    "enum": ['strict', 'liberal'],
+    "enum": ["strict", "liberal"],
     "default": "liberal",
     "description": "The mode for assignment bitwidth to activation quantizers. In the 'strict' mode,"
-                   "a group of quantizers that feed their output to one and more "
-                   "same modules as input (weight quantizers count as well) will have the same bitwidth in the "
-                   "'liberal' mode allows different precisions within the group.\n"
-                   "Bitwidth is assigned based on hardware constraints. If multiple "
-                   "variants are possible, the minimal compatible bitwidth is chosen."
+    "a group of quantizers that feed their output to one and more "
+    "same modules as input (weight quantizers count as well) will have the same bitwidth in the "
+    "'liberal' mode allows different precisions within the group.\n"
+    "Bitwidth is assigned based on hardware constraints. If multiple "
+    "variants are possible, the minimal compatible bitwidth is chosen.",
 }
 
 PRECISION_INIT_TYPES_VS_DESCRIPTION = {
     "hawq": f"Applies HAWQ algorithm to determine best bitwidths for each quantizer using a Hessian"
-            f"calculation approach. For more details see "
-            f"[Quantization.md]({ONLINE_DOCS_ROOT}/docs/compression_algorithms/Quantization.md#hawq)",
+    f"calculation approach. For more details see "
+    f"[Quantization.md]({ONLINE_DOCS_ROOT}/docs/compression_algorithms/Quantization.md#hawq)",
     "autoq": f"Applies AutoQ algorithm to determine best bitwidths for each quantizer using reinforcement learning. "
-             f"For more details see "
-             f"[Quantization.md]({ONLINE_DOCS_ROOT}/docs/compression_algorithms/Quantization.md#autoq)",
+    f"For more details see "
+    f"[Quantization.md]({ONLINE_DOCS_ROOT}/docs/compression_algorithms/Quantization.md#autoq)",
     "manual": "Allows to manually specify via following config options the exact bitwidth "
-              "for each quantizer location. "
+    "for each quantizer location. ",
 }
 
 
 PRECISION_INITIALIZER_SCHEMA = {
     "type": "object",
     "description": "This initializer performs advanced selection of bitwidth per each quantizer "
-                   "location, trying to achieve the best tradeoff between performance and "
-                   "quality of the resulting model.",
+    "location, trying to achieve the best tradeoff between performance and "
+    "quality of the resulting model.",
     "properties": {
         "required": ["type"],
-        "type": with_attributes(annotated_enum(PRECISION_INIT_TYPES_VS_DESCRIPTION),
-                                description="Type of precision initialization."),
-        "bits": with_attributes(ARRAY_OF_NUMBERS,
-                                description="A list of bitwidth to choose from when performing precision "
-                                            "initialization. Overrides bits constraints specified in "
-                                            "`weight` and `activation` sections.",
-                                examples=[[4, 8]],
-                                default=PRECISION_INIT_BITWIDTHS),
-        "num_data_points": with_attributes(NUMBER,
-                                           description="Number of data points to iteratively estimate "
-                                                       "Hessian trace.",
-                                           default=HAWQ_NUM_DATA_POINTS),
-        "iter_number": with_attributes(NUMBER,
-                                       description="Maximum number of iterations of Hutchinson algorithm "
-                                                   "to Estimate Hessian trace.",
-                                       default=HAWQ_ITER_NUMBER),
-        "tolerance": with_attributes(NUMBER,
-                                     description="Minimum relative tolerance for stopping the Hutchinson "
-                                                 "algorithm. It's calculated  between mean average trace "
-                                                 "from the previous iteration and the current one.",
-                                     default=HAWQ_TOLERANCE),
-        "compression_ratio": with_attributes(NUMBER,
-                                             description="For the `hawq` mode:\n"
-                                                         "The desired ratio between bit complexity of "
-                                                         "a fully INT8 model and a mixed-precision lower-bit "
-                                                         "one. On precision initialization stage the HAWQ "
-                                                         "algorithm chooses the most accurate "
-                                                         "mixed-precision configuration with a ratio no less "
-                                                         "than the specified. Bit complexity of the model "
-                                                         "is a sum of bit complexities for each quantized "
-                                                         "layer, which are a multiplication of FLOPS for "
-                                                         "the layer by the number of bits for its "
-                                                         "quantization.\n"
-                                                         "For the `autoq` mode:\n"
-                                                         "The target model size after quantization, relative to total "
-                                                         "parameters size in FP32. E.g. a uniform INT8-quantized model "
-                                                         "would have a `compression_ratio` equal to 0.25,"
-                                                         "and a uniform INT4-quantized model would have "
-                                                         "`compression_ratio` equal to 0.125."),
-        "eval_subset_ratio": with_attributes(NUMBER,
-                                             description="The desired ratio of dataloader to be iterated "
-                                                         "during each search iteration of AutoQ precision "
-                                                         "initialization. Specifically, this ratio applies "
-                                                         "to the registered autoq_eval_loader via "
-                                                         "register_default_init_args.",
-                                             default=AUTOQ_EVAL_SUBSET_RATIO),
-        "warmup_iter_number": with_attributes(NUMBER,
-                                              description="The number of random policy at the beginning of "
-                                                          "of AutoQ precision initialization to populate "
-                                                          "replay buffer with experiences. This key is "
-                                                          "meant for internal testing use. Users need not "
-                                                          "to configure.",
-                                              default=AUTOQ_WARMUP_ITER_NUMBER),
+        "type": with_attributes(
+            annotated_enum(PRECISION_INIT_TYPES_VS_DESCRIPTION), description="Type of precision initialization."
+        ),
+        "bits": with_attributes(
+            ARRAY_OF_NUMBERS,
+            description="A list of bitwidth to choose from when performing precision "
+            "initialization. Overrides bits constraints specified in "
+            "`weight` and `activation` sections.",
+            examples=[[4, 8]],
+            default=PRECISION_INIT_BITWIDTHS,
+        ),
+        "num_data_points": with_attributes(
+            NUMBER,
+            description="Number of data points to iteratively estimate Hessian trace.",
+            default=HAWQ_NUM_DATA_POINTS,
+        ),
+        "iter_number": with_attributes(
+            NUMBER,
+            description="Maximum number of iterations of Hutchinson algorithm to Estimate Hessian trace.",
+            default=HAWQ_ITER_NUMBER,
+        ),
+        "tolerance": with_attributes(
+            NUMBER,
+            description="Minimum relative tolerance for stopping the Hutchinson "
+            "algorithm. It's calculated  between mean average trace "
+            "from the previous iteration and the current one.",
+            default=HAWQ_TOLERANCE,
+        ),
+        "compression_ratio": with_attributes(
+            NUMBER,
+            description="For the `hawq` mode:\n"
+            "The desired ratio between bit complexity of "
+            "a fully INT8 model and a mixed-precision lower-bit "
+            "one. On precision initialization stage the HAWQ "
+            "algorithm chooses the most accurate "
+            "mixed-precision configuration with a ratio no less "
+            "than the specified. Bit complexity of the model "
+            "is a sum of bit complexities for each quantized "
+            "layer, which are a multiplication of FLOPS for "
+            "the layer by the number of bits for its "
+            "quantization.\n"
+            "For the `autoq` mode:\n"
+            "The target model size after quantization, relative to total "
+            "parameters size in FP32. E.g. a uniform INT8-quantized model "
+            "would have a `compression_ratio` equal to 0.25,"
+            "and a uniform INT4-quantized model would have "
+            "`compression_ratio` equal to 0.125.",
+        ),
+        "eval_subset_ratio": with_attributes(
+            NUMBER,
+            description="The desired ratio of dataloader to be iterated "
+            "during each search iteration of AutoQ precision "
+            "initialization. Specifically, this ratio applies "
+            "to the registered autoq_eval_loader via "
+            "register_default_init_args.",
+            default=AUTOQ_EVAL_SUBSET_RATIO,
+        ),
+        "warmup_iter_number": with_attributes(
+            NUMBER,
+            description="The number of random policy at the beginning of "
+            "of AutoQ precision initialization to populate "
+            "replay buffer with experiences. This key is "
+            "meant for internal testing use. Users need not "
+            "to configure.",
+            default=AUTOQ_WARMUP_ITER_NUMBER,
+        ),
         "bitwidth_per_scope": {
             "type": "array",
             "items": {
                 "type": "array",
-                "items":
-                    [
-                        NUMBER,
-                        STRING
-                    ],
-                "description": "A tuple of a bitwidth and a scope of the quantizer to assign the "
-                               "bitwidth to."
+                "items": [NUMBER, STRING],
+                "description": "A tuple of a bitwidth and a scope of the quantizer to assign the bitwidth to.",
             },
             "description": "Manual settings for the quantizer bitwidths. Scopes are used to identify "
-                           "the quantizers.",
+            "the quantizers.",
             "examples": [
                 [
                     [2, "ResNet/NNCFConv2d[conv1]/conv2d_0|WEIGHT"],
                     [8, "ResNet/ReLU[relu]/relu__0|OUTPUT"],
                 ]
-            ]
+            ],
         },
-        "traces_per_layer_path": with_attributes(STRING,
-                                                 description="Path to serialized PyTorch Tensor with "
-                                                             "average Hessian traces per quantized modules. "
-                                                             "It can be used to accelerate mixed precision "
-                                                             "initialization by using average Hessian "
-                                                             "traces from previous run of HAWQ algorithm."),
-        "dump_init_precision_data": with_attributes(BOOLEAN,
-                                                    description="Whether to dump data related to Precision "
-                                                                "Initialization algorithm. "
-                                                                "HAWQ dump includes bitwidth graph,"
-                                                                " average traces and different plots. "
-                                                                "AutoQ dump includes DDPG agent "
-                                                                "learning trajectory in tensorboard and "
-                                                                "mixed-precision environment metadata.",
-                                                    default=HAWQ_DUMP_INIT_PRECISION_DATA),
+        "traces_per_layer_path": with_attributes(
+            STRING,
+            description="Path to serialized PyTorch Tensor with "
+            "average Hessian traces per quantized modules. "
+            "It can be used to accelerate mixed precision "
+            "initialization by using average Hessian "
+            "traces from previous run of HAWQ algorithm.",
+        ),
+        "dump_init_precision_data": with_attributes(
+            BOOLEAN,
+            description="Whether to dump data related to Precision "
+            "Initialization algorithm. "
+            "HAWQ dump includes bitwidth graph,"
+            " average traces and different plots. "
+            "AutoQ dump includes DDPG agent "
+            "learning trajectory in tensorboard and "
+            "mixed-precision environment metadata.",
+            default=HAWQ_DUMP_INIT_PRECISION_DATA,
+        ),
         "bitwidth_assignment_mode": BITWIDTH_ASSIGNMENT_MODE_SCHEMA,
     },
     "additionalProperties": False,
 }
 
 QUANTIZATION_INITIALIZER_SCHEMA = {
     "type": "object",
     "description": "Specifies the kind of pre-training initialization used for the quantization algorithm.\n"
-                   "Some kind of initialization is usually required so that the trainable quantization "
-                   "parameters have a better chance to get fine-tuned to values that result in good accuracy.",
+    "Some kind of initialization is usually required so that the trainable quantization "
+    "parameters have a better chance to get fine-tuned to values that result in good accuracy.",
     "properties": {
         "batchnorm_adaptation": BATCHNORM_ADAPTATION_SCHEMA,
         **RANGE_INIT_CONFIG_PROPERTIES["initializer"]["properties"],
         "precision": PRECISION_INITIALIZER_SCHEMA,
     },
     "additionalProperties": False,
 }
 STAGED_QUANTIZATION_PARAMS = {
     "params": {
         "type": "object",
         "description": "Configures the staged quantization compression scheduler for the quantization algorithm. "
-                       "The quantizers will not be applied until a given epoch count is reached.",
+        "The quantizers will not be applied until a given epoch count is reached.",
         "properties": {
-            "batch_multiplier": with_attributes(NUMBER,
-                                                description="Gradients will be accumulated for this number of "
-                                                            "batches before doing a 'backward' call. Increasing "
-                                                            "this may improve training quality, since binarized "
-                                                            "networks exhibit noisy gradients and their training "
-                                                            "requires larger batch sizes than could be accommodated "
-                                                            "by GPUs."),
-            "activations_quant_start_epoch":
-                with_attributes(NUMBER,
-                                description="A zero-based index of the epoch, upon reaching which "
-                                            "the activations will start to be quantized.",
-                                default=ACTIVATIONS_QUANT_START_EPOCH),
-            "weights_quant_start_epoch": with_attributes(NUMBER,
-                                                         description="Epoch index upon which the weights will start "
-                                                                     "to be quantized.",
-                                                         default=WEIGHTS_QUANT_START_EPOCH),
-            "lr_poly_drop_start_epoch": with_attributes(NUMBER,
-                                                        description="Epoch index upon which the learning rate will "
-                                                                    "start to be dropped. If unspecified, "
-                                                                    "learning rate will not be dropped."),
-            "lr_poly_drop_duration_epochs": with_attributes(NUMBER,
-                                                            description="Duration, in epochs, of the learning "
-                                                                        "rate dropping process.",
-                                                            default=LR_POLY_DURATION_EPOCHS),
-            "disable_wd_start_epoch": with_attributes(NUMBER,
-                                                      description="Epoch to disable weight decay in the optimizer. "
-                                                                  "If unspecified, weight decay will not be disabled."),
-            "base_lr": with_attributes(NUMBER, description="Initial value of learning rate.",
-                                       default=STAGED_QUANTIZATION_BASE_LR),
-            "base_wd": with_attributes(NUMBER, description="Initial value of weight decay.",
-                                       default=STAGED_QUANTIZATION_BASE_WD),
+            "batch_multiplier": with_attributes(
+                NUMBER,
+                description="Gradients will be accumulated for this number of "
+                "batches before doing a 'backward' call. Increasing "
+                "this may improve training quality, since binarized "
+                "networks exhibit noisy gradients and their training "
+                "requires larger batch sizes than could be accommodated "
+                "by GPUs.",
+            ),
+            "activations_quant_start_epoch": with_attributes(
+                NUMBER,
+                description="A zero-based index of the epoch, upon reaching which "
+                "the activations will start to be quantized.",
+                default=ACTIVATIONS_QUANT_START_EPOCH,
+            ),
+            "weights_quant_start_epoch": with_attributes(
+                NUMBER,
+                description="Epoch index upon which the weights will start to be quantized.",
+                default=WEIGHTS_QUANT_START_EPOCH,
+            ),
+            "lr_poly_drop_start_epoch": with_attributes(
+                NUMBER,
+                description="Epoch index upon which the learning rate will "
+                "start to be dropped. If unspecified, "
+                "learning rate will not be dropped.",
+            ),
+            "lr_poly_drop_duration_epochs": with_attributes(
+                NUMBER,
+                description="Duration, in epochs, of the learning rate dropping process.",
+                default=LR_POLY_DURATION_EPOCHS,
+            ),
+            "disable_wd_start_epoch": with_attributes(
+                NUMBER,
+                description="Epoch to disable weight decay in the optimizer. "
+                "If unspecified, weight decay will not be disabled.",
+            ),
+            "base_lr": with_attributes(
+                NUMBER, description="Initial value of learning rate.", default=STAGED_QUANTIZATION_BASE_LR
+            ),
+            "base_wd": with_attributes(
+                NUMBER, description="Initial value of weight decay.", default=STAGED_QUANTIZATION_BASE_WD
+            ),
         },
-        "additionalProperties": False
+        "additionalProperties": False,
     }
 }
 
-QUANTIZATION_PRESETS_SCHEMA = {
-    "type": "string",
-    "enum": ["performance", "mixed"]
-}
+QUANTIZATION_PRESETS_SCHEMA = {"type": "string", "enum": ["performance", "mixed"]}
 
 QUANTIZER_GROUP_PROPERTIES = {
     **QUANTIZER_CONFIG_PROPERTIES,
     **SCOPING_PROPERTIES,
-    "logarithm_scale": with_attributes(BOOLEAN,
-                                       description="Whether to use log of scale as the optimization parameter "
-                                                   "instead of the scale itself. This serves as an optional "
-                                                   "regularization opportunity for training quantizer scales.",
-                                       default=QUANTIZATION_LOGARITHM_SCALE),
+    "logarithm_scale": with_attributes(
+        BOOLEAN,
+        description="Whether to use log of scale as the optimization parameter "
+        "instead of the scale itself. This serves as an optional "
+        "regularization opportunity for training quantizer scales.",
+        default=QUANTIZATION_LOGARITHM_SCALE,
+    ),
 }
 
 WEIGHTS_GROUP_SCHEMA = {
     "type": "object",
     "properties": {
         **QUANTIZER_GROUP_PROPERTIES,
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 ACTIVATIONS_GROUP_SCHEMA = {
     "type": "object",
     "properties": {
         **QUANTIZER_GROUP_PROPERTIES,
-        "unified_scale_ops": with_attributes(UNIFIED_SCALE_OP_SCOPES_SPECIFIER_SCHEMA,
-                                             description="Specifies operations in the model which will share "
-                                                         "the same quantizer module for activations. This "
-                                                         "is helpful in case one and the same quantizer scale "
-                                                         "is required for each input of this operation. Each "
-                                                         "sub-array will define a group of model operation "
-                                                         "inputs that have to share a single actual "
-                                                         "quantization module, each entry in this subarray "
-                                                         "should correspond to exactly one node in the NNCF "
-                                                         "graph and the groups should not overlap. The final "
-                                                         "quantizer for each sub-array will be associated "
-                                                         "with the first element of this sub-array.")
+        "unified_scale_ops": with_attributes(
+            UNIFIED_SCALE_OP_SCOPES_SPECIFIER_SCHEMA,
+            description="Specifies operations in the model which will share "
+            "the same quantizer module for activations. This "
+            "is helpful in case one and the same quantizer scale "
+            "is required for each input of this operation. Each "
+            "sub-array will define a group of model operation "
+            "inputs that have to share a single actual "
+            "quantization module, each entry in this subarray "
+            "should correspond to exactly one node in the NNCF "
+            "graph and the groups should not overlap. The final "
+            "quantizer for each sub-array will be associated "
+            "with the first element of this sub-array.",
+        ),
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
-OVERFLOW_FIX_OPTIONS = [
-    'enable',
-    'disable',
-    'first_layer_only'
-]
+OVERFLOW_FIX_OPTIONS = ["enable", "disable", "first_layer_only"]
 
 QUANTIZATION_SCHEMA = {
     **BASIC_COMPRESSION_ALGO_SCHEMA,
     "description": f"Applies quantization on top of the input model, simulating future low-precision execution "
-                   f"specifics, and selects the quantization layout and parameters to strive for the best possible "
-                   f"quantized model accuracy and performance. "
-                   f"\nSee [Quantization.md]"
-                   f"({ONLINE_DOCS_ROOT}"
-                   f"/docs/compression_algorithms/Quantization.md) and the rest of this schema for "
-                   f"more details and parameters.",
+    f"specifics, and selects the quantization layout and parameters to strive for the best possible "
+    f"quantized model accuracy and performance. "
+    f"\nSee [Quantization.md]"
+    f"({ONLINE_DOCS_ROOT}"
+    f"/docs/compression_algorithms/Quantization.md) and the rest of this schema for "
+    f"more details and parameters.",
     "properties": {
-        "algorithm": {
-            "const": QUANTIZATION_ALGO_NAME_IN_CONFIG
-        },
+        "algorithm": {"const": QUANTIZATION_ALGO_NAME_IN_CONFIG},
         "initializer": QUANTIZATION_INITIALIZER_SCHEMA,
-        "preset": with_attributes(QUANTIZATION_PRESETS_SCHEMA,
-                                  description="The preset defines the quantization schema for weights and activations. "
-                                              "The 'performance' mode sets up symmetric weight and activation "
-                                              "quantizers. The 'mixed' mode utilizes symmetric weight quantization and "
-                                              "asymmetric activation quantization.",
-                                  default=QUANTIZATION_PRESET),
-        "quantize_inputs": with_attributes(BOOLEAN,
-                                           description="Whether the model inputs should be immediately quantized prior "
-                                                       "to any other model operations.",
-                                           default=QUANTIZE_INPUTS),
-        "quantize_outputs": with_attributes(BOOLEAN,
-                                            description="Whether the model outputs should be additionally quantized.",
-                                            default=QUANTIZE_OUTPUTS),
-        "weights": with_attributes(WEIGHTS_GROUP_SCHEMA,
-                                   description="Constraints to be applied to model weights quantization only."),
-        "activations": with_attributes(ACTIVATIONS_GROUP_SCHEMA,
-                                       description="Constraints to be applied to model activations quantization only."),
-
-
+        "preset": with_attributes(
+            QUANTIZATION_PRESETS_SCHEMA,
+            description="The preset defines the quantization schema for weights and activations. "
+            "The 'performance' mode sets up symmetric weight and activation "
+            "quantizers. The 'mixed' mode utilizes symmetric weight quantization and "
+            "asymmetric activation quantization.",
+            default=QUANTIZATION_PRESET,
+        ),
+        "quantize_inputs": with_attributes(
+            BOOLEAN,
+            description="Whether the model inputs should be immediately quantized prior "
+            "to any other model operations.",
+            default=QUANTIZE_INPUTS,
+        ),
+        "quantize_outputs": with_attributes(
+            BOOLEAN, description="Whether the model outputs should be additionally quantized.", default=QUANTIZE_OUTPUTS
+        ),
+        "weights": with_attributes(
+            WEIGHTS_GROUP_SCHEMA, description="Constraints to be applied to model weights quantization only."
+        ),
+        "activations": with_attributes(
+            ACTIVATIONS_GROUP_SCHEMA, description="Constraints to be applied to model activations quantization only."
+        ),
         "scope_overrides": {
             "type": "object",
             "description": "This option is used to specify overriding quantization constraints for specific scope,"
-                           "e.g. in case you need to quantize a single operation differently than the rest of the "
-                           "model. Any other automatic or group-wise settings will be overridden.",
+            "e.g. in case you need to quantize a single operation differently than the rest of the "
+            "model. Any other automatic or group-wise settings will be overridden.",
             "examples": [
                 {
-                    'weights': {
-                        'QuantizeOutputsTestModel/NNCFConv2d[conv5]/conv2d_0':
-                            {
-                                "mode": "asymmetric",
-                            },
+                    "weights": {
+                        "QuantizeOutputsTestModel/NNCFConv2d[conv5]/conv2d_0": {
+                            "mode": "asymmetric",
+                        },
                         "activations": {
                             "{re}.*conv_first.*": {"mode": "asymmetric"},
-                            "{re}.*conv_second.*": {"mode": "symmetric"}
-                        }
+                            "{re}.*conv_second.*": {"mode": "symmetric"},
+                        },
                     }
                 }
             ],
             "properties": {
                 "weights": {
                     "type": "object",
                     "patternProperties": {
                         ".*": {
                             "type": "object",
                             "properties": {
                                 **QUANTIZER_CONFIG_PROPERTIES,
                                 **RANGE_INIT_CONFIG_PROPERTIES,
                             },
-                            "additionalProperties": False
+                            "additionalProperties": False,
                         },
                     },
                 },
                 "activations": {
                     "type": "object",
                     "patternProperties": {
                         ".*": {
                             "type": "object",
                             "properties": {
                                 **QUANTIZER_CONFIG_PROPERTIES,
                                 **RANGE_INIT_CONFIG_PROPERTIES,
                             },
-                            "additionalProperties": False
+                            "additionalProperties": False,
                         },
                     },
-                }
+                },
             },
             "additionalProperties": False,
         },
-        "export_to_onnx_standard_ops": with_attributes(BOOLEAN,
-                                                       description="Determines how should the additional quantization "
-                                                                   "operations be exported into the ONNX format. Set "
-                                                                   "this to true to export to ONNX "
-                                                                   "standard QuantizeLinear-DequantizeLinear "
-                                                                   "node pairs (8-bit quantization only) or to false "
-                                                                   "to export to OpenVINO-supported FakeQuantize ONNX"
-                                                                   "(all quantization settings supported).",
-                                                       default=QUANTIZATION_EXPORT_TO_ONNX_STANDARD_OPS),
-        "overflow_fix": with_attributes(STRING,
-                                        description="This option controls whether to apply the overflow "
-                                                    "issue fix for the appropriate NNCF config or not. "
-                                                    "If set to `disable`, the fix will not be applied. "
-                                                    "If set to `enable` or `first_layer_only`, "
-                                                    "while appropriate target_devices are chosen, "
-                                                    "the fix will be applied to all layers or to the first "
-                                                    "convolutional layer respectively.",
-                                        enum=OVERFLOW_FIX_OPTIONS,
-                                        default=QUANTIZATION_OVERFLOW_FIX),
+        "export_to_onnx_standard_ops": with_attributes(
+            BOOLEAN,
+            description="Determines how should the additional quantization "
+            "operations be exported into the ONNX format. Set "
+            "this to true to export to ONNX "
+            "standard QuantizeLinear-DequantizeLinear "
+            "node pairs (8-bit quantization only) or to false "
+            "to export to OpenVINO-supported FakeQuantize ONNX"
+            "(all quantization settings supported).",
+            default=QUANTIZATION_EXPORT_TO_ONNX_STANDARD_OPS,
+        ),
+        "overflow_fix": with_attributes(
+            STRING,
+            description="This option controls whether to apply the overflow "
+            "issue fix for the appropriate NNCF config or not. "
+            "If set to `disable`, the fix will not be applied. "
+            "If set to `enable` or `first_layer_only`, "
+            "while appropriate target_devices are chosen, "
+            "the fix will be applied to all layers or to the first "
+            "convolutional layer respectively.",
+            enum=OVERFLOW_FIX_OPTIONS,
+            default=QUANTIZATION_OVERFLOW_FIX,
+        ),
         **STAGED_QUANTIZATION_PARAMS,
         **SCOPING_PROPERTIES,
         **COMPRESSION_LR_MULTIPLIER_PROPERTY,
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/algo/rb_sparsity.py` & `nncf-2.5.0/nncf/config/schemata/algo/rb_sparsity.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,51 +1,41 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import ONLINE_DOCS_ROOT
 from nncf.config.definitions import RB_SPARSITY_ALGO_NAME_IN_CONFIG
-from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
-from nncf.config.schemata.common.compression import COMPRESSION_LR_MULTIPLIER_PROPERTY
 from nncf.config.schemata.basic import NUMBER
 from nncf.config.schemata.basic import with_attributes
+from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
+from nncf.config.schemata.common.compression import COMPRESSION_LR_MULTIPLIER_PROPERTY
 from nncf.config.schemata.common.sparsity import COMMON_SPARSITY_PARAM_PROPERTIES
 from nncf.config.schemata.common.targeting import SCOPING_PROPERTIES
 from nncf.config.schemata.defaults import SPARSITY_INIT
 
 RB_SPARSITY_SCHEMA = {
     **BASIC_COMPRESSION_ALGO_SCHEMA,
     "description": f"Applies sparsity on top of the current model. Each weight tensor value will be either kept as-is, "
-                   f"or set to 0 based on its importance as determined by the regularization-based sparsity algorithm. "
-                   f"For large sparsity levels, this will improve performance on "
-                   f"hardware that can profit from it. "
-                   f"See [Sparsity.md]"
-                   f"({ONLINE_DOCS_ROOT}"
-                   f"/docs/compression_algorithms/Sparsity.md#rb-sparsity) and the rest of this schema for "
-                   f"more details and parameters.",
+    f"or set to 0 based on its importance as determined by the regularization-based sparsity algorithm. "
+    f"For large sparsity levels, this will improve performance on "
+    f"hardware that can profit from it. "
+    f"See [Sparsity.md]"
+    f"({ONLINE_DOCS_ROOT}"
+    f"/docs/compression_algorithms/Sparsity.md#rb-sparsity) and the rest of this schema for "
+    f"more details and parameters.",
     "properties": {
-        "algorithm": {
-            "const": RB_SPARSITY_ALGO_NAME_IN_CONFIG
-        },
-        "sparsity_init": with_attributes(NUMBER,
-                                         description="Initial value of the sparsity level applied to the "
-                                                     "model",
-                                         default=SPARSITY_INIT),
-        "params":
-            {
-                "type": "object",
-                "properties": COMMON_SPARSITY_PARAM_PROPERTIES,
-                "additionalProperties": False
-            },
+        "algorithm": {"const": RB_SPARSITY_ALGO_NAME_IN_CONFIG},
+        "sparsity_init": with_attributes(
+            NUMBER, description="Initial value of the sparsity level applied to the model", default=SPARSITY_INIT
+        ),
+        "params": {"type": "object", "properties": COMMON_SPARSITY_PARAM_PROPERTIES, "additionalProperties": False},
         **SCOPING_PROPERTIES,
-        **COMPRESSION_LR_MULTIPLIER_PROPERTY
+        **COMPRESSION_LR_MULTIPLIER_PROPERTY,
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/basic.py` & `nncf-2.5.0/nncf/config/schemata/basic.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,74 +1,49 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Dict
 
-NUMBER = {
-    "type": "number"
-}
-STRING = {
-    "type": "string"
-}
-BOOLEAN = {
-    "type": "boolean"
-}
-ARRAY_OF_NUMBERS = {
-    "type": "array",
-    "items": NUMBER
-}
-ARRAY_OF_STRINGS = {
-    "type": "array",
-    "items": STRING
-}
+NUMBER = {"type": "number"}
+STRING = {"type": "string"}
+BOOLEAN = {"type": "boolean"}
+ARRAY_OF_NUMBERS = {"type": "array", "items": NUMBER}
+ARRAY_OF_STRINGS = {"type": "array", "items": STRING}
 
 
 def annotated_enum(names_vs_description: Dict[str, str]) -> Dict:
     retval_list = []
     for name, descr in names_vs_description.items():
         retval_list.append({"const": name, "title": name, "description": descr})
-    return {
-        "oneOf": retval_list
-    }
+    return {"oneOf": retval_list}
 
 
 def make_string_or_array_of_strings_schema(addtl_dict_entries: Dict = None) -> Dict:
     if addtl_dict_entries is None:
         addtl_dict_entries = {}
-    retval = {
-        "type": ["array", "string"],
-        "items": {
-            "type": "string"
-        }
-    }
+    retval = {"type": ["array", "string"], "items": {"type": "string"}}
     retval.update(addtl_dict_entries)
     return retval
 
 
 def make_object_or_array_of_objects_schema(single_object_schema: Dict = None) -> Dict:
     retval = {
         "oneOf": [
             {
                 "title": "single_object_version",
                 **single_object_schema,
             },
-            {
-                "title": "array_of_objects_version",
-                "type": "array",
-                "items": single_object_schema
-            },
+            {"title": "array_of_objects_version", "type": "array", "items": single_object_schema},
         ]
     }
     return retval
 
 
 def with_attributes(schema: Dict, **kwargs) -> Dict:
     retval = {**schema, **kwargs}
```

### Comparing `nncf-2.4.0/nncf/config/schemata/common/compression.py` & `nncf-2.5.0/nncf/config/schemata/common/compression.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,26 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.schemata.basic import NUMBER
 from nncf.config.schemata.basic import with_attributes
 
 COMPRESSION_LR_MULTIPLIER_PROPERTY = {
-    "compression_lr_multiplier":
-        with_attributes(NUMBER,
-                        description="PyTorch only - Used to increase/decrease gradients "
-                                    "for compression algorithms' parameters. The gradients will be multiplied "
-                                    "by the specified value. If unspecified, the gradients will not be adjusted.")
-}
-BASIC_COMPRESSION_ALGO_SCHEMA = {
-    "type": "object",
-    "required": ["algorithm"]
+    "compression_lr_multiplier": with_attributes(
+        NUMBER,
+        description="PyTorch only - Used to increase/decrease gradients "
+        "for compression algorithms' parameters. The gradients will be multiplied "
+        "by the specified value. If unspecified, the gradients will not be adjusted.",
+    )
 }
+BASIC_COMPRESSION_ALGO_SCHEMA = {"type": "object", "required": ["algorithm"]}
```

### Comparing `nncf-2.4.0/nncf/config/schemata/common/initialization.py` & `nncf-2.5.0/nncf/config/schemata/common/initialization.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.definitions import ONLINE_DOCS_ROOT
 from nncf.config.schemata.basic import NUMBER
 from nncf.config.schemata.basic import with_attributes
 from nncf.config.schemata.defaults import NUM_BN_ADAPTATION_SAMPLES
 
 BATCHNORM_ADAPTATION_SCHEMA = {
     "type": "object",
     "description": f"This initializer is applied by default to utilize batch norm statistics adaptation to the "
-                   f"current compression scenario. See "
-                   f"[documentation]"
-                   f"({ONLINE_DOCS_ROOT}docs/compression_algorithms/Quantization.md#batch-norm-statistics-adaptation) "
-                   f"for more details.",
+    f"current compression scenario. See "
+    f"[documentation]"
+    f"({ONLINE_DOCS_ROOT}docs/compression_algorithms/Quantization.md#batch-norm-statistics-adaptation) "
+    f"for more details.",
     "properties": {
-        "num_bn_adaptation_samples": with_attributes(NUMBER,
-                                                     description="Number of samples from the training "
-                                                                 "dataset to use for model inference "
-                                                                 "during the BatchNorm statistics "
-                                                                 "adaptation procedure for the compressed "
-                                                                 "model. The actual number of samples will "
-                                                                 "be a closest multiple of the batch "
-                                                                 "size. Set this to 0 to disable BN adaptation.",
-                                                     default=NUM_BN_ADAPTATION_SAMPLES)
+        "num_bn_adaptation_samples": with_attributes(
+            NUMBER,
+            description="Number of samples from the training "
+            "dataset to use for model inference "
+            "during the BatchNorm statistics "
+            "adaptation procedure for the compressed "
+            "model. The actual number of samples will "
+            "be a closest multiple of the batch "
+            "size. Set this to 0 to disable BN adaptation.",
+            default=NUM_BN_ADAPTATION_SAMPLES,
+        )
     },
     "additionalProperties": False,
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/common/sparsity.py` & `nncf-2.5.0/nncf/config/schemata/common/sparsity.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.schemata.basic import ARRAY_OF_NUMBERS
 from nncf.config.schemata.basic import BOOLEAN
 from nncf.config.schemata.basic import NUMBER
 from nncf.config.schemata.basic import STRING
 from nncf.config.schemata.basic import with_attributes
 from nncf.config.schemata.defaults import RB_SPARSITY_SCHEDULER
 from nncf.config.schemata.defaults import SPARSITY_FREEZE_EPOCH
@@ -24,75 +22,102 @@
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_CONCAVE
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_PATIENCE
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_POWER
 from nncf.config.schemata.defaults import SPARSITY_SCHEDULER_UPDATE_PER_OPTIMIZER_STEP
 from nncf.config.schemata.defaults import SPARSITY_TARGET
 from nncf.config.schemata.defaults import SPARSITY_TARGET_EPOCH
 
-SCHEDULE_OPTIONS = ['polynomial', 'exponential', 'adaptive', 'multistep', ]
+SCHEDULE_OPTIONS = [
+    "polynomial",
+    "exponential",
+    "adaptive",
+    "multistep",
+]
 
 COMMON_SPARSITY_PARAM_PROPERTIES = {
-    "sparsity_level_setting_mode": with_attributes(STRING,
-                                                   description="The mode of sparsity level setting. \n"
-                                                               "`global` - the sparsity level is calculated across all "
-                                                               "weight values in the network across layers, "
-                                                               "`local` - the sparsity level can be set per-layer and "
-                                                               "within each layer is computed with respect only to the "
-                                                               "weight values within that layer.",
-                                                   default=SPARSITY_LEVEL_SETTING_MODE),
-    "schedule": with_attributes(STRING,
-                                description=f"The type of scheduling to use for adjusting the target"
-                                            f"sparsity level. Default - {RB_SPARSITY_SCHEDULER} for `rb_sparsity`, "
-                                            f"{SPARSITY_SCHEDULER} otherwise",
-                                enum=SCHEDULE_OPTIONS),
-    "sparsity_target": with_attributes(NUMBER,
-                                       description="Target sparsity level for the model, to be reached at the end of "
-                                                   "the compression schedule.",
-                                       default=SPARSITY_TARGET),
-    "sparsity_target_epoch": with_attributes(NUMBER,
-                                             description="Index of the epoch upon which the sparsity level "
-                                                         "of the model is scheduled to become "
-                                                         "equal to `sparsity_target`.",
-                                             default=SPARSITY_TARGET_EPOCH),
-    "sparsity_freeze_epoch": with_attributes(NUMBER,
-                                             description="Index of the epoch upon which the sparsity mask will "
-                                                         "be frozen and no longer trained.",
-                                             default=SPARSITY_FREEZE_EPOCH),
-
-    "update_per_optimizer_step": with_attributes(BOOLEAN,
-                                                 description="Whether the function-based sparsity level schedulers "
-                                                             "should update the sparsity level after each optimizer "
-                                                             "step instead of each epoch step.",
-                                                 default=SPARSITY_SCHEDULER_UPDATE_PER_OPTIMIZER_STEP),
-    "steps_per_epoch": with_attributes(NUMBER,
-                                       description="Number of optimizer steps in one epoch. Required to start proper "
-                                                   "scheduling in the first training epoch if "
-                                                   "`update_per_optimizer_step` is `true.`"),
-    "multistep_steps": with_attributes(ARRAY_OF_NUMBERS,
-                                       description="A list of scheduler steps at which to transition "
-                                                   "to the next scheduled sparsity level (multistep "
-                                                   "scheduler only).",
-                                       default=SPARSITY_MULTISTEP_STEPS),
-    "multistep_sparsity_levels": with_attributes(ARRAY_OF_NUMBERS,
-                                                 description="Multistep scheduler only - Levels of sparsity to use at "
-                                                             "each step of the scheduler as specified in the "
-                                                             "`multistep_steps` attribute. The first sparsity level "
-                                                             "will be applied immediately, "
-                                                             "so the length of this list should be larger than the "
-                                                             "length of the `multistep_steps` by one. The last "
-                                                             "sparsity level will function as the ultimate sparsity "
-                                                             "target, overriding the \"sparsity_target\" setting if it "
-                                                             "is present.",
-                                                 default=SPARSITY_MULTISTEP_SPARSITY_LEVELS),
-    "patience": with_attributes(NUMBER,
-                                description="A conventional patience parameter for the scheduler, "
-                                            "as for any other standard scheduler. Specified in units "
-                                            "of scheduler steps.",
-                                default=SPARSITY_SCHEDULER_PATIENCE),
-    "power": with_attributes(NUMBER,
-                             description="For polynomial scheduler - determines the corresponding power value.",
-                             default=SPARSITY_SCHEDULER_POWER),
-    "concave": with_attributes(BOOLEAN, description="For polynomial scheduler - if `true`, then the target sparsity "
-                                                    "level will be approached in concave manner, and in convex "
-                                                    "manner otherwise.",
-                               default=SPARSITY_SCHEDULER_CONCAVE),
+    "sparsity_level_setting_mode": with_attributes(
+        STRING,
+        description="The mode of sparsity level setting. \n"
+        "`global` - the sparsity level is calculated across all "
+        "weight values in the network across layers, "
+        "`local` - the sparsity level can be set per-layer and "
+        "within each layer is computed with respect only to the "
+        "weight values within that layer.",
+        default=SPARSITY_LEVEL_SETTING_MODE,
+    ),
+    "schedule": with_attributes(
+        STRING,
+        description=f"The type of scheduling to use for adjusting the target"
+        f"sparsity level. Default - {RB_SPARSITY_SCHEDULER} for `rb_sparsity`, "
+        f"{SPARSITY_SCHEDULER} otherwise",
+        enum=SCHEDULE_OPTIONS,
+    ),
+    "sparsity_target": with_attributes(
+        NUMBER,
+        description="Target sparsity level for the model, to be reached at the end of the compression schedule.",
+        default=SPARSITY_TARGET,
+    ),
+    "sparsity_target_epoch": with_attributes(
+        NUMBER,
+        description="Index of the epoch upon which the sparsity level "
+        "of the model is scheduled to become "
+        "equal to `sparsity_target`.",
+        default=SPARSITY_TARGET_EPOCH,
+    ),
+    "sparsity_freeze_epoch": with_attributes(
+        NUMBER,
+        description="Index of the epoch upon which the sparsity mask will be frozen and no longer trained.",
+        default=SPARSITY_FREEZE_EPOCH,
+    ),
+    "update_per_optimizer_step": with_attributes(
+        BOOLEAN,
+        description="Whether the function-based sparsity level schedulers "
+        "should update the sparsity level after each optimizer "
+        "step instead of each epoch step.",
+        default=SPARSITY_SCHEDULER_UPDATE_PER_OPTIMIZER_STEP,
+    ),
+    "steps_per_epoch": with_attributes(
+        NUMBER,
+        description="Number of optimizer steps in one epoch. Required to start proper "
+        "scheduling in the first training epoch if "
+        "`update_per_optimizer_step` is `true.`",
+    ),
+    "multistep_steps": with_attributes(
+        ARRAY_OF_NUMBERS,
+        description="A list of scheduler steps at which to transition "
+        "to the next scheduled sparsity level (multistep "
+        "scheduler only).",
+        default=SPARSITY_MULTISTEP_STEPS,
+    ),
+    "multistep_sparsity_levels": with_attributes(
+        ARRAY_OF_NUMBERS,
+        description="Multistep scheduler only - Levels of sparsity to use at "
+        "each step of the scheduler as specified in the "
+        "`multistep_steps` attribute. The first sparsity level "
+        "will be applied immediately, "
+        "so the length of this list should be larger than the "
+        "length of the `multistep_steps` by one. The last "
+        "sparsity level will function as the ultimate sparsity "
+        'target, overriding the "sparsity_target" setting if it '
+        "is present.",
+        default=SPARSITY_MULTISTEP_SPARSITY_LEVELS,
+    ),
+    "patience": with_attributes(
+        NUMBER,
+        description="A conventional patience parameter for the scheduler, "
+        "as for any other standard scheduler. Specified in units "
+        "of scheduler steps.",
+        default=SPARSITY_SCHEDULER_PATIENCE,
+    ),
+    "power": with_attributes(
+        NUMBER,
+        description="For polynomial scheduler - determines the corresponding power value.",
+        default=SPARSITY_SCHEDULER_POWER,
+    ),
+    "concave": with_attributes(
+        BOOLEAN,
+        description="For polynomial scheduler - if `true`, then the target sparsity "
+        "level will be approached in concave manner, and in convex "
+        "manner otherwise.",
+        default=SPARSITY_SCHEDULER_CONCAVE,
+    ),
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/common/targeting.py` & `nncf-2.5.0/nncf/config/schemata/common/targeting.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,45 +1,47 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.config.schemata.basic import make_string_or_array_of_strings_schema
 from nncf.config.schemata.basic import with_attributes
 from nncf.config.schemata.common.initialization import BATCHNORM_ADAPTATION_SCHEMA
 
-IGNORED_SCOPES_DESCRIPTION = "A list of model control flow graph node scopes to be ignored for this " \
-                             "operation - functions as an 'allowlist'. Optional."
-TARGET_SCOPES_DESCRIPTION = "A list of model control flow graph node scopes to be considered for this operation" \
-                            " - functions as a 'denylist'. Optional."
+IGNORED_SCOPES_DESCRIPTION = (
+    "A list of model control flow graph node scopes to be ignored for this "
+    "operation - functions as an 'allowlist'. Optional."
+)
+TARGET_SCOPES_DESCRIPTION = (
+    "A list of model control flow graph node scopes to be considered for this operation"
+    " - functions as a 'denylist'. Optional."
+)
 SCOPING_PROPERTIES = {
-    "ignored_scopes": with_attributes(make_string_or_array_of_strings_schema(),
-                                      description=IGNORED_SCOPES_DESCRIPTION,
-                                      examples=[
-                                          "{re}conv.*",
-                                          ["LeNet/relu_0",
-                                           "LeNet/relu_1"]
-                                      ]),
-    "target_scopes": with_attributes(make_string_or_array_of_strings_schema(),
-                                     description=TARGET_SCOPES_DESCRIPTION,
-                                     examples=[
-                                         ["UNet/ModuleList[down_path]/UNetConvBlock[1]/Sequential[block]/Conv2d[0]",
-                                          "UNet/ModuleList[down_path]/UNetConvBlock[2]/Sequential[block]/Conv2d[0]",
-                                          "UNet/ModuleList[down_path]/UNetConvBlock[3]/Sequential[block]/Conv2d[0]",
-                                          "UNet/ModuleList[down_path]/UNetConvBlock[4]/Sequential[block]/Conv2d[0]"],
-                                         "UNet/ModuleList\\[up_path\\].*"
-                                     ]),
+    "ignored_scopes": with_attributes(
+        make_string_or_array_of_strings_schema(),
+        description=IGNORED_SCOPES_DESCRIPTION,
+        examples=["{re}conv.*", ["LeNet/relu_0", "LeNet/relu_1"]],
+    ),
+    "target_scopes": with_attributes(
+        make_string_or_array_of_strings_schema(),
+        description=TARGET_SCOPES_DESCRIPTION,
+        examples=[
+            [
+                "UNet/ModuleList[down_path]/UNetConvBlock[1]/Sequential[block]/Conv2d[0]",
+                "UNet/ModuleList[down_path]/UNetConvBlock[2]/Sequential[block]/Conv2d[0]",
+                "UNet/ModuleList[down_path]/UNetConvBlock[3]/Sequential[block]/Conv2d[0]",
+                "UNet/ModuleList[down_path]/UNetConvBlock[4]/Sequential[block]/Conv2d[0]",
+            ],
+            "UNet/ModuleList\\[up_path\\].*",
+        ],
+    ),
 }
 GENERIC_INITIALIZER_SCHEMA = {
     "type": "object",
-    "properties": {
-        "batchnorm_adaptation": BATCHNORM_ADAPTATION_SCHEMA
-    },
+    "properties": {"batchnorm_adaptation": BATCHNORM_ADAPTATION_SCHEMA},
     "additionalProperties": False,
 }
```

### Comparing `nncf-2.4.0/nncf/config/schemata/defaults.py` & `nncf-2.5.0/nncf/config/schemata/defaults.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,19 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-TARGET_DEVICE = 'ANY'
+TARGET_DEVICE = "ANY"
 
 NUM_BN_ADAPTATION_SAMPLES = 2000
 NUM_INIT_SAMPLES = 256
 MIN_PERCENTILE = 0.1
 MAX_PERCENTILE = 99.9
 
 PRECISION_INIT_BITWIDTHS = [2, 4, 8]
@@ -26,62 +24,62 @@
 HAWQ_DUMP_INIT_PRECISION_DATA = False
 
 AUTOQ_ITER_NUMBER = 0
 AUTOQ_COMPRESSION_RATIO = 0.15  # TODO (vshampor) do autoq and hawq follow the same convention here?
 AUTOQ_EVAL_SUBSET_RATIO = 1.0
 AUTOQ_WARMUP_ITER_NUMBER = 20
 
-QUANTIZATION_PRESET = 'performance'
+QUANTIZATION_PRESET = "performance"
 QUANTIZE_INPUTS = True
 QUANTIZE_OUTPUTS = False
 QUANTIZATION_EXPORT_TO_ONNX_STANDARD_OPS = False
-QUANTIZATION_OVERFLOW_FIX = 'enable'
+QUANTIZATION_OVERFLOW_FIX = "enable"
 QUANTIZATION_BITS = 8
 QUANTIZATION_PER_CHANNEL = False
 QUANTIZATION_LOGARITHM_SCALE = False
 
 ACTIVATIONS_QUANT_START_EPOCH = 1
 WEIGHTS_QUANT_START_EPOCH = 1
 LR_POLY_DURATION_EPOCHS = 30
 STAGED_QUANTIZATION_BASE_LR = 1e-3
 STAGED_QUANTIZATION_BASE_WD = 1e-5
 
-BINARIZATION_MODE = 'xnor'
+BINARIZATION_MODE = "xnor"
 
 PRUNING_INIT = 0.0
-PRUNING_SCHEDULE = 'exponential'
+PRUNING_SCHEDULE = "exponential"
 PRUNING_TARGET = 0.5
 PRUNING_NUM_INIT_STEPS = 0
 PRUNING_STEPS = 100
-PRUNING_FILTER_IMPORTANCE = 'L2'
-PRUNING_INTERLAYER_RANKING_TYPE = 'unweighted_ranking'
+PRUNING_FILTER_IMPORTANCE = "L2"
+PRUNING_INTERLAYER_RANKING_TYPE = "unweighted_ranking"
 PRUNING_ALL_WEIGHTS = False
 PRUNE_FIRST_CONV = False
 PRUNE_BATCH_NORMS = True
 PRUNE_DOWNSAMPLE_CONVS = False
 
 PRUNING_LEGR_GENERATIONS = 400
 PRUNING_LEGR_TRAIN_STEPS = 200
 PRUNING_LEGR_MAX_PRUNING = 0.8
 PRUNING_LEGR_RANDOM_SEED = 42
 
 SPARSITY_INIT = 0.0
-MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE = 'normed_abs'
-SPARSITY_SCHEDULER = 'polynomial'
-RB_SPARSITY_SCHEDULER = 'exponential'
+MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE = "normed_abs"
+SPARSITY_SCHEDULER = "polynomial"
+RB_SPARSITY_SCHEDULER = "exponential"
 SPARSITY_SCHEDULER_PATIENCE = 1
 SPARSITY_SCHEDULER_POWER = 0.9
 SPARSITY_SCHEDULER_CONCAVE = True
 SPARSITY_TARGET = 0.5
 SPARSITY_TARGET_EPOCH = 90
 SPARSITY_FREEZE_EPOCH = 100
 SPARSITY_SCHEDULER_UPDATE_PER_OPTIMIZER_STEP = False
 SPARSITY_MULTISTEP_STEPS = [90]
 SPARSITY_MULTISTEP_SPARSITY_LEVELS = [0.1, 0.5]
-SPARSITY_LEVEL_SETTING_MODE = 'global'
+SPARSITY_LEVEL_SETTING_MODE = "global"
 
 KNOWLEDGE_DISTILLATION_SCALE = 1.0
 KNOWLEDGE_DISTILLATION_TEMPERATURE = 1.0
 
 AA_PATIENCE_EPOCHS = 3
 AA_INITIAL_COMPRESSION_RATE_STEP = 0.1
 AA_INITIAL_TRAINING_PHASE_EPOCHS = 5
```

### Comparing `nncf-2.4.0/nncf/config/schemata/experimental_schema.py` & `nncf-2.5.0/nncf/config/schemata/experimental_schema.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,46 +1,43 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import copy
 
 from nncf.config.definitions import BOOTSTRAP_NAS_ALGO_NAME_IN_CONFIG
 from nncf.config.definitions import EXPERIMENTAL_QUANTIZATION_ALGO_NAME_IN_CONFIG
 from nncf.config.definitions import MOVEMENT_SPARSITY_ALGO_NAME_IN_CONFIG
-
+from nncf.config.schemata.algo.quantization import QUANTIZATION_SCHEMA
 from nncf.config.schemata.basic import ARRAY_OF_NUMBERS
 from nncf.config.schemata.basic import ARRAY_OF_STRINGS
+from nncf.config.schemata.basic import BOOLEAN
+from nncf.config.schemata.basic import NUMBER
+from nncf.config.schemata.basic import STRING
+from nncf.config.schemata.basic import make_string_or_array_of_strings_schema
+from nncf.config.schemata.basic import with_attributes
 from nncf.config.schemata.common.compression import BASIC_COMPRESSION_ALGO_SCHEMA
 from nncf.config.schemata.common.compression import COMPRESSION_LR_MULTIPLIER_PROPERTY
 from nncf.config.schemata.common.initialization import BATCHNORM_ADAPTATION_SCHEMA
-from nncf.config.schemata.basic import BOOLEAN
 from nncf.config.schemata.common.targeting import IGNORED_SCOPES_DESCRIPTION
 from nncf.config.schemata.common.targeting import SCOPING_PROPERTIES
-from nncf.config.schemata.basic import NUMBER
-from nncf.config.schemata.algo.quantization import QUANTIZATION_SCHEMA
-from nncf.config.schemata.basic import STRING
 from nncf.config.schemata.common.targeting import TARGET_SCOPES_DESCRIPTION
-from nncf.config.schemata.basic import make_string_or_array_of_strings_schema
-from nncf.config.schemata.basic import with_attributes
 
 ########################################################################################################################
 # Experimental Quantization
 ########################################################################################################################
 EXPERIMENTAL_QUANTIZATION_SCHEMA = copy.deepcopy(QUANTIZATION_SCHEMA)
-EXPERIMENTAL_QUANTIZATION_SCHEMA['properties']['algorithm']['const'] = EXPERIMENTAL_QUANTIZATION_ALGO_NAME_IN_CONFIG
+EXPERIMENTAL_QUANTIZATION_SCHEMA["properties"]["algorithm"]["const"] = EXPERIMENTAL_QUANTIZATION_ALGO_NAME_IN_CONFIG
 
 ########################################################################################################################
 # BootstrapNAS
 ########################################################################################################################
 
 TRAINING_ALGORITHMS_SCHEMA = {
     "type": "string",
@@ -55,334 +52,362 @@
 ELASTIC_DEPTH_SCHEMA = {
     "type": "object",
     "properties": {
         "skipped_blocks": {
             "type": "array",
             "items": ARRAY_OF_STRINGS,
             "description": "List of building blocks to be skipped. The block is defined by names of start and end "
-                           "nodes. The end node is skipped. In contrast, the start node is executed. It produces a "
-                           "tensor that is bypassed through the skipping nodes until the one after end node. ",
-            "examples": [
-                [
-                    ["start_op_1", "end_op_1"],
-                    ["start_op_2", "end_op_2"]
-                ]
-            ],
+            "nodes. The end node is skipped. In contrast, the start node is executed. It produces a "
+            "tensor that is bypassed through the skipping nodes until the one after end node. ",
+            "examples": [[["start_op_1", "end_op_1"], ["start_op_2", "end_op_2"]]],
         },
-        "min_block_size": with_attributes(NUMBER,
-                                          description="Defines minimal number of operations in the skipping block. "
-                                                      "Option is available for the auto mode only. "
-                                                      "Default value is 5"),
-        "max_block_size": with_attributes(NUMBER,
-                                          description="Defines maximal number of operations in the block. "
-                                                      "Option is available for the auto mode only. "
-                                                      "Default value is 50"),
-        "hw_fused_ops": with_attributes(BOOLEAN,
-                                        description="If True, automatic block search will not relate operations, "
-                                                    "which are fused on inference, into different blocks for skipping. "
-                                                    "True, by default"),
+        "min_block_size": with_attributes(
+            NUMBER,
+            description="Defines minimal number of operations in the skipping block. "
+            "Option is available for the auto mode only. "
+            "Default value is 5",
+        ),
+        "max_block_size": with_attributes(
+            NUMBER,
+            description="Defines maximal number of operations in the block. "
+            "Option is available for the auto mode only. "
+            "Default value is 50",
+        ),
+        "hw_fused_ops": with_attributes(
+            BOOLEAN,
+            description="If True, automatic block search will not relate operations, "
+            "which are fused on inference, into different blocks for skipping. "
+            "True, by default",
+        ),
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 ELASTIC_WIDTH_SCHEMA = {
     "type": "object",
     "properties": {
-        "min_width":
-        with_attributes(NUMBER,
-                        description="Minimal number of output channels that can be activated for "
-                                    "each layers with elastic width. Default value is 32."),
-        "max_num_widths":
-        with_attributes(NUMBER,
-                        description="Restricts total number of different elastic width values for "
-                                    "each layer. The default value is -1 means that there's no "
-                                    "restrictions."),
-        "width_step":
-        with_attributes(NUMBER,
-                        description="Defines a step size for a generation of the elastic width search "
-                                    "space - the list of all possible width values for each layer. The "
-                                    "generation starts from the number of output channels in the "
-                                    "original model and stops when it reaches whether a "
-                                    "`min_width` width value or number of generated width values "
-                                    "equal to `max_num_widths`"),
-        "width_multipliers":
-        with_attributes(ARRAY_OF_NUMBERS,
-                        description="Defines elastic width search space via a list of "
-                                    "multipliers. All possible width values are obtained by "
-                                    "multiplying the original width value with the values in the "
-                                    "given list."),
-        "filter_importance":
-        with_attributes(STRING,
-                        description="The type of filter importance metric. Can be"
-                                    " one of `L1`, `L2`, `geometric_median`."
-                                    " `L2` by default.")
+        "min_width": with_attributes(
+            NUMBER,
+            description="Minimal number of output channels that can be activated for "
+            "each layers with elastic width. Default value is 32.",
+        ),
+        "max_num_widths": with_attributes(
+            NUMBER,
+            description="Restricts total number of different elastic width values for "
+            "each layer. The default value is -1 means that there's no "
+            "restrictions.",
+        ),
+        "width_step": with_attributes(
+            NUMBER,
+            description="Defines a step size for a generation of the elastic width search "
+            "space - the list of all possible width values for each layer. The "
+            "generation starts from the number of output channels in the "
+            "original model and stops when it reaches whether a "
+            "`min_width` width value or number of generated width values "
+            "equal to `max_num_widths`",
+        ),
+        "width_multipliers": with_attributes(
+            ARRAY_OF_NUMBERS,
+            description="Defines elastic width search space via a list of "
+            "multipliers. All possible width values are obtained by "
+            "multiplying the original width value with the values in the "
+            "given list.",
+        ),
+        "filter_importance": with_attributes(
+            STRING,
+            description="The type of filter importance metric. Can be"
+            " one of `L1`, `L2`, `geometric_median`."
+            " `L2` by default.",
+        ),
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 ELASTIC_KERNEL_SCHEMA = {
     "type": "object",
     "properties": {
-        "max_num_kernels":
-        with_attributes(NUMBER,
-                        description="Restricts the total number of different elastic kernel values for "
-                                    "each layer. The default value is -1 means that there's no "
-                                    "restrictions."),
+        "max_num_kernels": with_attributes(
+            NUMBER,
+            description="Restricts the total number of different elastic kernel values for "
+            "each layer. The default value is -1 means that there's no "
+            "restrictions.",
+        ),
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 ELASTICITY_SCHEMA = {
     "type": "object",
     "properties": {
         "depth": ELASTIC_DEPTH_SCHEMA,
         "width": ELASTIC_WIDTH_SCHEMA,
         "kernel": ELASTIC_KERNEL_SCHEMA,
-        "available_elasticity_dims": with_attributes(ARRAY_OF_STRINGS,
-                                                     description="Defines the available elasticity dimension for "
-                                                                 "sampling subnets. By default, all elastic dimensions "
-                                                                 "are available - [width, depth, kernel]"),
-        "ignored_scopes": with_attributes(make_string_or_array_of_strings_schema(),
-                                          description=IGNORED_SCOPES_DESCRIPTION),
-        "target_scopes": with_attributes(make_string_or_array_of_strings_schema(),
-                                         description=TARGET_SCOPES_DESCRIPTION),
+        "available_elasticity_dims": with_attributes(
+            ARRAY_OF_STRINGS,
+            description="Defines the available elasticity dimension for "
+            "sampling subnets. By default, all elastic dimensions "
+            "are available - [width, depth, kernel]",
+        ),
+        "ignored_scopes": with_attributes(
+            make_string_or_array_of_strings_schema(), description=IGNORED_SCOPES_DESCRIPTION
+        ),
+        "target_scopes": with_attributes(
+            make_string_or_array_of_strings_schema(), description=TARGET_SCOPES_DESCRIPTION
+        ),
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 STAGE_DESCRIPTOR_SCHEMA = {
     "type": "object",
     "properties": {
-        "train_dims": with_attributes(ARRAY_OF_STRINGS,
-                                      description="Elasticity dimensions that are enabled for subnet sampling,"
-                                                  "the rest elastic dimensions are disabled"),
-        "epochs": with_attributes(NUMBER,
-                                  description="Duration of the training stage in epochs"),
-        "depth_indicator": with_attributes(NUMBER,
-                                           description="Restricts the maximum number of blocks in each "
-                                                       "independent group that can be skipped. For example, Resnet50 "
-                                                       "has 4 four independent groups, each group consists of a "
-                                                       "specific number of Bottleneck layers [3,4,6,3], that "
-                                                       "potentially can be skipped. If depth indicator equals to 1,"
-                                                       " only the last Bottleneck can be skipped in each group, if it "
-                                                       "equals 2 - the last two and etc. This allows to implement "
-                                                       "progressive shrinking logic from `Once for all` paper. Default "
-                                                       "value is 1."),
-        "width_indicator": with_attributes(NUMBER,
-                                           description="Restricts the maximum number of width values in each elastic "
-                                                       "layer. For example, some conv2d with elastic width can vary "
-                                                       "number of output channels from the following list: [8, 16, 32] "
-                                                       "If width indicator is equal to 1, it can only activate the "
-                                                       "maximum number of channels - 32. If it equals 2, then the last "
-                                                       " two can be selected - 16 or 32, or both of them."),
-        "reorg_weights": with_attributes(BOOLEAN,
-                                         description="if True, triggers reorganization of weights in order to have "
-                                                     "filters sorted by importance (e.g. by l2 norm) in the "
-                                                     "beginning of the stage"),
-        "bn_adapt": with_attributes(BOOLEAN,
-                                    description="if True, triggers batchnorm adaptation in the beginning of the stage"),
-        "init_lr": with_attributes(NUMBER,
-                                   description="Initial learning rate for a stage. If specified in the stage "
-                                               " descriptor, it will trigger a reset of the learning rate at "
-                                               "the beginning of the stage."),
-        "epochs_lr": with_attributes(NUMBER,
-                                     description="Number of epochs to compute the adjustment of the learning rate."),
-        "sample_rate": with_attributes(NUMBER,
-                                       description="Number of iterations to activate the random subnet."
-                                                   "Default value is 1.")
+        "train_dims": with_attributes(
+            ARRAY_OF_STRINGS,
+            description="Elasticity dimensions that are enabled for subnet sampling,"
+            "the rest elastic dimensions are disabled",
+        ),
+        "epochs": with_attributes(NUMBER, description="Duration of the training stage in epochs"),
+        "depth_indicator": with_attributes(
+            NUMBER,
+            description="Restricts the maximum number of blocks in each "
+            "independent group that can be skipped. For example, Resnet50 "
+            "has 4 four independent groups, each group consists of a "
+            "specific number of Bottleneck layers [3,4,6,3], that "
+            "potentially can be skipped. If depth indicator equals to 1,"
+            " only the last Bottleneck can be skipped in each group, if it "
+            "equals 2 - the last two and etc. This allows to implement "
+            "progressive shrinking logic from `Once for all` paper. Default "
+            "value is 1.",
+        ),
+        "width_indicator": with_attributes(
+            NUMBER,
+            description="Restricts the maximum number of width values in each elastic "
+            "layer. For example, some conv2d with elastic width can vary "
+            "number of output channels from the following list: [8, 16, 32] "
+            "If width indicator is equal to 1, it can only activate the "
+            "maximum number of channels - 32. If it equals 2, then the last "
+            " two can be selected - 16 or 32, or both of them.",
+        ),
+        "reorg_weights": with_attributes(
+            BOOLEAN,
+            description="if True, triggers reorganization of weights in order to have "
+            "filters sorted by importance (e.g. by l2 norm) in the "
+            "beginning of the stage",
+        ),
+        "bn_adapt": with_attributes(
+            BOOLEAN, description="if True, triggers batchnorm adaptation in the beginning of the stage"
+        ),
+        "init_lr": with_attributes(
+            NUMBER,
+            description="Initial learning rate for a stage. If specified in the stage "
+            " descriptor, it will trigger a reset of the learning rate at "
+            "the beginning of the stage.",
+        ),
+        "epochs_lr": with_attributes(
+            NUMBER, description="Number of epochs to compute the adjustment of the learning rate."
+        ),
+        "sample_rate": with_attributes(
+            NUMBER, description="Number of iterations to activate the random subnet. Default value is 1."
+        ),
     },
     "description": "Defines a supernet training stage: how many epochs it takes, which elasticities with which "
-                   "settings are enabled, whether some operation should happen in the beginning",
-    "additionalProperties": False
-
+    "settings are enabled, whether some operation should happen in the beginning",
+    "additionalProperties": False,
 }
 NAS_SCHEDULE_SCHEMA = {
     "type": "object",
     "properties": {
         "list_stage_descriptions": {
             "type": "array",
             "items": STAGE_DESCRIPTOR_SCHEMA,
-            "description": "List of parameters per each supernet training stage"
+            "description": "List of parameters per each supernet training stage",
         }
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 LR_SCHEDULE_SCHEMA = {
     "type": "object",
     "properties": {
         "params": {
             "type": "object",
             "properties": {
-                "base_lr":
-                with_attributes(NUMBER,
-                                description="Defines a global learning rate scheduler."
-                                "If these parameters are not set, a stage learning rate scheduler will be used."),
+                "base_lr": with_attributes(
+                    NUMBER,
+                    description="Defines a global learning rate scheduler."
+                    "If these parameters are not set, a stage learning rate scheduler will be used.",
+                ),
             },
-            "additionalProperties": False
+            "additionalProperties": False,
         }
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 BOOTSTRAP_NAS_TRAINING_SCHEMA = {
     "type": "object",
     "properties": {
-        "algorithm": with_attributes(TRAINING_ALGORITHMS_SCHEMA,
-                                     description="Defines training strategy for tuning supernet. By default, "
-                                                 "progressive shrinking"),
-        "progressivity_of_elasticity": with_attributes(ARRAY_OF_STRINGS,
-                                                       description="Defines the order of adding a new elasticity "
-                                                                   "dimension from stage to stage",
-                                                       examples=[["width", "depth", "kernel"]]),
+        "algorithm": with_attributes(
+            TRAINING_ALGORITHMS_SCHEMA,
+            description="Defines training strategy for tuning supernet. By default, progressive shrinking",
+        ),
+        "progressivity_of_elasticity": with_attributes(
+            ARRAY_OF_STRINGS,
+            description="Defines the order of adding a new elasticity dimension from stage to stage",
+            examples=[["width", "depth", "kernel"]],
+        ),
         "batchnorm_adaptation": BATCHNORM_ADAPTATION_SCHEMA,
         "schedule": NAS_SCHEDULE_SCHEMA,
         "elasticity": ELASTICITY_SCHEMA,
         "lr_schedule": LR_SCHEDULE_SCHEMA,
-        "train_steps": with_attributes(NUMBER,
-                                       description="Defines the number of samples used for each training epoch."),
+        "train_steps": with_attributes(
+            NUMBER, description="Defines the number of samples used for each training epoch."
+        ),
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 SEARCH_ALGORITHMS_SCHEMA = {
     "type": "string",
     "enum": ["NSGA2"],
 }
 
 BOOTSTRAP_NAS_SEARCH_SCHEMA = {
     "type": "object",
     "properties": {
-        "algorithm":
-        with_attributes(SEARCH_ALGORITHMS_SCHEMA,
-                        description="Defines the search algorithm. Default algorithm is NSGA-II."),
+        "algorithm": with_attributes(
+            SEARCH_ALGORITHMS_SCHEMA, description="Defines the search algorithm. Default algorithm is NSGA-II."
+        ),
         "batchnorm_adaptation": BATCHNORM_ADAPTATION_SCHEMA,
-        "num_evals":
-        with_attributes(NUMBER,
-                        description="Defines the number of evaluations that will be used by the search algorithm."),
-        "population":
-        with_attributes(NUMBER,
-                        description="Defines the population size when using an evolutionary search algorithm."),
-        "acc_delta":
-        with_attributes(NUMBER,
-                        description="Defines the absolute difference in accuracy that is tolerated "
-                                    "when looking for a subnetwork."),
-        "ref_acc":
-        with_attributes(NUMBER,
-                        description="Defines the reference accuracy from the pre-trained model used "
-                                    "to generate the super-network."),
+        "num_evals": with_attributes(
+            NUMBER, description="Defines the number of evaluations that will be used by the search algorithm."
+        ),
+        "population": with_attributes(
+            NUMBER, description="Defines the population size when using an evolutionary search algorithm."
+        ),
+        "acc_delta": with_attributes(
+            NUMBER,
+            description="Defines the absolute difference in accuracy that is tolerated "
+            "when looking for a subnetwork.",
+        ),
+        "ref_acc": with_attributes(
+            NUMBER,
+            description="Defines the reference accuracy from the pre-trained model used "
+            "to generate the super-network.",
+        ),
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 
 BOOTSTRAP_NAS_SCHEMA = {
     "type": "object",
     "properties": {
         "training": BOOTSTRAP_NAS_TRAINING_SCHEMA,
         "search": BOOTSTRAP_NAS_SEARCH_SCHEMA,
     },
-    "additionalProperties": False
+    "additionalProperties": False,
 }
 
 ########################################################################################################################
 # Movement Sparsity
 ########################################################################################################################
 
-MOVEMENT_SPARSE_STRUCTURE_MODE = ['fine', 'block', 'per_dim']
-MOVEMENT_POWER = 3.
-MOVEMENT_FINAL_IMPORTANCE_THRESHOLD = 0.
+MOVEMENT_SPARSE_STRUCTURE_MODE = ["fine", "block", "per_dim"]
+MOVEMENT_POWER = 3.0
+MOVEMENT_FINAL_IMPORTANCE_THRESHOLD = 0.0
 MOVEMENT_ENABLE_STRUCTURED_MASKING = True
 
 MOVEMENT_SPARSE_STRUCTURE_BY_SCOPES_SCHEMA = {
-    'type': 'object',
-    'properties': {
-        'mode':
-            with_attributes(STRING,
-                            description='Defines in which mode a supported layer will be sparsified.',
-                            enum=MOVEMENT_SPARSE_STRUCTURE_MODE),
-        'sparse_factors':
-            with_attributes(ARRAY_OF_NUMBERS,
-                            description='The block shape for weights to sparsify. Required when `mode`="block".'),
-        'axis':
-            with_attributes(NUMBER,
-                            description='The dimension for weights to sparsify. Required when `mode`="per_dim".'),
-        'target_scopes':
-            with_attributes(make_string_or_array_of_strings_schema(),
-                            description='Model control flow graph node scopes to be considered in this mode.')
+    "type": "object",
+    "properties": {
+        "mode": with_attributes(
+            STRING,
+            description="Defines in which mode a supported layer will be sparsified.",
+            enum=MOVEMENT_SPARSE_STRUCTURE_MODE,
+        ),
+        "sparse_factors": with_attributes(
+            ARRAY_OF_NUMBERS, description='The block shape for weights to sparsify. Required when `mode`="block".'
+        ),
+        "axis": with_attributes(
+            NUMBER, description='The dimension for weights to sparsify. Required when `mode`="per_dim".'
+        ),
+        "target_scopes": with_attributes(
+            make_string_or_array_of_strings_schema(),
+            description="Model control flow graph node scopes to be considered in this mode.",
+        ),
     },
-    'additionalProperties': False,
-    'required': ['mode', 'target_scopes']
+    "additionalProperties": False,
+    "required": ["mode", "target_scopes"],
 }
 
 MOVEMENT_SCHEDULER_PARAMS_SCHEMA = {
-    'type': 'object',
-    'properties': {
-        'warmup_start_epoch':
-            with_attributes(NUMBER,
-                            description='Index of the starting epoch (include) for warmup stage.'),
-        'warmup_end_epoch':
-            with_attributes(NUMBER,
-                            description='Index of the end epoch (exclude) for warmup stage.'),
-        'importance_regularization_factor':
-            with_attributes(NUMBER,
-                            description='The regularization factor on weight importance scores. With a larger '
-                                        'positive value, more model weights will be regarded as less important '
-                                        'and thus be sparsified.'),
-        'enable_structured_masking':
-            with_attributes(BOOLEAN,
-                            description='Whether to do structured mask resolution after warmup stage. Only '
-                                        'supports structured masking on multi-head self-attention blocks and '
-                                        'feed-forward networks now.',
-                            default=MOVEMENT_ENABLE_STRUCTURED_MASKING),
-        'power':
-            with_attributes(NUMBER,
-                            description='The power value of polynomial decay for threshold and '
-                                        'regularization factor update during warmup stage.',
-                            default=MOVEMENT_POWER),
-        'init_importance_threshold':
-            with_attributes(NUMBER,
-                            description='The initial value of importance threshold during warmup stage. If not '
-                                        'specified, this will be automatically decided during training so that '
-                                        'the model is with about 0.1% linear layer sparsity on involved layers at '
-                                        'the beginning of warmup stage.'),
-        'final_importance_threshold':
-            with_attributes(NUMBER,
-                            description='The final value of importance threshold during warmup stage.',
-                            default=MOVEMENT_FINAL_IMPORTANCE_THRESHOLD),
-
-        'steps_per_epoch':
-            with_attributes(NUMBER,
-                            description='Number of training steps in one epoch, used for proper threshold and '
-                                        'regularization factor updates. Optional if warmup_start_epoch >=1 since '
-                                        'this can be counted in the 1st epoch. Otherwise users have to specify it.'),
-    },
-    'additionalProperties': False,
-    'required': ['warmup_start_epoch',
-                 'warmup_end_epoch',
-                 'importance_regularization_factor']
+    "type": "object",
+    "properties": {
+        "warmup_start_epoch": with_attributes(
+            NUMBER, description="Index of the starting epoch (include) for warmup stage."
+        ),
+        "warmup_end_epoch": with_attributes(NUMBER, description="Index of the end epoch (exclude) for warmup stage."),
+        "importance_regularization_factor": with_attributes(
+            NUMBER,
+            description="The regularization factor on weight importance scores. With a larger "
+            "positive value, more model weights will be regarded as less important "
+            "and thus be sparsified.",
+        ),
+        "enable_structured_masking": with_attributes(
+            BOOLEAN,
+            description="Whether to do structured mask resolution after warmup stage. Only "
+            "supports structured masking on multi-head self-attention blocks and "
+            "feed-forward networks now.",
+            default=MOVEMENT_ENABLE_STRUCTURED_MASKING,
+        ),
+        "power": with_attributes(
+            NUMBER,
+            description="The power value of polynomial decay for threshold and "
+            "regularization factor update during warmup stage.",
+            default=MOVEMENT_POWER,
+        ),
+        "init_importance_threshold": with_attributes(
+            NUMBER,
+            description="The initial value of importance threshold during warmup stage. If not "
+            "specified, this will be automatically decided during training so that "
+            "the model is with about 0.1% linear layer sparsity on involved layers at "
+            "the beginning of warmup stage.",
+        ),
+        "final_importance_threshold": with_attributes(
+            NUMBER,
+            description="The final value of importance threshold during warmup stage.",
+            default=MOVEMENT_FINAL_IMPORTANCE_THRESHOLD,
+        ),
+        "steps_per_epoch": with_attributes(
+            NUMBER,
+            description="Number of training steps in one epoch, used for proper threshold and "
+            "regularization factor updates. Optional if warmup_start_epoch >=1 since "
+            "this can be counted in the 1st epoch. Otherwise users have to specify it.",
+        ),
+    },
+    "additionalProperties": False,
+    "required": ["warmup_start_epoch", "warmup_end_epoch", "importance_regularization_factor"],
 }
 
 
 MOVEMENT_SPARSITY_SCHEMA = {
     **BASIC_COMPRESSION_ALGO_SCHEMA,
-    'properties': {
-        'algorithm': {
-            'const': MOVEMENT_SPARSITY_ALGO_NAME_IN_CONFIG
-        },
-        'params': MOVEMENT_SCHEDULER_PARAMS_SCHEMA,
-        'sparse_structure_by_scopes': {
-            'type': 'array',
-            'items': MOVEMENT_SPARSE_STRUCTURE_BY_SCOPES_SCHEMA,
-            'description': 'Describes how each supported layer will be sparsified.'
+    "properties": {
+        "algorithm": {"const": MOVEMENT_SPARSITY_ALGO_NAME_IN_CONFIG},
+        "params": MOVEMENT_SCHEDULER_PARAMS_SCHEMA,
+        "sparse_structure_by_scopes": {
+            "type": "array",
+            "items": MOVEMENT_SPARSE_STRUCTURE_BY_SCOPES_SCHEMA,
+            "description": "Describes how each supported layer will be sparsified.",
         },
         **SCOPING_PROPERTIES,
-        **COMPRESSION_LR_MULTIPLIER_PROPERTY
+        **COMPRESSION_LR_MULTIPLIER_PROPERTY,
     },
-    'additionalProperties': False
+    "additionalProperties": False,
 }
 
 ########################################################################################################################
 # All experimental schemas
 ########################################################################################################################
 
 EXPERIMENTAL_REF_VS_ALGO_SCHEMA = {
```

### Comparing `nncf-2.4.0/nncf/config/structures.py` & `nncf-2.5.0/nncf/config/structures.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,101 +1,99 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Structures for passing live Python objects into NNCF algorithms.
 """
-
-from typing import Optional, Callable
+from typing import Callable, Optional
 
 from nncf.common.initialization.dataloader import NNCFDataLoader
+from nncf.common.utils.api_marker import api
 
 
 class NNCFExtraConfigStruct:
     """
     This is the class from which all extra structures that define additional
     NNCFConfig arguments inherit.
     """
 
     @classmethod
     def get_id(cls) -> str:
         raise NotImplementedError
 
 
+@api()
 class QuantizationRangeInitArgs(NNCFExtraConfigStruct):
     """
     Stores additional arguments for quantization range initialization algorithms.
+
+    :param data_loader: Provides an iterable over the given dataset.
+    :param device: Device to perform initialization. If `device` is `None`
+        then the device of the model parameters will be used.
     """
 
-    def __init__(self,
-                 data_loader: NNCFDataLoader,
-                 device: Optional[str] = None):
-        """
-        Initializes additional arguments for quantization range initialization
-        algorithms.
-
-        :param data_loader: Provides an iterable over the given dataset.
-        :param device: Device to perform initialization. If `device` is `None`
-            then the device of the model parameters will be used.
-        """
+    def __init__(self, data_loader: NNCFDataLoader, device: Optional[str] = None):
         self._data_loader = data_loader
         self._device = device
 
     @property
     def data_loader(self) -> NNCFDataLoader:
         return self._data_loader
 
     @property
     def device(self) -> str:
         return self._device
 
     @classmethod
     def get_id(cls) -> str:
-        return 'quantization_range_init_args'
+        return "quantization_range_init_args"
 
 
+@api()
 class BNAdaptationInitArgs(NNCFExtraConfigStruct):
     """
     Stores additional arguments for batchnorm statistics adaptation algorithm.
+
+    :param data_loader: Provides an iterable over the given dataset.
+    :param device: Device to perform initialization. If `device` is `None`
+        then the device of the model parameters will be used.
     """
 
-    def __init__(self,
-                 data_loader: NNCFDataLoader,
-                 device: Optional[str] = None):
-        """
-        Initializes additional arguments for batchnorm statistics adaptation
-        algorithm.
-
-        :param data_loader: Provides an iterable over the given dataset.
-        :param device: Device to perform initialization. If `device` is `None`
-            then the device of the model parameters will be used.
-        """
+    def __init__(self, data_loader: NNCFDataLoader, device: Optional[str] = None):
         self._data_loader = data_loader
         self._device = device
 
     @property
     def data_loader(self) -> NNCFDataLoader:
         return self._data_loader
 
     @property
     def device(self) -> str:
         return self._device
 
     @classmethod
     def get_id(cls) -> str:
-        return 'bn_adaptation_init_args'
+        return "bn_adaptation_init_args"
 
 
+@api()
 class ModelEvaluationArgs(NNCFExtraConfigStruct):
-    def __init__(self,
-                 eval_fn: Callable):
+    """
+    Stores additional arguments for running the model in the evaluation mode, should this be required for an algorithm.
+
+    :param eval_fn: A function accepting a single argument - the model object - and returning the model's metric on
+        the evaluation split of the dataset corresponding to the model.
+    """
+
+    def __init__(self, eval_fn: Callable):
         self.eval_fn = eval_fn
 
     @classmethod
     def get_id(cls) -> str:
         return "model_evaluation_args"
```

### Comparing `nncf-2.4.0/nncf/config/telemetry_extractors.py` & `nncf-2.5.0/nncf/config/telemetry_extractors.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,24 +1,21 @@
-"""
- Copyright (c) 2020-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from nncf.telemetry.extractors import CollectedEvent
-from nncf.telemetry.extractors import TelemetryExtractor
 from nncf.config import NNCFConfig
 from nncf.config.extractors import extract_algorithm_names
+from nncf.telemetry.extractors import CollectedEvent
+from nncf.telemetry.extractors import TelemetryExtractor
 
 
 class CompressionStartedFromConfig(TelemetryExtractor):
     def extract(self, argvalue: NNCFConfig) -> CollectedEvent:
         algo_names = extract_algorithm_names(argvalue)
-        return CollectedEvent(name="compression_started",
-                              data=",".join(algo_names))
+        return CollectedEvent(name="compression_started", data=",".join(algo_names))
```

### Comparing `nncf-2.4.0/nncf/config/utils.py` & `nncf-2.5.0/nncf/config/utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,34 +1,32 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf import NNCFConfig
 
 
 def is_accuracy_aware_training(nncf_config: NNCFConfig) -> bool:
     if nncf_config.get("accuracy_aware_training") is not None:
         return True
     return False
 
 
 def is_experimental_quantization(nncf_config: NNCFConfig) -> bool:
-    compression_section = nncf_config.get('compression', [])
+    compression_section = nncf_config.get("compression", [])
     algorithms_names = []
 
     if isinstance(compression_section, dict):
-        algorithms_names.append(compression_section['algorithm'])
+        algorithms_names.append(compression_section["algorithm"])
 
     if isinstance(compression_section, list):
         for sec in compression_section:
-            algorithms_names.append(sec['algorithm'])
+            algorithms_names.append(sec["algorithm"])
 
-    return 'experimental_quantization' in algorithms_names
+    return "experimental_quantization" in algorithms_names
```

### Comparing `nncf-2.4.0/nncf/data/__init__.py` & `nncf-2.5.0/nncf/tensorflow/pruning/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,13 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Backend-specific implementations of pruning algorithms.
 """
-
-from nncf.data.dataset import Dataset
```

### Comparing `nncf-2.4.0/nncf/data/dataset.py` & `nncf-2.5.0/nncf/data/dataset.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,59 +1,53 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Iterable
-from typing import Callable
-from typing import Optional
-from typing import List
-from typing import Generic
-from typing import TypeVar
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Callable, Generic, Iterable, List, Optional, TypeVar
 
+from nncf.common.utils.api_marker import api
 
-DataItem = TypeVar('DataItem')
-ModelInput = TypeVar('ModelInput')
+DataItem = TypeVar("DataItem")
+ModelInput = TypeVar("ModelInput")
 
 
+@api(canonical_alias="nncf.Dataset")
 class Dataset(Generic[DataItem, ModelInput]):
     """
-    The `nncf.Dataset` class defines the interface by which compression algorithms
+    Wrapper for passing custom user datasets into NNCF algorithms.
+
+    This class defines the interface by which compression algorithms
     retrieve data items from the passed data source object. These data items are used
-    for different purposes, for example, model inference and model validation. It depends
-    on the compression algorithm.
+    for different purposes, for example, model inference and model validation, based
+    on the choice of the exact compression algorithm.
 
     If the data item has been returned from the data source per iteration and it cannot be
     used as input for model inference, the transformation function is used to extract the
     model's input from this data item. For example, in supervised learning, the data item
     usually contains both examples and labels. So transformation function should extract
     the examples from the data item.
-    """
 
-    def __init__(self,
-                 data_source: Iterable[DataItem],
-                 transform_func: Optional[Callable[[DataItem], ModelInput]] = None):
-        """
-        Initializes the `nncf.Dataset` object.
+    :param data_source: The iterable object serving as the source of data items.
+    :param transform_func: The function that is used to extract the model's input
+        from the data item. The data item here is the data item that is returned from
+        the data source per iteration. This function should be passed when
+        the data item cannot be directly used as model's input. If this is not specified, then the data item
+        will be passed into the model as-is.
+    """
 
-        :param data_source: It is the iterable object where data items are from.
-        :param transform_func: The function that is used to extract the model's input
-            from the data item. The data item here is the data item that is returned from
-            the data source per iteration. This function should be passed when
-            the data item cannot be used as model's input. The identity function is used
-            by default.
-        """
+    def __init__(
+        self, data_source: Iterable[DataItem], transform_func: Optional[Callable[[DataItem], ModelInput]] = None
+    ):
         self._data_source = data_source
         self._transform_func = transform_func
 
     def get_data(self, indices: Optional[List[int]] = None) -> Iterable[DataItem]:
         """
         Returns the iterable object that contains selected data items from the data source as-is.
 
@@ -76,44 +70,46 @@
         :return: The iterable object that contains selected data items from the data source, for which
             the transformation function was applied.
         """
         return DataProvider(self._data_source, self._transform_func, indices)
 
 
 class DataProvider(Generic[DataItem, ModelInput]):
-    def __init__(self,
-                 data_source: Iterable[DataItem],
-                 transform_func: Callable[[DataItem], ModelInput],
-                 indices: Optional[List[int]] = None):
+    def __init__(
+        self,
+        data_source: Iterable[DataItem],
+        transform_func: Callable[[DataItem], ModelInput],
+        indices: Optional[List[int]] = None,
+    ):
         self._data_source = data_source
         if transform_func is None:
             transform_func = lambda x: x
         self._transform_func = transform_func
         self._indices = indices
 
     def __iter__(self):
         if self._indices is None:
             return map(self._transform_func, self._data_source)
 
-        if hasattr(self._data_source, '__getitem__'):
+        if hasattr(self._data_source, "__getitem__"):
             return DataProvider._get_iterator_for_map_style(self._data_source, self._transform_func, self._indices)
 
         return DataProvider._get_iterator_for_iter(self._data_source, self._transform_func, sorted(self._indices))
 
     @staticmethod
-    def _get_iterator_for_map_style(data_source: Iterable[DataItem],
-                                    transform_func: Callable[[DataItem], ModelInput],
-                                    indices: List[int]):
+    def _get_iterator_for_map_style(
+        data_source: Iterable[DataItem], transform_func: Callable[[DataItem], ModelInput], indices: List[int]
+    ):
         for index in indices:
             yield transform_func(data_source[index])
 
     @staticmethod
-    def _get_iterator_for_iter(data_source: Iterable[DataItem],
-                               transform_func: Callable[[DataItem], ModelInput],
-                               indices: List[int]):
+    def _get_iterator_for_iter(
+        data_source: Iterable[DataItem], transform_func: Callable[[DataItem], ModelInput], indices: List[int]
+    ):
         pos = 0
         num_indices = len(indices)
         for idx, data_item in enumerate(data_source):
             if pos == num_indices:
                 # All specified data items were selected.
                 break
             if idx == indices[pos]:
```

### Comparing `nncf-2.4.0/nncf/definitions.py` & `nncf-2.5.0/nncf/definitions.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,22 +1,25 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import os
+from pathlib import Path
+
 NNCF_PACKAGE_ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
 HW_CONFIG_RELATIVE_DIR = "common/hardware/configs"
 
+NNCF_CACHE_PATH = Path.home() / Path(".cache/nncf/")
+CACHE_MODELS_PATH = NNCF_CACHE_PATH / Path("models")
+
 # Environment variables below, if set, mark the execution environment
 # so that certain actions within NNCF proper, such as telemetry event collection or
 # debug dumps, are performed or not performed
 NNCF_CI_ENV_VAR_NAME = "NNCF_CI"  # Must be set in CI environments
 NNCF_DEV_ENV_VAR_NAME = "NNCF_DEV"  # Must be set in environments of the NNCF dev team machines
```

### Comparing `nncf-2.4.0/nncf/experimental/openvino_native/graph/metatypes/openvino_metatypes.py` & `nncf-2.5.0/nncf/tensorflow/graph/metatypes/tf_ops.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,469 +1,444 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
-from typing import Type
 
-from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
-from nncf.common.graph.operator_metatypes import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.operator_metatypes import OperatorMetatypeRegistry
 from nncf.common.hardware.opset import HWConfigOpName
 
-OV_OPERATION_METATYPES = OperatorMetatypeRegistry('openvino_operator_metatypes')
+TF_OPERATION_METATYPES = OperatorMetatypeRegistry("tf_operation_metatypes")
 
 
-class OVOpMetatype(OperatorMetatype):
+class TFOpMetatype(OperatorMetatype):
     op_names = []  # type: List[str]
 
     @classmethod
     def get_all_aliases(cls) -> List[str]:
         return cls.op_names
 
 
-@OV_OPERATION_METATYPES.register()
-class OVConvolutionMetatype(OVOpMetatype):
-    name = 'ConvOp'
-    op_names = ['Convolution']
-    hw_config_names = [HWConfigOpName.CONVOLUTION]
-
-
-@OV_OPERATION_METATYPES.register()
-class OVConvolutionBackpropDataMetatype(OVOpMetatype):
-    name = 'ConvTransposeOp'
-    op_names = ['ConvolutionBackpropData']
-    hw_config_names = [HWConfigOpName.CONVOLUTION]
+class OpWeightDef:
+    """
+    Contains information about the weight of operation.
+    """
 
+    def __init__(self, port_id: int, channel_axes):
+        """
+        Initializes a definition of the weight.
 
-@OV_OPERATION_METATYPES.register()
-class OVReluMetatype(OVOpMetatype):
-    name = 'ReluOp'
-    op_names = ['Relu']
+        :param port_id: Zero-based argument number of the operation to
+            which this weight tensor corresponds.
+        :param channel_axes: Channel axes for weight tensor.
+        """
+        # TODO(andrey-churkin): Seems like we can determine the port id
+        # dynamically during the NNCF Graph building.
+        self.port_id = port_id
+        self.channel_axes = channel_axes
 
 
-@OV_OPERATION_METATYPES.register()
-class OVGeluMetatype(OVOpMetatype):
-    name = 'GeluOp'
-    op_names = ['Gelu']
-    hw_config_names = [HWConfigOpName.GELU]
+class TFOpWithWeightsMetatype(TFOpMetatype):
+    weight_definitions = []  # type: List[OpWeightDef]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVEluMetatype(OVOpMetatype):
-    name = 'EluOp'
-    op_names = ['Elu']
+@TF_OPERATION_METATYPES.register()
+class TFNoopMetatype(OperatorMetatype):
+    name = "noop"
+    op_names = ["noop"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVPReluMetatype(OVOpMetatype):
-    name = 'PReluOp'
-    op_names = ['PReLU']
+@TF_OPERATION_METATYPES.register()
+class TFIdentityOpMetatype(TFOpMetatype):
+    name = "IdentityOp"
+    op_names = ["Identity", "identity"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVSigmoidMetatype(OVOpMetatype):
-    name = 'SigmoidOp'
-    op_names = ['Sigmoid']
+@TF_OPERATION_METATYPES.register()
+class TFPackOpMetatype(TFOpMetatype):
+    # Unsqueezes->Concat pattern
+    name = "PackOp"
+    op_names = ["Pack", "stack"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVHardSigmoidMetatype(OVOpMetatype):
-    name = 'HardSigmoidOp'
-    op_names = ['HardSigmoid']
+@TF_OPERATION_METATYPES.register()
+class TFUnPackOpMetatype(TFOpMetatype):
+    name = "UnPackOp"
+    op_names = ["Unpack", "unstack"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVAveragePoolMetatype(OVOpMetatype):
-    name = 'AveragePoolOp'
-    op_names = ['AvgPool']
-    hw_config_names = [HWConfigOpName.AVGPOOL]
+@TF_OPERATION_METATYPES.register()
+class TFPadOpMetatype(TFOpMetatype):
+    name = "PadOp"
+    op_names = ["Pad", "compat.v1.pad", "pad"]
+    hw_config_names = [HWConfigOpName.PAD]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVMaxPoolMetatype(OVOpMetatype):
-    name = 'MaxPoolOp'
-    op_names = ['MaxPool']
-    hw_config_names = [HWConfigOpName.MAXPOOL]
+@TF_OPERATION_METATYPES.register()
+class TFStridedSliceOpMetatype(TFOpMetatype):
+    name = "StridedSliceOp"
+    op_names = ["StridedSlice", "__operators__.getitem"]
+    hw_config_names = [HWConfigOpName.STRIDEDSLICE]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVConstantMetatype(OVOpMetatype):
-    name = 'ConstantOp'
-    op_names = ['Constant']
+@TF_OPERATION_METATYPES.register()
+class TFConcatOpMetatype(TFOpMetatype):
+    name = "ConcatOp"
+    op_names = ["Concat", "ConcatV2", "concat"]
+    hw_config_names = [HWConfigOpName.CONCAT]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVAddMetatype(OVOpMetatype):
-    name = 'AddOp'
-    op_names = ['Add']
+@TF_OPERATION_METATYPES.register()
+class TFAddOpMetatype(TFOpMetatype):
+    name = "AddOp"
+    op_names = ["Add", "AddV2", "__operators__.add"]
     hw_config_names = [HWConfigOpName.ADD]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVSubMetatype(OVOpMetatype):
-    name = 'SubOp'
-    op_names = ['Subtract']
+@TF_OPERATION_METATYPES.register()
+class TFSubOpMetatype(TFOpMetatype):
+    name = "SubOp"
+    op_names = ["Sub", "math.subtract"]
     hw_config_names = [HWConfigOpName.SUBTRACT]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVMulMetatype(OVOpMetatype):
-    name = 'MulOp'
-    op_names = ['Multiply']
+@TF_OPERATION_METATYPES.register()
+class TFMulOpMetatype(TFOpMetatype):
+    name = "MulOp"
+    op_names = ["Mul", "math.multiply"]
     hw_config_names = [HWConfigOpName.MULTIPLY]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVDivMetatype(OVOpMetatype):
-    name = 'DivOp'
-    op_names = ['Divide']
-    hw_config_names = [HWConfigOpName.DIVIDE]
-
+@TF_OPERATION_METATYPES.register()
+class TFAvgPoolOpMetatype(TFOpMetatype):
+    name = "AvgPoolOp"
+    op_names = ["AvgPool", "nn.avg_pool"]
+    hw_config_names = [HWConfigOpName.AVGPOOL]
 
-@OV_OPERATION_METATYPES.register()
-class OVSumMetatype(OVOpMetatype):
-    name = 'SumOp'
-    op_names = ['ReduceSum']
-    hw_config_names = [HWConfigOpName.REDUCESUM]
 
+@TF_OPERATION_METATYPES.register()
+class TFAvgPool3DOpMetatype(TFOpMetatype):
+    name = "AvgPool3DOp"
+    op_names = ["AvgPool3D"]
+    hw_config_names = [HWConfigOpName.AVGPOOL]
 
-@OV_OPERATION_METATYPES.register()
-class OVConcatMetatype(OVOpMetatype):
-    name = 'ConcatOp'
-    op_names = ['Concat']
-    hw_config_names = [HWConfigOpName.CONCAT]
 
+@TF_OPERATION_METATYPES.register()
+class TFReluOpMetatype(TFOpMetatype):
+    name = "ReluOp"
+    op_names = ["Relu", "ReLU", "nn.relu"]
+
+
+@TF_OPERATION_METATYPES.register()
+class TFRelu6OpMetatype(TFOpMetatype):
+    name = "Relu6Op"
+    op_names = ["Relu6"]
+
+
+@TF_OPERATION_METATYPES.register()
+class TFMatMulOpMetatype(TFOpWithWeightsMetatype):
+    name = "MatMulOp"
+    op_names = ["MatMul", "linalg.matmul"]
+    weight_definitions = [OpWeightDef(port_id=1, channel_axes=-1)]
+    hw_config_names = [HWConfigOpName.MATMUL]
 
-@OV_OPERATION_METATYPES.register()
-class OVBatchNormMetatype(OVOpMetatype):
-    name = 'BatchNormalizationOp'
-    op_names = ['BatchNormInference']
 
+@TF_OPERATION_METATYPES.register()
+class TFBatchMatMulV2OpMetatype(TFOpMetatype):
+    name = "BatchMatMulV2Op"
+    op_names = ["BatchMatMulV2"]
+    hw_config_names = [HWConfigOpName.MATMUL]
 
-@OV_OPERATION_METATYPES.register()
-class OVInterpolateMetatype(OVOpMetatype):
-    name = 'InterpolateOp'
-    op_names = ['Interpolate']
-    hw_config_names = [HWConfigOpName.INTERPOLATE]
 
+@TF_OPERATION_METATYPES.register()
+class TFConv2DOpMetatype(TFOpWithWeightsMetatype):
+    name = "Conv2DOp"
+    op_names = ["Conv2D"]
+    weight_definitions = [OpWeightDef(port_id=1, channel_axes=-1)]
+    hw_config_names = [HWConfigOpName.CONVOLUTION]
 
-@OV_OPERATION_METATYPES.register()
-class OVMVNMetatype(OVOpMetatype):
-    name = 'MVNOp'
-    op_names = ['MVN']
-    hw_config_names = [HWConfigOpName.MVN]
-
-
-@OV_OPERATION_METATYPES.register()
-class OVNormalizeL2Metatype(OVOpMetatype):
-    name = 'NormalizeL2Op'
-    op_names = ['NormalizeL2']
-
-
-@OV_OPERATION_METATYPES.register()
-class OVReshapeMetatype(OVOpMetatype):
-    name = 'ReshapeOp'
-    op_names = ['Reshape']
-    hw_config_names = [HWConfigOpName.RESHAPE]
 
+@TF_OPERATION_METATYPES.register()
+class TFConv3DOpMetatype(TFOpWithWeightsMetatype):
+    name = "Conv3DOp"
+    op_names = ["Conv3D"]
+    weight_definitions = [OpWeightDef(port_id=1, channel_axes=-1)]
+    hw_config_names = [HWConfigOpName.CONVOLUTION]
 
-@OV_OPERATION_METATYPES.register()
-class OVShapeMetatype(OVOpMetatype):
-    name = 'ShapeOp'
-    op_names = ['ShapeOf']
-
-
-@OV_OPERATION_METATYPES.register()
-class OVNonZeroMetatype(OVOpMetatype):
-    name = 'NonZeroOp'
-    op_names = ['NonZero']
-
-
-@OV_OPERATION_METATYPES.register()
-class OVSplitMetatype(OVOpMetatype):
-    name = 'SplitOp'
-    op_names = ['Split']
-    hw_config_names = [HWConfigOpName.SPLIT]
 
+@TF_OPERATION_METATYPES.register()
+class TFDepthwiseConv2dNativeOpMetatype(TFOpWithWeightsMetatype):
+    name = "DepthwiseConv2dNativeOp"
+    op_names = ["DepthwiseConv2dNative"]
+    weight_definitions = [OpWeightDef(port_id=1, channel_axes=[2, 3])]
+    hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]
+
+
+@TF_OPERATION_METATYPES.register()
+class TFQuantizedConv2DOpMetatype(TFOpMetatype):
+    name = "QuantizedConv2DOp"
+    op_names = ["QuantizedConv2D"]
+
+
+@TF_OPERATION_METATYPES.register()
+class TFReshapeOpMetatype(TFOpMetatype):
+    name = "ReshapeOp"
+    op_names = ["Reshape", "reshape"]
+    hw_config_names = [HWConfigOpName.RESHAPE]
 
-@OV_OPERATION_METATYPES.register()
-class OVLessMetatype(OVOpMetatype):
-    name = 'LessOp'
-    op_names = ['Less']
-    hw_config_names = [HWConfigOpName.LESS]
-
-
-@OV_OPERATION_METATYPES.register()
-class OVLessEqualMetatype(OVOpMetatype):
-    name = 'LessEqualOp'
-    op_names = ['LessEqual']
-    hw_config_names = [HWConfigOpName.LESSEQUAL]
-
-
-@OV_OPERATION_METATYPES.register()
-class OVGreaterMetatype(OVOpMetatype):
-    name = 'GreaterOp'
-    op_names = ['Greater']
-    hw_config_names = [HWConfigOpName.GREATER]
 
+@TF_OPERATION_METATYPES.register()
+class TFExpandDimsOpMetatype(TFOpMetatype):
+    name = "ExpandDimsOp"
+    op_names = ["ExpandDims", "expand_dims"]
 
-@OV_OPERATION_METATYPES.register()
-class OVGreaterEqualMetatype(OVOpMetatype):
-    name = 'GreaterEqualOp'
-    op_names = ['GreaterEqual']
-    hw_config_names = [HWConfigOpName.GREATEREQUAL]
 
+@TF_OPERATION_METATYPES.register()
+class TFSplitOpMetatype(TFOpMetatype):
+    name = "SplitOp"
+    op_names = ["Split", "split"]
+    hw_config_names = [HWConfigOpName.SPLIT]
 
-@OV_OPERATION_METATYPES.register()
-class OVEqualMetatype(OVOpMetatype):
-    name = 'EqualOp'
-    op_names = ['Equal']
-    hw_config_names = [HWConfigOpName.EQUAL]
 
+@TF_OPERATION_METATYPES.register()
+class TFMinimumOpMetatype(TFOpMetatype):
+    name = "MinimumOp"
+    op_names = ["Minimum", "math.minimum"]
+    hw_config_names = [HWConfigOpName.MINIMUM]
 
-@OV_OPERATION_METATYPES.register()
-class OVNotEqualMetatype(OVOpMetatype):
-    name = 'NotEqualOp'
-    op_names = ['NotEqual']
-    hw_config_names = [HWConfigOpName.NOTEQUAL]
 
+@TF_OPERATION_METATYPES.register()
+class TFMaximumOpMetatype(TFOpMetatype):
+    name = "MaximumOp"
+    op_names = ["Maximum", "math.maximum"]
+    hw_config_names = [HWConfigOpName.MAXIMUM]
 
-@OV_OPERATION_METATYPES.register()
-class OVNotMetatype(OVOpMetatype):
-    name = 'NotOp'
-    op_names = ['LogicalNot']
-    hw_config_names = [HWConfigOpName.LOGICALNOT]
 
+@TF_OPERATION_METATYPES.register()
+class TFExpOpMetatype(TFOpMetatype):
+    name = "ExpOp"
+    op_names = ["Exp", "math.exp"]
 
-@OV_OPERATION_METATYPES.register()
-class OVAndMetatype(OVOpMetatype):
-    name = 'AndOp'
-    op_names = ['LogicalAnd']
-    hw_config_names = [HWConfigOpName.LOGICALAND]
 
+@TF_OPERATION_METATYPES.register()
+class TFPlaceholderOpMetatype(TFOpMetatype):
+    name = "PlaceholderOp"
+    op_names = ["Placeholder"]
 
-@OV_OPERATION_METATYPES.register()
-class OVOrMetatype(OVOpMetatype):
-    name = 'OrOp'
-    op_names = ['LogicalOr']
-    hw_config_names = [HWConfigOpName.LOGICALOR]
 
+@TF_OPERATION_METATYPES.register()
+class TFShapeOpMetatype(TFOpMetatype):
+    name = "ShapeOp"
+    op_names = ["Shape", "compat.v1.shape"]
 
-@OV_OPERATION_METATYPES.register()
-class OVXorMetatype(OVOpMetatype):
-    name = 'XorOp'
-    op_names = ['LogicalXor']
-    hw_config_names = [HWConfigOpName.LOGICALXOR]
 
+@TF_OPERATION_METATYPES.register()
+class TFBiasAddOpMetatype(TFOpMetatype):
+    name = "BiasAddOp"
+    op_names = ["BiasAdd"]
+    hw_config_names = [HWConfigOpName.ADD]
 
-@OV_OPERATION_METATYPES.register()
-class OVFloorMetatype(OVOpMetatype):
-    name = 'FloorOp'
-    op_names = ['Floor']
 
+@TF_OPERATION_METATYPES.register()
+class TFMeanOpMetatype(TFOpMetatype):
+    name = "MeanOp"
+    op_names = ["Mean", "math.reduce_mean"]
+    hw_config_names = [
+        HWConfigOpName.REDUCEMEAN,
+        HWConfigOpName.AVGPOOL,
+    ]
+
+
+@TF_OPERATION_METATYPES.register()
+class TFFusedBatchNormV3OpMetatype(TFOpMetatype):
+    name = "FusedBatchNormV3Op"
+    op_names = ["FusedBatchNormV3"]
+
+
+@TF_OPERATION_METATYPES.register()
+class TFSqueezeOpMetatype(TFOpMetatype):
+    name = "SqueezeOp"
+    op_names = ["Squeeze", "squeeze"]
+    hw_config_names = [HWConfigOpName.SQUEEZE]
 
-@OV_OPERATION_METATYPES.register()
-class OVFloorModMetatype(OVOpMetatype):
-    name = 'FloorModOp'
-    op_names = ['FloorMod']
-    hw_config_names = [HWConfigOpName.FLOORMOD]
 
+@TF_OPERATION_METATYPES.register()
+class TFSigmoidOpMetatype(TFOpMetatype):
+    name = "SigmoidOp"
+    op_names = ["Sigmoid", "math.sigmoid"]
 
-@OV_OPERATION_METATYPES.register()
-class OVMaximumMetatype(OVOpMetatype):
-    name = 'MaximumOp'
-    op_names = ['Maximum']
-    hw_config_names = [HWConfigOpName.MAXIMUM]
 
-@OV_OPERATION_METATYPES.register()
-class OVMinimumMetatype(OVOpMetatype):
-    name = 'MinimumOp'
-    op_names = ['Minimum']
-    hw_config_names = [HWConfigOpName.MINIMUM]
+@TF_OPERATION_METATYPES.register()
+class TFCombinedNonMaxSuppressionOpMetatype(TFOpMetatype):
+    name = "CombinedNonMaxSuppressionOp"
+    op_names = ["CombinedNonMaxSuppression", "image.combined_non_max_suppression"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVSqrtMetatype(OVOpMetatype):
-    name = 'SqrtOp'
-    op_names = ['Sqrt']
-    hw_config_names = [HWConfigOpName.POWER]
+@TF_OPERATION_METATYPES.register()
+class TFTopKV2OpMetatype(TFOpMetatype):
+    name = "TopKV2Op"
+    op_names = ["TopKV2", "math.top_k"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVPowerMetatype(OVOpMetatype):
-    name = 'PowerOp'
-    op_names = ['Power']
-    hw_config_names = [HWConfigOpName.POWER]
+@TF_OPERATION_METATYPES.register()
+class TFGatherOpMetatype(TFOpMetatype):
+    name = "GatherOp"
+    op_names = ["GatherNd", "GatherV2", "compat.v1.gather", "compat.v1.gather_nd"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVLogMetatype(OVOpMetatype):
-    name = 'LogOp'
-    op_names = ['Log']
-
-
-@OV_OPERATION_METATYPES.register()
-class OVRoiAlignMetatype(OVOpMetatype):
-    name = 'RoiAlignOp'
-    op_names = ['ROIAlign']
-
-
-@OV_OPERATION_METATYPES.register()
-class OVMatMulMetatype(OVOpMetatype):
-    name = 'MatMulOp'
-    op_names = ['MatMul']
-    hw_config_names = [HWConfigOpName.MATMUL]
+@TF_OPERATION_METATYPES.register()
+class TFPowOpMetatype(TFOpMetatype):
+    name = "PowOp"
+    op_names = ["Pow", "math.pow"]
+    hw_config_names = [HWConfigOpName.POWER]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVGatherMetatype(OVOpMetatype):
-    name = 'GatherOp'
-    op_names = ['Gather']
-
-
-@OV_OPERATION_METATYPES.register()
-class OVUnsqueezeMetatype(OVOpMetatype):
-    name = 'UnsqueezeOp'
-    op_names = ['Unsqueeze']
-    hw_config_names = [HWConfigOpName.UNSQUEEZE]
-
-
-@OV_OPERATION_METATYPES.register()
-class OVSqueezeMetatype(OVOpMetatype):
-    name = 'SqueezeOp'
-    op_names = ['Squeeze']
-    hw_config_names = [HWConfigOpName.SQUEEZE]
+@TF_OPERATION_METATYPES.register()
+class TFSqrtOpMetatype(TFOpMetatype):
+    name = "SqrtOp"
+    op_names = ["Sqrt", "math.sqrt"]
+    hw_config_names = [HWConfigOpName.POWER]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVNonMaxSuppressionMetatype(OVOpMetatype):
-    name = 'NonMaxSuppressionOp'
-    op_names = ['NonMaxSuppression']
+@TF_OPERATION_METATYPES.register()
+class TFTrueDivOpMetatype(TFOpMetatype):
+    name = "TrueDivOp"
+    op_names = ["RealDiv", "math.divide", "math.truediv"]
+    hw_config_names = [HWConfigOpName.DIVIDE]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVReduceMinMetatype(OVOpMetatype):
-    name = 'ReduceMinOp'
-    op_names = ['ReduceMin']
+@TF_OPERATION_METATYPES.register()
+class TFLogOpMetatype(TFOpMetatype):
+    name = "LogOp"
+    op_names = ["Log", "math.log"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVReduceMaxMetatype(OVOpMetatype):
-    name = 'ReduceMaxOp'
-    op_names = ['ReduceMax']
-    hw_config_names = [HWConfigOpName.REDUCEMAX]
+@TF_OPERATION_METATYPES.register()
+class TFFloorOpMetatype(TFOpMetatype):
+    name = "FloorOp"
+    op_names = ["Floor", "math.floor"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVReduceMeanMetatype(OVOpMetatype):
-    name = 'ReduceMeanOp'
-    op_names = ['ReduceMean']
-    hw_config_names = [HWConfigOpName.REDUCEMEAN]
+@TF_OPERATION_METATYPES.register()
+class TFAbsOpMetatype(TFOpMetatype):
+    name = "AbsOp"
+    op_names = ["Abs", "math.abs"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVReduceL1Metatype(OVOpMetatype):
-    name = 'ReduceL1Op'
-    op_names = ['ReduceL1']
+@TF_OPERATION_METATYPES.register()
+class TFFloorDivOpMetatype(TFOpMetatype):
+    name = "FloorDivOp"
+    op_names = ["FloorDiv", "math.floordiv", "compat.v1.floor_div"]
+    hw_config_names = [HWConfigOpName.FLOORMOD]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVReduceL2Metatype(OVOpMetatype):
-    name = 'ReduceL2Op'
-    op_names = ['ReduceL2']
-    hw_config_names = [HWConfigOpName.REDUCEL2]
+@TF_OPERATION_METATYPES.register()
+class TFCastOpMetatype(TFOpMetatype):
+    name = "CastOp"
+    op_names = ["Cast", "cast"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVTopKMetatype(OVOpMetatype):
-    name = 'TopKOp'
-    op_names = ['TopK']
+@TF_OPERATION_METATYPES.register()
+class TFMaxOpMetatype(TFOpMetatype):
+    name = "MaxOp"
+    op_names = ["Max", "math.reduce_max"]
+    hw_config_names = [HWConfigOpName.MAXIMUM]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVSliceMetatype(OVOpMetatype):
-    name = 'SliceOp'
-    op_names = ['StridedSlice']
-    hw_config_names = [HWConfigOpName.STRIDEDSLICE]
+@TF_OPERATION_METATYPES.register()
+class TFTanhOpMetatype(TFOpMetatype):
+    name = "TanhOp"
+    op_names = ["Tanh", "math.tanh"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVExpMetatype(OVOpMetatype):
-    name = 'ExpOp'
-    op_names = ['Exp']
+@TF_OPERATION_METATYPES.register()
+class TFSeluOpMetatype(TFOpMetatype):
+    name = "SeluOp"
+    op_names = ["Selu"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVTransposeMetatype(OVOpMetatype):
-    name = 'TransposeOp'
-    op_names = ['Transpose']
-    hw_config_names = [HWConfigOpName.TRANSPOSE]
+@TF_OPERATION_METATYPES.register()
+class TFEluOpMetatype(TFOpMetatype):
+    name = "EluOp"
+    op_names = ["Elu"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVTileMetatype(OVOpMetatype):
-    name = 'TileOp'
-    op_names = ['Tile']
-    hw_config_names = [HWConfigOpName.TILE]
+@TF_OPERATION_METATYPES.register()
+class TFLeakyReluOpMetatype(TFOpMetatype):
+    name = "LeakyReluOp"
+    op_names = ["LeakyRelu"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVSoftmaxMetatype(OVOpMetatype):
-    name = 'SoftmaxOp'
-    op_names = ['SoftMax']
+@TF_OPERATION_METATYPES.register()
+class TFMaxPoolOpMetatype(TFOpMetatype):
+    name = "MaxPoolOp"
+    op_names = ["MaxPool"]
+    hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVPadMetatype(OVOpMetatype):
-    name = 'PadOp'
-    op_names = ['Pad']
-    hw_config_names = [HWConfigOpName.PAD]
+@TF_OPERATION_METATYPES.register()
+class TFMaxPool3DOpMetatype(TFOpMetatype):
+    name = "MaxPool3DOp"
+    op_names = ["MaxPool3D"]
+    hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVReadValueMetatype(OVOpMetatype):
-    name = 'ReadValueOp'
-    op_names = ['ReadValue']
+@TF_OPERATION_METATYPES.register()
+class TFNegOpMetatype(TFOpMetatype):
+    name = "NegOp"
+    op_names = ["Neg", "math.negative"]
 
 
-@OV_OPERATION_METATYPES.register()
-class OVAssignMetatype(OVOpMetatype):
-    name = 'AssignOp'
-    op_names = ['Assign']
+@TF_OPERATION_METATYPES.register()
+class TFTileOpMetatype(TFOpMetatype):
+    name = "TileOp"
+    op_names = ["Tile", "tile"]
+    hw_config_names = [HWConfigOpName.TILE]
 
 
-@OV_OPERATION_METATYPES.register()
-@INPUT_NOOP_METATYPES.register()
-class OVParameterMetatype(OVOpMetatype):
-    name = 'ParameterOp'
-    op_names = ['Parameter']
+@TF_OPERATION_METATYPES.register()
+class TFSliceOpMetatype(TFOpMetatype):
+    name = "SliceOp"
+    op_names = ["Slice", "slice"]
+
+
+@TF_OPERATION_METATYPES.register()
+class TFSoftmaxOpMetatype(TFOpMetatype):
+    name = "SoftmaxOp"
+    op_names = ["Softmax"]
+
+
+@TF_OPERATION_METATYPES.register()
+class TFTransposeOpMetatype(TFOpMetatype):
+    name = "TransposeOp"
+    op_names = ["Transpose", "transpose", "compat.v1.transpose"]
+    hw_config_names = [HWConfigOpName.TRANSPOSE]
 
 
-@OV_OPERATION_METATYPES.register()
-@OUTPUT_NOOP_METATYPES.register()
-class OVResultMetatype(OVOpMetatype):
-    name = 'ResultOp'
-    op_names = ['Result']
+@TF_OPERATION_METATYPES.register()
+class TFGreaterOpMetatype(TFOpMetatype):
+    name = "GreaterOp"
+    op_names = ["Greater", "math.greater"]
+    hw_config_names = [HWConfigOpName.GREATER]
 
 
-GENERAL_WEIGHT_LAYER_METATYPES = [OVConvolutionMetatype,
-                                  OVConvolutionBackpropDataMetatype,
-                                  OVMatMulMetatype]
+@TF_OPERATION_METATYPES.register()
+class TFResizeNearestNeighborOpMetatype(TFOpMetatype):
+    name = "ResizeNearestNeighborOp"
+    op_names = ["ResizeNearestNeighbor"]
+    hw_config_names = [HWConfigOpName.INTERPOLATE]
 
 
-def get_operator_metatypes() -> List[Type[OperatorMetatype]]:
-    """
-    Returns a list of the operator metatypes.
-    :return: List of operator metatypes .
-    """
-    return list(OV_OPERATION_METATYPES.registry_dict.values())
+WEIGHTABLE_TF_OP_METATYPES = [
+    TFConv2DOpMetatype,
+    TFConv3DOpMetatype,
+    TFDepthwiseConv2dNativeOpMetatype,
+    TFQuantizedConv2DOpMetatype,
+]
```

### Comparing `nncf-2.4.0/nncf/experimental/openvino_native/graph/model_transformer.py` & `nncf-2.5.0/nncf/tensorflow/graph/transformations/layout.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,129 +1,129 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List
+from typing import Callable, Optional
 
-import openvino.runtime as ov
-from openvino.runtime import opset9 as opset
-
-from nncf.common.graph.model_transformer import ModelTransformer
-from nncf.common.graph.transformations.layout import TransformationLayout
+from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
-from nncf.experimental.openvino_native.graph.transformations.commands import OVOutputInsertionCommand
-from nncf.experimental.openvino_native.graph.transformations.commands import OVFQNodeRemovingCommand
-
+from nncf.common.graph.transformations.commands import TransformationCommand
+from nncf.common.graph.transformations.commands import TransformationType
+from nncf.common.graph.transformations.layout import TransformationLayout
+from nncf.tensorflow.graph.transformations.commands import TFLayer
+from nncf.tensorflow.graph.transformations.commands import TFLayerPoint
+from nncf.tensorflow.graph.transformations.commands import TFMultipleInsertionCommands
+
+GRAPH_NODE_TYPES = [TargetType.LAYER, TargetType.OPERATION_WITH_WEIGHTS]
+
+
+class TFTransformationLayout(TransformationLayout):
+    def register(self, transformation: TransformationCommand) -> None:
+        if transformation.type == TransformationType.REMOVE:
+            self._transformations.append(transformation)
+        elif transformation.type == TransformationType.INSERT:
+            self._register_insertion_transformation(transformation)
+        elif transformation.type == TransformationType.MULTI_INSERT:
+            self._register_multiple_insertion_transformation(transformation)
+
+    def _register_insertion_transformation(self, transformation: TransformationCommand) -> None:
+        start_idx = self._find_transformation(
+            lambda t: t.type == TransformationType.REMOVE
+            and is_object_removed(t.target_point, transformation.target_point),
+            reverse=True,
+        )
+        start_idx = 0 if start_idx is None else start_idx + 1
+
+        idx = self._find_transformation(lambda t: t.check_command_compatibility(transformation), start_idx=start_idx)
+        if idx is not None:
+            self.transformations[idx] = self.transformations[idx] + transformation
+            return
+
+        idx = self._find_transformation(
+            lambda t: t.type == TransformationType.MULTI_INSERT and t.check_insertion_command(transformation),
+            start_idx=start_idx,
+        )
+        if idx is not None:
+            self.transformations[idx].add_insertion_command(transformation)
+            return
+
+        idx = self._find_transformation(
+            lambda t: t.type == TransformationType.INSERT
+            and check_target_points(t.target_point, transformation.target_point),
+            start_idx=start_idx,
+        )
+        if idx is not None:
+            self.transformations[idx] = TFMultipleInsertionCommands(
+                target_point=TFLayer(transformation.target_point.layer_name),
+                check_target_points_fn=check_target_points,
+                commands=[self.transformations[idx], transformation],
+            )
+            return
+
+        self.transformations.append(transformation)
+
+    def _register_multiple_insertion_transformation(self, transformation: TransformationCommand) -> None:
+        start_idx = self._find_transformation(
+            lambda t: t.type == TransformationType.REMOVE
+            and is_object_removed(t.target_point, transformation.target_point),
+            reverse=True,
+        )
+        start_idx = 0 if start_idx is None else start_idx + 1
+
+        idx = self._find_transformation(lambda t: t.check_command_compatibility(transformation), start_idx=start_idx)
+        if idx is not None:
+            self.transformations[idx] = self.transformations[idx] + transformation
+            return
+
+        merged_transformations = []
+        for t in self.transformations[start_idx:]:
+            if transformation.check_insertion_command(t):
+                transformation.add_insertion_command(t)
+                merged_transformations.append(t)
+        for t in merged_transformations:
+            self.transformations.remove(t)
+        self.transformations.append(transformation)
+
+    def _find_transformation(self, condition: Callable, start_idx: int = 0, reverse: bool = False) -> Optional[int]:
+        transformations_iterator = (
+            reversed(list(enumerate(self.transformations[start_idx:])))
+            if reverse
+            else enumerate(self.transformations[start_idx:])
+        )
+        for idx, t in transformations_iterator:
+            if condition(t):
+                return idx
+        return None
+
+
+def check_target_points(tp0: TargetPoint, tp1: TargetPoint) -> bool:
+    return (
+        isinstance(tp0, TFLayerPoint)
+        and isinstance(tp1, TFLayerPoint)
+        and tp0.type in GRAPH_NODE_TYPES
+        and tp1.type in GRAPH_NODE_TYPES
+        and tp0.layer_name == tp1.layer_name
+    )
+
+
+def is_object_removed(removed_target: TargetPoint, command_target: TargetPoint) -> bool:
+    layer_removed = (
+        removed_target.type == TargetType.LAYER
+        and command_target.type in GRAPH_NODE_TYPES
+        and removed_target.layer_name == command_target.layer_name
+    )
+
+    operation_removed = (
+        removed_target.type == TargetType.OPERATION_WITH_WEIGHTS
+        and removed_target.type == command_target.type
+        and removed_target.layer_name == command_target.layer_name
+        and removed_target.weights_attr_name == command_target.weights_attr_name
+    )
 
-class OVModelTransformer(ModelTransformer):
-    """
-    Applies transformations to an OpenVINO model.
-    """
-
-    def __init__(self, model: ov.Model):
-        """
-        Initializes Model Transformer.
-
-        :param model: OpenVINO model to be transformed.
-        """
-        super().__init__(model)
-        self._model = model.clone()
-        self.name_to_node_mapping = {op.get_friendly_name(): op for op in self._model.get_ops()}
-
-    def transform(self, transformation_layout: TransformationLayout) -> ov.Model:
-        """
-        Applies transformations by type-callback on the model.
-
-        :param transformations: lisf of the TransformationCommand transformations.
-        """
-        output_insert_transformations = []
-        fq_nodes_removing_transformations = []
-        transformations = transformation_layout.transformations
-
-        for transformation in transformations:
-            if isinstance(transformation, OVOutputInsertionCommand):
-                output_insert_transformations.append(transformation)
-            elif isinstance(transformation, OVFQNodeRemovingCommand):
-                fq_nodes_removing_transformations.append(transformation)
-
-        if output_insert_transformations:
-            self._apply_output_insertion_transformations(output_insert_transformations)
-        if fq_nodes_removing_transformations:
-            self._apply_fq_nodes_removing_transformation(fq_nodes_removing_transformations)
-
-        return self._model
-
-    def _apply_output_insertion_transformations(self, transformations: List[OVOutputInsertionCommand]) -> None:
-        """
-        Applies incoming transformations to the model.
-
-        :param transformations: list of the OVOutputInsertionCommand transformations.
-        """
-        extra_model_outputs = self._get_extra_model_outputs(transformations)
-        self._model = self._insert_outputs(self._model, outputs=extra_model_outputs)
-
-    def _get_extra_model_outputs(self,
-                                 transformations: List[OVOutputInsertionCommand]) -> List[ov.Output]:
-        """
-        Collects extra model outputs based on transformations.
-
-        :param transformations: lisf of the OVOutputInsertionCommand.
-        :return: list of the output names.
-        """
-        extra_model_outputs = []
-        for transformation in transformations:
-            node_name = transformation.target_point.target_node_name
-            node = self.name_to_node_mapping[node_name]
-            port_id = transformation.target_point.port_id
-            if transformation.target_point.type == TargetType.POST_LAYER_OPERATION:
-                output = node.output(port_id)
-                extra_model_outputs.append(output)
-            elif transformation.target_point.type == TargetType.PRE_LAYER_OPERATION:
-                output = node.input_value(port_id)
-                extra_model_outputs.append(output)
-            else:
-                raise NotImplementedError(f'Unsupported target point type {transformation.target_point.type}')
-
-        return extra_model_outputs
-
-    @staticmethod
-    def _insert_outputs(model: ov.Model, outputs: List[ov.Output]) -> ov.Model:
-        """
-        Takes a model and adds outputs based on the list of ov.Output.
-
-        :param model: OpenVINO model.
-        :param outputs: list of ov.Output.
-        :return: modified model.
-        """
-        model_outputs = model.get_results()
-        params = model.get_parameters()
-        extra_model_outputs = []
-        for output in outputs:
-            output_name = output.get_node().get_friendly_name()
-            port_id = output.get_index()
-            result = opset.result(output, name=f'Result_{output_name}.{port_id}')
-            extra_model_outputs.append(result)
-
-        return ov.Model(model_outputs + extra_model_outputs, params)
-
-    def _apply_fq_nodes_removing_transformation(self, transformations: List[OVFQNodeRemovingCommand]) -> None:
-        """
-        Removes the layers from the model.
-        :param transformations: lisf of the node removing transformations.
-        """
-        for transformation in transformations:
-            node = self.name_to_node_mapping[transformation.target_point.target_node_name]
-
-            node_input = node.input_value(0)
-            for node_output in node.outputs():
-                for target_in in node_output.get_target_inputs():
-                    target_in.replace_source_output(node_input)
-            del self.name_to_node_mapping[transformation.target_point.target_node_name]
+    return layer_removed or operation_removed
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/__init__.py` & `nncf-2.5.0/nncf/experimental/tensorflow/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,12 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.experimental.tensorflow.quantization import algorithm as experimental_quantization_algorithm
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/context.py` & `nncf-2.5.0/nncf/experimental/tensorflow/context.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,51 +1,45 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Optional
 import threading
+from typing import Optional
 
 import tensorflow as tf
 
-
 _CURRENT_CONTEXT = threading.local()
 
 
 def get_current_context():
     """
     Returns current active `TFTracingContext`.
 
     :return: Tracing context.
     """
-    tracing_context = getattr(_CURRENT_CONTEXT, 'tracing_context', None)
+    tracing_context = getattr(_CURRENT_CONTEXT, "tracing_context", None)
     if tracing_context is None:
         tracing_context = TFTracingContext()
-        setattr(_CURRENT_CONTEXT, 'tracing_context', tracing_context)
+        setattr(_CURRENT_CONTEXT, "tracing_context", tracing_context)
     return tracing_context
 
 
 class TFTracingContextState:
     """
     Contains values that describe a state of the `TFTracingContext`.
     """
 
-    def __init__(self,
-                 in_call: bool = False,
-                 wrap_ops: bool = False,
-                 model: Optional[tf.keras.Model] = None):
+    def __init__(self, in_call: bool = False, wrap_ops: bool = False, model: Optional[tf.keras.Model] = None):
         """
         Initializes the `TFTracingContextState` instance.
 
         :param in_call: Whether currently inside the `call()` method of a `tf.keras.Model`.
         :param wrap_ops: Whether currently adding the compression pre-hooks and post-hooks
             to TensorFlow operations.
         :param model: The Keras model whose `call()` method is currently active.
@@ -53,17 +47,17 @@
             possible when `in_call` is equal to `False`.
         """
         self._in_call = in_call
         self._wrap_ops = wrap_ops
 
         if model is None and in_call:
             raise ValueError(
-                f'Inconsisten values `{in_call}` and `{model}` for `in_call` and `model` parameters. '
-                'The `None` value is specified that model is undefined at this moment. This is only '
-                'possible when `in_call` is equal to `False`.'
+                f"Inconsisten values `{in_call}` and `{model}` for `in_call` and `model` parameters. "
+                "The `None` value is specified that model is undefined at this moment. This is only "
+                "possible when `in_call` is equal to `False`."
             )
 
         self._model = model
 
     @property
     def in_call(self) -> bool:
         return self._in_call
@@ -99,18 +93,15 @@
     def in_call(self) -> bool:
         return self.state.in_call
 
     @property
     def wrap_ops(self) -> bool:
         return self.state.wrap_ops
 
-    def enter(self,
-              in_call: bool,
-              wrap_ops: bool,
-              model: Optional[tf.keras.Model] = None):
+    def enter(self, in_call: bool, wrap_ops: bool, model: Optional[tf.keras.Model] = None):
         """
         Pushes parameters onto the tracing context.
 
         :param in_call: Whether currently inside the `call()` method of a model.
         :param wrap_ops: Whether currently adding the compression pre-hooks and post-hooks
             to TensorFlow operations.
         :param model: The Keras model whose `call()` method is currently active.
@@ -134,23 +125,23 @@
         i = self.names_in_use.get(name_key, 0)
         self.names_in_use[name_key] = i + 1
 
         if i > 0:
             base_name_key = name_key
             # Make sure the composed name key is not already used.
             while name_key in self.names_in_use:
-                name_key = f'{base_name_key}_{i}'
+                name_key = f"{base_name_key}_{i}"
                 i += 1
 
             # Mark the composed name_key as used in case someone wants
             # to call unique_name('name_1').
             self.names_in_use[name_key] = 1
 
             # Return the new name with the original capitalization of the given name.
-            name = f'{name}_{i - 1}'
+            name = f"{name}_{i - 1}"
         return name
 
     @property
     def state(self) -> TFTracingContextState:
         return self._state
 
     def load_state(self, state: TFTracingContextState) -> None:
@@ -158,17 +149,15 @@
 
 
 class TFTracingContextManager:
     """
     Context manager for the tracing context.
     """
 
-    def __init__(self,
-                 tracing_context: TFTracingContext,
-                 next_state: TFTracingContextState):
+    def __init__(self, tracing_context: TFTracingContext, next_state: TFTracingContextState):
         """
         Initializes the tracing context manager.
 
         :param tracing_context: Tracing context.
         :param next_state: Next state of the tracing context which
             should be applied.
         """
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/graph/argprovider.py` & `nncf-2.5.0/nncf/experimental/tensorflow/graph/argprovider.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,19 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Any
-from typing import Tuple
+from typing import Any, Tuple
 
 import tensorflow as tf
 
 from nncf.common.utils.registry import Registry
 
 
 def replace_value_by_index(xs: Tuple[Any, ...], pos: int, value: Any) -> Tuple[Any, ...]:
@@ -30,18 +27,18 @@
     """
     return tuple(value if idx == pos else elem for idx, elem in enumerate(xs))
 
 
 def check_port_id(port_id: int, min_port_id: int, max_port_id: int):
     if min_port_id <= port_id <= max_port_id:
         return
-    raise ValueError(f'Unexpected `port_id`: {port_id}')
+    raise ValueError(f"Unexpected `port_id`: {port_id}")
 
 
-TF_ARG_PROVIDERS = Registry('TF_ARG_PROVIDERS')
+TF_ARG_PROVIDERS = Registry("TF_ARG_PROVIDERS")
 
 
 class ArgProvider:
     """
     Base class for all argument providers. An `ArgProvider` instance
     describes how to extract input or output argument with specified
     `port_id` from the `args` and `kwargs`.
@@ -76,90 +73,90 @@
         Updates output Tensor with specified `output_port_id`.
         :param output_port_id: Zero-based index an output Tensor
             for TensorFlow operation which should be updated.
         :return: A tuple (args, kwargs) with updated Tensor.
         """
 
 
-@TF_ARG_PROVIDERS.register('Relu6')
-@TF_ARG_PROVIDERS.register('Relu')
-@TF_ARG_PROVIDERS.register('Mean')
-@TF_ARG_PROVIDERS.register('AddV2')
-@TF_ARG_PROVIDERS.register('Placeholder')
-@TF_ARG_PROVIDERS.register('BiasAdd')
-@TF_ARG_PROVIDERS.register('GatherV2')
-@TF_ARG_PROVIDERS.register('Cast')
-@TF_ARG_PROVIDERS.register('Sub')
-@TF_ARG_PROVIDERS.register('SquaredDifference')
-@TF_ARG_PROVIDERS.register('Rsqrt')
-@TF_ARG_PROVIDERS.register('Mul')
-@TF_ARG_PROVIDERS.register('Erf')
-@TF_ARG_PROVIDERS.register('BatchMatMulV2')
-@TF_ARG_PROVIDERS.register('RealDiv')
+@TF_ARG_PROVIDERS.register("Relu6")
+@TF_ARG_PROVIDERS.register("Relu")
+@TF_ARG_PROVIDERS.register("Mean")
+@TF_ARG_PROVIDERS.register("AddV2")
+@TF_ARG_PROVIDERS.register("Placeholder")
+@TF_ARG_PROVIDERS.register("BiasAdd")
+@TF_ARG_PROVIDERS.register("GatherV2")
+@TF_ARG_PROVIDERS.register("Cast")
+@TF_ARG_PROVIDERS.register("Sub")
+@TF_ARG_PROVIDERS.register("SquaredDifference")
+@TF_ARG_PROVIDERS.register("Rsqrt")
+@TF_ARG_PROVIDERS.register("Mul")
+@TF_ARG_PROVIDERS.register("Erf")
+@TF_ARG_PROVIDERS.register("BatchMatMulV2")
+@TF_ARG_PROVIDERS.register("RealDiv")
 class SimpleOutputArgProvider(ArgProvider):
     def get_output(self, output_port_id: int, args, kwargs) -> tf.Tensor:
         check_port_id(output_port_id, min_port_id=0, max_port_id=0)
 
         if len(args) > 1:
-            raise ValueError(f'Unexpected `args`: {args}')
+            raise ValueError(f"Unexpected `args`: {args}")
 
         return args[output_port_id]
 
     def set_output(self, output_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(output_port_id, min_port_id=0, max_port_id=0)
 
         if len(args) > 1:
-            raise ValueError(f'Unexpected `args`: {args}')
+            raise ValueError(f"Unexpected `args`: {args}")
 
         return replace_value_by_index(args, output_port_id, value), kwargs
 
 
-@TF_ARG_PROVIDERS.register('Transpose')
+@TF_ARG_PROVIDERS.register("Transpose")
 class TransposeArgProvider(ArgProvider):
     def get_input(self, input_port_id: int, args, kwargs) -> tf.Tensor:
         check_port_id(input_port_id, min_port_id=0, max_port_id=0)
         return args[input_port_id]
 
     def set_input(self, input_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(input_port_id, min_port_id=0, max_port_id=0)
         return replace_value_by_index(args, input_port_id, value), kwargs
 
 
-@TF_ARG_PROVIDERS.register('ResizeNearestNeighbor')
+@TF_ARG_PROVIDERS.register("ResizeNearestNeighbor")
 class ResizeNearestNeighborArgProvider(ArgProvider):
     """
     Argument provider for the `ResizeNearestNeighbor` operation.
     """
 
     def get_output(self, output_port_id: int, args, kwargs) -> tf.Tensor:
         check_port_id(output_port_id, min_port_id=0, max_port_id=0)
 
         if len(args) > 1:
-            raise ValueError(f'Unexpected `args`: {args}')
+            raise ValueError(f"Unexpected `args`: {args}")
 
         return args[output_port_id]
 
     def set_output(self, output_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(output_port_id, min_port_id=0, max_port_id=0)
 
         if len(args) > 1:
-            raise ValueError(f'Unexpected `args`: {args}')
+            raise ValueError(f"Unexpected `args`: {args}")
 
         return replace_value_by_index(args, output_port_id, value), kwargs
 
     def get_input(self, input_port_id: int, args, kwargs) -> tf.Tensor:
         check_port_id(input_port_id, min_port_id=0, max_port_id=0)
         return args[0]
 
     def set_input(self, input_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(input_port_id, min_port_id=0, max_port_id=0)
         return replace_value_by_index(args, input_port_id, value), kwargs
 
 
-@TF_ARG_PROVIDERS.register('Conv2D')
+@TF_ARG_PROVIDERS.register("Conv2D")
 class Conv2DArgProvider(ArgProvider):
     """
     Argument provider of the `Conv2D` operation.
     Inputs:
         port_id: 0 - input tensor.
         port_id: 1 - filter tensor.
     Outputs:
@@ -168,43 +165,43 @@
 
     def get_input(self, input_port_id: int, args, kwargs) -> tf.Tensor:
         check_port_id(input_port_id, min_port_id=0, max_port_id=1)
 
         if input_port_id == 0:
             return args[0]
 
-        return kwargs['filter']  # input_port_id == 1
+        return kwargs["filter"]  # input_port_id == 1
 
     def set_input(self, input_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(input_port_id, min_port_id=0, max_port_id=1)
 
         if input_port_id == 0:
             return replace_value_by_index(args, input_port_id, value), kwargs
 
-        kwargs['filter'] = value
+        kwargs["filter"] = value
         return args, kwargs
 
     def get_output(self, output_port_id: int, args, kwargs) -> tf.Tensor:
         check_port_id(output_port_id, min_port_id=0, max_port_id=0)
 
         if len(args) > 1:
-            raise ValueError(f'Unexpected `args`: {args}')
+            raise ValueError(f"Unexpected `args`: {args}")
 
         return args[output_port_id]
 
     def set_output(self, output_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(output_port_id, min_port_id=0, max_port_id=0)
 
         if len(args) > 1:
-            raise ValueError(f'Unexpected `args`: {args}')
+            raise ValueError(f"Unexpected `args`: {args}")
 
         return replace_value_by_index(args, output_port_id, value), kwargs
 
 
-@TF_ARG_PROVIDERS.register('FusedBatchNormV3')
+@TF_ARG_PROVIDERS.register("FusedBatchNormV3")
 class FusedBatchNormV3ArgProvider(ArgProvider):
     """
     Argument provider of the `FusedBatchNormV3` operation.
     Outputs:
         port_id: 0 - output tensor (`y` key).
     """
 
@@ -214,15 +211,15 @@
 
     def set_output(self, output_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(output_port_id, min_port_id=0, max_port_id=0)
         x = args[0]._replace(y=value)
         return replace_value_by_index(args, 0, x), kwargs
 
 
-@TF_ARG_PROVIDERS.register('DepthwiseConv2dNative')
+@TF_ARG_PROVIDERS.register("DepthwiseConv2dNative")
 class DepthwiseConv2dNativeArgProvider(ArgProvider):
     """
     Argument provider of the `DepthwiseConv2dNative` operation.
     Inputs:
         port_id: 0 - input tensor (args[0]).
         port_id: 1 - filter tensor (args[1]).
     """
@@ -232,30 +229,30 @@
         return args[input_port_id]
 
     def set_input(self, input_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(input_port_id, min_port_id=0, max_port_id=1)
         return replace_value_by_index(args, input_port_id, value), kwargs
 
 
-@TF_ARG_PROVIDERS.register('MatMul')
+@TF_ARG_PROVIDERS.register("MatMul")
 class MatMulArgProvider(SimpleOutputArgProvider):
     """
     Argument provider of the `MatMul` operation.
     Inputs:
         port_id: 0 - input tensor (args[0]).
         port_id: 1 - filter (always?) tensor (args[1]).
     """
 
     def get_input(self, input_port_id: int, args, kwargs) -> tf.Tensor:
         check_port_id(input_port_id, min_port_id=0, max_port_id=1)
 
         if len(args) == 0:
-            return kwargs['a' if input_port_id == 0 else 'b']
+            return kwargs["a" if input_port_id == 0 else "b"]
         return args[input_port_id]
 
     def set_input(self, input_port_id: int, value: tf.Tensor, args, kwargs):
         check_port_id(input_port_id, min_port_id=0, max_port_id=1)
 
         if len(args) == 0:
-            kwargs['a' if input_port_id == 0 else 'b'] = value
+            kwargs["a" if input_port_id == 0 else "b"] = value
             return args, kwargs
         return replace_value_by_index(args, input_port_id, value), kwargs
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/graph/converter.py` & `nncf-2.5.0/nncf/experimental/tensorflow/graph/converter.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,40 +1,34 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Optional
-from typing import Dict
-from typing import Any
-from typing import List
-from typing import Tuple
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from collections import deque
+from typing import Any, Dict, List, Optional, Tuple
 
 import tensorflow as tf
-from tensorflow.python.keras.saving import saving_utils as _saving_utils
 from tensorflow.lite.python.util import get_grappler_config as _get_grappler_config
 from tensorflow.lite.python.util import run_graph_optimizations as _run_graph_optimizations
 from tensorflow.python.framework import convert_to_constants as _convert_to_constants
+from tensorflow.python.keras.saving import saving_utils as _saving_utils
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph.layer_attributes import Dtype
-from nncf.tensorflow.graph.converter import TFModelConverter
-from nncf.tensorflow.graph.metatypes.matcher import get_op_metatype
-from nncf.tensorflow.graph.metatypes.common import ALL_LAYER_METATYPES_WITH_WEIGHTS
 from nncf.experimental.tensorflow.graph.node_attributes import TFNodeAttributes
 from nncf.experimental.tensorflow.graph.node_attributes import TFWeightedNodeAttributes
+from nncf.tensorflow.graph.converter import TFModelConverter
+from nncf.tensorflow.graph.metatypes.common import ALL_LAYER_METATYPES_WITH_WEIGHTS
+from nncf.tensorflow.graph.metatypes.matcher import get_op_metatype
 
 
 class TensorFlowGraphBuilder:
     """
     Converts given Keras model to the tf.Graph.
     """
 
@@ -62,57 +56,56 @@
             Look at the [guide](https://www.tensorflow.org/guide/graph_optimization) for more information.
         :return: The optimized tf.Graph.
         """
         # Step 1: Freeze Keras model to frozen graph
 
         func = _saving_utils.trace_model_call(self._model, self._input_signature)
         concrete_func = func.get_concrete_function()
-        frozen_func, graph_def = \
-            _convert_to_constants.convert_variables_to_constants_v2_as_graph(concrete_func, lower_control_flow=False)
+        frozen_func, graph_def = _convert_to_constants.convert_variables_to_constants_v2_as_graph(
+            concrete_func, lower_control_flow=False
+        )
         # List of input tensors
         input_tensors = [tensor for tensor in frozen_func.inputs if tensor.dtype != tf.dtypes.resource]
         # List of output tensors
         output_tensors = frozen_func.outputs
 
         # Step 2: Run a Grappler pass to oprimize the TensorFlow graph.
 
         # Creates a ConfigProto for configuring Grappler
         grappler_config = _get_grappler_config(graph_optimizers)
         # Skip running grappler when there are no optimizers to run. If not,
         # grappler will run with the default optimizer set and it will lead to
         # causing an unexpected behavior.
         if grappler_config.graph_options.rewrite_options.optimizers:
             graph_def = _run_graph_optimizations(
-                graph_def,
-                input_tensors,
-                output_tensors,
-                config=grappler_config,
-                graph=frozen_func.graph
+                graph_def, input_tensors, output_tensors, config=grappler_config, graph=frozen_func.graph
             )
 
         # Step 3: Convert the GraphDef to a tf.Graph
         with tf.Graph().as_default() as graph:  # pylint:disable=not-context-manager
-            tf.graph_util.import_graph_def(graph_def, name='')
+            tf.graph_util.import_graph_def(graph_def, name="")
 
         return graph, input_tensors, output_tensors
 
 
 class NodeDesc:
     """
     Contains description of a node in the TensorFlow `FuncGraph`.
     This information is required for `NNCFNode` creation.
     """
 
-    def __init__(self,
-                 op_name: str,
-                 op_type_name: str,
-                 metatype,
-                 is_shared: bool,
-                 resource_name: Optional[str] = None,
-                 attrs: Optional[Any] = None):
+    def __init__(
+        self,
+        op_name: str,
+        op_type_name: str,
+        metatype,
+        is_shared: bool,
+        resource_name: Optional[str] = None,
+        attrs: Optional[Any] = None,
+    ):
         """
         Initializes description of a node.
 
         :param op_name: Name of a node in the `FuncGraph`.
         :param op_type_name: Type of operation.
         :param metatype: NNCF meta type which corresponds to operation.
         :param is_shared: `True` if the node is shared, `False` otherwise.
@@ -129,21 +122,23 @@
 
 class EdgeDesc:
     """
     Contains description of an edge in the TensorFlow `FuncGraph`.
     This information is required for `NNCFGraphEdge` creation.
     """
 
-    def __init__(self,
-                 producer_op_name: str,
-                 output_port_id: int,
-                 consumer_op_name: str,
-                 input_port_id: int,
-                 tensor_shape: List[int],
-                 tensor_dtype: Dtype):
+    def __init__(
+        self,
+        producer_op_name: str,
+        output_port_id: int,
+        consumer_op_name: str,
+        input_port_id: int,
+        tensor_shape: List[int],
+        tensor_dtype: Dtype,
+    ):
         """
         Initializes description of an edge.
 
         :param producer_op_name: Name of the node where the edge comes out.
         :param output_port_id: Output port id.
         :param consumer_op_name: Name of the node where the edge comes in.
         :param input_port_id: Input port id.
@@ -163,19 +158,21 @@
     Converts the Keras model, which was created via `tf.keras.Model`
     subclass, to the `NNCFGraph`.
 
     Uses the frozen TF graph for the provided model with applied
     graph optimizers to create `NNCFGraph`
     """
 
-    def __init__(self,
-                 model: tf.keras.Model,
-                 input_signature: List[tf.TensorSpec],
-                 training: bool = False,
-                 graph_optimizers: Optional[List[str]] = None):
+    def __init__(
+        self,
+        model: tf.keras.Model,
+        input_signature: List[tf.TensorSpec],
+        training: bool = False,
+        graph_optimizers: Optional[List[str]] = None,
+    ):
         """
         Initializes the subclassed converter.
 
         :param model: Instance of the `tf.keras.Model` class.
         :param input_signature: A list of `tf.TensorSpec` objects specifying the
             inputs to the model.
         :param training: Mode of the model.
@@ -191,15 +188,15 @@
             If `graph_optimizers` is `None` `dependency` optimizer will be used.
         """
         self._model = model
         self._input_signature = input_signature
         self._training = training
 
         if graph_optimizers is None:
-            graph_optimizers = ['dependency']
+            graph_optimizers = ["dependency"]
         self._graph_optimizers = graph_optimizers
 
         self._tfgraph_builder = TensorFlowGraphBuilder(self._model, self._input_signature)
 
     def convert(self) -> NNCFGraph:
         """
         Converts the Keras model to the `NNCFGraph` object.
@@ -213,16 +210,15 @@
         node_descs, edge_descs = SubclassedConverter._collect_tfgraph_descs(tfgraph, input_op_names)
 
         nncf_graph = SubclassedConverter._create_nncf_graph_from_descs(node_descs, edge_descs)
 
         return nncf_graph
 
     @staticmethod
-    def _create_nncf_graph_from_descs(node_descs: List[NodeDesc],
-                                      edge_descs: List[EdgeDesc]) -> NNCFGraph:
+    def _create_nncf_graph_from_descs(node_descs: List[NodeDesc], edge_descs: List[EdgeDesc]) -> NNCFGraph:
         """
         Creates the NNCF graph from the provided nodes and edges descriptions.
 
         :param node_descs: A list of `NodeDesc` objects.
         :param edge_descs: A list of `EdgeDesc` objects.
         :return: An instance of the `NNCFGraph` class.
         """
@@ -231,61 +227,60 @@
         for desc in node_descs:
             nncf_node = nncf_graph.add_nncf_node(
                 node_name=desc.op_name,
                 node_type=desc.op_type_name,
                 node_metatype=desc.metatype,
                 layer_attributes=desc.attrs,
                 layer_name=desc.resource_name if desc.resource_name else desc.op_name,
-                is_shared=desc.is_shared
+                is_shared=desc.is_shared,
             )
             op_name_to_node_id_map[desc.op_name] = nncf_node.node_id
 
         for desc in edge_descs:
             from_node_id = op_name_to_node_id_map[desc.producer_op_name]
             to_node_id = op_name_to_node_id_map[desc.consumer_op_name]
 
             nncf_graph.add_edge_between_nncf_nodes(
                 from_node_id,
                 to_node_id,
                 tensor_shape=desc.tensor_shape,
                 input_port_id=desc.input_port_id,
                 output_port_id=desc.output_port_id,
-                dtype=desc.tensor_dtype
+                dtype=desc.tensor_dtype,
             )
 
         return nncf_graph
 
     @staticmethod
     def _get_data_format(op: tf.Operation) -> str:
         """
         Returns data format for the TensorFlow operation in the Keras format.
         Returns default data format if the operation does not have it.
 
         :param op: TensorFlow operation.
         :return: String `channels_last` or `channels_first`.
         """
         try:
-            data_format = op.get_attr('data_format')
+            data_format = op.get_attr("data_format")
         except ValueError:
             data_format = None
 
         if data_format:
             to_keras_data_format = {
-                'NHWC': 'channels_last',
-                'NCHW': 'channels_first',
-                'NDHWC': 'channels_last',
-                'NCDHW': 'channels_first',
+                "NHWC": "channels_last",
+                "NCHW": "channels_first",
+                "NDHWC": "channels_last",
+                "NCDHW": "channels_first",
             }
-            return to_keras_data_format[data_format.decode('utf-8')]
+            return to_keras_data_format[data_format.decode("utf-8")]
 
         return tf.keras.backend.image_data_format()
 
     @staticmethod
-    def _collect_tfgraph_descs(graph: tf.Graph,
-                               op_names: List[str]) -> Tuple[List[NodeDesc], List[EdgeDesc]]:
+    def _collect_tfgraph_descs(graph: tf.Graph, op_names: List[str]) -> Tuple[List[NodeDesc], List[EdgeDesc]]:
         """
         Traverses the TF graph and collects information about nodes and edges
         which should be included to the NNCF graph.
 
         :param graph: Frozen `tf.Graph` with applied `dependency` optimization.
         :param op_names: A list of names for the input operations ()
         :return: A description of nodes and edges which should be included to the NNCF graph.
@@ -316,17 +311,15 @@
 
         # Collect descriptions for all visited ops. The directed edge (v, u)
         # of the `graph` is added only if ops v, u were visited.
         node_descs = []
         edge_descs = []
         for op in visited_ops.values():
             metatype = get_op_metatype(op.type)
-            node_attributes = TFNodeAttributes(
-                SubclassedConverter._get_data_format(op)
-            )
+            node_attributes = TFNodeAttributes(SubclassedConverter._get_data_format(op))
 
             is_shared = False
             const_op_name = None
             if metatype in ALL_LAYER_METATYPES_WITH_WEIGHTS:
                 const_op_names = op_name_to_const_op_names_map[op.name]
                 if const_op_names:
                     # TODO(andrey-churkin): Currently, we don't have metatypes with
@@ -336,27 +329,24 @@
                     const_op_name, is_shared = const_op_names[0]
 
                     # TODO(andrey-churkin): Seems like we can dynamically collect
                     # this information. Need to look at this.
                     port_id = metatype.weight_definitions[0].port_id
                     weight_shape = op.inputs[port_id].shape.as_list()
 
-                    node_attributes = TFWeightedNodeAttributes(
-                        node_attributes.get_data_format(),
-                        weight_shape
-                    )
+                    node_attributes = TFWeightedNodeAttributes(node_attributes.get_data_format(), weight_shape)
 
             node_descs.append(
                 NodeDesc(
                     op_name=op.name,
                     op_type_name=op.type,
                     metatype=metatype,
                     is_shared=is_shared,
                     resource_name=const_op_name,
-                    attrs=node_attributes
+                    attrs=node_attributes,
                 )
             )
 
             for input_port_id, tensor in enumerate(op.inputs):
                 producer_op = tensor.op
                 if producer_op.name not in visited_ops:
                     continue
@@ -364,15 +354,15 @@
                 edge_descs.append(
                     EdgeDesc(
                         producer_op_name=producer_op.name,
                         output_port_id=tensor.value_index,
                         consumer_op_name=op.name,
                         input_port_id=input_port_id,
                         tensor_shape=tensor.shape.as_list(),
-                        tensor_dtype=SubclassedConverter._convert_dtype_to_nncf_format(tensor.dtype)
+                        tensor_dtype=SubclassedConverter._convert_dtype_to_nncf_format(tensor.dtype),
                     )
                 )
 
         return node_descs, edge_descs
 
     @staticmethod
     def _convert_dtype_to_nncf_format(dtype: tf.dtypes.DType) -> Dtype:
@@ -383,35 +373,34 @@
         :return: An instance of the `Dtype`.
         """
         if dtype.is_floating:
             tensor_dtype = Dtype.FLOAT
         elif dtype.is_integer:
             tensor_dtype = Dtype.INTEGER
         else:
-            raise RuntimeError(f'Unexpected dtype of tensor: {dtype}')
+            raise RuntimeError(f"Unexpected dtype of tensor: {dtype}")
 
         return tensor_dtype
 
     @staticmethod
-    def _get_op_name_to_const_op_names_map(graph,
-                                           marked_ops: Dict[str, tf.Operation]) -> Dict[str, List[Tuple[str, bool]]]:
+    def _get_op_name_to_const_op_names_map(
+        graph, marked_ops: Dict[str, tf.Operation]
+    ) -> Dict[str, List[Tuple[str, bool]]]:
         """
         Returns information about constant operations for the `marked_ops`.
 
         :param graph: Frozen tf.Graph.
         :param marked_ops: A mapping from operation name to operation. The marked operations are
             the operations which reachable from the input operations (`input_ops`) in the `graph`.
         :return: A mapping from the operation name to the list of (const_op_name, is_shared) tuples where
             - `const_op_name` is the name of constant operation which is used by this operation
             - `is_shared` is the boolean flag. Takes one of the following values:
             `True` if the number of operations that use it is greater than 1, `False` otherwise.
         """
-        const_ops = [
-            op for op in graph.get_operations() if op.type == 'Const'
-        ]
+        const_ops = [op for op in graph.get_operations() if op.type == "Const"]
 
         const_op_name_to_op_names_map = {}  # type: Dict[str, List[str]]
         for const_op in const_ops:
             # Traverse the `graph` from the `const_op` and find the reachable ops
             # which are marked as visited (i.e. it's name in `marked_ops` dict)
             # from the `input_ops` in the previous traverse.
             queue = deque([const_op])
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/graph/model_transformer.py` & `nncf-2.5.0/nncf/experimental/tensorflow/graph/model_transformer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.commands import TransformationType
-from nncf.experimental.tensorflow.nncf_network import NNCFNetwork
 from nncf.experimental.tensorflow.graph.transformations.layout import TFTransformationLayoutV2
+from nncf.experimental.tensorflow.nncf_network import NNCFNetwork
 
 
 class TFModelTransformerV2(ModelTransformer):
     """
     Applies transformations to the NNCF network.
 
     The `TFModelTransformerV2` does not modify the model config to insert
@@ -46,10 +44,10 @@
         for command in transformation_layout.transformations:
             if command.type == TransformationType.INSERT:
                 self._model.insert_at_point(command.target_point, command.insertion_objects)
             elif command.type == TransformationType.REMOVE:
                 # TODO(andrey-churkin): Add support
                 pass
             else:
-                raise ValueError(f'Transformation type {command.type} does not support.')
+                raise ValueError(f"Transformation type {command.type} does not support.")
 
         return self._model
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/graph/node_attributes.py` & `nncf-2.5.0/nncf/experimental/tensorflow/graph/node_attributes.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 
 
 class TFNodeAttributes(BaseLayerAttributes):
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/graph/transformations/commands.py` & `nncf-2.5.0/nncf/experimental/tensorflow/graph/transformations/commands.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,76 +1,71 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict
-from typing import Any
+from typing import Any, Dict
 
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.stateful_classes_registry import TF_STATEFUL_CLASSES
 
 
 class TFTargetPointStateNames:
-    OP_NAME = 'op_name'
-    OP_TYPE_NAME = 'op_type_name'
-    PORT_ID = 'port_id'
-    TARGET_TYPE = 'target_type'
+    OP_NAME = "op_name"
+    OP_TYPE_NAME = "op_type_name"
+    PORT_ID = "port_id"
+    TARGET_TYPE = "target_type"
 
 
 @TF_STATEFUL_CLASSES.register()
 class TFTargetPoint(TargetPoint):
     """
     Describes where the compression operation should be placed.
     """
 
     _state_names = TFTargetPointStateNames
 
-    def __init__(self,
-                 op_name: str,
-                 op_type_name: str,
-                 port_id: int,
-                 target_type: TargetType):
+    def __init__(self, op_name: str, op_type_name: str, port_id: int, target_type: TargetType):
         """
         Initializes target point for TensorFlow backend.
 
         :param op_name: Name of a node in the `FuncGraph`.
         :param op_type_name: Type of operation.
         :param port_id: Port id.
         :param target_type: Type of the target point.
         """
         super().__init__(target_type)
         self.op_name = op_name
         self.op_type_name = op_type_name
         self.port_id = port_id
 
-    def __eq__(self, other: 'TFTargetPoint') -> bool:
-        return isinstance(other, TFTargetPoint) and \
-               self.type == other.type and \
-               self.op_name == other.op_name and \
-               self.op_type_name == other.op_type_name and \
-               self.port_id == other.port_id
+    def __eq__(self, other: "TFTargetPoint") -> bool:
+        return (
+            isinstance(other, TFTargetPoint)
+            and self.type == other.type
+            and self.op_name == other.op_name
+            and self.op_type_name == other.op_type_name
+            and self.port_id == other.port_id
+        )
 
     def __str__(self) -> str:
         items = [
             super().__str__(),
             self.op_name,
             self.op_type_name,
             str(self.port_id),
         ]
-        return ' '.join(items)
+        return " ".join(items)
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: State of the object.
@@ -80,15 +75,15 @@
             self._state_names.OP_TYPE_NAME: self.op_type_name,
             self._state_names.PORT_ID: self.port_id,
             self._state_names.TARGET_TYPE: self.type.get_state(),
         }
         return state
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFTargetPoint':
+    def from_state(cls, state: Dict[str, Any]) -> "TFTargetPoint":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         kwargs = {
             cls._state_names.OP_NAME: state[cls._state_names.OP_NAME],
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/graph/transformations/layout.py` & `nncf-2.5.0/nncf/experimental/tensorflow/graph/transformations/layout.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TransformationType
 from nncf.common.graph.transformations.layout import TransformationLayout
 
 
 class TFTransformationLayoutV2(TransformationLayout):
@@ -30,15 +28,15 @@
         """
         if transformation.type == TransformationType.REMOVE:
             # TODO(andrey-churkin): Add support.
             pass
         elif transformation.type == TransformationType.INSERT:
             self._register_insertion_transformation(transformation)
         else:
-            raise ValueError(f'Unknown type of transformation command: {transformation.type}')
+            raise ValueError(f"Unknown type of transformation command: {transformation.type}")
 
     def _register_insertion_transformation(self, transformation: TransformationCommand) -> None:
         idx = None
         for curr_idx, t in enumerate(self.transformations):
             if t.check_command_compatibility(transformation):
                 assert idx is None
                 idx = curr_idx
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/nncf_network.py` & `nncf-2.5.0/nncf/experimental/tensorflow/nncf_network.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,97 +1,83 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Union, Dict, Tuple, List, Any
 import itertools
+from typing import Any, Dict, List, Tuple, Union
 
 import tensorflow as tf
 
-from nncf.tensorflow.layers.operation import NNCFOperation
 from nncf.experimental.tensorflow.context import get_current_context
 from nncf.experimental.tensorflow.graph.transformations.commands import TFTargetPoint
 from nncf.experimental.tensorflow.patch_tf import Hook
-
+from nncf.tensorflow.layers.operation import NNCFOperation
 
 InputSignature = Union[tf.TensorSpec, Dict[str, tf.TensorSpec], Tuple[tf.TensorSpec, ...], List[tf.TensorSpec]]
 
 
 def _add_names_to_input_signature(input_signature: InputSignature):
     xs = tf.nest.flatten(input_signature)
     ys = []
     for i, spec in enumerate(xs):
-        ys.append(
-            tf.TensorSpec.from_spec(
-                spec,
-                name=spec.name if spec.name else f'input_{i}'
-            )
-        )
+        ys.append(tf.TensorSpec.from_spec(spec, name=spec.name if spec.name else f"input_{i}"))
 
     return tf.nest.pack_sequence_as(input_signature, ys)
 
 
 class NNCFNetwork(tf.keras.Model):
     """
     Wraps the Keras model.
     """
 
-    def __init__(self,
-                 model: tf.keras.Model,
-                 input_signature: InputSignature,
-                 **kwargs):
+    def __init__(self, model: tf.keras.Model, input_signature: InputSignature, **kwargs):
         """
         Initializes the NNCF network.
 
         :param model: Keras model.
         :param input_signature: Input signature of the moodel.
         """
         super().__init__(**kwargs)
         self._model = model
         self._input_signature = _add_names_to_input_signature(input_signature)
 
         # The `__setattr__` was was overridden inside superclasses.
         # This workaround allows not add dependencies from hooks to the model.
         # See `tensorflow.python.training.tracking.autotrackable.AutoTrackable`
         # class for more details.
-        self.__dict__['_pre_hooks'] = {}  # type: Dict[str, List[Hook]]
-        self.__dict__['_post_hooks'] = {}  # type: Dict[str, List[Hook]]
+        self.__dict__["_pre_hooks"] = {}  # type: Dict[str, List[Hook]]
+        self.__dict__["_post_hooks"] = {}  # type: Dict[str, List[Hook]]
 
     @property
     def nncf_operations(self) -> List[NNCFOperation]:
         """
         Returns list of the NNCF operations which were added to the NNCF network.
 
         :return: List of the NNCF operations.
         """
-        return [op for hook in getattr(self, '_hooks') for op in hook.operations]
+        return [op for hook in getattr(self, "_hooks") for op in hook.operations]
 
     @property
     def input_signature(self) -> InputSignature:
         """
         Returns input signature of the model.
 
         :return: Input signature of the model.
         """
         return self._input_signature
 
     def get_nncf_operations_with_params(self) -> List[Tuple[NNCFOperation, Any]]:
-        return [
-            (op, hook.get_operation_weights(op.name)) \
-                 for hook in getattr(self, '_hooks') for op in hook.operations
-        ]
+        return [(op, hook.get_operation_weights(op.name)) for hook in getattr(self, "_hooks") for op in hook.operations]
 
     def get_config(self):
         raise NotImplementedError
 
     def call(self, inputs, **kwargs):
         """
         Calls the model on new inputs and returns the outputs as tensors.
@@ -112,35 +98,36 @@
         Inserts the list of the NNCF operations according to the target point.
 
         :param point: The location where operations should be inserted.
         :param ops: List of the NNCF operarions.
         """
         ops_weights = {op.name: op.create_variables(self) for op in ops}
         hook = Hook(ops, point, ops_weights)
-        hooks = getattr(self, '_pre_hooks') if hook.is_pre_hook else getattr(self, '_post_hooks')
+        hooks = getattr(self, "_pre_hooks") if hook.is_pre_hook else getattr(self, "_post_hooks")
         # TODO(andrey-churkin): What we should do if the hook with the same `target_point`
         # already exists inside `hooks`? Is it a valid case?
         hooks.setdefault(hook.target_point.op_name, []).append(hook)
 
     @property
     def _hooks(self):
-        pre_hooks = getattr(self, '_pre_hooks')
-        post_hooks = getattr(self, '_post_hooks')
+        pre_hooks = getattr(self, "_pre_hooks")
+        post_hooks = getattr(self, "_post_hooks")
         return itertools.chain(*pre_hooks.values(), *post_hooks.values())
 
     def _apply_post_hooks_for_inputs(self, inputs):
         """
         Applies post-hooks to inputs.
 
         :param inputs: Input tensor, or dict/list/tuple of input tensors.
         :return: Modified input tensor, or dict/list/tuple of input tensors.
         """
         input_name_to_post_hook_map = {
-            hook.target_point.op_name: hook for hook in getattr(self, '_hooks') \
-            if hook.target_point.op_type_name == 'Placeholder'
+            hook.target_point.op_name: hook
+            for hook in getattr(self, "_hooks")
+            if hook.target_point.op_type_name == "Placeholder"
         }
 
         if not input_name_to_post_hook_map:
             return inputs
 
         xs = tf.nest.flatten(inputs)
         ys = tf.nest.flatten(self.input_signature)
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/patch_tf.py` & `nncf-2.5.0/nncf/experimental/tensorflow/patch_tf.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,52 +1,44 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import functools
 import inspect
-from typing import Optional
-from typing import List
-from typing import Dict
-from typing import Any
+from typing import Any, Dict, List, Optional
 
 import tensorflow as tf
 from tensorflow.python.eager import context
 from tensorflow.python.framework import ops
-from tensorflow.python.ops import nn
 from tensorflow.python.ops import math_ops
+from tensorflow.python.ops import nn
 
 from nncf.common.graph.transformations.commands import TargetType
-from nncf.tensorflow.layers.operation import NNCFOperation
 from nncf.experimental.tensorflow.context import get_current_context
-from nncf.experimental.tensorflow.scope import get_op_name
+from nncf.experimental.tensorflow.graph.argprovider import TF_ARG_PROVIDERS
 from nncf.experimental.tensorflow.graph.argprovider import replace_value_by_index
 from nncf.experimental.tensorflow.graph.transformations.commands import TFTargetPoint
-from nncf.experimental.tensorflow.graph.argprovider import TF_ARG_PROVIDERS
+from nncf.experimental.tensorflow.scope import get_op_name
+from nncf.tensorflow.layers.operation import NNCFOperation
 
 
 class Hook:
     """
     Contains the NNCF operations and target point where
     these operations should be applied.
     """
 
-    def __init__(self,
-                 operations: List[NNCFOperation],
-                 target_point: TFTargetPoint,
-                 ops_weights: Dict[str, Any]):
+    def __init__(self, operations: List[NNCFOperation], target_point: TFTargetPoint, ops_weights: Dict[str, Any]):
         """
         Initializes the hook.
 
         :param operations: List of the NNCF operations in the correct order.
             The operation at index 0 is applied first, with index -1 last.
         :param target_point: A target point. Contains information on where the
             operations should be applied.
@@ -54,17 +46,19 @@
         """
         self._operations = operations
         self._target_point = target_point
         self._ops_weights = ops_weights
 
         arg_provider_cls = TF_ARG_PROVIDERS.registry_dict.get(self._target_point.op_type_name)
         if arg_provider_cls is None:
-            raise ValueError(f'Unexpected type of the TensorFlow operation: {self._target_point.op_type_name}. '
-                             'Register an `ArgProvider` instance for this type in the '
-                             '`TF_ARG_PROVIDERS` registry, please.')
+            raise ValueError(
+                f"Unexpected type of the TensorFlow operation: {self._target_point.op_type_name}. "
+                "Register an `ArgProvider` instance for this type in the "
+                "`TF_ARG_PROVIDERS` registry, please."
+            )
 
         self._arg_provider = arg_provider_cls()
 
     @property
     def operations(self) -> List[NNCFOperation]:
         return self._operations
 
@@ -136,36 +130,28 @@
         """
         tracing_context = get_current_context()
 
         # Should we wrap current operation?
         if not tracing_context.wrap_ops:
             return self._op(*args, **kwargs)
 
-        op_name = get_op_name(self._op_type_name, kwargs.get('name'))
+        op_name = get_op_name(self._op_type_name, kwargs.get("name"))
 
-        _pre_hooks = getattr(get_current_context().model, '_pre_hooks')
-        _post_hooks = getattr(get_current_context().model, '_post_hooks')
+        _pre_hooks = getattr(get_current_context().model, "_pre_hooks")
+        _post_hooks = getattr(get_current_context().model, "_post_hooks")
 
         with tracing_context.enter(in_call=True, wrap_ops=False):
             # Apply pre-hooks
-            args, kwargs = TensorFlowOpWrapper._apply_hooks(
-                _pre_hooks.get(op_name, []),
-                args,
-                kwargs
-            )
+            args, kwargs = TensorFlowOpWrapper._apply_hooks(_pre_hooks.get(op_name, []), args, kwargs)
 
             # Apply TensorFlow operation
             outputs = self._op(*args, **kwargs)
 
             # Apply post-hooks
-            (outputs,), _ = TensorFlowOpWrapper._apply_hooks(
-                _post_hooks.get(op_name, []),
-                (outputs,),
-                {}
-            )
+            (outputs,), _ = TensorFlowOpWrapper._apply_hooks(_post_hooks.get(op_name, []), (outputs,), {})
 
         return outputs
 
     def __getattr__(self, name):
         return getattr(self._op, name)
 
     @staticmethod
@@ -196,18 +182,18 @@
             tf_op_wrapper = TensorFlowOpWrapper(fn, op_type_name)
             setattr(module, fn_name, tf_op_wrapper)
 
             if hasattr(module, op_type_name):
                 setattr(module, op_type_name, tf_op_wrapper)
 
             # Wraps `fn` from the public API
-            if hasattr(fn, '_tf_api_names'):
-                tf_api_names = getattr(fn, '_tf_api_names')
+            if hasattr(fn, "_tf_api_names"):
+                tf_api_names = getattr(fn, "_tf_api_names")
                 for api_name in tf_api_names:
-                    items = api_name.split('.')
+                    items = api_name.split(".")
                     module_names = items[:-1]
                     name = items[-1]
 
                     curr_module = tf
                     for curr_name in module_names:
                         curr_module = getattr(curr_module, curr_name)
                     setattr(curr_module, name, tf_op_wrapper)
@@ -250,39 +236,39 @@
     @staticmethod
     def _wrap_name_scope_internal_fn(func):
         @functools.wraps(func)
         def wrapper(*args, **kwargs):
             if len(args) == 4:
                 args = replace_value_by_index(args, 3, False)
             else:
-                kwargs['skip_on_eager'] = False
+                kwargs["skip_on_eager"] = False
             return func(*args, **kwargs)
 
         return wrapper
 
     @staticmethod
     def _wrap_name_scope_v2_enter_fn(func):
         @functools.wraps(func)
         def wrapper(*args, **kwargs):
             tracing_context = get_current_context()
 
             if tf.executing_eagerly() and tracing_context.in_call:
-                obj, = args  # self
+                (obj,) = args  # self
                 eager_context = context.context()
                 old_name = eager_context.scope_name
                 name = obj._name  # pylint: disable=protected-access
 
                 if not name:
-                    scope_name = ''
-                elif name[-1] == '/':
+                    scope_name = ""
+                elif name[-1] == "/":
                     scope_name = name
                 elif old_name:
-                    scope_name = tracing_context.unique_name(old_name + name) + '/'
+                    scope_name = tracing_context.unique_name(old_name + name) + "/"
                 else:
-                    scope_name = name + '/'
+                    scope_name = name + "/"
                 eager_context.scope_name = scope_name
 
                 def _restore_name_scope(*_):
                     eager_context.scope_name = old_name
 
                 obj._exit_fns.append(_restore_name_scope)  # pylint: disable=protected-access
             else:
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/quantization/algorithm.py` & `nncf-2.5.0/nncf/experimental/tensorflow/quantization/algorithm.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,75 +1,73 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List, Optional, Dict, Any
+from typing import Any, Dict, List, Optional
 
-from nncf.common.logging import nncf_logger
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
-from nncf.common.graph.utils import get_first_nodes_of_type
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationPriority
-from nncf.common.quantization.structs import QuantizerConfig
-from nncf.common.quantization.quantizer_setup import QuantizationPointId
+from nncf.common.graph.utils import get_first_nodes_of_type
+from nncf.common.logging import nncf_logger
 from nncf.common.quantization.quantizer_setup import ActivationQuantizationInsertionPoint
+from nncf.common.quantization.quantizer_setup import QuantizationPointId
+from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.stateful_classes_registry import TF_STATEFUL_CLASSES
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.backend import copy_model
 from nncf.config.extractors import extract_range_init_params
-from nncf.tensorflow.algorithm_selector import TF_COMPRESSION_ALGORITHMS
-from nncf.tensorflow.graph.transformations.commands import TFInsertionCommand
-from nncf.tensorflow.graph.metatypes.tf_ops import TFOpWithWeightsMetatype
-from nncf.tensorflow.quantization.quantizers import TFQuantizerSpec
-from nncf.tensorflow.quantization.algorithm import QuantizationController
-from nncf.tensorflow.quantization.algorithm import TFQuantizationPointStateNames
-from nncf.tensorflow.quantization.algorithm import TFQuantizationPoint
-from nncf.tensorflow.quantization.algorithm import TFQuantizationSetup
-from nncf.tensorflow.quantization.algorithm import QuantizationBuilder
-from nncf.experimental.tensorflow.nncf_network import NNCFNetwork
 from nncf.experimental.tensorflow.graph.converter import SubclassedConverter
 from nncf.experimental.tensorflow.graph.model_transformer import TFModelTransformerV2
+from nncf.experimental.tensorflow.graph.transformations.commands import TFTargetPoint
 from nncf.experimental.tensorflow.graph.transformations.layout import TFTransformationLayoutV2
-from nncf.experimental.tensorflow.quantization.init_range import TFRangeInitParamsV2
+from nncf.experimental.tensorflow.nncf_network import NNCFNetwork
 from nncf.experimental.tensorflow.quantization.init_range import RangeInitializerV2
+from nncf.experimental.tensorflow.quantization.init_range import TFRangeInitParamsV2
 from nncf.experimental.tensorflow.quantization.quantizers import create_quantizer
-from nncf.experimental.tensorflow.graph.transformations.commands import TFTargetPoint
-
+from nncf.tensorflow.algorithm_selector import TF_COMPRESSION_ALGORITHMS
+from nncf.tensorflow.graph.metatypes.tf_ops import TFOpWithWeightsMetatype
+from nncf.tensorflow.graph.transformations.commands import TFInsertionCommand
+from nncf.tensorflow.quantization.algorithm import QuantizationBuilder
+from nncf.tensorflow.quantization.algorithm import QuantizationController
+from nncf.tensorflow.quantization.algorithm import TFQuantizationPoint
+from nncf.tensorflow.quantization.algorithm import TFQuantizationPointStateNames
+from nncf.tensorflow.quantization.algorithm import TFQuantizationSetup
+from nncf.tensorflow.quantization.quantizers import TFQuantizerSpec
 
-UNSUPPORTED_TF_OP_METATYPES = [
-]
+UNSUPPORTED_TF_OP_METATYPES = []
 
 
 class TFQuantizationPointV2StateNames(TFQuantizationPointStateNames):
-    IS_WEIGHT_QUANTIZATION = 'is_weight_quantization'
-    INPUT_SHAPE = 'input_shape'
-    CHANNEL_AXES = 'channel_axes'
+    IS_WEIGHT_QUANTIZATION = "is_weight_quantization"
+    INPUT_SHAPE = "input_shape"
+    CHANNEL_AXES = "channel_axes"
 
 
 class TFQuantizationPointV2(TFQuantizationPoint):
-
     _state_names = TFQuantizationPointV2StateNames
 
-    def __init__(self,
-                 op_name: str,
-                 quantizer_spec: TFQuantizerSpec,
-                 target_point: TargetPoint,
-                 is_weight_quantization: bool,
-                 input_shape: Optional[List[int]] = None,
-                 channel_axes: Optional[List[int]] = None):
+    def __init__(
+        self,
+        op_name: str,
+        quantizer_spec: TFQuantizerSpec,
+        target_point: TargetPoint,
+        is_weight_quantization: bool,
+        input_shape: Optional[List[int]] = None,
+        channel_axes: Optional[List[int]] = None,
+    ):
         super().__init__(op_name, quantizer_spec, target_point)
         self.is_weight_quantization = is_weight_quantization
         self.input_shape = input_shape
         self.channel_axes = channel_axes
 
     def get_state(self) -> Dict[str, Any]:
         state = super().get_state()
@@ -79,55 +77,52 @@
                 self._state_names.INPUT_SHAPE: self.input_shape,
                 self._state_names.CHANNEL_AXES: self.channel_axes,
             }
         )
         return state
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFQuantizationPointV2':
+    def from_state(cls, state: Dict[str, Any]) -> "TFQuantizationPointV2":
         target_point_cls = TF_STATEFUL_CLASSES.get_registered_class(state[cls._state_names.TARGET_POINT_CLASS_NAME])
         kwargs = {
             cls._state_names.OP_NAME: state[cls._state_names.OP_NAME],
             cls._state_names.QUANTIZER_SPEC: TFQuantizerSpec.from_state(state[cls._state_names.QUANTIZER_SPEC]),
             cls._state_names.TARGET_POINT: target_point_cls.from_state(state[cls._state_names.TARGET_POINT]),
             cls._state_names.IS_WEIGHT_QUANTIZATION: state[cls._state_names.IS_WEIGHT_QUANTIZATION],
             cls._state_names.INPUT_SHAPE: state[cls._state_names.INPUT_SHAPE],
             cls._state_names.CHANNEL_AXES: state[cls._state_names.CHANNEL_AXES],
         }
         return cls(**kwargs)
 
 
 class TFQuantizationSetupV2(TFQuantizationSetup):
-
     @classmethod
-    def from_state(cls, state: Dict) -> 'TFQuantizationSetupV2':
+    def from_state(cls, state: Dict) -> "TFQuantizationSetupV2":
         setup = TFQuantizationSetupV2()
         for quantization_point_state in state[cls._state_names.QUANTIZATION_POINTS]:
             quantization_point = TFQuantizationPointV2.from_state(quantization_point_state)
             setup.add_quantization_point(quantization_point)
 
         if cls._state_names.UNIFIED_SCALE_GROUPS in state:
             for quantization_group in state[cls._state_names.UNIFIED_SCALE_GROUPS]:
                 setup.register_unified_scale_group(quantization_group)
         return setup
 
 
 def _get_quantizer_op_name(prefix: str, is_wq: bool, port_id: int, target_type) -> str:
-    pos = 'pre_hook' if target_type == TargetType.OPERATOR_PRE_HOOK else 'post_hook'
-    qtype = 'W' if is_wq else 'A'
-    name = '_'.join([pos, qtype, str(port_id)])
-    quantizer_op_name = f'{prefix}/{name}'
+    pos = "pre_hook" if target_type == TargetType.OPERATOR_PRE_HOOK else "post_hook"
+    qtype = "W" if is_wq else "A"
+    name = "_".join([pos, qtype, str(port_id)])
+    quantizer_op_name = f"{prefix}/{name}"
     return quantizer_op_name
 
 
-def _get_tensor_specs(node: NNCFNode,
-                      nncf_graph: NNCFGraph,
-                      port_ids: List[int],
-                      is_input_tensors: bool,
-                      is_weight_tensors: bool):
+def _get_tensor_specs(
+    node: NNCFNode, nncf_graph: NNCFGraph, port_ids: List[int], is_input_tensors: bool, is_weight_tensors: bool
+):
     """
     Returns specification of tensors for `node` according to `port_ids`.
     """
     tensor_specs = []
 
     if is_weight_tensors:
         assert is_input_tensors
@@ -137,22 +132,20 @@
         assert len(metatype.weight_definitions) == 1
 
         channel_axes = metatype.weight_definitions[0].channel_axes
         weight_shape = node.layer_attributes.get_weight_shape()
         tensor_specs.append((weight_shape, channel_axes))
     else:
         data_format = node.layer_attributes.get_data_format()
-        channel_axes = [-1] if data_format == 'channels_last' else [1]
+        channel_axes = [-1] if data_format == "channels_last" else [1]
 
         if is_input_tensors:
             edges = nncf_graph.get_input_edges(node)
             for input_port_id in port_ids:
-                tensor_specs.extend(
-                    (e.tensor_shape, channel_axes) for e in edges if e.input_port_id == input_port_id
-                )
+                tensor_specs.extend((e.tensor_shape, channel_axes) for e in edges if e.input_port_id == input_port_id)
         else:
             edges = nncf_graph.get_output_edges(node)
             for output_port_id in port_ids:
                 filtered_edges = [e for e in edges if e.output_port_id == output_port_id]
 
                 shape = filtered_edges[0].tensor_shape
                 for e in filtered_edges:
@@ -161,95 +154,85 @@
                 tensor_specs.append((shape, channel_axes))
 
     assert len(tensor_specs) == len(port_ids)
 
     return tensor_specs
 
 
-@TF_COMPRESSION_ALGORITHMS.register('experimental_quantization')
+@TF_COMPRESSION_ALGORITHMS.register("experimental_quantization")
 class QuantizationBuilderV2(QuantizationBuilder):
-
     def _load_state_without_name(self, state_without_name: Dict[str, Any]):
         quantizer_setup_state = state_without_name[self._state_names.QUANTIZER_SETUP]
         self._quantizer_setup = TFQuantizationSetupV2.from_state(quantizer_setup_state)
 
     def _parse_range_init_params(self) -> TFRangeInitParamsV2:
         range_init_params = extract_range_init_params(self.config, self.name)
         return TFRangeInitParamsV2(**range_init_params) if range_init_params is not None else None
 
-    def _build_insertion_commands_for_quantizer_setup(self, quantizer_setup: TFQuantizationSetupV2) \
-             -> List[TFInsertionCommand]:
+    def _build_insertion_commands_for_quantizer_setup(
+        self, quantizer_setup: TFQuantizationSetupV2
+    ) -> List[TFInsertionCommand]:
         insertion_commands = []
         quantization_points = quantizer_setup.get_quantization_points()
         # quantization point id is her index inside the `quantization_points` list
         was_processed = {qp_id: False for qp_id in range(len(quantization_points))}
 
         for unified_scales_group in quantizer_setup.get_unified_scale_groups():
             qp = quantization_points[unified_scales_group[0]]
             quantizer = create_quantizer(
-                f'{qp.op_name}/unified_scale_group',
+                f"{qp.op_name}/unified_scale_group",
                 qp.quantizer_spec,
                 qp.is_weight_quantization,
                 qp.input_shape,
-                qp.channel_axes
+                qp.channel_axes,
             )
 
             self._op_names.append(quantizer.name)
 
             for qp_id in unified_scales_group:
                 if was_processed[qp_id]:
-                    raise RuntimeError('Unexpected behavior')
+                    raise RuntimeError("Unexpected behavior")
                 was_processed[qp_id] = True
 
                 curr_qp = quantization_points[qp_id]
                 # Checks
                 assert curr_qp.quantizer_spec.get_state() == qp.quantizer_spec.get_state()
                 assert curr_qp.input_shape == qp.input_shape
                 assert curr_qp.channel_axes == qp.channel_axes
 
                 command = TFInsertionCommand(
-                    curr_qp.target_point,
-                    quantizer,
-                    TransformationPriority.QUANTIZATION_PRIORITY
+                    curr_qp.target_point, quantizer, TransformationPriority.QUANTIZATION_PRIORITY
                 )
                 insertion_commands.append(command)
 
         for qp_id, qp in enumerate(quantization_points):
             if was_processed[qp_id]:
                 continue
 
             quantizer = create_quantizer(
-                qp.op_name,
-                qp.quantizer_spec,
-                qp.is_weight_quantization,
-                qp.input_shape,
-                qp.channel_axes
+                qp.op_name, qp.quantizer_spec, qp.is_weight_quantization, qp.input_shape, qp.channel_axes
             )
 
             self._op_names.append(quantizer.name)
 
-            command = TFInsertionCommand(
-                qp.target_point,
-                quantizer,
-                TransformationPriority.QUANTIZATION_PRIORITY
-            )
+            command = TFInsertionCommand(qp.target_point, quantizer, TransformationPriority.QUANTIZATION_PRIORITY)
             insertion_commands.append(command)
 
         return insertion_commands
 
     def get_transformation_layout(self, model: NNCFNetwork):
         transformations = TFTransformationLayoutV2()
         if self._quantizer_setup is None:
             self._quantizer_setup = self._get_quantizer_setup(model)
         insertion_commands = self._build_insertion_commands_for_quantizer_setup(self._quantizer_setup)
         for command in insertion_commands:
             transformations.register(command)
         return transformations
 
-    def _build_controller(self, model: NNCFNetwork) -> 'QuantizationControllerV2':
+    def _build_controller(self, model: NNCFNetwork) -> "QuantizationControllerV2":
         return QuantizationControllerV2(model, self.config, self._op_names)
 
     def _run_range_initialization(self, model: NNCFNetwork) -> None:
         if self._range_initializer is None:
             self._range_initializer = RangeInitializerV2(self._range_init_params)
         self._range_initializer.run(model)
 
@@ -259,61 +242,62 @@
     def _get_quantizer_setup(self, model: NNCFNetwork) -> TFQuantizationSetupV2:
         converter = SubclassedConverter(model, model.input_signature)
         nncf_graph = converter.convert()
         # Find out which metatypes unsupported by the quantization algorithm
         for node in nncf_graph.get_all_nodes():
             if node.metatype in UNSUPPORTED_TF_OP_METATYPES:
                 nncf_logger.warning(
-                    'The operation {} is unsupported by the quantization algorithm.'.format(node.node_name)
+                    "The operation {} is unsupported by the quantization algorithm.".format(node.node_name)
                 )
 
         # Possible configurations of quantizer for nodes with weights.
         possible_qconfigs_for_nodes_with_weight = self._get_quantizable_weighted_layer_nodes(nncf_graph)
-        qp_solution = self._get_quantizer_propagation_solution(nncf_graph,
-                                                               possible_qconfigs_for_nodes_with_weight,
-                                                               [],
-                                                               model)
+        qp_solution = self._get_quantizer_propagation_solution(
+            nncf_graph, possible_qconfigs_for_nodes_with_weight, [], model
+        )
 
         # Logic of the TFQuantizationSetupV2 creation
 
         quantization_setup = TFQuantizationSetupV2()
         node_name_to_qconfig_map = {}  # type: Dict[str, QuantizerConfig]
         qp_id_to_setup_index_map = {}  # type: Dict[QuantizationPointId, int]
-        first_conv_nodes = get_first_nodes_of_type(nncf_graph, ['Conv2D'])
+        first_conv_nodes = get_first_nodes_of_type(nncf_graph, ["Conv2D", "Conv3D"])
 
         for idx, (qp_id, qp) in enumerate(qp_solution.quantization_points.items()):
             qp_id_to_setup_index_map[qp_id] = idx
             target_node = nncf_graph.get_node_by_name(qp.insertion_point.target_node_name)
 
             if qp.is_weight_quantization_point():
                 # Check correctness
                 if target_node.node_name in node_name_to_qconfig_map:
                     assigned_qconfig = node_name_to_qconfig_map[target_node.node_name]
                     if qp.qconfig != assigned_qconfig:
-                        raise RuntimeError('Inconsistent quantizer configurations selected by solver for one '
-                                            f'and the same quantizable op! Tried to assign {qp.qconfig} to '
-                                            f'{target_node.node_name} as specified by QP {qp_id}, but the op '
-                                            f'already has quantizer config {assigned_qconfig} assigned to it!')
+                        raise RuntimeError(
+                            "Inconsistent quantizer configurations selected by solver for one "
+                            f"and the same quantizable op! Tried to assign {qp.qconfig} to "
+                            f"{target_node.node_name} as specified by QP {qp_id}, but the op "
+                            f"already has quantizer config {assigned_qconfig} assigned to it!"
+                        )
                     continue  # The operation has already been quantized
                 node_name_to_qconfig_map[target_node.node_name] = qp.qconfig
 
                 # Parameters
                 half_range = self._get_half_range(qp.qconfig, target_node, first_conv_nodes)
                 narrow_range = not half_range
                 target_type = TargetType.OPERATOR_PRE_HOOK
                 if not issubclass(target_node.metatype, TFOpWithWeightsMetatype):
-                    raise RuntimeError(f'Unexpected type of metatype: {type(target_node.metatype)}')
+                    raise RuntimeError(f"Unexpected type of metatype: {type(target_node.metatype)}")
                 port_ids = [weight_def.port_id for weight_def in target_node.metatype.weight_definitions]
 
             else:
                 assert qp.is_activation_quantization_point()
 
                 # Check correctness
                 if not isinstance(qp.insertion_point, ActivationQuantizationInsertionPoint):
-                    raise RuntimeError(f'Unexpected type of insertion point: {type(qp.insertion_point)}')
+                    raise RuntimeError(f"Unexpected type of insertion point: {type(qp.insertion_point)}")
 
                 # Parameters
                 half_range = False
                 narrow_range = False
                 if qp.insertion_point.input_port_id is not None:
                     port_ids = [qp.insertion_point.input_port_id]  # Input port ids
                     target_type = TargetType.OPERATOR_PRE_HOOK
@@ -322,36 +306,37 @@
                     target_type = TargetType.OPERATOR_POST_HOOK
 
             tensor_specs = _get_tensor_specs(
                 target_node,
                 nncf_graph,
                 port_ids,
                 target_type == TargetType.OPERATOR_PRE_HOOK,
-                qp.is_weight_quantization_point()
+                qp.is_weight_quantization_point(),
             )
 
             for port_id, (tensor_shape, channel_axes) in zip(port_ids, tensor_specs):
                 quantizer_op_name = _get_quantizer_op_name(
-                    target_node.node_name,
-                    qp.is_weight_quantization_point(),
-                    port_id,
-                    target_type
+                    target_node.node_name, qp.is_weight_quantization_point(), port_id, target_type
                 )
                 quantizer_spec = TFQuantizerSpec.from_config(qp.qconfig, narrow_range, half_range)
                 target_point = TFTargetPoint(target_node.node_name, target_node.node_type, port_id, target_type)
-                qpoint = TFQuantizationPointV2(quantizer_op_name, quantizer_spec, target_point,
-                                               qp.is_weight_quantization_point(), tensor_shape, channel_axes)
+                qpoint = TFQuantizationPointV2(
+                    quantizer_op_name,
+                    quantizer_spec,
+                    target_point,
+                    qp.is_weight_quantization_point(),
+                    tensor_shape,
+                    channel_axes,
+                )
 
                 quantization_setup.add_quantization_point(qpoint)
 
         # Registration of unified scale groups
         for unified_group in qp_solution.unified_scale_groups.values():
-            us_group = [
-                qp_id_to_setup_index_map[qp_id] for qp_id in unified_group
-            ]
+            us_group = [qp_id_to_setup_index_map[qp_id] for qp_id in unified_group]
             quantization_setup.register_unified_scale_group(us_group)
 
         return quantization_setup
 
     def apply_to(self, model: NNCFNetwork) -> NNCFNetwork:
         transformation_layout = self.get_transformation_layout(model)
         transformer = TFModelTransformerV2(model)
@@ -360,15 +345,17 @@
         if self.should_init:
             self.initialize(transformed_model)
 
         return transformed_model
 
 
 class QuantizationControllerV2(QuantizationController):
-    def strip_model(self, model: NNCFNetwork) -> NNCFNetwork:
+    def strip_model(self, model: NNCFNetwork, do_copy: bool = False) -> NNCFNetwork:
+        if do_copy:
+            model = copy_model(model)
         return model
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         return NNCFStatistics()
 
     def prepare_for_export(self) -> None:
         self._model.compute_output_shape(self._model.input_signature.shape.as_list())
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/quantization/init_range.py` & `nncf-2.5.0/nncf/experimental/tensorflow/quantization/init_range.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,42 +1,38 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from itertools import islice
 
 import numpy as np
 import tensorflow as tf
 
+from nncf.common.logging.progress_bar import ProgressBar
 from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.initialization.range import RangeInitConfig
 from nncf.common.quantization.structs import QuantizerGroup
-from nncf.common.logging.progress_bar import ProgressBar
-from nncf.tensorflow.quantization.init_range import TFRangeInitParams
+from nncf.experimental.tensorflow.nncf_network import NNCFNetwork
+from nncf.experimental.tensorflow.quantization.quantizers import NNCF_QUANTIZATION_OPERATIONS_V2
+from nncf.experimental.tensorflow.quantization.quantizers import InputType
 from nncf.tensorflow.quantization.init_range import RangeInitializer
+from nncf.tensorflow.quantization.init_range import TFRangeInitParams
 from nncf.tensorflow.tensor_statistics.reduction import get_axes
-from nncf.experimental.tensorflow.quantization.quantizers import InputType
-from nncf.experimental.tensorflow.quantization.quantizers import NNCF_QUANTIZATION_OPERATIONS_V2
-from nncf.experimental.tensorflow.nncf_network import NNCFNetwork
 from nncf.tensorflow.tensor_statistics.statistics import tf_convert_stat_to_min_max_tensor_stat
 
 
 class TFRangeInitParamsV2(TFRangeInitParams):
-    def get_init_config_for_quantization_point_v2(self,
-                                                  node_name: str,
-                                                  input_type: str) -> RangeInitConfig:
+    def get_init_config_for_quantization_point_v2(self, node_name: str, input_type: str) -> RangeInitConfig:
         group = QuantizerGroup.WEIGHTS if input_type == InputType.WEIGHTS else QuantizerGroup.ACTIVATIONS
         return self.get_init_config_for_scope_and_group(node_name, group)
 
 
 def _get_reduction_shape(tensor_shape, channel_axes, per_channel):
     ndims = len(tensor_shape)
     channel_axes_ = channel_axes if isinstance(channel_axes, (list, tuple)) else [channel_axes]
@@ -46,55 +42,47 @@
 
 class RangeInitializerV2(RangeInitializer):
     def __init__(self, range_init_params: TFRangeInitParamsV2):
         super().__init__(range_init_params)
         self.nncf_quantization_operation_classes = NNCF_QUANTIZATION_OPERATIONS_V2.registry_dict.values()
 
     def _register_op_collector(self, op, collectors, handles, op_weights):
-        node_name = ''  # TODO(andrey-churkin): Use correct node_name
-        init_config = self.range_init_params.get_init_config_for_quantization_point_v2(
-            node_name,
-            op.input_type
-        )
+        node_name = ""  # TODO(andrey-churkin): Use correct node_name
+        init_config = self.range_init_params.get_init_config_for_quantization_point_v2(node_name, op.input_type)
 
         is_weights = op.input_type == InputType.WEIGHTS
         collector_params = RangeInitCollectorParams(is_weights, op.mode, op.per_channel)
 
         reduction_shape = _get_reduction_shape(op.input_shape, op.channel_axes, op.per_channel)
         if is_weights:
             num_batches = 1
         else:
             num_batches = int(np.ceil(init_config.num_init_samples / self.dataset.batch_size))
 
-            per_sample_stats = init_config.init_type in ['mixed_min_max', 'mean_min_max']
+            per_sample_stats = init_config.init_type in ["mixed_min_max", "mean_min_max"]
             if collector_params.use_per_sample_stats(per_sample_stats):
                 reduction_shape = reduction_shape[1:]
 
         collector = RangeInitializerV2.generate_stat_collector(
-            reduction_shape,
-            collector_params,
-            init_config,
-            num_batches
+            reduction_shape, collector_params, init_config, num_batches
         )
         handles.append(op.register_hook_pre_call(collector.register_input))
         op.enabled = False
         collectors.append((op, collector, op_weights))
 
     def run(self, model: NNCFNetwork) -> None:
         handles = []
         collectors = []
         for op, op_weights in model.get_nncf_operations_with_params():
             if op.__class__ not in self.nncf_quantization_operation_classes:
                 continue
             self._register_op_collector(op, collectors, handles, op_weights)
 
-        for (x, _) in ProgressBar(
-                islice(self.dataset, self.num_steps),
-                total=self.num_steps,
-                desc='Collecting tensor statistics/data'
+        for x, _ in ProgressBar(
+            islice(self.dataset, self.num_steps), total=self.num_steps, desc="Collecting tensor statistics/data"
         ):
             model(x, training=False)
 
         for op, collector, op_weights in collectors:
             target_stat = collector.get_statistics()
             minmax_stats = tf_convert_stat_to_min_max_tensor_stat(target_stat)
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/quantization/quantizers.py` & `nncf-2.5.0/nncf/experimental/tensorflow/quantization/quantizers.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,43 +1,37 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Dict
-from typing import Optional
-from typing import List
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Dict, List, Optional
 
 import tensorflow as tf
 
-from nncf.common.utils.registry import Registry
 from nncf.common.quantization.structs import QuantizationMode
+from nncf.common.utils.registry import Registry
 from nncf.tensorflow.layers.operation import InputType
-from nncf.tensorflow.quantization.quantizers import TFQuantizerSpec
-from nncf.tensorflow.quantization.quantizers import SymmetricQuantizer
 from nncf.tensorflow.quantization.quantizers import AsymmetricQuantizer
+from nncf.tensorflow.quantization.quantizers import SymmetricQuantizer
+from nncf.tensorflow.quantization.quantizers import TFQuantizerSpec
 
-
-NNCF_QUANTIZATION_OPERATIONS_V2 = Registry('nncf_quantization_operations_v2')
+NNCF_QUANTIZATION_OPERATIONS_V2 = Registry("nncf_quantization_operations_v2")
 
 
 @NNCF_QUANTIZATION_OPERATIONS_V2.register(QuantizationMode.SYMMETRIC)
 class SymmetricQuantizerV2(SymmetricQuantizer):
-    def set_input_spec(self,
-                       input_type: str,
-                       input_shape: Optional[List[int]] = None,
-                       channel_axes: Optional[List[int]] = None):
+    def set_input_spec(
+        self, input_type: str, input_shape: Optional[List[int]] = None, channel_axes: Optional[List[int]] = None
+    ):
         """
         Sets input tensor specification for the quantizer.
 
         :param input_type: Indicates the type of input tensor: `inputs` or `weights`.
         :param input_shape: Shape of the input tensor for which the
             quantization is applied. Required only for per-channel
             quantization.
@@ -53,26 +47,26 @@
         """
         Creates quantizer variables using `layer.add_weight()` method.
 
         :param layer: Instance of the `tf.keras.layers.Layer` class.
         :return: Quantizer variables.
         """
         if self.per_channel and (self.input_shape is None or self.channel_axes is None):
-            raise ValueError('The `input_shape` and `channel_axes` arguments are required when'
-                             'using per-channel quantization.')
+            raise ValueError(
+                "The `input_shape` and `channel_axes` arguments are required when using per-channel quantization."
+            )
         prefix = self.name
         return self._create_variables(layer, self.input_shape, self.channel_axes, prefix)
 
 
 @NNCF_QUANTIZATION_OPERATIONS_V2.register(QuantizationMode.ASYMMETRIC)
 class AsymmetricQuantizerV2(AsymmetricQuantizer):
-    def set_input_spec(self,
-                       input_type: str,
-                       input_shape: Optional[List[int]] = None,
-                       channel_axes: Optional[List[int]] = None):
+    def set_input_spec(
+        self, input_type: str, input_shape: Optional[List[int]] = None, channel_axes: Optional[List[int]] = None
+    ):
         """
         Sets input tensor specification for the quantizer.
 
         :param input_type: Indicates the type of input tensor: `inputs` or `weights`.
         :param input_shape: Shape of the input tensor for which the
             quantization is applied. Required only for per-channel
             quantization.
@@ -88,25 +82,28 @@
         """
         Creates quantizer variables using `layer.add_weight()` method.
 
         :param layer: Instance of the `tf.keras.layers.Layer` class.
         :return: Quantizer variables.
         """
         if self.per_channel and (self.input_shape is None or self.channel_axes is None):
-            raise ValueError('The `input_shape` and `channel_axes` arguments are required when'
-                             'using per-channel quantization.')
+            raise ValueError(
+                "The `input_shape` and `channel_axes` arguments are required when using per-channel quantization."
+            )
         prefix = self.name
         return self._create_variables(layer, self.input_shape, self.channel_axes, prefix)
 
 
-def create_quantizer(name: str,
-                     qspec: TFQuantizerSpec,
-                     is_weight_quantization: bool,
-                     input_shape: Optional[List[int]] = None,
-                     channel_axes: Optional[List[int]] = None):
+def create_quantizer(
+    name: str,
+    qspec: TFQuantizerSpec,
+    is_weight_quantization: bool,
+    input_shape: Optional[List[int]] = None,
+    channel_axes: Optional[List[int]] = None,
+):
     """
     Factory method to create quantizer.
 
     :param name: Name of the quantizer. Should be unique.
     :param qspec: Specification of the quantizer.
     :param is_weight_quantization: A boolean flag.
         Takes one of the following values:
```

### Comparing `nncf-2.4.0/nncf/experimental/tensorflow/scope.py` & `nncf-2.5.0/nncf/experimental/tensorflow/scope.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,55 +1,53 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Optional
 
 import tensorflow as tf
 from tensorflow.python.eager import context
 
 
 def get_current_name_scope() -> str:
     """
     Returns current full name scope specified by `tf.name_scope(...)` method.
 
     :return: The name of scope.
     """
     if tf.executing_eagerly():
-        return context.context().scope_name.rstrip('/')
+        return context.context().scope_name.rstrip("/")
 
     return tf.compat.v1.get_default_graph().get_name_scope()
 
 
 def get_op_name(op_type_name: str, scope: Optional[str] = None) -> str:
     """
     Returns the name of operation from the current name of scope.
 
     :param op_type_name: Type name of operation.
     :param scope: The name of scope.
     :return: The name of operation.
     """
     if scope:
-        if not scope.endswith('/'):
+        if not scope.endswith("/"):
             return scope
         op_name = scope[:-1]
     else:
         current_scope = get_current_name_scope()
-        if current_scope[current_scope.rfind('/') + 1:].startswith(op_type_name.lower()):
+        if current_scope[current_scope.rfind("/") + 1 :].startswith(op_type_name.lower()):
             op_name = current_scope
         else:
-            op_name = f'{current_scope}/{op_type_name}'
+            op_name = f"{current_scope}/{op_type_name}"
 
     # Remove `replica_*/` prefix from `op_name`.
-    if op_name.startswith('replica'):
-        op_name = op_name[op_name.find('/') + 1:]
+    if op_name.startswith("replica"):
+        op_name = op_name[op_name.find("/") + 1 :]
 
     return op_name
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/__init__.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,16 +1,14 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity import elasticity_builder as elasticity_algo
+from nncf.experimental.torch.nas.bootstrapNAS.search.search import SearchAlgorithm
 from nncf.experimental.torch.nas.bootstrapNAS.training import progressive_shrinking_builder as ps_algo
 from nncf.experimental.torch.nas.bootstrapNAS.training.training_algorithm import EpochBasedTrainingAlgorithm
-from nncf.experimental.torch.nas.bootstrapNAS.search.search import SearchAlgorithm
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/base_handler.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/base_handler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,37 +1,31 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from abc import ABC
 from abc import abstractmethod
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import TypeVar
+from typing import Any, Dict, List, Optional, TypeVar
 
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.utils.registry import Registry
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
 from nncf.torch.nncf_network import NNCFNetwork
 
-ELASTICITY_HANDLERS_MAP = Dict[ElasticityDim, 'ElasticityHandler']
-ElasticSearchSpace = TypeVar('ElasticSearchSpace')
-ElasticityConfig = TypeVar('ElasticityConfig')
-ELASTICITY_BUILDERS = Registry('Elasticity builder', add_name_as_attr=True)
-ELASTICITY_PARAMS = Registry('Elasticity builder', add_name_as_attr=True)
+ELASTICITY_HANDLERS_MAP = Dict[ElasticityDim, "ElasticityHandler"]
+ElasticSearchSpace = TypeVar("ElasticSearchSpace")
+ElasticityConfig = TypeVar("ElasticityConfig")
+ELASTICITY_BUILDERS = Registry("Elasticity builder", add_name_as_attr=True)
+ELASTICITY_PARAMS = Registry("Elasticity builder", add_name_as_attr=True)
 
 
 class ElasticityHandler(ABC):
     """
     An interface for handling elasticity in the network. The elasticity defines variable values in properties of the
     layers or the network, e.g. variable number of channels in the Conv or variable number of layers in the network.
     By applying elasticity it's possible to derive a smaller models (Subnets) that have some elements in common with
@@ -124,21 +118,22 @@
         Activates a maximum Subnet that corresponds to the maximum values of elastic properties.
         """
         config = self.get_maximum_config()
         self.activate_subnet_for_config(config)
 
 
 class SEHandlerStateNames:
-    ACTIVE_CONFIG = 'active_config'
+    ACTIVE_CONFIG = "active_config"
 
 
 class SingleElasticityHandler(ElasticityHandler, ABC):
     """
     An interface for handling a single elasticity dimension in the network, e.g. elastic width or depth.
     """
+
     _state_names = SEHandlerStateNames
 
     @abstractmethod
     def get_search_space(self) -> ElasticSearchSpace:
         """
         :return: search space that is produced by iterating over all elastic parameters
         """
@@ -146,17 +141,17 @@
     @abstractmethod
     def get_transformation_commands(self) -> List[TransformationCommand]:
         """
         :return: transformation commands for introducing the elasticity to NNCFNetwork
         """
 
     @abstractmethod
-    def resolve_conflicts_with_other_elasticities(self,
-                                                  config: ElasticityConfig,
-                                                  elasticity_handlers: ELASTICITY_HANDLERS_MAP) -> ElasticityConfig:
+    def resolve_conflicts_with_other_elasticities(
+        self, config: ElasticityConfig, elasticity_handlers: ELASTICITY_HANDLERS_MAP
+    ) -> ElasticityConfig:
         """
         Resolves a conflict between the given elasticity config and active elasticity configs of the given handlers.
         For example, elastic width configuration may contradict to elastic depth one. When we activate some
         configuration in the Elastic Width Handler, i.e. define number of output channels for some layers, we
         change output shapes of the layers. Consequently, it affects the blocks that can be skipped by Elastic Depth
         Handler, because input and output shapes may not be identical now.
 
@@ -185,22 +180,22 @@
         return {
             self._state_names.ACTIVE_CONFIG: active_config,
         }
 
 
 class BaseElasticityParams:
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'BaseElasticityParams':
+    def from_config(cls, config: Dict[str, Any]) -> "BaseElasticityParams":
         """
         Creates the object from its config.
         """
 
     @classmethod
     @abstractmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'BaseElasticityParams':
+    def from_state(cls, state: Dict[str, Any]) -> "BaseElasticityParams":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
 
     @abstractmethod
@@ -209,28 +204,31 @@
         Returns the compression loss state.
 
         :return: The compression loss state.
         """
 
 
 class SEHBuilderStateNames:
-    ELASTICITY_PARAMS = 'elasticity_params'
+    ELASTICITY_PARAMS = "elasticity_params"
 
 
 class SingleElasticityBuilder:
     """
     Determines which modifications should be made to the original FP32 model in order to introduce elasticity
     to the model.
     """
+
     _state_names = SEHBuilderStateNames
 
-    def __init__(self,
-                 params: BaseElasticityParams,
-                 ignored_scopes: Optional[List[str]] = None,
-                 target_scopes: Optional[List[str]] = None):
+    def __init__(
+        self,
+        params: BaseElasticityParams,
+        ignored_scopes: Optional[List[str]] = None,
+        target_scopes: Optional[List[str]] = None,
+    ):
         self._target_scopes = target_scopes
         self._ignored_scopes = ignored_scopes
         self._params = params
 
     @abstractmethod
     def build(self, target_model: NNCFNetwork) -> SingleElasticityHandler:
         """
@@ -255,15 +253,17 @@
         """
         Initializes object from the state.
 
         :param state: Output of `get_state()` method.
         """
 
 
-def create_elasticity_builder_from_config(config: Dict[str, Any],
-                                          elasticity_dim: ElasticityDim,
-                                          ignored_scopes: Optional[List[str]] = None,
-                                          target_scopes: Optional[List[str]] = None) -> SingleElasticityBuilder:
+def create_elasticity_builder_from_config(
+    config: Dict[str, Any],
+    elasticity_dim: ElasticityDim,
+    ignored_scopes: Optional[List[str]] = None,
+    target_scopes: Optional[List[str]] = None,
+) -> SingleElasticityBuilder:
     params_cls = ELASTICITY_PARAMS.get(elasticity_dim)
     params = params_cls.from_config(config)
     elasticity_builder_cls = ELASTICITY_BUILDERS.get(elasticity_dim)
     return elasticity_builder_cls(params, ignored_scopes, target_scopes)
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_depth.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_depth.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,33 +1,28 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import random
 from itertools import combinations
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
+from typing import Any, Dict, List, Optional
 
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.logging import nncf_logger
-from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import BaseElasticityParams
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_BUILDERS
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_HANDLERS_MAP
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_PARAMS
+from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import BaseElasticityParams
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import SingleElasticityBuilder
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import SingleElasticityHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_width import ElasticWidthHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
 from nncf.experimental.torch.search_building_blocks.search_blocks import BuildingBlock
 from nncf.experimental.torch.search_building_blocks.search_blocks import BuildingBlocks
 from nncf.experimental.torch.search_building_blocks.search_blocks import ExtendedBuildingBlocks
@@ -39,39 +34,43 @@
 
 BlockId = int
 ElasticDepthConfig = List[BlockId]  # list of block indexes
 ElasticDepthSearchSpace = List[ElasticDepthConfig]  # grouped list of block indexes
 
 
 class EDHandlerStateNames:
-    DEPTH_INDICATOR = 'depth_indicator'
+    DEPTH_INDICATOR = "depth_indicator"
 
 
 class ElasticDepthHandler(SingleElasticityHandler):
     """
     An interface for handling elastic depth dimension in the network, i.e. skip some layers in the model.
     """
+
     _depth_state_names = EDHandlerStateNames
 
-    def __init__(self, target_model: NNCFNetwork,
-                 skipped_blocks: BuildingBlocks,
-                 skip_dependencies: GroupedBlockIDs,
-                 node_names_per_block: Dict[int, List[NNCFNodeName]]):
+    def __init__(
+        self,
+        target_model: NNCFNetwork,
+        skipped_blocks: BuildingBlocks,
+        skip_dependencies: GroupedBlockIDs,
+        node_names_per_block: Dict[int, List[NNCFNodeName]],
+    ):
         """
         Constructor
 
         :param target_model: a target NNCFNetwork for adding modifications
         :param skipped_blocks: list of building blocks to be skipped.
         :param skip_dependencies: indexes of building blocks grouped by connectivity.
         Blocks that follow each other in the graph (i.e. they are connected by one edge) belong to the same group.
         :param node_names_per_block: mapping of block id and all node names inside the block.
         """
         super().__init__()
         self._target_model = target_model
-        self._tracing_context = self._target_model.get_tracing_context()  # type: TracingContext
+        self._tracing_context = self._target_model.nncf.get_tracing_context()  # type: TracingContext
         self._skipped_blocks = skipped_blocks
         self._skip_dependencies = skip_dependencies
         self._node_names_per_block = node_names_per_block
         self._depth_indicator = 1
         self._is_search_space_obsolete = True
         self._cached_search_space = None
 
@@ -187,17 +186,17 @@
         Activates a Subnet that doesn't execute the blocks specified in the given elasticity configuration
 
         :param config: list of blocks' indexes to skip
         """
         config = self._remove_inconsistent_blocks(config)
         self._tracing_context.set_active_skipped_block(config)
 
-    def resolve_conflicts_with_other_elasticities(self,
-                                                  config: ElasticDepthConfig,
-                                                  elasticity_handlers: ELASTICITY_HANDLERS_MAP) -> ElasticDepthConfig:
+    def resolve_conflicts_with_other_elasticities(
+        self, config: ElasticDepthConfig, elasticity_handlers: ELASTICITY_HANDLERS_MAP
+    ) -> ElasticDepthConfig:
         """
         Resolves a conflict between the given elasticity config and active elasticity configs of the given handlers.
         For example, elastic width configuration may contradict to elastic depth one. When we activate some
         configuration in the Elastic Width Handler, i.e. define number of output channels for some layers, we
         change output shapes of the layers. Consequently, it affects the blocks that can be skipped by Elastic Depth
         Handler, because input and output shapes may not be identical now.
 
@@ -212,15 +211,16 @@
             blocks = [self._tracing_context.skipped_blocks[block_idx] for block_idx in config]
             pairs_of_nodes = [(block.start_node_name, block.end_node_name) for block in blocks]
             if pairs_of_nodes:
                 indexes_of_pairs = width_handler.find_pairs_of_nodes_with_different_width(pairs_of_nodes)
                 if indexes_of_pairs:
                     result = [element for idx, element in enumerate(config) if idx not in indexes_of_pairs]
                     nncf_logger.debug(
-                        f'The blocks with indexes {indexes_of_pairs} are not skipped to avoid inconsistency with width')
+                        f"The blocks with indexes {indexes_of_pairs} are not skipped to avoid inconsistency with width"
+                    )
         return result
 
     def get_names_of_skipped_nodes(self) -> List[NNCFNodeName]:
         """
         :return: names of nodes that are currently skipped by Elastic Depth
         """
         names_of_skipped_nodes = []
@@ -230,72 +230,76 @@
                 names_of_skipped_nodes.extend(self._node_names_per_block[idx])
         return names_of_skipped_nodes
 
     def _remove_inconsistent_blocks(self, config: ElasticDepthConfig) -> ElasticDepthConfig:
         return self._remove_blocks_skipped_non_progressively(config)
 
     def _remove_blocks_skipped_non_progressively(self, config: ElasticDepthConfig) -> ElasticDepthConfig:
-        assert self._skip_dependencies is not None, 'Please include depth dependencies in conf. Pending automation.'
+        assert self._skip_dependencies is not None, "Please include depth dependencies in conf. Pending automation."
         block_indexes_to_remove = []
         for block_index in config:
             tmp_block_indexes_to_remove = self._get_block_indexes_to_remove(block_index, config)
             block_indexes_to_remove.extend(tmp_block_indexes_to_remove)
 
         for block_index in block_indexes_to_remove:
             config.remove(block_index)
-            nncf_logger.debug(f'The block #{block_index} is not skipped to not violate progressive shrinking')
+            nncf_logger.debug(f"The block #{block_index} is not skipped to not violate progressive shrinking")
         return config
 
     def _get_block_indexes_to_remove(self, block_index: int, config: ElasticDepthConfig) -> ElasticDepthConfig:
         block_indexes_to_remove = []
         for group in self._skip_dependencies.values():
             found = False
             group_index = group.index(block_index) if block_index in group else None
             if group_index is not None:
                 found = True
                 if len(group) - group_index > self.depth_indicator:
-                    nncf_logger.debug(f'The block with {block_index} did not pass the depth_indicator test')
+                    nncf_logger.debug(f"The block with {block_index} did not pass the depth_indicator test")
                     block_indexes_to_remove.append(block_index)
                     break
                 valid_block_indexes = [group[group_index]]
                 for i in range(group_index + 1, len(group)):
                     if group[i] in config:
                         valid_block_indexes.append(group[i])
                     else:
-                        nncf_logger.debug(f'The block #{block_index} or #{valid_block_indexes} '
-                                          f'did not satisfy requirement of next static block')
+                        nncf_logger.debug(
+                            f"The block #{block_index} or #{valid_block_indexes} "
+                            f"did not satisfy requirement of next static block"
+                        )
                         for valid_block_index in valid_block_indexes:
                             block_indexes_to_remove.append(valid_block_index)
                         break
             if found:
                 break
         return block_indexes_to_remove
 
 
 class EDBuilderStateNames:
-    SKIPPED_BLOCKS = 'skipped_blocks'
-    SKIPPED_BLOCKS_DEPENDENCIES = 'skipped_blocks_dependencies'
+    SKIPPED_BLOCKS = "skipped_blocks"
+    SKIPPED_BLOCKS_DEPENDENCIES = "skipped_blocks_dependencies"
 
 
 class EDParamsStateNames:
-    MAX_BLOCK_SIZE = 'max_block_size'
-    MIN_BLOCK_SIZE = 'min_block_size'
-    HW_FUSED_OPS = 'hw_fused_ops'
-    SKIPPED_BLOCKS = 'skipped_blocks'
+    MAX_BLOCK_SIZE = "max_block_size"
+    MIN_BLOCK_SIZE = "min_block_size"
+    HW_FUSED_OPS = "hw_fused_ops"
+    SKIPPED_BLOCKS = "skipped_blocks"
 
 
 @ELASTICITY_PARAMS.register(ElasticityDim.DEPTH)
 class ElasticDepthParams(BaseElasticityParams):
     _state_names = EDParamsStateNames
 
-    def __init__(self,
-                 max_block_size: int,
-                 min_block_size: int,
-                 hw_fused_ops: bool = True,
-                 skipped_blocks: Optional[List[List[NNCFNodeName]]] = None):
+    def __init__(
+        self,
+        max_block_size: int,
+        min_block_size: int,
+        hw_fused_ops: bool = True,
+        skipped_blocks: Optional[List[List[NNCFNodeName]]] = None,
+    ):
         """
         Constructor
 
         :param max_block_size: Defines minimal number of operations in the block. Option is available for the auto mode
          only. Default value is 50.
         :param min_block_size: Defines minimal number of operations in the skipping block. Option is available for the
          auto mode only. Default value is 6.
@@ -306,28 +310,28 @@
         """
         self.max_block_size = max_block_size
         self.min_block_size = min_block_size
         self.hw_fused_ops = hw_fused_ops
         self.skipped_blocks = skipped_blocks
 
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'ElasticDepthParams':
+    def from_config(cls, config: Dict[str, Any]) -> "ElasticDepthParams":
         """
         Creates the object from its config.
         """
         kwargs = {
             cls._state_names.MAX_BLOCK_SIZE: config.get(cls._state_names.MAX_BLOCK_SIZE, 50),
             cls._state_names.MIN_BLOCK_SIZE: config.get(cls._state_names.MIN_BLOCK_SIZE, 5),
             cls._state_names.HW_FUSED_OPS: config.get(cls._state_names.HW_FUSED_OPS, True),
             cls._state_names.SKIPPED_BLOCKS: config.get(cls._state_names.SKIPPED_BLOCKS),
         }
         return cls(**kwargs)
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'ElasticDepthParams':
+    def from_state(cls, state: Dict[str, Any]) -> "ElasticDepthParams":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
@@ -340,35 +344,40 @@
         return {
             self._state_names.MAX_BLOCK_SIZE: self.max_block_size,
             self._state_names.MIN_BLOCK_SIZE: self.min_block_size,
             self._state_names.HW_FUSED_OPS: self.hw_fused_ops,
             self._state_names.SKIPPED_BLOCKS: self.skipped_blocks,
         }
 
-    def __eq__(self, other: 'ElasticDepthParams') -> bool:
+    def __eq__(self, other: "ElasticDepthParams") -> bool:
         return self.__dict__ == other.__dict__
 
     def __str__(self):
-        return f"{self.__class__.__name__}: max_block_size: {self.max_block_size} " \
-               f"min_block_size: {self.min_block_size} " \
-               f"hw_fused_ops: {self.hw_fused_ops} skipped_blocks: {self.skipped_blocks}"
+        return (
+            f"{self.__class__.__name__}: max_block_size: {self.max_block_size} "
+            f"min_block_size: {self.min_block_size} "
+            f"hw_fused_ops: {self.hw_fused_ops} skipped_blocks: {self.skipped_blocks}"
+        )
 
 
 @ELASTICITY_BUILDERS.register(ElasticityDim.DEPTH)
 class ElasticDepthBuilder(SingleElasticityBuilder):
     """
     Determines which modifications should be made to the original FP32 model in order to introduce elastic depth
     to the model.
     """
+
     _state_names = EDBuilderStateNames
 
-    def __init__(self,
-                 params: ElasticDepthParams,
-                 ignored_scopes: Optional[List[str]] = None,
-                 target_scopes: Optional[List[str]] = None):
+    def __init__(
+        self,
+        params: ElasticDepthParams,
+        ignored_scopes: Optional[List[str]] = None,
+        target_scopes: Optional[List[str]] = None,
+    ):
         """
         :param params: parameters to configure elastic depth.
         :param ignored_scopes: A list of strings to match against NNCFGraph node names
           and ignore matching nodes. Ignored nodes will not considered for skipping.
         :param target_scopes: A list of strings to match against NNCFGraph and define a set of nodes
           to be considered for skipping. When `ignored_scopes` is a "denylist",
           the `target_scopes` is an "allowlist"; otherwise, same effects apply as for `ignored_scopes`.
@@ -385,29 +394,28 @@
         """
         Creates modifications to the given NNCFNetwork for introducing elastic depth and creates a handler object that
         can manipulate this elasticity.
 
         :param target_model: a target NNCFNetwork for adding modifications
         :return: a handler object that can manipulate the elastic depth.
         """
-        tracing_context = target_model.get_tracing_context()
+        tracing_context = target_model.nncf.get_tracing_context()
         tracing_context.elastic_depth = True
         blocks_for_info = self._skipped_blocks
         if self._skipped_blocks is None:
-            extended_blocks, self._skip_dependencies = \
-                get_building_blocks(
-                    target_model,
-                    max_block_size=self._params.max_block_size,
-                    min_block_size=self._params.min_block_size,
-                    hw_fused_ops=self._params.hw_fused_ops,
-                )
+            extended_blocks, self._skip_dependencies = get_building_blocks(
+                target_model,
+                max_block_size=self._params.max_block_size,
+                min_block_size=self._params.min_block_size,
+                hw_fused_ops=self._params.hw_fused_ops,
+            )
             self._skipped_blocks = [eb.basic_block for eb in extended_blocks]
             blocks_for_info = extended_blocks
         nncf_logger.info("Blocks for skipping (changing the depth of model):")
-        new_line = '\n'
+        new_line = "\n"
         str_bs = [str(block) for block in blocks_for_info]
         nncf_logger.info(f"\n[{new_line.join(str_bs)}]\n\n")
 
         tracing_context.set_elastic_blocks(self._skipped_blocks)
         node_names_per_block = self._get_node_names_per_block(target_model, self._skipped_blocks)
 
         return ElasticDepthHandler(target_model, self._skipped_blocks, self._skip_dependencies, node_names_per_block)
@@ -417,16 +425,18 @@
         Initializes object from the state.
 
         :param state: Output of `get_state()` method.
         """
         params_from_state = state[SingleElasticityBuilder._state_names.ELASTICITY_PARAMS]
         params = ElasticDepthParams.from_state(params_from_state)
         if self._params and self._params != params:
-            nncf_logger.warning('Different elasticity parameters were provided in two places: on init and on loading '
-                                'state. The one from state is taken by ignoring the ones from init.')
+            nncf_logger.warning(
+                "Different elasticity parameters were provided in two places: on init and on loading "
+                "state. The one from state is taken by ignoring the ones from init."
+            )
         self._params = params
         skipped_blocks_from_state = state[self._state_names.SKIPPED_BLOCKS]
         self._skip_dependencies = state[self._state_names.SKIPPED_BLOCKS_DEPENDENCIES]
         self._skipped_blocks = [BuildingBlock.from_state(bb_state) for bb_state in skipped_blocks_from_state]
 
     def get_state(self) -> Dict[str, Any]:
         """
@@ -442,15 +452,15 @@
             SingleElasticityBuilder._state_names.ELASTICITY_PARAMS: self._params.get_state(),
             self._state_names.SKIPPED_BLOCKS: skipped_blocks_from_state,
             self._state_names.SKIPPED_BLOCKS_DEPENDENCIES: self._skip_dependencies,
         }
 
     @staticmethod
     def _get_node_names_per_block(target_model: NNCFNetwork, skipped_blocks) -> Dict[int, List[NNCFNodeName]]:
-        graph = target_model.get_original_graph()
+        graph = target_model.nncf.get_original_graph()
         all_node_names_per_block = {}
         for idx, block in enumerate(skipped_blocks):
             simple_paths = graph.get_all_simple_paths(block.start_node_name, block.end_node_name)
 
             node_names_in_block = set()
             for node_keys_in_path in simple_paths:
                 for node_key in node_keys_in_path:
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_kernel.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_kernel.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,42 +1,36 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import random
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Optional
+from typing import Any, Callable, Dict, List, Optional
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 from torch.nn import Parameter
 
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.logging import nncf_logger
-from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import BaseElasticityParams
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_BUILDERS
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_HANDLERS_MAP
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_PARAMS
+from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import BaseElasticityParams
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import SingleElasticityBuilder
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import SingleElasticityHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.layers import NNCFConv2d
 from nncf.torch.module_operations import UpdateInputs
@@ -48,43 +42,42 @@
 
 KernelSizeType = int
 ElasticKernelConfig = List[KernelSizeType]  # list of kernel sizes per layer
 ElasticKernelSearchSpace = List[List[KernelSizeType]]
 
 
 class EKParamsStateNames:
-    MAX_NUM_KERNELS = 'max_num_kernels'
+    MAX_NUM_KERNELS = "max_num_kernels"
 
 
 @ELASTICITY_PARAMS.register(ElasticityDim.KERNEL)
 class ElasticKernelParams(BaseElasticityParams):
     _state_names = EKParamsStateNames
 
-    def __init__(self,
-                 max_num_kernels: int = -1):
+    def __init__(self, max_num_kernels: int = -1):
         """
         Constructor
 
         :param max_num_kernels: Restricts total number of different elastic kernel values for each layer.
         The default value is -1 means that there's no restrictions.
         """
         self.max_num_kernels = max_num_kernels
 
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'ElasticKernelParams':
+    def from_config(cls, config: Dict[str, Any]) -> "ElasticKernelParams":
         """
         Creates the object from its config.
         """
         kwargs = {
             cls._state_names.MAX_NUM_KERNELS: config.get(cls._state_names.MAX_NUM_KERNELS, -1),
         }
         return cls(**kwargs)
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'ElasticKernelParams':
+    def from_state(cls, state: Dict[str, Any]) -> "ElasticKernelParams":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
@@ -94,15 +87,15 @@
 
         :return: The compression loss state.
         """
         return {
             self._state_names.MAX_NUM_KERNELS: self.max_num_kernels,
         }
 
-    def __eq__(self, other: 'ElasticKernelParams') -> bool:
+    def __eq__(self, other: "ElasticKernelParams") -> bool:
         return self.__dict__ == other.__dict__
 
 
 class ElasticKernelOp:
     """
     Base class for introducing elastic kernel for the operations. On the forward pass it takes parameters of operations
     and modifies in the way that kernel size is changing to a given value.
@@ -132,16 +125,19 @@
         Sets current level of elasticity for the operation - kernel size value that parameters should have.
         The actual modification of parameters happens on forward call.
         The value should be less the original kernel size and more than one.
 
         :param kernel_size: kernel size value
         """
         if kernel_size is None or kernel_size > self.max_kernel_size or kernel_size < 1:
-            raise AttributeError('Invalid kernel size={} in scope={}.\nIt should be within the range: [1, {}]'.format(
-                kernel_size, self.node_name, self.max_kernel_size))
+            raise AttributeError(
+                "Invalid kernel size={} in scope={}.\nIt should be within the range: [1, {}]".format(
+                    kernel_size, self.node_name, self.max_kernel_size
+                )
+            )
 
         self._active_kernel_size = kernel_size
 
     @property
     def kernel_size_list(self) -> List[KernelSizeType]:
         """
         Gets list of all available kernel sizes to select from. Each value corresponds to a single element in the
@@ -168,17 +164,15 @@
 
 class ElasticKernelConv2DOp(ElasticKernelOp, nn.Module):
     """
     Introduces elastic kernel for the 2D convolution. On the forward pass it takes parameters of operations
     and modifies in the way that kernel size is changing to a given value.
     """
 
-    def __init__(self, max_kernel_size: KernelSizeType,
-                 node_name: NNCFNodeName,
-                 params: ElasticKernelParams):
+    def __init__(self, max_kernel_size: KernelSizeType, node_name: NNCFNodeName, params: ElasticKernelParams):
         """
         Constructor.
 
         :param max_kernel_size: maximum kernel size value in the original operation.
         :param node_name: string representation of operation address. It's used for more informative messages only.
         :param params: parameters to configure elastic kernel for the operation.
         """
@@ -189,29 +183,29 @@
         self._ks_set = list(set(self.kernel_size_list))
         self._ks_set.sort()
 
         scale_params = {}
         for i in range(len(self._ks_set) - 1):
             ks_small = self._ks_set[i]
             ks_larger = self._ks_set[i + 1]
-            param_name = '%dto%d' % (ks_larger, ks_small)
+            param_name = "%dto%d" % (ks_larger, ks_small)
             # noinspection PyArgumentList
-            scale_params['%s_matrix' % param_name] = Parameter(torch.eye(ks_small ** 2))
+            scale_params["%s_matrix" % param_name] = Parameter(torch.eye(ks_small**2))
         for name, param in scale_params.items():
             self.register_parameter(name, param)
 
     def generate_kernel_size_list(self, max_kernel_size: KernelSizeType) -> List[KernelSizeType]:
         """
         Generates list of available kernel size values.
 
         :param max_kernel_size: maximum value of kernel size, it's supposed to be odd
         :return: list of kernel size values.
         """
         DEFAULT_KERNEL_SIZE_STEP = 2
-        assert max_kernel_size % 2 > 0, 'kernel size should be odd number'
+        assert max_kernel_size % 2 > 0, "kernel size should be odd number"
         if max_kernel_size == 1:
             return [1]
         kernel = max_kernel_size
         ks_list = []
         while kernel > 1:
             ks_list.append(kernel)
             kernel -= DEFAULT_KERNEL_SIZE_STEP
@@ -223,15 +217,15 @@
         """
         Modifies weight to have kernel size equals to active kernel size value.
 
         :param weight: weight tensor to be modified
         :return: modified weight
         """
         kernel_size = self.get_active_kernel_size()
-        nncf_logger.debug(f'Conv2d with active kernel size={kernel_size} in scope={self.node_name}')
+        nncf_logger.debug(f"Conv2d with active kernel size={kernel_size} in scope={self.node_name}")
 
         result = weight
         if is_tracing_state():
             with no_jit_trace():
                 if kernel_size > 1:
                     result = self._get_active_filter(kernel_size, weight).detach()
         else:
@@ -243,19 +237,18 @@
         """
         Sets current level of elasticity for the operation - kernel size value that parameters should have.
         The actual modification of parameters happens on forward call.
         The value should be less the original kernel size and more than one.
 
         :param kernel_size: kernel size value
         """
-        nncf_logger.debug(f'set active elastic_kernel={kernel_size} in scope={self.node_name}')
-        assert kernel_size % 2 > 0, 'kernel size should be odd number'
+        nncf_logger.debug(f"set active elastic_kernel={kernel_size} in scope={self.node_name}")
+        assert kernel_size % 2 > 0, "kernel size should be odd number"
         if kernel_size not in self.kernel_size_list and kernel_size != self.max_kernel_size:
-            raise ValueError(
-                'invalid kernel size to set. Should be a number in {}'.format(self.kernel_size_list))
+            raise ValueError("invalid kernel size to set. Should be a number in {}".format(self.kernel_size_list))
         super().set_active_kernel_size(kernel_size)
 
     @staticmethod
     def _sub_filter_start_end(kernel_size, sub_kernel_size):
         center = kernel_size // 2
         dev = sub_kernel_size // 2
         start, end = center - dev, center + dev + 1
@@ -275,17 +268,18 @@
                 target_ks = self._ks_set[i - 1]
                 start, end = self._sub_filter_start_end(src_ks, target_ks)
                 _input_filter = start_filter[:, :, start:end, start:end]
                 _input_filter = _input_filter.contiguous()
                 _input_filter = _input_filter.view(_input_filter.size(0), _input_filter.size(1), -1)
                 _input_filter = _input_filter.view(-1, _input_filter.size(2))
                 _input_filter = F.linear(
-                    _input_filter, self.__getattr__('%dto%d_matrix' % (src_ks, target_ks)),
+                    _input_filter,
+                    self.__getattr__("%dto%d_matrix" % (src_ks, target_ks)),
                 )
-                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks ** 2)
+                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks**2)
                 _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks, target_ks)
                 start_filter = _input_filter
             filters = start_filter
         return filters
 
 
 class ElasticKernelPaddingAdjustment:
@@ -315,26 +309,24 @@
         self._max_kernel_size = max_kernel_size
 
     def __call__(self, x) -> int:
         active_kernel_size = self._elastic_k_w_op.get_active_kernel_size()
         diff = (self._max_kernel_size - active_kernel_size) // 2
         h = x.size(2)
         w = x.size(3)
-        new_x = x[:, :, diff:h - diff, diff:w - diff]
+        new_x = x[:, :, diff : h - diff, diff : w - diff]
         return new_x
 
 
 class ElasticKernelHandler(SingleElasticityHandler):
     """
     An interface for handling elastic kernel dimension in the network, i.e. define size of kernels in the conv layers.
     """
 
-    def __init__(self,
-                 elastic_kernel_ops: List[ElasticKernelOp],
-                 transformation_commands: List[TransformationCommand]):
+    def __init__(self, elastic_kernel_ops: List[ElasticKernelOp], transformation_commands: List[TransformationCommand]):
         super().__init__()
         self._elastic_kernel_ops = elastic_kernel_ops
         self._transformation_commands = transformation_commands
 
     def get_transformation_commands(self) -> List[TransformationCommand]:
         """
         :return: transformation commands for introducing the elasticity to NNCFNetwork
@@ -404,17 +396,17 @@
         """
         active_kernel_sizes = {}
         for elastic_kernel_op in self._elastic_kernel_ops:
             active_kernel_size = elastic_kernel_op.get_active_kernel_size()
             active_kernel_sizes[elastic_kernel_op.node_name] = (active_kernel_size, active_kernel_size)
         return active_kernel_sizes
 
-    def resolve_conflicts_with_other_elasticities(self,
-                                                  config: ElasticKernelConfig,
-                                                  elasticity_handlers: ELASTICITY_HANDLERS_MAP) -> ElasticKernelConfig:
+    def resolve_conflicts_with_other_elasticities(
+        self, config: ElasticKernelConfig, elasticity_handlers: ELASTICITY_HANDLERS_MAP
+    ) -> ElasticKernelConfig:
         """
         Resolves a conflict between the given elasticity config and active elasticity configs of the given handlers.
         For example, elastic width configuration may contradict to elastic depth one. When we activate some
         configuration in the Elastic Width Handler, i.e. define number of output channels for some layers, we
         change output shapes of the layers. Consequently, it affects the blocks that can be skipped by Elastic Depth
         Handler, because input and output shapes may not be identical now.
 
@@ -425,24 +417,27 @@
         return config
 
     def _collect_ops_data_by_selection_rule(self, selection_rule: Callable) -> List[Any]:
         return list(map(selection_rule, self._elastic_kernel_ops))
 
 
 class EKBuilderStateNames:
-    NODE_NAMES_TO_MAKE_ELASTIC = 'node_names_to_make_elastic'
+    NODE_NAMES_TO_MAKE_ELASTIC = "node_names_to_make_elastic"
 
 
 @ELASTICITY_BUILDERS.register(ElasticityDim.KERNEL)
 class ElasticKernelBuilder(SingleElasticityBuilder):
     _state_names = EKBuilderStateNames
 
-    def __init__(self, params: ElasticKernelParams,
-                 ignored_scopes: Optional[List[str]] = None,
-                 target_scopes: Optional[List[str]] = None):
+    def __init__(
+        self,
+        params: ElasticKernelParams,
+        ignored_scopes: Optional[List[str]] = None,
+        target_scopes: Optional[List[str]] = None,
+    ):
         super().__init__(ignored_scopes, target_scopes)
         self._node_names_to_make_elastic = []  # type: List[NNCFNodeName]
         self._params = params
 
     def build(self, target_model: NNCFNetwork) -> ElasticKernelHandler:
         """
         Creates modifications to the given NNCFNetwork for introducing elastic kernel and creates a handler object that
@@ -450,80 +445,71 @@
 
         :param target_model: a target NNCFNetwork for adding modifications
         :return: a handler object that can manipulate the elastic kernel.
         """
         elastic_kernel_ops = []  # type: List[ElasticKernelOp]
         transformation_commands = []
 
-        graph = target_model.get_original_graph()
+        graph = target_model.nncf.get_original_graph()
         device = next(target_model.parameters()).device
         pad_commands = []
 
         if not self._node_names_to_make_elastic:
             elastic_kernel_types = [NNCFConv2d.op_func_name]
             all_elastic_kernel_nodes = graph.get_nodes_by_types(elastic_kernel_types)  # type: List[NNCFNode]
             self._node_names_to_make_elastic = [node.node_name for node in all_elastic_kernel_nodes]
 
         for node_name in self._node_names_to_make_elastic:
             nncf_logger.debug(f"Adding Elastic Kernel op for Conv2D in scope: {node_name}")
             node = graph.get_node_by_name(node_name)
             layer_attrs = node.layer_attributes
-            assert isinstance(layer_attrs, ConvolutionLayerAttributes), 'Conv2D can have elastic kernel only'
+            assert isinstance(layer_attrs, ConvolutionLayerAttributes), "Conv2D can have elastic kernel only"
             max_kernel_size = layer_attrs.kernel_size[0]
             elastic_kernel_op = ElasticKernelConv2DOp(max_kernel_size, node_name, self._params)
             elastic_kernel_op.to(device)
             update_conv_params_op = UpdateWeight(elastic_kernel_op)
             transformation_commands.append(
                 PTInsertionCommand(
-                    PTTargetPoint(
-                        TargetType.PRE_LAYER_OPERATION,
-                        target_node_name=node_name
-                    ),
+                    PTTargetPoint(TargetType.PRE_LAYER_OPERATION, target_node_name=node_name),
                     update_conv_params_op,
-                    TransformationPriority.PRUNING_PRIORITY
+                    TransformationPriority.PRUNING_PRIORITY,
                 )
             )
             # TODO(nlyalyus): ticket 71613: remove hardcode for EfficientNet
             #   should be pairing of pad and conv operations by graph analysis
             #   get
             #       1) input size before pad
             #       2) input size after pad
             #       3) active kernel size
             #       4) max kernel size
             #       5) built-in conv padding
             #   output
             #       how to change padded input (e.g. center crop)
-            if 'NNCFUserConv2dStaticSamePadding' in node.node_name:
+            if "NNCFUserConv2dStaticSamePadding" in node.node_name:
                 if max_kernel_size >= 3:
                     crop_op = ElasticKernelInputForExternalPadding(elastic_kernel_op, max_kernel_size)
                     op = UpdateInputs(crop_op).to(device)
-                    nncf_logger.debug(f'Padded input will be cropped for {node_name}')
+                    nncf_logger.debug(f"Padded input will be cropped for {node_name}")
                     pad_commands.append(
                         PTInsertionCommand(
-                            PTTargetPoint(
-                                target_type=TargetType.PRE_LAYER_OPERATION,
-                                target_node_name=node_name
-                            ),
+                            PTTargetPoint(target_type=TargetType.PRE_LAYER_OPERATION, target_node_name=node_name),
                             op,
-                            TransformationPriority.DEFAULT_PRIORITY
+                            TransformationPriority.DEFAULT_PRIORITY,
                         )
                     )
             else:
                 # Padding
                 ap = ElasticKernelPaddingAdjustment(elastic_kernel_op)
                 pad_op = UpdatePadding(ap).to(device)
-                nncf_logger.debug(f'Padding will be adjusted for {node_name}')
+                nncf_logger.debug(f"Padding will be adjusted for {node_name}")
                 pad_commands.append(
                     PTInsertionCommand(
-                        PTTargetPoint(
-                            target_type=TargetType.PRE_LAYER_OPERATION,
-                            target_node_name=node_name
-                        ),
+                        PTTargetPoint(target_type=TargetType.PRE_LAYER_OPERATION, target_node_name=node_name),
                         pad_op,
-                        TransformationPriority.DEFAULT_PRIORITY
+                        TransformationPriority.DEFAULT_PRIORITY,
                     )
                 )
             elastic_kernel_ops.append(elastic_kernel_op)
         if pad_commands:
             transformation_commands += pad_commands
 
         return ElasticKernelHandler(elastic_kernel_ops, transformation_commands)
@@ -533,23 +519,25 @@
         Initializes object from the state.
 
         :param state: Output of `get_state()` method.
         """
         params_from_state = state[SingleElasticityBuilder._state_names.ELASTICITY_PARAMS]
         params = ElasticKernelParams.from_state(params_from_state)
         if self._params and self._params != params:
-            nncf_logger.warning('Different elasticity parameters were provided in two places: on init and on loading '
-                                'state. The one from state is taken by ignoring the ones from init.')
+            nncf_logger.warning(
+                "Different elasticity parameters were provided in two places: on init and on loading "
+                "state. The one from state is taken by ignoring the ones from init."
+            )
         self._params = params
         self._node_names_to_make_elastic = state[self._state_names.NODE_NAMES_TO_MAKE_ELASTIC]
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
         return {
             SingleElasticityBuilder._state_names.ELASTICITY_PARAMS: self._params.get_state(),
-            self._state_names.NODE_NAMES_TO_MAKE_ELASTIC: self._node_names_to_make_elastic
+            self._state_names.NODE_NAMES_TO_MAKE_ELASTIC: self._node_names_to_make_elastic,
         }
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_width.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elastic_width.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,81 +1,74 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 # pylint: disable=too-many-lines,too-many-nested-blocks,too-many-branches
 
 import random
 from collections import OrderedDict
 from copy import deepcopy
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
+from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import torch
 from torch import nn
 
 from nncf.common.graph import BaseLayerAttributes
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.layer_attributes import GenericWeightedLayerAttributes
 from nncf.common.graph.layer_attributes import LinearLayerAttributes
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TransformationPriority
+from nncf.common.logging import nncf_logger
 from nncf.common.pruning.clusterization import Cluster
 from nncf.common.pruning.clusterization import Clusterization
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
 from nncf.common.pruning.node_selector import PruningNodeSelector
+from nncf.common.pruning.shape_pruning_processor import ShapePruningProcessor
 from nncf.common.pruning.structs import PrunedLayerInfoBase
 from nncf.common.pruning.utils import get_input_masks
 from nncf.common.pruning.utils import get_prunable_layers_in_out_channels
 from nncf.common.pruning.utils import is_prunable_depthwise_conv
-from nncf.common.pruning.shape_pruning_processor import ShapePruningProcessor
 from nncf.common.tensor import NNCFTensor
-from nncf.common.logging import nncf_logger
-from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import BaseElasticityParams
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_BUILDERS
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_HANDLERS_MAP
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ELASTICITY_PARAMS
+from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import BaseElasticityParams
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import SingleElasticityBuilder
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import SingleElasticityHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.filter_reorder import FilterReorderingAlgorithm
 from nncf.torch.graph.graph import PTNNCFGraph
+from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
 from nncf.torch.graph.operator_metatypes import PTModuleBatchNormMetatype
-from nncf.torch.graph.operator_metatypes import PTModuleLayerNormMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConv2dMetatype
-from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
+from nncf.torch.graph.operator_metatypes import PTModuleLayerNormMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleLinearMetatype
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.layers import NNCFConv2d
 from nncf.torch.layers import NNCFLinear
 from nncf.torch.module_operations import UpdateBatchNormParams
 from nncf.torch.module_operations import UpdateLayerNormParams
 from nncf.torch.module_operations import UpdateNumGroups
 from nncf.torch.module_operations import UpdateWeight
 from nncf.torch.module_operations import UpdateWeightAndOptionalBias
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.pruning.filter_pruning.functions import FILTER_IMPORTANCE_FUNCTIONS
-from nncf.torch.pruning.operations import PTElementwisePruningOp
 from nncf.torch.pruning.operations import PT_PRUNING_OPERATOR_METATYPES
+from nncf.torch.pruning.operations import PTElementwisePruningOp
 from nncf.torch.pruning.tensor_processor import PTNNCFPruningTensorProcessor
 from nncf.torch.tensor import PTNNCFTensor
 from nncf.torch.utils import get_filters_num
 from nncf.torch.utils import get_model_device
 
 PruningGroupID = int
 WidthType = int
@@ -122,44 +115,49 @@
         The actual trimming of specified number of channels happens on forward call.
         The value should be less the original width and more than one. Zero number of channels is
         supported through Dynamic Depth feature.
 
         :param width: number of channels
         """
         if width is None or width > self._max_width or width < 1:
-            raise AttributeError('Invalid width={} in scope={}.\nIt should be within the range: [1, {}]'.format(
-                width, self._node_name, self._max_width))
+            raise AttributeError(
+                "Invalid width={} in scope={}.\nIt should be within the range: [1, {}]".format(
+                    width, self._node_name, self._max_width
+                )
+            )
 
         self._active_width = width
 
 
 class EWParamsStateNames:
-    MIN_WIDTH = 'min_width'
-    MAX_NUM_WIDTHS = 'max_num_widths'
-    WIDTH_STEP = 'width_step'
-    WIDTH_MULTIPLIERS = 'width_multipliers'
-    FILTER_IMPORTANCE = 'filter_importance'
-    OVERWRITE_GROUPS = 'overwrite_groups'
-    OVERWRITE_GROUPS_WIDTHS = 'overwrite_groups_widths'
-    ADD_DYNAMIC_INPUTS = 'add_dynamic_inputs'
+    MIN_WIDTH = "min_width"
+    MAX_NUM_WIDTHS = "max_num_widths"
+    WIDTH_STEP = "width_step"
+    WIDTH_MULTIPLIERS = "width_multipliers"
+    FILTER_IMPORTANCE = "filter_importance"
+    OVERWRITE_GROUPS = "overwrite_groups"
+    OVERWRITE_GROUPS_WIDTHS = "overwrite_groups_widths"
+    ADD_DYNAMIC_INPUTS = "add_dynamic_inputs"
 
 
 @ELASTICITY_PARAMS.register(ElasticityDim.WIDTH)
 class ElasticWidthParams(BaseElasticityParams):
     _state_names = EWParamsStateNames
 
-    def __init__(self,
-                 min_width: int,
-                 max_num_widths: int,
-                 width_step: int,
-                 width_multipliers: List[float],
-                 filter_importance: str,
-                 overwrite_groups: Optional[List[str]] = None,
-                 overwrite_groups_widths: Optional[List[str]] = None,
-                 add_dynamic_inputs: Optional[List[str]] = None):
+    def __init__(
+        self,
+        min_width: int,
+        max_num_widths: int,
+        width_step: int,
+        width_multipliers: List[float],
+        filter_importance: str,
+        overwrite_groups: Optional[List[str]] = None,
+        overwrite_groups_widths: Optional[List[str]] = None,
+        add_dynamic_inputs: Optional[List[str]] = None,
+    ):
         """
         Constructor
 
         :param min_width: Minimal number of output channels that can be activated for each layers with elastic width.
         Default value is 32.
         :param max_num_widths: Restricts total number of different elastic width values for each layer.
         The default value is -1 means that there's no restrictions.
@@ -182,32 +180,32 @@
         self.filter_importance = filter_importance
 
         self.overwrite_groups = overwrite_groups
         self.overwrite_groups_widths = overwrite_groups_widths
         self.add_dynamic_inputs = add_dynamic_inputs
 
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'ElasticWidthParams':
+    def from_config(cls, config: Dict[str, Any]) -> "ElasticWidthParams":
         """
         Creates the object from its config.
         """
         kwargs = {
             cls._state_names.MIN_WIDTH: config.get(cls._state_names.MIN_WIDTH, 32),
             cls._state_names.MAX_NUM_WIDTHS: config.get(cls._state_names.MAX_NUM_WIDTHS, -1),
             cls._state_names.WIDTH_STEP: config.get(cls._state_names.WIDTH_STEP, 32),
             cls._state_names.WIDTH_MULTIPLIERS: config.get(cls._state_names.WIDTH_MULTIPLIERS),
-            cls._state_names.FILTER_IMPORTANCE: config.get(cls._state_names.FILTER_IMPORTANCE, 'L1'),
+            cls._state_names.FILTER_IMPORTANCE: config.get(cls._state_names.FILTER_IMPORTANCE, "L1"),
             cls._state_names.OVERWRITE_GROUPS: config.get(cls._state_names.OVERWRITE_GROUPS, None),
             cls._state_names.OVERWRITE_GROUPS_WIDTHS: config.get(cls._state_names.OVERWRITE_GROUPS_WIDTHS, None),
-            cls._state_names.ADD_DYNAMIC_INPUTS: config.get(cls._state_names.ADD_DYNAMIC_INPUTS, None)
+            cls._state_names.ADD_DYNAMIC_INPUTS: config.get(cls._state_names.ADD_DYNAMIC_INPUTS, None),
         }
         return cls(**kwargs)
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'ElasticWidthParams':
+    def from_state(cls, state: Dict[str, Any]) -> "ElasticWidthParams":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
@@ -221,49 +219,53 @@
             self._state_names.MIN_WIDTH: self.min_width,
             self._state_names.MAX_NUM_WIDTHS: self.max_num_widths,
             self._state_names.WIDTH_STEP: self.width_step,
             self._state_names.WIDTH_MULTIPLIERS: self.width_multipliers,
             self._state_names.FILTER_IMPORTANCE: self.filter_importance,
             self._state_names.OVERWRITE_GROUPS: self.overwrite_groups,
             self._state_names.OVERWRITE_GROUPS_WIDTHS: self.overwrite_groups_widths,
-            self._state_names.ADD_DYNAMIC_INPUTS: self.add_dynamic_inputs
+            self._state_names.ADD_DYNAMIC_INPUTS: self.add_dynamic_inputs,
         }
 
-    def __eq__(self, other: 'ElasticWidthParams') -> bool:
+    def __eq__(self, other: "ElasticWidthParams") -> bool:
         return self.__dict__ == other.__dict__
 
     def __str__(self):
-        return f"{self.__class__.__name__}: width_step: {self.width_step} " \
-               f"min_width: {self.min_width} width_multipliers: {self.width_multipliers} " \
-               f"max_num_widths: {self.max_num_widths} overwrite_groups: {self.overwrite_groups} " \
-               f"overwrite_group_widths: {self.overwrite_groups_widths}"
+        return (
+            f"{self.__class__.__name__}: width_step: {self.width_step} "
+            f"min_width: {self.min_width} width_multipliers: {self.width_multipliers} "
+            f"max_num_widths: {self.max_num_widths} overwrite_groups: {self.overwrite_groups} "
+            f"overwrite_group_widths: {self.overwrite_groups_widths}"
+        )
 
 
 class ElasticOutputWidthOp(ElasticWidthOp):
     """
     Base class for trimming output channels (output width) of the operations.
     """
 
-    def __init__(self, max_width: int, node_name: str, params: ElasticWidthParams,
-                 fixed_width_list: Optional[List[int]] = None):
+    def __init__(
+        self, max_width: int, node_name: str, params: ElasticWidthParams, fixed_width_list: Optional[List[int]] = None
+    ):
         """
         Constructor.
 
         :param max_width: maximum number of output channels in the original operation.
         :param node_name: string representation of operation address. It's used for more informative messages only.
         :param params: parameters to configure elastic width for the operation.
         """
         super().__init__(max_width=max_width, node_name=node_name)
         if fixed_width_list is None:
             fixed_width_list = []
         if fixed_width_list:
             fixed_width_list.sort(reverse=True)
             if fixed_width_list[0] > max_width:
-                raise RuntimeError(f"Width list for {node_name} "
-                                   f"contains invalid values: {fixed_width_list}, {max_width}")
+                raise RuntimeError(
+                    f"Width list for {node_name} " f"contains invalid values: {fixed_width_list}, {max_width}"
+                )
             if fixed_width_list[0] != max_width:
                 raise RuntimeError(f"Max width for {node_name} is not aligned with pre-trained model")
             self._width_list = fixed_width_list
         else:
             self._width_list = self._generate_width_list(self._max_width, params)
 
     @property
@@ -286,16 +288,18 @@
         The actual trimming of specified number of channels happens on forward call.
         The value should be less the original width and more than one. Zero number of channels is
         supported through Dynamic Depth feature.
 
         :param width: number of output channels
         """
         if width not in self.width_list and width != self.max_width:
-            raise ValueError(f'Invalid number of output channels to set: {width} in scope={self._node_name}. '
-                             f'Should be a number in {self.width_list}')
+            raise ValueError(
+                f"Invalid number of output channels to set: {width} in scope={self._node_name}. "
+                f"Should be a number in {self.width_list}"
+            )
         super().set_active_width(width)
 
     @staticmethod
     def _generate_width_list(max_width: int, params: ElasticWidthParams) -> List[int]:
         """
         Generates list of available width values.
         There are two mutually exclusive modes: using `width_step` and using `width_multipliers`.
@@ -344,58 +348,61 @@
 
 
 class ElasticWidthInfo(PrunedLayerInfoBase):
     """
     List of attributes describing operation with elastic width
     """
 
-    def __init__(self,
-                 node_name: NNCFNodeName,
-                 module: nn.Module,
-                 elastic_op: ElasticOutputWidthOp,
-                 node_id: int,
-                 is_depthwise: bool):
+    def __init__(
+        self,
+        node_name: NNCFNodeName,
+        module: nn.Module,
+        elastic_op: ElasticOutputWidthOp,
+        node_id: int,
+        is_depthwise: bool,
+    ):
         super().__init__(node_name, node_id, is_depthwise)
         self.module = module
         self.elastic_op = elastic_op
 
     def __str__(self):
-        return f"{self.__class__.__name__}: node_name: {self.node_name} module: {self.module} " \
-               f"elastic_op: {self.elastic_op} node_id: {self.nncf_node_id} is_depthwise: {self.is_depthwise}"
-
+        return (
+            f"{self.__class__.__name__}: node_name: {self.node_name} module: {self.module} "
+            f"elastic_op: {self.elastic_op} node_id: {self.nncf_node_id} is_depthwise: {self.is_depthwise}"
+        )
 
 
 class ElasticInputWidthLinearOp(ElasticWidthOp, nn.Module):
     """
     Introduces elastic input width for linear layer.
     """
 
     def forward(self, weight: torch.Tensor) -> torch.Tensor:
         """
         Trims weight according to active number of input channels
 
         :param weight: weight tensor to be trimmed
         :return: trimmed weight
-        """""
-        return weight[:, :self._active_width]
+        """
+        return weight[:, : self._active_width]
 
 
 class ElasticInputWidthConvOp(ElasticWidthOp, nn.Module):
     """
     Introduces elastic input width for 2D convolution.
     """
 
     def forward(self, weight: torch.Tensor) -> torch.Tensor:
         """
         Trims weight according to active number of input channels
 
         :param weight: weight tensor to be trimmed
         :return: trimmed weight
-        """""
-        return weight[:, :self._active_width, :, :]
+        """
+        return weight[:, : self._active_width, :, :]
 
 
 class ElasticInputWidthDWConvOp(ElasticWidthOp, nn.Module):
     """
     Introduces elastic input width for depthwise convolution.
     """
 
@@ -407,41 +414,42 @@
         return self._active_width
 
 
 class ElasticInputWidthBatchNormOp(ElasticWidthOp, nn.Module):
     """
     Introduces elastic input width for batchnorm layer.
     """
+
     SET_RUNNING_STATISTICS = False
 
     def forward(self, **bn_params: torch.Tensor) -> List[torch.Tensor]:
         """
         Trims batchnorm parameters according to active number of input channels.
 
         :param bn_params: map of name and tensor to be trimmed
         :return: trimmed batchnorm parameters
-        """""
-        return [param[:self._active_width] for param in bn_params.values()]
+        """
+        return [param[: self._active_width] for param in bn_params.values()]
 
 
 class ElasticInputWidthLayerNormOp(ElasticWidthOp, nn.Module):
     """
     Introduces elastic input width for layernorm layer.
     """
 
     def forward(self, weight: torch.Tensor, bias: torch.Tensor, normalized_shape: torch.Tensor) -> List[torch.Tensor]:
         """
         Trims layernorm parameters according to active number of input channels.
         :param weight: weight tensor to be trimmed
         :param bias: bias tensor to be trimmed
         :param normalized_shape: normalized_shape to be trimmed
         :return: list of trimmed layernorm parameters
-        """""
+        """
         assert len(normalized_shape) == 1, "Currently only 1-dimensional shape is supported."
-        return [weight[:self._active_width], bias[:self._active_width], (self._active_width,)]
+        return [weight[: self._active_width], bias[: self._active_width], (self._active_width,)]
 
 
 class ElasticOutputWidthConv2DOp(ElasticOutputWidthOp, nn.Module):
     """
     Introduces elastic output width for 2D convolution.
     """
 
@@ -449,15 +457,15 @@
         """
         Trims convolution parameters according to active number of output channels.
 
         :param weight: weight tensor to be trimmed
         :param bias: bias tensor to be trimmed
         :return: list of trimmed convolution parameters
         """
-        nncf_logger.debug(f'Conv2d with active width={self._active_width} in scope={self._node_name}')
+        nncf_logger.debug(f"Conv2d with active width={self._active_width} in scope={self._node_name}")
         num_out_channels = self._active_width
         new_bias = None if bias is None else bias[:num_out_channels]
         new_weights = weight[:num_out_channels, :, :, :]
         return [new_weights, new_bias]
 
 
 class ElasticOutputWidthLinearOp(ElasticOutputWidthOp, nn.Module):
@@ -469,34 +477,39 @@
         """
         Trims linear layer's parameters according to active number of output channels.
 
         :param weight: weight tensor to be trimmed
         :param bias: bias tensor to be trimmed
         :return: list of trimmed linear parameters
         """
-        new_bias = None if bias is None else bias[:self._active_width]
-        return [weight[:self._active_width, :], new_bias]
+        new_bias = None if bias is None else bias[: self._active_width]
+        return [weight[: self._active_width, :], new_bias]
+
 
 class EWHandlerStateNames:
-    WIDTH_NUM_PARAMS_INDICATOR = 'width_num_params_indicator'
+    WIDTH_NUM_PARAMS_INDICATOR = "width_num_params_indicator"
 
 
 class ElasticWidthHandler(SingleElasticityHandler):
     """
     An interface for handling elastic width dimension in the network, i.e. define number of channels in the layers.
     """
+
     _width_state_names = EWHandlerStateNames
 
-    def __init__(self, target_model: NNCFNetwork,
-                 filter_importance_fn: Callable[[torch.Tensor, int], torch.Tensor],
-                 weights_normalizer_fn: Optional[Callable[[torch.Tensor], torch.Tensor]],
-                 node_name_vs_dynamic_input_width_op_map: Dict[NNCFNodeName, ElasticWidthOp],
-                 pruned_module_groups_info: Clusterization[ElasticWidthInfo](id_fn=lambda x: x.node_name),
-                 transformation_commands: List[TransformationCommand],
-                 add_dynamic_inputs: Optional[List[str]] = None):
+    def __init__(
+        self,
+        target_model: NNCFNetwork,
+        filter_importance_fn: Callable[[torch.Tensor, int], torch.Tensor],
+        weights_normalizer_fn: Optional[Callable[[torch.Tensor], torch.Tensor]],
+        node_name_vs_dynamic_input_width_op_map: Dict[NNCFNodeName, ElasticWidthOp],
+        pruned_module_groups_info: Clusterization[ElasticWidthInfo](id_fn=lambda x: x.node_name),
+        transformation_commands: List[TransformationCommand],
+        add_dynamic_inputs: Optional[List[str]] = None,
+    ):
         """
         Constructor
 
         :param target_model: a target NNCFNetwork for adding elasticity.
         :param filter_importance_fn: a callable that implements calculation of the importance of filters along a given
         dimension for a given weight tensor.
         :param weights_normalizer_fn: a callable that implements normalization of weight tensor
@@ -510,19 +523,19 @@
         self._node_name_vs_dynamic_input_width_op_map = node_name_vs_dynamic_input_width_op_map
         self._pruned_module_groups_info = pruned_module_groups_info
         self._transformation_commands = transformation_commands
         self._filter_importance_fn = filter_importance_fn
         self._weights_normalizer_fn = weights_normalizer_fn
         self._add_dynamic_inputs = add_dynamic_inputs
 
-        graph = self._target_model.get_original_graph()
+        graph = self._target_model.nncf.get_original_graph()
         prunable_types = [NNCFConv2d.op_func_name, NNCFLinear.op_func_name]
         self._shape_pruning_processor = ShapePruningProcessor(
-            prunable_types=prunable_types,
-            pruning_operations_metatype=PT_PRUNING_OPERATOR_METATYPES)
+            prunable_types=prunable_types, pruning_operations_metatype=PT_PRUNING_OPERATOR_METATYPES
+        )
         self._next_nodes = self._shape_pruning_processor.get_next_nodes(graph, pruned_module_groups_info)
         # Need a copy because it will be used for adding `output_mask`/`input_masks` to nodes that are relevant to
         # Elastic Width only and therefore it should be isolated to not intercept with other algorithms.
         self._propagation_graph = deepcopy(graph)
 
         self._width_num_params_indicator = -1
 
@@ -570,15 +583,16 @@
     def get_search_space(self) -> ElasticWidthSearchSpace:
         """
         :return: search space that is produced by iterating over all elastic parameters
         """
         if self._width_num_params_indicator == -1:
             return self._collect_ops_data_by_selection_rule(lambda op: op.width_list)
         return self._collect_ops_data_by_selection_rule(
-            lambda op: op.width_list[:min(self._width_num_params_indicator, len(op.width_list))])
+            lambda op: op.width_list[: min(self._width_num_params_indicator, len(op.width_list))]
+        )
 
     def get_active_config(self) -> ElasticWidthConfig:
         """
         Forms an elasticity configuration that describes currently activated Subnet
 
         :return: map of pruning group id to width value
         """
@@ -620,63 +634,64 @@
     def activate_subnet_for_config(self, config: ElasticWidthConfig) -> None:
         """
         Activates a Subnet that corresponds to the given elasticity configuration
 
         :param config: map of pruning group id to width value
         """
         for node in self._propagation_graph.get_all_nodes():
-            node.data.pop('output_mask', None)
+            node.data.pop("output_mask", None)
 
         names_of_processed_nodes = set()
         for cluster_id, width in config.items():
             cluster = self._pruned_module_groups_info.get_cluster_by_id(cluster_id)
             for elastic_width_info in cluster.elements:
                 node_id = elastic_width_info.nncf_node_id
                 node = self._propagation_graph.get_node_by_id(node_id)
                 max_width = elastic_width_info.elastic_op.max_width
                 device = get_model_device(self._target_model)
                 mask = self._width_to_mask(width, max_width, device)
-                node.data['output_mask'] = mask
+                node.data["output_mask"] = mask
                 elastic_width_info.elastic_op.set_active_width(width)
                 names_of_processed_nodes.add(node_id)
 
         algo = MaskPropagationAlgorithm(
-            self._propagation_graph, PT_PRUNING_OPERATOR_METATYPES, PTNNCFPruningTensorProcessor)
+            self._propagation_graph, PT_PRUNING_OPERATOR_METATYPES, PTNNCFPruningTensorProcessor
+        )
         algo.mask_propagation()
 
         for node_name, dynamic_input_width_op in self._node_name_vs_dynamic_input_width_op_map.items():
             node = self._propagation_graph.get_node_by_name(node_name)
             input_masks = get_input_masks(node, self._propagation_graph)
             was_set = False
             if input_masks:
                 input_mask = input_masks[0]
                 input_width = self.mask_to_width(input_mask)
                 if input_width:
                     dynamic_input_width_op.set_active_width(input_width)
                     was_set = True
 
             if not was_set and node_name not in names_of_processed_nodes:
-                nncf_logger.debug(f'input width was not set in scope={node.node_name}')
+                nncf_logger.debug(f"input width was not set in scope={node.node_name}")
 
             if self._add_dynamic_inputs:
                 if node_name in self._add_dynamic_inputs and not was_set:
                     nncf_logger.debug(f"setting input width by user's request for scope={node_name}")
                     nodes_to_check = [node]
                     while any(elem is None for elem in input_masks):
                         previous_nodes = []
                         for node in nodes_to_check:
                             previous_nodes.append(self._propagation_graph.get_previous_nodes(node))
                         nodes_to_check.clear()
                         previous_nodes = [item for nodes in previous_nodes for item in nodes]
                         if not previous_nodes:
                             break
                         for previous in previous_nodes:
-                            if 'output_mask' in previous.data:
-                                if previous.data['output_mask'] is not None:
-                                    input_masks.append(previous.data['output_mask'])
+                            if "output_mask" in previous.data:
+                                if previous.data["output_mask"] is not None:
+                                    input_masks.append(previous.data["output_mask"])
                                     input_masks = [i for i in input_masks if i]
                                 else:
                                     nodes_to_check.append(previous)
                             else:
                                 nodes_to_check.append(previous)
                     if input_masks:
                         input_mask = input_masks[0]
@@ -692,33 +707,34 @@
         Collects the active number of input and output channels (width) for each elastic layer in the graph.
 
         :return Dictionary with the number of input channels to convolution and linear layers:
             {node_name: input_channels_num}
             Dictionary with the number of output channels from convolution and linear layers:
             {node_name: output_channels_num}
         """
-        graph = self._target_model.get_graph()
+        graph = self._target_model.nncf.get_graph()
         in_channels, out_channels = get_prunable_layers_in_out_channels(graph)
 
         for group in self._pruned_module_groups_info.get_all_clusters():
-            assert all(out_channels[group.elements[0].node_name] == out_channels[node.node_name]
-                       for node in group.elements)
+            assert all(
+                out_channels[group.elements[0].node_name] == out_channels[node.node_name] for node in group.elements
+            )
             first_elastic_width_info = group.elements[0]  # type: ElasticWidthInfo
             first_elastic_op = first_elastic_width_info.elastic_op  # type: ElasticOutputWidthOp
             new_out_channels_num = first_elastic_op.get_active_width()
             num_of_pruned_elems = first_elastic_op.max_width - new_out_channels_num
-            self._shape_pruning_processor.prune_cluster_shapes(group, num_of_pruned_elems,
-                                                               self._next_nodes,
-                                                               in_channels, out_channels)
+            self._shape_pruning_processor.prune_cluster_shapes(
+                group, num_of_pruned_elems, self._next_nodes, in_channels, out_channels
+            )
 
         return in_channels, out_channels
 
-    def resolve_conflicts_with_other_elasticities(self,
-                                                  config: ElasticWidthConfig,
-                                                  elasticity_handlers: ELASTICITY_HANDLERS_MAP) -> ElasticWidthConfig:
+    def resolve_conflicts_with_other_elasticities(
+        self, config: ElasticWidthConfig, elasticity_handlers: ELASTICITY_HANDLERS_MAP
+    ) -> ElasticWidthConfig:
         """
         Resolves a conflict between the given elasticity config and active elasticity configs of the given handlers.
         For example, elastic width configuration may contradict to elastic depth one. When we activate some
         configuration in the Elastic Width Handler, i.e. define number of output channels for some layers, we
         change output shapes of the layers. Consequently, it affects the blocks that can be skipped by Elastic Depth
         Handler, because input and output shapes may not be identical now.
 
@@ -744,45 +760,43 @@
         return group_id
 
     def reorganize_weights(self) -> None:
         """
         Reorder output filters in descending order of their importance.
         """
         for node in self._propagation_graph.get_all_nodes():
-            node.data.pop('output_mask', None)
+            node.data.pop("output_mask", None)
 
         # 1. Calculate filter importance for all groups of prunable layers
         for group in self._pruned_module_groups_info.get_all_clusters():
             filters_num = torch.tensor([get_filters_num(minfo.module) for minfo in group.elements])
             assert torch.all(filters_num == filters_num[0])
             device = group.elements[0].module.weight.device
 
             cumulative_filters_importance = torch.zeros(filters_num[0]).to(device)
             # 1.1 Calculate cumulative importance for all filters in this group
             for minfo in group.elements:
                 weight = minfo.module.weight
                 if self._weights_normalizer_fn:
                     weight = self._weights_normalizer_fn(minfo.module.weight)
-                filters_importance = self._filter_importance_fn(weight,
-                                                                minfo.module.target_weight_dim_for_compression)
+                filters_importance = self._filter_importance_fn(weight, minfo.module.target_weight_dim_for_compression)
                 cumulative_filters_importance += filters_importance
 
             _, reorder_indexes = torch.sort(cumulative_filters_importance, dim=0, descending=True)
             device = get_model_device(self._target_model)
             reorder_indexes.to(device)
             # 1.2 Setup reorder indexes as output mask to reorganize filters
             for minfo in group.elements:
                 node = self._propagation_graph.get_node_by_id(minfo.nncf_node_id)
-                node.data['output_mask'] = PTNNCFTensor(reorder_indexes)
+                node.data["output_mask"] = PTNNCFTensor(reorder_indexes)
 
         # 2. Propagating masks across the graph
-        reorder_algo = FilterReorderingAlgorithm(self._target_model,
-                                                 self._propagation_graph,
-                                                 PT_PRUNING_OPERATOR_METATYPES,
-                                                 PTNNCFPruningTensorProcessor)
+        reorder_algo = FilterReorderingAlgorithm(
+            self._target_model, self._propagation_graph, PT_PRUNING_OPERATOR_METATYPES, PTNNCFPruningTensorProcessor
+        )
         reorder_algo.reorder_filters()
 
     def find_pairs_of_nodes_with_different_width(self, pairs_of_nodes: List[Tuple[str, str]]) -> List[int]:
         """
         Find pairs of nodes that have different output shapes. It's need to resolve conflict between elastic width and
         elastic depth. The conflicting situation happens when layers are trimmed and skipped independently.
         There might be a situation when channels are trimmed for the layers in the beginning of skipped block, and
@@ -791,37 +805,40 @@
 
         :param pairs_of_nodes: list of pairs of node names
         :return: index of nodes pair in the given list that have different output shapes.
         """
         pair_indexes = []
         for idx, (start_node_name, end_node_name) in enumerate(pairs_of_nodes):
             start_node = self._propagation_graph.get_node_by_name(start_node_name)
-            start_mask = start_node.data['output_mask']
+            start_mask = start_node.data["output_mask"]
             end_node = self._propagation_graph.get_node_by_name(end_node_name)
-            end_mask = end_node.data['output_mask']
+            end_mask = end_node.data["output_mask"]
 
             all_start_output_shapes = self._propagation_graph.get_output_shapes_for_node(start_node_name)
             start_output_shape = list(OrderedDict.fromkeys(all_start_output_shapes))
             all_end_output_shape = self._propagation_graph.get_output_shapes_for_node(end_node_name)
             end_output_shape = list(OrderedDict.fromkeys(all_end_output_shape))
 
             start_width = ElasticWidthHandler.mask_to_width(start_mask)
             end_width = ElasticWidthHandler.mask_to_width(end_mask)
 
             if start_width is None and end_width is None and start_output_shape != end_output_shape:
-                reason = f'it has a different shapes on boundaries: {start_output_shape} != {end_output_shape}'
+                reason = f"it has a different shapes on boundaries: {start_output_shape} != {end_output_shape}"
             elif start_width is not None and end_width is not None and start_width != end_width:
-                reason = f'it has a different width on boundaries: {start_width} != {end_width}'
+                reason = f"it has a different width on boundaries: {start_width} != {end_width}"
             elif (start_width is None or end_width is None) and not (start_width is None and end_width is None):
-                reason = f'it has empty width on one of the boundaries. ' \
-                         f'Width: {start_width} vs {end_width}. Shapes: {start_output_shape} vs {end_output_shape}'
+                reason = (
+                    f"it has empty width on one of the boundaries. "
+                    f"Width: {start_width} vs {end_width}. Shapes: {start_output_shape} vs {end_output_shape}"
+                )
             else:
                 continue
             nncf_logger.debug(
-                f'The block [\n\t{start_node_name},\n\t{end_node_name}\n]\n can`t be skipped, because {reason}')
+                f"The block [\n\t{start_node_name},\n\t{end_node_name}\n]\n can`t be skipped, because {reason}"
+            )
             pair_indexes.append(idx)
         return pair_indexes
 
     def _get_width_list_len(self, op: ElasticOutputWidthOp) -> int:
         N = len(op.width_list)
         if 0 < self._width_num_params_indicator < N:
             return self._width_num_params_indicator
@@ -832,21 +849,21 @@
         return op.width_list[:width_list_len]
 
     def _collect_ops_data_by_selection_rule(self, selection_rule: Callable) -> Dict[PruningGroupID, Any]:
         elastic_width_config = {}
         for cluster in self._pruned_module_groups_info.get_all_clusters():
             all_max_out_channels = {el.elastic_op.max_width for el in cluster.elements}
             if len(all_max_out_channels) != 1:
-                raise RuntimeError('Invalid grouping of layers with different number of output channels')
+                raise RuntimeError("Invalid grouping of layers with different number of output channels")
 
             first_elastic_width_info = next(iter(cluster.elements))
             op = first_elastic_width_info.elastic_op
             selected_width = selection_rule(op)
             elastic_width_config[cluster.id] = selected_width
-            nncf_logger.debug(f'Select width={cluster.id} for group #{selected_width}')
+            nncf_logger.debug(f"Select width={cluster.id} for group #{selected_width}")
         return elastic_width_config
 
     @staticmethod
     def mask_to_width(mask: NNCFTensor) -> Optional[int]:
         """
         Decodes mask to a single integer. We assume that mask was constructed in a way that first N values are equal
         to 1, and the rest values are 0. The N encodes width value.
@@ -857,16 +874,17 @@
         result = None
         if mask is not None:
             actual_mask = mask.tensor
             mask_len = sum(actual_mask.size())
             width = int(sum(actual_mask))
             device = actual_mask.device
             ref_mask = ElasticWidthHandler._width_to_mask(width, mask_len, device).tensor
-            assert torch.equal(ref_mask, actual_mask), \
-                f'Invalid mask {actual_mask}: the first {width} values must be ones, the rest - zeros.'
+            assert torch.equal(
+                ref_mask, actual_mask
+            ), f"Invalid mask {actual_mask}: the first {width} values must be ones, the rest - zeros."
             result = width
         return result
 
     @staticmethod
     def _width_to_mask(active_width: int, max_width: int, device: torch.device) -> PTNNCFTensor:
         """
         Encodes width to tensor filled by 1 and 0. We intentionally construct mask in a way that first N values are
@@ -880,34 +898,40 @@
         """
         mask = torch.ones(max_width).to(device)
         mask[active_width:].fill_(0)
         return PTNNCFTensor(mask)
 
 
 class EWBuilderStateNames:
-    GROUPED_NODE_NAMES_TO_PRUNE = 'grouped_node_names_to_prune'
-    OVERWRITE_GROUP_WIDTHS = 'overwrite_groups_widths'
-    ADD_DYNAMIC_INPUTS = 'add_dynamic_inputs'
+    GROUPED_NODE_NAMES_TO_PRUNE = "grouped_node_names_to_prune"
+    OVERWRITE_GROUP_WIDTHS = "overwrite_groups_widths"
+    ADD_DYNAMIC_INPUTS = "add_dynamic_inputs"
+
 
 @ELASTICITY_BUILDERS.register(ElasticityDim.WIDTH)
 class ElasticWidthBuilder(SingleElasticityBuilder):
     """
-     Determines which modifications should be made to the original FP32 model in order to introduce elastic width
-     to the model.
-     """
+    Determines which modifications should be made to the original FP32 model in order to introduce elastic width
+    to the model.
+    """
+
     _state_names = EWBuilderStateNames
 
-    def __init__(self, params: ElasticWidthParams,
-                 ignored_scopes: Optional[List[str]] = None,
-                 target_scopes: Optional[List[str]] = None):
+    def __init__(
+        self,
+        params: ElasticWidthParams,
+        ignored_scopes: Optional[List[str]] = None,
+        target_scopes: Optional[List[str]] = None,
+    ):
         super().__init__(params, ignored_scopes, target_scopes)
         self._weights_normalizer = None
         self._overwriting_pruning_groups = params.overwrite_groups is not None
-        self._grouped_node_names_to_prune = params.overwrite_groups \
-            if params.overwrite_groups is not None else [] # type: List[List[NNCFNodeName]]
+        self._grouped_node_names_to_prune = (
+            params.overwrite_groups if params.overwrite_groups is not None else []
+        )  # type: List[List[NNCFNodeName]]
         self._overwrite_groups_widths = params.overwrite_groups_widths
         self._add_dynamic_inputs = params.add_dynamic_inputs
         self._params = params
 
     def build(self, target_model: NNCFNetwork) -> ElasticWidthHandler:
         """
         Creates modifications to the given NNCFNetwork for introducing elastic width and creates a handler object that
@@ -915,128 +939,136 @@
 
         :param target_model: a target NNCFNetwork for adding modifications
         :return: a handler object that can manipulate the elastic width.
         """
         filter_importance_str = self._params.filter_importance
         filter_importance = FILTER_IMPORTANCE_FUNCTIONS.get(filter_importance_str)
 
-        graph = target_model.get_original_graph()
+        graph = target_model.nncf.get_original_graph()
         device = next(target_model.parameters()).device
 
         if not self._grouped_node_names_to_prune:
             prunable_types = [NNCFConv2d, NNCFLinear]
             prune_operations_types = [pt.op_func_name for pt in prunable_types]
             types_of_grouping_ops = PTElementwisePruningOp.get_all_op_aliases()
-            pruning_node_selector = PruningNodeSelector(PT_PRUNING_OPERATOR_METATYPES,
-                                                        prune_operations_types,
-                                                        types_of_grouping_ops,
-                                                        ignored_scopes=self._ignored_scopes,
-                                                        target_scopes=self._target_scopes,
-                                                        prune_first=True,
-                                                        prune_downsample_convs=True)
+            pruning_node_selector = PruningNodeSelector(
+                PT_PRUNING_OPERATOR_METATYPES,
+                prune_operations_types,
+                types_of_grouping_ops,
+                ignored_scopes=self._ignored_scopes,
+                target_scopes=self._target_scopes,
+                prune_first=True,
+                prune_downsample_convs=True,
+            )
             groups_of_nodes_to_prune = pruning_node_selector.create_pruning_groups(graph)
             for group in groups_of_nodes_to_prune.get_all_clusters():
                 grouped_node_names = [node.node_name for node in group.elements]
                 self._grouped_node_names_to_prune.append(grouped_node_names)
 
         transformation_commands = []
         pruned_module_groups_info = Clusterization[ElasticWidthInfo](id_fn=lambda x: x.node_name)
         node_name_vs_dynamic_input_width_op_map = OrderedDict()
 
         metatype_vs_elastic_op_creator = {
             PTModuleConv2dMetatype: self._create_elastic_conv_width_op,
             PTDepthwiseConv2dSubtype: self._create_elastic_conv_width_op,
-            PTModuleLinearMetatype: self._create_elastic_linear_width_op
+            PTModuleLinearMetatype: self._create_elastic_linear_width_op,
         }
 
         for i, grouped_node_names in enumerate(self._grouped_node_names_to_prune):
             group_minfos = []
             list_of_node_ids = []
             for node_name in grouped_node_names:
                 node = graph.get_node_by_name(node_name)
                 metatype = node.metatype
                 list_of_node_ids.append(node.node_id)
                 layer_attrs = node.layer_attributes
                 if metatype not in metatype_vs_elastic_op_creator:
-                    raise RuntimeError(f'Elastic width is not supported for {metatype}')
+                    raise RuntimeError(f"Elastic width is not supported for {metatype}")
                 elastic_op_creator = metatype_vs_elastic_op_creator[metatype]
 
                 elastic_width_operation = elastic_op_creator(
-                        layer_attrs,
-                        node_name,
-                        self._params,
-                        self._overwrite_groups_widths[i] if self._overwriting_pruning_groups
-                                                        else [])
+                    layer_attrs,
+                    node_name,
+                    self._params,
+                    self._overwrite_groups_widths[i] if self._overwriting_pruning_groups else [],
+                )
                 elastic_width_operation.to(device)
                 update_conv_params_op = UpdateWeightAndOptionalBias(elastic_width_operation)
                 transformation_commands.append(
                     PTInsertionCommand(
-                        PTTargetPoint(
-                            TargetType.PRE_LAYER_OPERATION,
-                            target_node_name=node_name
-                        ),
+                        PTTargetPoint(TargetType.PRE_LAYER_OPERATION, target_node_name=node_name),
                         update_conv_params_op,
-                        TransformationPriority.PRUNING_PRIORITY
+                        TransformationPriority.PRUNING_PRIORITY,
+                    )
+                )
+                pruned_module = target_model.nncf.get_containing_module(node_name)
+                assert isinstance(
+                    pruned_module, (nn.Conv2d, nn.Linear)
+                ), "currently prune only 2D Convolutions and Linear layers"
+
+                group_minfos.append(
+                    ElasticWidthInfo(
+                        node_name=node_name,
+                        module=pruned_module,
+                        elastic_op=elastic_width_operation,
+                        node_id=node.node_id,
+                        is_depthwise=is_prunable_depthwise_conv(node),
                     )
                 )
-                pruned_module = target_model.get_containing_module(node_name)
-                assert isinstance(pruned_module, (nn.Conv2d, nn.Linear)), \
-                    'currently prune only 2D Convolutions and Linear layers'
-
-                group_minfos.append(ElasticWidthInfo(node_name=node_name,
-                                                     module=pruned_module,
-                                                     elastic_op=elastic_width_operation,
-                                                     node_id=node.node_id,
-                                                     is_depthwise=is_prunable_depthwise_conv(node)))
 
             cluster = Cluster[ElasticWidthInfo](i, group_minfos, list_of_node_ids)
             pruned_module_groups_info.add_cluster(cluster)
 
         metatype_vs_dynamic_input_op_creator = {
             PTModuleConv2dMetatype: self._create_dynamic_conv_input_op,
             PTDepthwiseConv2dSubtype: self._create_dynamic_dw_conv_input_op,
             PTModuleBatchNormMetatype: self._create_dynamic_bn_input_op,
             PTModuleLayerNormMetatype: self._create_dynamic_ln_input_op,
-            PTModuleLinearMetatype: self._create_dynamic_linear_input_op
+            PTModuleLinearMetatype: self._create_dynamic_linear_input_op,
         }
         for metatype, op_creator in metatype_vs_dynamic_input_op_creator.items():
             nodes = graph.get_nodes_by_metatypes([metatype])
             for node in nodes:
                 node_name = node.node_name
                 nncf_logger.debug(f"Adding Dynamic Input Op for {metatype.name} in scope: {node_name}")
                 layer_attrs = node.layer_attributes
                 update_module_params = op_creator(layer_attrs, node_name).to(device)
                 node_name_vs_dynamic_input_width_op_map[node_name] = update_module_params.op
                 transformation_commands.append(
                     PTInsertionCommand(
-                        PTTargetPoint(
-                            TargetType.PRE_LAYER_OPERATION,
-                            target_node_name=node_name
-                        ),
+                        PTTargetPoint(TargetType.PRE_LAYER_OPERATION, target_node_name=node_name),
                         update_module_params,
-                        priority=TransformationPriority.DEFAULT_PRIORITY
+                        priority=TransformationPriority.DEFAULT_PRIORITY,
                     )
                 )
 
-        return ElasticWidthHandler(target_model, filter_importance, self._weights_normalizer,
-                                   node_name_vs_dynamic_input_width_op_map,
-                                   pruned_module_groups_info, transformation_commands, self._add_dynamic_inputs)
+        return ElasticWidthHandler(
+            target_model,
+            filter_importance,
+            self._weights_normalizer,
+            node_name_vs_dynamic_input_width_op_map,
+            pruned_module_groups_info,
+            transformation_commands,
+            self._add_dynamic_inputs,
+        )
 
     def load_state(self, state: Dict[str, Any]) -> None:
         """
         Initializes object from the state.
 
         :param state: Output of `get_state()` method.
         """
         params_from_state = state[SingleElasticityBuilder._state_names.ELASTICITY_PARAMS]
         params = ElasticWidthParams.from_state(params_from_state)
         if self._params and self._params != params:
             nncf_logger.warning(
-                'Different elasticity parameters were provided in two places: on init and on loading '
-                'state. The one from state is taken by ignoring the ones from init.')
+                "Different elasticity parameters were provided in two places: on init and on loading "
+                "state. The one from state is taken by ignoring the ones from init."
+            )
         self._params = params
         self._grouped_node_names_to_prune = state[self._state_names.GROUPED_NODE_NAMES_TO_PRUNE]
 
         if params_from_state.get(self._state_names.OVERWRITE_GROUP_WIDTHS, None) is not None:
             self._overwrite_groups_widths = params_from_state[self._state_names.OVERWRITE_GROUP_WIDTHS]
             self._overwriting_pruning_groups = True
             if len(self._grouped_node_names_to_prune) != len(self._overwrite_groups_widths):
@@ -1053,36 +1085,42 @@
         """
         return {
             SingleElasticityBuilder._state_names.ELASTICITY_PARAMS: self._params.get_state(),
             self._state_names.GROUPED_NODE_NAMES_TO_PRUNE: self._grouped_node_names_to_prune,
         }
 
     @staticmethod
-    def _create_elastic_conv_width_op(conv_layer_attrs: BaseLayerAttributes,
-                                      node_name: str,
-                                      params: ElasticWidthParams,
-                                      fixed_width_list: Optional[List[int]] = None) -> ElasticOutputWidthConv2DOp:
+    def _create_elastic_conv_width_op(
+        conv_layer_attrs: BaseLayerAttributes,
+        node_name: str,
+        params: ElasticWidthParams,
+        fixed_width_list: Optional[List[int]] = None,
+    ) -> ElasticOutputWidthConv2DOp:
         assert isinstance(conv_layer_attrs, ConvolutionLayerAttributes)
         nncf_logger.debug(f"Adding Dynamic Conv2D Layer in scope: {str(node_name)}")
         if fixed_width_list is None:
             fixed_width_list = []
-        return ElasticOutputWidthConv2DOp(conv_layer_attrs.out_channels, node_name,
-                                          params, fixed_width_list=fixed_width_list)
+        return ElasticOutputWidthConv2DOp(
+            conv_layer_attrs.out_channels, node_name, params, fixed_width_list=fixed_width_list
+        )
 
     @staticmethod
-    def _create_elastic_linear_width_op(linear_layer_attrs: BaseLayerAttributes,
-                                        node_name: str,
-                                        params: ElasticWidthParams,
-                                        fixed_width_list: Optional[List[int]] = None) -> ElasticOutputWidthLinearOp:
+    def _create_elastic_linear_width_op(
+        linear_layer_attrs: BaseLayerAttributes,
+        node_name: str,
+        params: ElasticWidthParams,
+        fixed_width_list: Optional[List[int]] = None,
+    ) -> ElasticOutputWidthLinearOp:
         assert isinstance(linear_layer_attrs, LinearLayerAttributes)
         if fixed_width_list is None:
             fixed_width_list = []
         nncf_logger.debug(f"Adding Dynamic Linear Layer in scope: {str(node_name)}")
-        return ElasticOutputWidthLinearOp(linear_layer_attrs.out_features, node_name, params,
-                                          fixed_width_list=fixed_width_list)
+        return ElasticOutputWidthLinearOp(
+            linear_layer_attrs.out_features, node_name, params, fixed_width_list=fixed_width_list
+        )
 
     @staticmethod
     def _create_dynamic_conv_input_op(conv_layer_attrs: BaseLayerAttributes, node_name: str) -> UpdateWeight:
         assert isinstance(conv_layer_attrs, ConvolutionLayerAttributes)
         dynamic_conv_input_op = ElasticInputWidthConvOp(max_width=conv_layer_attrs.in_channels, node_name=node_name)
         return UpdateWeight(dynamic_conv_input_op)
 
@@ -1091,24 +1129,27 @@
         assert isinstance(conv_layer_attrs, ConvolutionLayerAttributes)
         dynamic_dw_conv_input_op = ElasticInputWidthDWConvOp(max_width=conv_layer_attrs.groups, node_name=node_name)
         return UpdateNumGroups(dynamic_dw_conv_input_op)
 
     @staticmethod
     def _create_dynamic_bn_input_op(generic_layer_attrs: BaseLayerAttributes, node_name: str) -> UpdateBatchNormParams:
         assert isinstance(generic_layer_attrs, GenericWeightedLayerAttributes)
-        dynamic_bn_input_op = ElasticInputWidthBatchNormOp(max_width=generic_layer_attrs.get_num_filters(),
-                                                           node_name=node_name)
+        dynamic_bn_input_op = ElasticInputWidthBatchNormOp(
+            max_width=generic_layer_attrs.get_num_filters(), node_name=node_name
+        )
         return UpdateBatchNormParams(dynamic_bn_input_op)
 
     @staticmethod
     def _create_dynamic_ln_input_op(generic_layer_attrs: BaseLayerAttributes, node_name: str) -> UpdateLayerNormParams:
         assert isinstance(generic_layer_attrs, GenericWeightedLayerAttributes)
-        dynamic_ln_input_op = ElasticInputWidthLayerNormOp(max_width=generic_layer_attrs.get_num_filters(),
-                                                           node_name=node_name)
+        dynamic_ln_input_op = ElasticInputWidthLayerNormOp(
+            max_width=generic_layer_attrs.get_num_filters(), node_name=node_name
+        )
         return UpdateLayerNormParams(dynamic_ln_input_op)
 
     @staticmethod
     def _create_dynamic_linear_input_op(linear_layer_attrs: BaseLayerAttributes, node_name: str) -> UpdateWeight:
         assert isinstance(linear_layer_attrs, LinearLayerAttributes)
-        dynamic_linear_input_op = ElasticInputWidthLinearOp(max_width=linear_layer_attrs.in_features,
-                                                            node_name=node_name)
+        dynamic_linear_input_op = ElasticInputWidthLinearOp(
+            max_width=linear_layer_attrs.in_features, node_name=node_name
+        )
         return UpdateWeight(dynamic_linear_input_op)
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_builder.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,47 +1,44 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import OrderedDict
-from typing import Any
-from typing import Dict
-from typing import List
+from typing import Any, Dict, List
 
 from nncf import NNCFConfig
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import SingleElasticityBuilder
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import create_elasticity_builder_from_config
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_controller import ElasticityController
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.multi_elasticity_handler import MultiElasticityHandler
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.nncf_network import NNCFNetwork
 
 
 class EBuilderStateNames:
-    AVAILABLE_ELASTICITY_DIMS = 'available_elasticity_dims'
-    BUILDER_STATES = 'builder_states'
+    AVAILABLE_ELASTICITY_DIMS = "available_elasticity_dims"
+    BUILDER_STATES = "builder_states"
 
 
-@PT_COMPRESSION_ALGORITHMS.register('elasticity')
+@PT_COMPRESSION_ALGORITHMS.register("elasticity")
 class ElasticityBuilder(PTCompressionAlgorithmBuilder):
     """
     Determines which modifications should be made to the original FP32 model in order to introduce elasticity
     to the model.
     """
+
     _state_names = EBuilderStateNames
 
     # NOTE: This is the order of activation elasticity dimensions when multiple of them are enabled.
     # Don't confuse with the order of adding elasticity dimension on training stages (progressiveness of
     # elasticity). For vanilla progressive shrinking the stages order is the following:
     #   1st stage: kernel
     #   2nd stage: kernel + depth
@@ -56,20 +53,20 @@
     # but currently, it's not supported.
     ALL_DIMS_IN_EXECUTION_ORDER = [ElasticityDim.WIDTH, ElasticityDim.KERNEL, ElasticityDim.DEPTH]
 
     def __init__(self, nncf_config: NNCFConfig, should_init: bool = True):
         super().__init__(nncf_config, should_init)
         self._multi_elasticity_handler = None
         # TODO(nlyalyus): ignored/target scope is not supported (ticket 68052)
-        self._ignored_scopes = self.config.get('ignored_scopes', None)
-        self._target_scopes = self.config.get('target_scopes', None)
+        self._ignored_scopes = self.config.get("ignored_scopes", None)
+        self._target_scopes = self.config.get("target_scopes", None)
         self._multi_elasticity_handler_state = None
 
         all_elasticity_dims = {e.value for e in ElasticityDim}
-        available_elasticity_dims_str = self._algo_config.get('available_elasticity_dims', all_elasticity_dims)
+        available_elasticity_dims_str = self._algo_config.get("available_elasticity_dims", all_elasticity_dims)
         self._available_elasticity_dims = list(map(ElasticityDim, available_elasticity_dims_str))
         self._elasticity_builders = OrderedDict()  # type: Dict[ElasticityDim, SingleElasticityBuilder]
         self._builder_states = None
 
     def initialize(self, model: NNCFNetwork) -> None:
         """
         Initialize model parameters before training
@@ -81,38 +78,38 @@
     def get_available_elasticity_dims(self) -> List[ElasticityDim]:
         """
         :return: list of available elasticity dimensions
         """
         return self._available_elasticity_dims
 
     def _get_algo_specific_config_section(self) -> Dict:
-        return self.config.get('bootstrapNAS', {}).get('training', {}).get('elasticity', {})
+        return self.config.get("bootstrapNAS", {}).get("training", {}).get("elasticity", {})
 
-    def _build_controller(self, model: NNCFNetwork) -> 'ElasticityController':
+    def _build_controller(self, model: NNCFNetwork) -> "ElasticityController":
         """
         Simple implementation of building controller without setting builder state and loading controller's one.
 
         :param model: The model with additional modifications necessary to enable
             algorithm-specific compression during fine-tuning.
         :return: The instance of the `ElasticityController`.
         """
         return ElasticityController(model, self._algo_config, self._multi_elasticity_handler)
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         sorted_elasticity_dims = list(
-            filter(lambda x: x in self._available_elasticity_dims, self.ALL_DIMS_IN_EXECUTION_ORDER))
+            filter(lambda x: x in self._available_elasticity_dims, self.ALL_DIMS_IN_EXECUTION_ORDER)
+        )
         ignored_scopes = self._ignored_scopes
         target_scopes = self._target_scopes
 
         for elasticity_dim in sorted_elasticity_dims:
             elasticity_config = self._algo_config.get(elasticity_dim.value, {})
-            elasticity_builder = create_elasticity_builder_from_config(elasticity_config,
-                                                                       elasticity_dim,
-                                                                       ignored_scopes,
-                                                                       target_scopes)
+            elasticity_builder = create_elasticity_builder_from_config(
+                elasticity_config, elasticity_dim, ignored_scopes, target_scopes
+            )
             self._elasticity_builders[elasticity_dim] = elasticity_builder
 
         if self._builder_states is not None:
             for dim_str, builder_state in self._builder_states.items():
                 dim = ElasticityDim(dim_str)
                 if dim in self._elasticity_builders:
                     self._elasticity_builders[dim].load_state(builder_state)
@@ -137,15 +134,15 @@
         :return: Returns a dictionary with Python data structures
             (dict, list, tuple, str, int, float, True, False, None) that represents state of the object.
         """
         builder_states = {dim.value: builder.get_state() for dim, builder in self._elasticity_builders.items()}
         available_elasticity_dims_state = list(map(lambda x: x.value, self.get_available_elasticity_dims()))
         return {
             self._state_names.BUILDER_STATES: builder_states,
-            self._state_names.AVAILABLE_ELASTICITY_DIMS: available_elasticity_dims_state
+            self._state_names.AVAILABLE_ELASTICITY_DIMS: available_elasticity_dims_state,
         }
 
     def _load_state_without_name(self, state_without_name: Dict[str, Any]):
         """
         Implementation of load state that takes state without builder name.
 
         :param state_without_name: Output of `_get_state_without_name()` method.
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_controller.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_controller.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,42 +1,40 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Any
-from typing import Dict
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Any, Dict
 
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.statistics import NNCFStatistics
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.multi_elasticity_handler import MultiElasticityHandler
 from nncf.torch.algo_selector import ZeroCompressionLoss
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.nncf_network import NNCFNetwork
 
 
 class EControllerStateNames:
-    MULTI_ELASTICITY_HANDLER_STATE = 'multi_elasticity_handler_state'
+    MULTI_ELASTICITY_HANDLER_STATE = "multi_elasticity_handler_state"
 
 
 class ElasticityController(PTCompressionAlgorithmController):
     """
     Serves as a handle to the additional modules, parameters and hooks inserted
     into the original uncompressed model in order to control elasticity in the model.
     """
+
     _ec_state_names = EControllerStateNames
 
     def __init__(self, target_model: NNCFNetwork, algo_config: Dict, multi_elasticity_handler: MultiElasticityHandler):
         super().__init__(target_model)
         self.target_model = target_model
         self._algo_config = algo_config
         self._loss = ZeroCompressionLoss(next(target_model.parameters()).device)
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/elasticity_dim.py` & `nncf-2.5.0/nncf/torch/sparsity/magnitude/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,22 +1,13 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Backend-specific implementation of magnitude sparsity algorithm.
 """
-from enum import Enum
-
-
-class ElasticityDim(Enum):
-    """
-    Defines elasticity dimension or type of elasticity applied to the model
-    """
-    KERNEL = 'kernel'
-    WIDTH = 'width'
-    DEPTH = 'depth'
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/filter_reorder.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/filter_reorder.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,64 +1,65 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-import torch
-from typing import Type
-
-from typing import Optional
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Optional, Type
 
-from nncf.common.pruning.tensor_processor import NNCFPruningBaseTensorProcessor
+import torch
 
 from nncf.common.graph import NNCFGraph
+from nncf.common.logging import nncf_logger
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
+from nncf.common.pruning.tensor_processor import NNCFPruningBaseTensorProcessor
 from nncf.common.pruning.utils import PruningOperationsMetatypeRegistry
 from nncf.torch.nncf_network import NNCFNetwork
-from nncf.common.logging import nncf_logger
 
 
 class FilterReorderingAlgorithm(MaskPropagationAlgorithm):
     """
     Reorders filters based on reordering indexes encoded in the `output_mask` attribute in the nodes of
     model graph.
     """
-    def __init__(self, model: NNCFNetwork, graph: NNCFGraph,
-                 pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
-                 tensor_processor: Optional[Type[NNCFPruningBaseTensorProcessor]] = None):
+
+    def __init__(
+        self,
+        model: NNCFNetwork,
+        graph: NNCFGraph,
+        pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
+        tensor_processor: Optional[Type[NNCFPruningBaseTensorProcessor]] = None,
+    ):
         super().__init__(graph, pruning_operator_metatypes, tensor_processor)
         self._model = model
 
     def apply_reordering_indexes(self) -> None:
         """
         Applying propagated masks (which encodes indexes to reorder filters) for all nodes in topological order:
         1. running input_reorder method for this node
         2. running output_reorder method for this node
         """
         pruned_node_modules = []
         with torch.no_grad():
             for node in self._graph.topological_sort():
                 node_cls = self.get_meta_operation_by_type_name(node.node_type)
-                node_module = self._model.get_containing_module(node.node_name)
+                node_module = self._model.nncf.get_containing_module(node.node_name)
                 if node_module not in pruned_node_modules:
                     node_cls.input_reorder(self._model, node, self._graph)
                     node_cls.output_reorder(self._model, node, self._graph)
                     pruned_node_modules.append(node_module)
-            nncf_logger.debug('Finished mask applying step')
+            nncf_logger.debug("Finished mask applying step")
 
     def reorder_filters(self) -> None:
         """
         Model pruner work in two stages:
         1. Mask propagation: propagate pruning masks through the graph.
         2. Applying calculated masks
         """
-        nncf_logger.info('Start reordering filters')
+        nncf_logger.info("Start reordering filters")
         self.mask_propagation()
         self.apply_reordering_indexes()
-        nncf_logger.info('Finished reordering filters')
+        nncf_logger.info("Finished reordering filters")
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/multi_elasticity_handler.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/multi_elasticity_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,93 +1,85 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import inspect
 from collections import OrderedDict
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
+from typing import Any, Dict, List, Optional
 from typing import OrderedDict as OrderedDictType
 from typing import Tuple
 
-from nncf.common.pruning.weights_flops_calculator import WeightsFlopsCalculator
 from nncf.common.logging import nncf_logger
+from nncf.common.pruning.weights_flops_calculator import WeightsFlopsCalculator
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ElasticityConfig
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import ElasticityHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.base_handler import SingleElasticityHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_depth import ElasticDepthHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_depth import ElasticDepthSearchSpace
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_kernel import ElasticKernelHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_kernel import ElasticKernelSearchSpace
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_width import ElasticWidthHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_width import ElasticWidthSearchSpace
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
+from nncf.torch.graph.operator_metatypes import PTDepthwiseConv1dSubtype
+from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
+from nncf.torch.graph.operator_metatypes import PTDepthwiseConv3dSubtype
 from nncf.torch.graph.operator_metatypes import PTModuleConv1dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConv2dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConv3dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConvTranspose2dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConvTranspose3dMetatype
-from nncf.torch.graph.operator_metatypes import PTDepthwiseConv1dSubtype
-from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
-from nncf.torch.graph.operator_metatypes import PTDepthwiseConv3dSubtype
 from nncf.torch.graph.operator_metatypes import PTModuleLinearMetatype
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.pruning.utils import collect_output_shapes
 
 SubnetConfig = OrderedDictType[ElasticityDim, ElasticityConfig]
 
 
 class MEHandlerStateNames:
-    IS_HANDLER_ENABLED_MAP = 'is_handler_enabled_map'
-    STATES_OF_HANDLERS = 'states_of_handlers'
+    IS_HANDLER_ENABLED_MAP = "is_handler_enabled_map"
+    STATES_OF_HANDLERS = "states_of_handlers"
 
 
-#pylint: disable=too-many-public-methods
+# pylint: disable=too-many-public-methods
 class MultiElasticityHandler(ElasticityHandler):
     """
     An interface for handling multiple elasticity in the network. The elasticity defines variable values in properties
     of the layers or the network, e.g. variable number of channels in the Conv or variable number of layers in the
     network. By applying elasticity it's possible to derive a smaller models (Subnets) that have some elements in
     common with the original model.
     The interface defines methods for activation Subnets.
     """
+
     _state_names = MEHandlerStateNames
 
-    def __init__(self,
-                 handlers: OrderedDictType[ElasticityDim, SingleElasticityHandler],
-                 target_model: NNCFNetwork):
+    def __init__(self, handlers: OrderedDictType[ElasticityDim, SingleElasticityHandler], target_model: NNCFNetwork):
         GENERAL_CONV_LAYER_METATYPES = [
             PTModuleConv1dMetatype,
             PTDepthwiseConv1dSubtype,
             PTModuleConv2dMetatype,
             PTDepthwiseConv2dSubtype,
             PTModuleConv3dMetatype,
             PTDepthwiseConv3dSubtype,
             PTModuleConvTranspose2dMetatype,
             PTModuleConvTranspose3dMetatype,
         ]
-        LINEAR_LAYER_METATYPES = [
-            PTModuleLinearMetatype
-        ]
+        LINEAR_LAYER_METATYPES = [PTModuleLinearMetatype]
         self._handlers = handlers
         self._target_model = target_model
         self._is_handler_enabled_map = {elasticity_dim: True for elasticity_dim in handlers}
         self._weights_calc = WeightsFlopsCalculator(
-            conv_op_metatypes=GENERAL_CONV_LAYER_METATYPES,
-            linear_op_metatypes=LINEAR_LAYER_METATYPES)
+            conv_op_metatypes=GENERAL_CONV_LAYER_METATYPES, linear_op_metatypes=LINEAR_LAYER_METATYPES
+        )
         self.activate_supernet()
 
     @property
     def width_search_space(self) -> ElasticWidthSearchSpace:
         return self.width_handler.get_search_space()
 
     @property
@@ -158,26 +150,25 @@
 
     def activate_subnet_for_config(self, config: SubnetConfig) -> None:
         """
         Activates a Subnet that corresponds to the given elasticity configuration
 
         :param config: elasticity configuration
         """
-        active_handlers = {
-            dim: self._handlers[dim] for dim in self._handlers if self._is_handler_enabled_map[dim]
-        }
+        active_handlers = {dim: self._handlers[dim] for dim in self._handlers if self._is_handler_enabled_map[dim]}
         for handler_id, handler in self._handlers.items():
             if handler_id in config:
                 sub_config = config[handler_id]
                 other_active_handlers = dict(filter(lambda pair: pair[0] != handler_id, active_handlers.items()))
                 resolved_config = handler.resolve_conflicts_with_other_elasticities(sub_config, other_active_handlers)
                 handler.activate_subnet_for_config(resolved_config)
                 if sub_config != resolved_config:
                     nncf_logger.warning(
-                        f'Config for {handler_id} mismatch. Requested: {sub_config}. Resolved: {resolved_config}')
+                        f"Config for {handler_id} mismatch. Requested: {sub_config}. Resolved: {resolved_config}"
+                    )
 
     def load_state(self, state: Dict[str, Any]) -> None:
         """
         Initializes object from the state.
 
         :param state: Output of `get_state()` method.
         """
@@ -200,15 +191,15 @@
 
         :return: state of the object
         """
         states_of_handlers = {dim.value: handler.get_state() for dim, handler in self._handlers.items()}
         is_handler_enabled_map = {dim.value: is_enabled for dim, is_enabled in self._is_handler_enabled_map.items()}
         return {
             self._state_names.STATES_OF_HANDLERS: states_of_handlers,
-            self._state_names.IS_HANDLER_ENABLED_MAP: is_handler_enabled_map
+            self._state_names.IS_HANDLER_ENABLED_MAP: is_handler_enabled_map,
         }
 
     def enable_all(self) -> None:
         """
         Enables all elasticities for being selected on sampling subnets.
         """
         self._is_handler_enabled_map = {elasticity_dim: True for elasticity_dim in self._is_handler_enabled_map}
@@ -243,16 +234,15 @@
         if self.depth_handler is not None:
             names_of_skipped_nodes = self.depth_handler.get_names_of_skipped_nodes()
 
         input_width_values, output_width_values = None, None
         if self.width_handler is not None:
             input_width_values, output_width_values = self.width_handler.get_active_in_out_width_values()
 
-
-        graph = self._target_model.get_graph()
+        graph = self._target_model.nncf.get_graph()
         output_shapes = collect_output_shapes(graph)
 
         flops, num_weights = self._weights_calc.count_flops_and_weights(
             graph=graph,
             output_shapes=output_shapes,
             input_channels=input_width_values,
             output_channels=output_width_values,
@@ -278,41 +268,40 @@
         return result
 
     @staticmethod
     def _get_current_method_name() -> str:
         return inspect.stack()[1].function
 
     def get_design_vars_info(self) -> float:
-        active_handlers = {
-            dim: self._handlers[dim] for dim in self._handlers if self._is_handler_enabled_map[dim]
-        }
+        active_handlers = {dim: self._handlers[dim] for dim in self._handlers if self._is_handler_enabled_map[dim]}
         num_vars = 0
         vars_upper = []
         for handler_id, handler in active_handlers.items():
             if handler_id is ElasticityDim.DEPTH:
                 num_vars += 1
                 vars_upper.append(len(handler.get_search_space()) - 1)
             else:
                 num_vars += len(handler.get_search_space())
-                vars_upper += [len(handler.get_search_space()[i]) - 1 for i in
-                                     range(len(handler.get_search_space()))]
+                vars_upper += [len(handler.get_search_space()[i]) - 1 for i in range(len(handler.get_search_space()))]
         return num_vars, vars_upper
 
-    def get_config_from_pymoo(self, x : List) -> SubnetConfig:
-        active_handlers = {
-            dim: self._handlers[dim] for dim in self._handlers if self._is_handler_enabled_map[dim]
-        }
+    def get_config_from_pymoo(self, x: List) -> SubnetConfig:
+        active_handlers = {dim: self._handlers[dim] for dim in self._handlers if self._is_handler_enabled_map[dim]}
         index_pos = 0
         sample = SubnetConfig()
         for handler_id, _ in active_handlers.items():
             if handler_id is ElasticityDim.KERNEL:
-                sample[handler_id] = [self.kernel_search_space[j - index_pos][x[j]] for j in
-                               range(index_pos, index_pos + len(self.kernel_search_space))]
+                sample[handler_id] = [
+                    self.kernel_search_space[j - index_pos][x[j]]
+                    for j in range(index_pos, index_pos + len(self.kernel_search_space))
+                ]
                 index_pos += len(self.kernel_search_space)
             elif handler_id is ElasticityDim.WIDTH:
-                sample[handler_id] = {key - index_pos: self.width_search_space[key - index_pos][x[key]] for
-                               key in range(index_pos, index_pos + len(self.width_search_space))}
+                sample[handler_id] = {
+                    key - index_pos: self.width_search_space[key - index_pos][x[key]]
+                    for key in range(index_pos, index_pos + len(self.width_search_space))
+                }
                 index_pos += len(self.width_search_space)
             elif handler_id is ElasticityDim.DEPTH:
                 sample[handler_id] = self.depth_search_space[x[index_pos]]
                 index_pos += 1
         return sample
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/visualization.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/elasticity/visualization.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,98 +1,97 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Optional
 
 import networkx as nx
 
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.pruning.utils import get_input_masks
-from nncf.torch.graph.graph import PTNNCFGraph
-from nncf.torch.graph.operator_metatypes import PTModuleConv2dMetatype
-from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elastic_width import ElasticWidthHandler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.multi_elasticity_handler import MultiElasticityHandler
+from nncf.torch.graph.graph import PTNNCFGraph
+from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
+from nncf.torch.graph.operator_metatypes import PTModuleConv2dMetatype
 
 
 class SubnetGraph:
     """
     Graph that represents active subnet in convenient way for visualization.
     """
+
     def __init__(self, compression_graph: PTNNCFGraph, multi_elasticity_handler: MultiElasticityHandler):
         # TODO: visualize other elastic dimension: depth, kernel (ticket 76870)
         self._width_graph = compression_graph.get_graph_for_structure_analysis(extended=True)
         for node_key in compression_graph.get_all_node_keys():
             compression_node = compression_graph.get_node_by_key(node_key)
 
             operator_name = self._get_operator_name(compression_node, multi_elasticity_handler.width_handler)
 
             metatype = compression_node.metatype
             color = None
             if metatype == PTModuleConv2dMetatype:
-                color = 'lightblue'
+                color = "lightblue"
             if metatype == PTDepthwiseConv2dSubtype:
-                operator_name = f'DW_{operator_name}'
-                color = 'purple'
+                operator_name = f"DW_{operator_name}"
+                color = "purple"
 
             target_node_to_draw = self._width_graph.nodes[node_key]
-            target_node_to_draw['label'] = operator_name
-            target_node_to_draw['style'] = 'filled'
+            target_node_to_draw["label"] = operator_name
+            target_node_to_draw["style"] = "filled"
             if color is not None:
-                target_node_to_draw['color'] = color
+                target_node_to_draw["color"] = color
 
     def get(self) -> nx.DiGraph:
         return self._width_graph
 
     @staticmethod
     def _get_operator_name(compressed_node: NNCFNode, width_handler: ElasticWidthHandler):
         operator_name = compressed_node.node_type
         node_id = compressed_node.node_id
-        node = SubnetGraph._get_original_node_from_compressed_node_name(
-            compressed_node.node_name, width_handler)
+        node = SubnetGraph._get_original_node_from_compressed_node_name(compressed_node.node_name, width_handler)
         if node is not None:
             operator_name = node.node_type
             node_id = node.node_id
             input_masks = get_input_masks(node, width_handler.propagation_graph)
             input_widths = None
             if input_masks:
                 input_widths = [ElasticWidthHandler.mask_to_width(input_mask) for input_mask in input_masks]
-            output_width = ElasticWidthHandler.mask_to_width(node.data['output_mask'])
+            output_width = ElasticWidthHandler.mask_to_width(node.data["output_mask"])
 
             if input_widths:
                 IW = None
                 if len(input_widths) == 1 and input_widths[0]:
                     IW = input_widths[0]
                 if len(input_widths) > 1 and all(input_widths):
                     IW = input_widths
                 if IW is not None:
-                    operator_name += f'_IW{IW}'
+                    operator_name += f"_IW{IW}"
 
             if output_width is not None:
-                operator_name += f'_OW{output_width}'
+                operator_name += f"_OW{output_width}"
 
             group_id = width_handler.get_group_id_by_node_name(node.node_name)
             if group_id is not None:
-                operator_name += f'_G{group_id}'
+                operator_name += f"_G{group_id}"
 
-        operator_name += f'_#{node_id}'
+        operator_name += f"_#{node_id}"
         return operator_name
 
     @staticmethod
-    def _get_original_node_from_compressed_node_name(node_name: NNCFNodeName,
-                                                     width_handler: ElasticWidthHandler) -> Optional[NNCFNode]:
+    def _get_original_node_from_compressed_node_name(
+        node_name: NNCFNodeName, width_handler: ElasticWidthHandler
+    ) -> Optional[NNCFNode]:
         try:
             propagation_graph = width_handler.propagation_graph  # type: PTNNCFGraph
             result = propagation_graph.get_node_by_name(node_name)
         except RuntimeError:
             result = None
         return result
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,71 +1,53 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import csv
 from abc import abstractmethod
 from pathlib import Path
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import NoReturn
-from typing import Optional
-from typing import Tuple
-from typing import TypeVar
-import csv
+from typing import Any, Callable, Dict, NoReturn, Optional, Tuple, TypeVar
 
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.os import safe_open
 
-DataLoaderType = TypeVar('DataLoaderType')
-TModel = TypeVar('TModel')
-EvalFnType = Callable[
-    [
-        TModel
-    ],
-    float
-]
-AccValFnType = Callable[
-    [
-        TModel,
-        DataLoaderType
-    ],
-    float
-]
+DataLoaderType = TypeVar("DataLoaderType")
+TModel = TypeVar("TModel")
+EvalFnType = Callable[[TModel], float]
+AccValFnType = Callable[[TModel, DataLoaderType], float]
 
 
 class BNASEvaluatorStateNames:
-    BNAS_EVALUATOR_STAGE = 'evaluator_state'
+    BNAS_EVALUATOR_STAGE = "evaluator_state"
 
 
 class BaseEvaluator:
     """
     An interface for handling measurements collected on a target device. Evaluators make use
     of functions provided by the users to measure a particular property, e.g., accuracy, latency, etc.
     """
+
     def __init__(self, name: str, ideal_val: float):
         """
         Initializes evaluator
 
         :param name: Name of the evaluator
         :param ideal_val: Ideal value for the metric computed by the evaluator
         """
         self.name = name
         self._current_value = -1
         self._ideal_value = ideal_val
         self.cache = {}
-        #TODO(pablo): Here we should store some super-network signature that is associated with this evaluator
+        # TODO(pablo): Here we should store some super-network signature that is associated with this evaluator
 
     @property
     def current_value(self):
         """
         :return: current value
         """
         return self._current_value
@@ -119,55 +101,55 @@
     def get_state(self) -> Dict[str, Any]:
         """
         Returns state of the evaluatar
 
         :return: Dict with the state of the evaluator
         """
         state_dict = {
-            'name': self.name,
-            'current_value': self._current_value,
-            'ideal_value': self._ideal_value,
-            'cache': self.cache,
+            "name": self.name,
+            "current_value": self._current_value,
+            "ideal_value": self._ideal_value,
+            "cache": self.cache,
         }
         return state_dict
 
     def update_from_state(self, state: Dict[str, Any]) -> NoReturn:
         """
         Updates the cache and other values in the evaluator from a saved state.
 
         :param state: dict with state that should be used for updating this evaluator
         :return:
         """
         new_dict = state.copy()
-        self.name = new_dict['name']
-        self._ideal_value = new_dict['ideal_value']
-        self._current_value = new_dict['current_value']
-        self.cache = new_dict['cache']
+        self.name = new_dict["name"]
+        self._ideal_value = new_dict["ideal_value"]
+        self._current_value = new_dict["current_value"]
+        self.cache = new_dict["cache"]
 
     def load_cache_from_csv(self, cache_file_path: str) -> NoReturn:
         """
         Loads cache from CSV file.
 
         :param cache_file_path: Path to CSV file containing the cache information.
         :return:
         """
-        with safe_open(Path(cache_file_path), 'r', encoding='utf8') as cache_file:
+        with safe_open(Path(cache_file_path), "r", encoding="utf8") as cache_file:
             reader = csv.reader(cache_file)
             for row in reader:
-                rep_tuple = tuple(map(int, row[0][1:len(row[0])-1].split(',')))
+                rep_tuple = tuple(map(int, row[0][1 : len(row[0]) - 1].split(",")))
                 self.add_to_cache(rep_tuple, float(row[1]))
 
     def export_cache_to_csv(self, cache_file_path: str) -> NoReturn:
         """
         Exports cache information to CSV.
 
         :param cache_file_path: Path to export a CSV file with the cache information.
         :return:
         """
-        with safe_open(Path(cache_file_path) / f'cache_{self.name}.csv', 'w', encoding='utf8') as cache_dump:
+        with safe_open(Path(cache_file_path) / f"cache_{self.name}.csv", "w", encoding="utf8") as cache_dump:
             writer = csv.writer(cache_dump)
             for key in self.cache:
                 row = [key, self.cache[key]]
                 writer.writerow(row)
 
 
 class MACsEvaluator(BaseEvaluator):
@@ -187,16 +169,17 @@
 
 
 class AccuracyEvaluator(BaseEvaluator):
     """
     A particular kind of evaluator for collecting model's accuracy measurements
     """
 
-    def __init__(self, model: TModel, eval_func: AccValFnType,
-                 val_loader: DataLoaderType, is_top1: Optional[bool] = True):
+    def __init__(
+        self, model: TModel, eval_func: AccValFnType, val_loader: DataLoaderType, is_top1: Optional[bool] = True
+    ):
         """
         Initializes Accuracy operator
 
         :param eval_func: function used to validate a sub-network
         :param val_loader: Datq loader used by the validation function
         :param is_top1: Whether is top 1 accuracy or top 5.
         :param ref_acc: Accuracy from a model that is used as input to BootstrapNAS
@@ -220,20 +203,20 @@
     def get_state(self) -> Dict[str, Any]:
         """
         Get state of Accuracy evaluator.
 
         :return: Dict with state of evaluator
         """
         state = super().get_state()
-        state['is_top1'] = self._is_top1
+        state["is_top1"] = self._is_top1
         return state
 
     def update_from_state(self, state: Dict[str, Any]) -> NoReturn:
         """
 
         :param state: dict with state that should be used for updating this evaluator
         :return:
         """
 
         super().update_from_state(state)
         new_dict = state.copy()
-        self._is_top1 = new_dict['is_top1']
+        self._is_top1 = new_dict["is_top1"]
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator_handler.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/search/evaluator_handler.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,23 @@
-from typing import NoReturn
-from typing import Optional
-from typing import Tuple
-from typing import TypeVar
+from typing import NoReturn, Optional, Tuple, TypeVar
 
 from nncf.common.logging import nncf_logger
 
-BaseEvaluatorType = TypeVar('BaseEvaluatorType')
-ElasticControllerType = TypeVar('ElasticControllerType')
-SearchParametersType = TypeVar('SearchParametersType')
+BaseEvaluatorType = TypeVar("BaseEvaluatorType")
+ElasticControllerType = TypeVar("ElasticControllerType")
+SearchParametersType = TypeVar("SearchParametersType")
+
 
 class BaseEvaluatorHandler:
     """
     An interface for handling an evaluator. Evaluator handlers initialize the underlying
     evaluator and can be used to update the evaluator's properties.
 
     """
+
     def __init__(self, evaluator: BaseEvaluatorType, elasticity_ctr: ElasticControllerType):
         """
         Initializes the evaluator handler
 
         :param evaluator: An interface for collecting measurements at a target device.
         :param elasticity_ctr: interface to manage the elasticity of the super-network.
         """
@@ -52,14 +51,15 @@
 
 
 class AccuracyEvaluatorHandler(BaseEvaluatorHandler):
     """
     An interface for handling accuracy evaluators
 
     """
+
     def __init__(self, accuracy_evaluator, elasticity_ctrl, ref_acc: Optional[float] = 100):
         super().__init__(accuracy_evaluator, elasticity_ctrl)
         self._ref_acc = ref_acc
 
     @property
     def ref_acc(self) -> float:
         """
@@ -82,15 +82,16 @@
         :param search_params: parameters of the search algorithm
         :return:
         """
         self.ref_acc = search_params.ref_acc
         if self.input_model_value > self.ref_acc - 0.01 or self.input_model_value < self.ref_acc + 0.01:
             nncf_logger.warning(
                 f"Accuracy obtained from evaluation {self.input_model_value} "
-                f"differs from reference accuracy {self.ref_acc}")
+                f"differs from reference accuracy {self.ref_acc}"
+            )
             if self.ref_acc == -1:
                 nncf_logger.info("Adjusting reference accuracy to accuracy obtained from evaluation")
                 self.ref_acc = self.input_model_value
             else:
                 if self.ref_acc >= 100:
                     nncf_logger.error(f"Reference accuracy value is invalid: {self.ref_acc}")
                 nncf_logger.info("Using reference accuracy.")
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/search/search.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/search/search.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,31 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import csv
 from abc import abstractmethod
 from enum import Enum
 from pathlib import Path
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import NoReturn
-from typing import Optional
-from typing import Tuple
-from typing import TypeVar
+from typing import Any, Callable, Dict, List, NoReturn, Optional, Tuple, TypeVar
 
 import numpy as np
 import torch
 from pymoo.algorithms.moo.nsga2 import NSGA2
 from pymoo.core.problem import Problem
 from pymoo.factory import get_crossover
 from pymoo.factory import get_mutation
@@ -46,36 +37,41 @@
 from nncf.experimental.torch.nas.bootstrapNAS.search.evaluator import BaseEvaluator
 from nncf.experimental.torch.nas.bootstrapNAS.search.evaluator import MACsEvaluator
 from nncf.experimental.torch.nas.bootstrapNAS.search.evaluator_handler import AccuracyEvaluatorHandler
 from nncf.experimental.torch.nas.bootstrapNAS.search.evaluator_handler import BaseEvaluatorHandler
 from nncf.experimental.torch.nas.bootstrapNAS.search.evaluator_handler import EfficiencyEvaluatorHandler
 from nncf.torch.nncf_network import NNCFNetwork
 
-DataLoaderType = TypeVar('DataLoaderType')
-TModel = TypeVar('TModel')
-ValFnType = Callable[
-    [
-        TModel,
-        DataLoaderType
-    ],
-    float
-]
+DataLoaderType = TypeVar("DataLoaderType")
+TModel = TypeVar("TModel")
+ValFnType = Callable[[TModel, DataLoaderType], float]
 
 
 class EvolutionaryAlgorithms(Enum):
-    NSGA2 = 'NSGA2'
+    NSGA2 = "NSGA2"
 
 
 class SearchParams:
     """
     Storage class for search parameters.
     """
-    def __init__(self, num_evals: float, num_constraints: float, population: float,
-                 seed: float, crossover_prob: float, crossover_eta: float,
-                 mutation_prob: float, mutation_eta: float, acc_delta: float, ref_acc: float):
+
+    def __init__(
+        self,
+        num_evals: float,
+        num_constraints: float,
+        population: float,
+        seed: float,
+        crossover_prob: float,
+        crossover_eta: float,
+        mutation_prob: float,
+        mutation_eta: float,
+        acc_delta: float,
+        ref_acc: float,
+    ):
         """
         Initializes storage class for search parameters.
 
         :param num_evals: Number of evaluations for the search algorithm.
         :param num_constraints: Number of constraints in search problem
         :param population: Population size
         :param seed: Seed used by the search algorithm.
@@ -96,129 +92,145 @@
         self.crossover_eta = crossover_eta
         self.mutation_prob = mutation_prob
         self.mutation_eta = mutation_eta
         self.acc_delta = acc_delta
         self.ref_acc = ref_acc
 
     @classmethod
-    def from_dict(cls, search_config: Dict[str, Any]) -> 'SearchParams':
+    def from_dict(cls, search_config: Dict[str, Any]) -> "SearchParams":
         """
         Initializes search params storage class from Dict.
 
         :param search_config: Dictionary with search configuration.
         :return: Instance of the storage class
         """
-        num_evals = search_config.get('num_evals', 3000)
-        num_constraints = search_config.get('num_constraints', 0)
-        population = search_config.get('population', 40)
-        seed = search_config.get('seed', 0)
-        crossover_prob = search_config.get('crossover_prob', 0.9)
-        crossover_eta = search_config.get('crossover_eta', 10.0)
-        mutation_prob = search_config.get('mutation_prob', 0.02)
-        mutation_eta = search_config.get('mutation_eta', 3.0)
-        acc_delta = search_config.get('acc_delta', 1)
-        ref_acc = search_config.get('ref_acc', -1)
-
-        return cls(num_evals, num_constraints, population,
-                   seed, crossover_prob, crossover_eta,
-                   mutation_prob, mutation_eta, acc_delta, ref_acc)
+        num_evals = search_config.get("num_evals", 3000)
+        num_constraints = search_config.get("num_constraints", 0)
+        population = search_config.get("population", 40)
+        seed = search_config.get("seed", 0)
+        crossover_prob = search_config.get("crossover_prob", 0.9)
+        crossover_eta = search_config.get("crossover_eta", 10.0)
+        mutation_prob = search_config.get("mutation_prob", 0.02)
+        mutation_eta = search_config.get("mutation_eta", 3.0)
+        acc_delta = search_config.get("acc_delta", 1)
+        ref_acc = search_config.get("ref_acc", -1)
+
+        return cls(
+            num_evals,
+            num_constraints,
+            population,
+            seed,
+            crossover_prob,
+            crossover_eta,
+            mutation_prob,
+            mutation_eta,
+            acc_delta,
+            ref_acc,
+        )
 
 
 class BaseSearchAlgorithm:
     """
     Base class for search algorithms. It contains the evaluators used by search approches.
     """
+
     def __init__(self):
         """
         Initialize BaseSearchAlgorithm class.
 
         """
         self._use_default_evaluators = True
         self._evaluator_handlers = []
         self._accuracy_evaluator_handler = None
         self._efficiency_evaluator_handler = None
         self._log_dir = None
         self._search_records = []
         self.bad_requests = []
         self.best_config = None
         self.best_vals = None
-        self.best_pair_objective = float('inf')
+        self.best_pair_objective = float("inf")
         self._tb = None
 
     @property
     def search_records(self):
         return self._search_records
 
     @abstractmethod
-    def run(self, validate_fn: Callable, val_loader: DataLoader, checkpoint_save_dir: str,
-            efficiency_evaluator: Optional[BaseEvaluator] = None, ref_acc: Optional[float] = 100,
-            tensorboard_writer: Optional[SummaryWriter] = None) -> Tuple[ElasticityController,
-                                                                         SubnetConfig, Tuple[float, ...]]:
+    def run(
+        self,
+        validate_fn: Callable,
+        val_loader: DataLoader,
+        checkpoint_save_dir: str,
+        efficiency_evaluator: Optional[BaseEvaluator] = None,
+        ref_acc: Optional[float] = 100,
+        tensorboard_writer: Optional[SummaryWriter] = None,
+    ) -> Tuple[ElasticityController, SubnetConfig, Tuple[float, ...]]:
         """This method should implement how to run the search algorithm."""
 
-    def search_progression_to_csv(self, filename='search_progression.csv') -> NoReturn:
+    def search_progression_to_csv(self, filename="search_progression.csv") -> NoReturn:
         """
         Exports search progression to CSV file
 
         :param filename: path to save the CSV file.
         :return:
         """
-        with safe_open(Path(self._log_dir) / filename, 'w', encoding='utf8') as progression:
+        with safe_open(Path(self._log_dir) / filename, "w", encoding="utf8") as progression:
             writer = csv.writer(progression)
             for record in self.search_records:
                 writer.writerow(record)
 
 
 class SearchAlgorithm(BaseSearchAlgorithm):
-    def __init__(self,
-                 model: NNCFNetwork,
-                 elasticity_ctrl: ElasticityController,
-                 nncf_config: NNCFConfig,
-                 verbose=True):
+    def __init__(
+        self, model: NNCFNetwork, elasticity_ctrl: ElasticityController, nncf_config: NNCFConfig, verbose=True
+    ):
         """
         Initializes search algorithm
 
         :param model: Super-network
         :param elasticity_ctrl: interface to manage the elasticity of the super-network.
         :param nncf_config: Configuration file.
         :param verbose:
         """
         super().__init__()
         self._model = model
         self._elasticity_ctrl = elasticity_ctrl
-        search_config = nncf_config.get('bootstrapNAS', {}).get('search', {})
+        search_config = nncf_config.get("bootstrapNAS", {}).get("search", {})
         self.num_obj = None
         self.search_params = SearchParams.from_dict(search_config)
         self._log_dir = nncf_config.get("log_dir", ".")
         self._verbose = verbose
         self._top1_accuracy_validation_fn = None
         self._val_loader = None
-        evo_algo = search_config['algorithm']
+        evo_algo = search_config["algorithm"]
         if evo_algo == EvolutionaryAlgorithms.NSGA2.value:
-            self._algorithm = NSGA2(pop_size=self.search_params.population,
-                                    sampling=get_sampling("int_lhs"),
-                                    crossover=get_crossover("int_sbx", prob=self.search_params.crossover_prob,
-                                                            eta=self.search_params.crossover_eta),
-                                    mutation=get_mutation("int_pm", prob=self.search_params.mutation_prob,
-                                                          eta=self.search_params.mutation_eta),
-                                    eliminate_duplicates=True,
-                                    save_history=True,
-                                    )
+            self._algorithm = NSGA2(
+                pop_size=self.search_params.population,
+                sampling=get_sampling("int_lhs"),
+                crossover=get_crossover(
+                    "int_sbx", prob=self.search_params.crossover_prob, eta=self.search_params.crossover_eta
+                ),
+                mutation=get_mutation(
+                    "int_pm", prob=self.search_params.mutation_prob, eta=self.search_params.mutation_eta
+                ),
+                eliminate_duplicates=True,
+                save_history=True,
+            )
         else:
             raise NotImplementedError(f"Evolutionary Search Algorithm {evo_algo} not implemented")
         self._num_vars = 0
         self._vars_lower = 0
         self._vars_upper = []
 
         self._num_vars, self._vars_upper = self._elasticity_ctrl.multi_elasticity_handler.get_design_vars_info()
         if self._num_vars == 0 or self._vars_lower is None:
             raise RuntimeError("Search space is empty")
 
         self._result = None
-        bn_adapt_params = search_config.get('batchnorm_adaptation', {})
+        bn_adapt_params = search_config.get("batchnorm_adaptation", {})
         bn_adapt_algo_kwargs = get_bn_adapt_algo_kwargs(nncf_config, bn_adapt_params)
         self.bn_adaptation = BatchnormAdaptationAlgorithm(**bn_adapt_algo_kwargs) if bn_adapt_algo_kwargs else None
 
         self._problem = None
         self.checkpoint_save_dir = None
         self.type_var = np.int
 
@@ -283,22 +295,29 @@
         :param elasticity_ctrl: Interface to manage elasticity of super-network
         :param nncf_config: Dict with configuration for search algorithm
         :return: instance of the search algorithm.
         """
         return cls(model, elasticity_ctrl, nncf_config)
 
     @classmethod
-    def from_checkpoint(cls, model: NNCFNetwork, elasticity_ctrl: ElasticityController, bn_adapt_args,
-                        resuming_checkpoint_path: str) -> 'SearchAlgorithm':
+    def from_checkpoint(
+        cls, model: NNCFNetwork, elasticity_ctrl: ElasticityController, bn_adapt_args, resuming_checkpoint_path: str
+    ) -> "SearchAlgorithm":
         raise NotImplementedError
 
-    def run(self, validate_fn: ValFnType, val_loader: DataLoaderType, checkpoint_save_dir: str,
-            efficiency_evaluator: Optional[BaseEvaluator] = None, ref_acc: Optional[float] = 100,
-            tensorboard_writer: Optional[SummaryWriter] = None, evaluator_checkpoint = None) \
-            -> Tuple[ElasticityController, SubnetConfig, Tuple[float, ...]]:
+    def run(
+        self,
+        validate_fn: ValFnType,
+        val_loader: DataLoaderType,
+        checkpoint_save_dir: str,
+        efficiency_evaluator: Optional[BaseEvaluator] = None,
+        ref_acc: Optional[float] = 100,
+        tensorboard_writer: Optional[SummaryWriter] = None,
+        evaluator_checkpoint=None,
+    ) -> Tuple[ElasticityController, SubnetConfig, Tuple[float, ...]]:
         """
         Runs the search algorithm
 
         :param validate_fn: Function used to validate the accuracy of the model.
         :param val_loader: Data loader used by the validation function.
         :param checkpoint_save_dir: Path to save checkpoints.
         :param efficiency_evaluator: External efficiency evaluator.
@@ -310,41 +329,45 @@
         """
         self._elasticity_ctrl.multi_elasticity_handler.activate_maximum_subnet()
         nncf_logger.info("Searching for optimal subnet.")
         if ref_acc != 100:
             self.search_params.ref_acc = ref_acc
         self._tb = tensorboard_writer
         self.checkpoint_save_dir = checkpoint_save_dir
-        self._accuracy_evaluator_handler = AccuracyEvaluatorHandler(AccuracyEvaluator(
-                                                                    self._model, validate_fn, val_loader),
-                                                                    self._elasticity_ctrl)
+        self._accuracy_evaluator_handler = AccuracyEvaluatorHandler(
+            AccuracyEvaluator(self._model, validate_fn, val_loader), self._elasticity_ctrl
+        )
         self._accuracy_evaluator_handler.update_reference_accuracy(self.search_params)
         self.num_obj = 2
         if efficiency_evaluator is not None:
             self._use_default_evaluators = False
             self._efficiency_evaluator_handler = EfficiencyEvaluatorHandler(efficiency_evaluator, self._elasticity_ctrl)
         else:
             self._use_default_evaluators = True
 
             def get_macs_for_active_subnet() -> float:
                 flops, _ = self._elasticity_ctrl.multi_elasticity_handler.count_flops_and_weights_for_active_subnet()
                 return flops / 2000000  # MACs
-            self._efficiency_evaluator_handler = EfficiencyEvaluatorHandler(MACsEvaluator(get_macs_for_active_subnet),
-                                                                            self._elasticity_ctrl)
+
+            self._efficiency_evaluator_handler = EfficiencyEvaluatorHandler(
+                MACsEvaluator(get_macs_for_active_subnet), self._elasticity_ctrl
+            )
         self._evaluator_handlers.append(self._efficiency_evaluator_handler)
         self._evaluator_handlers.append(self._accuracy_evaluator_handler)
-        self.maximal_vals = [evaluator_handler.current_value for evaluator_handler
-                                  in self._evaluator_handlers]
+        self.maximal_vals = [evaluator_handler.current_value for evaluator_handler in self._evaluator_handlers]
 
         self._problem = SearchProblem(self)
-        self._result = minimize(self._problem, self._algorithm,
-                                ('n_gen', int(self.search_params.num_evals / self.search_params.population)),
-                                seed=self.search_params.seed,
-                                # save_history=True,
-                                verbose=self._verbose)
+        self._result = minimize(
+            self._problem,
+            self._algorithm,
+            ("n_gen", int(self.search_params.num_evals / self.search_params.population)),
+            seed=self.search_params.seed,
+            # save_history=True,
+            verbose=self._verbose,
+        )
 
         if self.best_config is not None:
             self._elasticity_ctrl.multi_elasticity_handler.activate_subnet_for_config(self.best_config)
             if self.bn_adaptation is not None:
                 self.bn_adaptation.run(self._model)
             ret_vals = self.best_vals
         else:
@@ -354,54 +377,74 @@
                 self.bn_adaptation.run(self._model)
             self.best_config = self._elasticity_ctrl.multi_elasticity_handler.get_active_config()
             self.best_vals = [None, None]
             ret_vals = self.maximal_vals
 
         return self._elasticity_ctrl, self.best_config, [abs(elem) for elem in ret_vals if elem is not None]
 
-    @skip_if_dependency_unavailable(dependencies=['matplotlib.pyplot'])
-    def visualize_search_progression(self, filename='search_progression') -> NoReturn:
+    @skip_if_dependency_unavailable(dependencies=["matplotlib.pyplot"])
+    def visualize_search_progression(self, filename="search_progression") -> NoReturn:
         """
         Visualizes search progression and saves the resulting figure.
 
         :param filename:
         :return:
         """
         import matplotlib.pyplot as plt
+
         plt.figure()
-        colormap = plt.cm.get_cmap('viridis')
+        colormap = plt.cm.get_cmap("viridis")
         col = range(int(self.search_params.num_evals / self.search_params.population))
         for i in range(0, len(self.search_records), self.search_params.population):
-            c = [col[int(i/self.search_params.population)]]*len(self.search_records[i:i+self.search_params.population])
-            plt.scatter([abs(row[2]) for row in self.search_records][i:i+self.search_params.population],
-                        [abs(row[4]) for row in self.search_records][i:i+self.search_params.population],
-                        s=9, c=c, alpha=0.5,
-                        marker='D', cmap=colormap)
-        plt.scatter(*tuple(abs(ev.input_model_value) for ev in self.evaluator_handlers),
-                    marker='s', s=120, color='blue', label='Input Model', edgecolors='black')
+            c = [col[int(i / self.search_params.population)]] * len(
+                self.search_records[i : i + self.search_params.population]
+            )
+            plt.scatter(
+                [abs(row[2]) for row in self.search_records][i : i + self.search_params.population],
+                [abs(row[4]) for row in self.search_records][i : i + self.search_params.population],
+                s=9,
+                c=c,
+                alpha=0.5,
+                marker="D",
+                cmap=colormap,
+            )
+        plt.scatter(
+            *tuple(abs(ev.input_model_value) for ev in self.evaluator_handlers),
+            marker="s",
+            s=120,
+            color="blue",
+            label="Input Model",
+            edgecolors="black",
+        )
         if None not in self.best_vals:
-            plt.scatter(*tuple(abs(val) for val in self.best_vals),
-                        marker='o', s=120,color='yellow', label='BootstrapNAS A',
-                        edgecolors='black', linewidth=2.5)
+            plt.scatter(
+                *tuple(abs(val) for val in self.best_vals),
+                marker="o",
+                s=120,
+                color="yellow",
+                label="BootstrapNAS A",
+                edgecolors="black",
+                linewidth=2.5,
+            )
         plt.legend()
-        plt.title('Search Progression')
+        plt.title("Search Progression")
         plt.xlabel(self.efficiency_evaluator_handler.name)
         plt.ylabel(self.accuracy_evaluator_handler.name)
-        plt.savefig(f'{self._log_dir}/{filename}.png')
+        plt.savefig(f"{self._log_dir}/{filename}.png")
 
     def save_evaluators_state(self) -> NoReturn:
         """
         Save state of evaluators used in search.
         :return:
         """
         evaluator_handlers_state = []
         for evaluator_handler in self._evaluator_handlers:
             eval_state = evaluator_handler.evaluator.get_state()
             evaluator_handlers_state.append(eval_state)
-        torch.save(evaluator_handlers_state, Path(self.checkpoint_save_dir, 'evaluators_state.pth'))
+        torch.save(evaluator_handlers_state, Path(self.checkpoint_save_dir, "evaluators_state.pth"))
 
     def evaluators_to_csv(self) -> NoReturn:
         """
         Export evaluators' information used by search algorithm to CSV
         :return:
         """
         for evaluator_handler in self.evaluator_handlers:
@@ -423,20 +466,22 @@
 
     def __init__(self, search: SearchAlgorithm):
         """
         Initializes search problem
 
         :param search: search algorithm.
         """
-        super().__init__(n_var=search.num_vars,
-                         n_obj=search.num_obj,
-                         n_constr=search.search_params.num_constraints,
-                         xl=search.vars_lower,
-                         xu=search.vars_upper,
-                         type_var=search.type_var)
+        super().__init__(
+            n_var=search.num_vars,
+            n_obj=search.num_obj,
+            n_constr=search.search_params.num_constraints,
+            xl=search.vars_lower,
+            xu=search.vars_upper,
+            type_var=search.type_var,
+        )
         self._search = search
         self._search_records = search.search_records
         self._elasticity_handler = self._search._elasticity_ctrl.multi_elasticity_handler
         self._dims_enabled = self._elasticity_handler.get_available_elasticity_dims()
         self._iter = 0
         self._evaluator_handlers = search.evaluator_handlers
         self._accuracy_evaluator_handler = search.accuracy_evaluator_handler
@@ -497,16 +542,17 @@
         """
         acc_within_tolerance = self._accuracy_evaluator_handler.current_value
         pair_objective = self._efficiency_evaluator_handler.current_value
         if acc_within_tolerance < (self._lower_bound_acc * -1.0):
             if pair_objective < self._search.best_pair_objective:
                 self._search.best_pair_objective = pair_objective
                 self._search.best_config = config
-                self._search.best_vals = [evaluator_handler.current_value for evaluator_handler
-                                          in self._evaluator_handlers]
-                checkpoint_path = Path(self._search.checkpoint_save_dir, 'subnetwork_best.pth')
+                self._search.best_vals = [
+                    evaluator_handler.current_value for evaluator_handler in self._evaluator_handlers
+                ]
+                checkpoint_path = Path(self._search.checkpoint_save_dir, "subnetwork_best.pth")
                 checkpoint = {
-                    'best_acc1': acc_within_tolerance * -1.0,
-                    'best_efficiency': pair_objective,
-                    'subnet_config': config
+                    "best_acc1": acc_within_tolerance * -1.0,
+                    "best_efficiency": pair_objective,
+                    "subnet_config": config,
                 }
                 torch.save(checkpoint, checkpoint_path)
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/base_training.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/base_training.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from abc import ABC
 from abc import abstractmethod
 
-from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_controller import ElasticityController
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.multi_elasticity_handler import MultiElasticityHandler
 from nncf.experimental.torch.nas.bootstrapNAS.training.stage_descriptor import StageDescriptor
+from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 
 
 class BNASTrainingAlgorithm(ABC):
     """
     Base training algorithm for supernet-based NAS methods.
     """
 
@@ -54,14 +52,15 @@
         """
 
 
 class BNASTrainingController(PTCompressionAlgorithmController, BNASTrainingAlgorithm, ABC):
     """
     A base class for BootstrapNAS training controllers that provides capabilities for supernet training.
     """
+
     @property
     @abstractmethod
     def multi_elasticity_handler(self) -> MultiElasticityHandler:
         """
         Gets access to multi elasticity handler to perform some actions with supernet or subnets.
 
         :return: multi elasticity handler
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,70 +1,91 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from abc import abstractmethod
 import math
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import TypeVar
+from abc import abstractmethod
+from typing import Any, Dict, List, Optional, TypeVar
 
 from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.experimental.torch.nas.bootstrapNAS.training.stage_descriptor import StageDescriptor
 
-OptimizerType = TypeVar('OptimizerType')
+OptimizerType = TypeVar("OptimizerType")
 
 
-def adjust_learning_rate(optimizer: OptimizerType, epoch: float, init_lr: float, epochs: float,
-                         batch: float = 0, n_batch: float = 0, lr_schedule_type: str = 'cosine'):
+def adjust_learning_rate(
+    optimizer: OptimizerType,
+    epoch: float,
+    init_lr: float,
+    epochs: float,
+    batch: float = 0,
+    n_batch: float = 0,
+    lr_schedule_type: str = "cosine",
+):
     new_lr = calc_learning_rate(epoch, init_lr, epochs, batch, n_batch, lr_schedule_type)
     for param_group in optimizer.param_groups:
-        param_group['lr'] = new_lr
+        param_group["lr"] = new_lr
     return new_lr
 
 
-def warmup_adjust_learning_rate(optimizer: OptimizerType, init_lr: float, t_total: float, n_batch: float,
-                                epoch: float, batch: float = 0, warmup_lr: float = 0):
+def warmup_adjust_learning_rate(
+    optimizer: OptimizerType,
+    init_lr: float,
+    t_total: float,
+    n_batch: float,
+    epoch: float,
+    batch: float = 0,
+    warmup_lr: float = 0,
+):
     t_cur = epoch * n_batch + batch + 1
     new_lr = t_cur / t_total * (init_lr - warmup_lr) + warmup_lr
     for param_group in optimizer.param_groups:
-        param_group['lr'] = new_lr
+        param_group["lr"] = new_lr
     return new_lr
 
 
-def calc_learning_rate(epoch: float, init_lr: float, n_epochs: float,
-                       batch: float = 0, n_batch: float = 0, lr_schedule_type: str = 'cosine'):
-    if lr_schedule_type == 'cosine':
+def calc_learning_rate(
+    epoch: float,
+    init_lr: float,
+    n_epochs: float,
+    batch: float = 0,
+    n_batch: float = 0,
+    lr_schedule_type: str = "cosine",
+):
+    if lr_schedule_type == "cosine":
         t_total = n_epochs * n_batch
         t_cur = epoch * n_batch + batch
         lr = 0.5 * init_lr * (1 + math.cos(math.pi * t_cur / t_total))
     elif lr_schedule_type is None:
         lr = init_lr
     else:
-        raise ValueError('do not support: %s' % lr_schedule_type)
+        raise ValueError("do not support: %s" % lr_schedule_type)
     return lr
 
 
 class LRSchedulerParams:
     """
     Storage class for LR Scheduler parameters
     """
-    def __init__(self, num_steps_in_epoch: float, base_lr: float = 3.4e-4, num_epochs: float = 0,
-                 warmup_epochs: float = 0, warmup_lr: float = 0):
+
+    def __init__(
+        self,
+        num_steps_in_epoch: float,
+        base_lr: float = 3.4e-4,
+        num_epochs: float = 0,
+        warmup_epochs: float = 0,
+        warmup_lr: float = 0,
+    ):
         """
         Initializes storage class for learning rate scheduler parameters
 
         :param num_steps_in_epoch:
         :param base_lr:
         :param num_epochs:
         :param warmup_epochs:
@@ -73,130 +94,149 @@
         self.num_steps_in_epoch = num_steps_in_epoch
         self.base_lr = base_lr
         self.num_epochs = num_epochs
         self.warmup_epochs = warmup_epochs
         self.warmup_lr = warmup_lr
 
     @classmethod
-    def from_dict(cls, lr_scheduler_config: Dict[str, Any]) -> 'LRSchedulerParams':
+    def from_dict(cls, lr_scheduler_config: Dict[str, Any]) -> "LRSchedulerParams":
         """
         Initialize learning rate scheduler parameters storage clas from Dict.
         :param lr_scheduler_config: Dict with parameters of learning rate scheduler.
         :return:
         """
-        num_steps_in_epoch = lr_scheduler_config.get('num_steps_in_epoch', 0)
-        base_lr = lr_scheduler_config.get('base_lr', 3.4e-4)
-        num_epochs = lr_scheduler_config.get('num_epochs', 0)
-        warmup_epochs = lr_scheduler_config.get('warmup_epochs', 0)
-        warmup_lr = lr_scheduler_config.get('warmup_lr', 3.4e-4)
+        num_steps_in_epoch = lr_scheduler_config.get("num_steps_in_epoch", 0)
+        base_lr = lr_scheduler_config.get("base_lr", 3.4e-4)
+        num_epochs = lr_scheduler_config.get("num_epochs", 0)
+        warmup_epochs = lr_scheduler_config.get("warmup_epochs", 0)
+        warmup_lr = lr_scheduler_config.get("warmup_lr", 3.4e-4)
         return cls(num_steps_in_epoch, base_lr, num_epochs, warmup_epochs, warmup_lr)
 
+
 class BaseLRScheduler(BaseCompressionScheduler):
     """
     Base class for the learning rate scheduler
     """
+
     def __init__(self, optimizer: OptimizerType, num_steps_in_epoch: float):
         super().__init__()
         self._optimizer = optimizer
         self._num_steps_in_epoch = num_steps_in_epoch
 
     @abstractmethod
     def stage_step(self, stage_desc: StageDescriptor):
         pass
 
     @classmethod
     def from_state(cls, state: Dict[str, Any], optimizer: OptimizerType):
         return cls(optimizer, **state)
 
     def get_last_lr(self) -> List[Any]:
-        return [group['lr'] for group in self._optimizer.param_groups]
+        return [group["lr"] for group in self._optimizer.param_groups]
+
 
 class GlobalLRScheduler(BaseLRScheduler):
     """
     Global LR scheduler prevents LR adjustments per stage.
     """
-    def __init__(self, optimizer: OptimizerType, num_steps_in_epoch: float, *,
-                 base_lr: float, num_epochs: float, warmup_epochs: float = 0,
-                 warmup_lr: float = 3.4e-4):
+
+    def __init__(
+        self,
+        optimizer: OptimizerType,
+        num_steps_in_epoch: float,
+        *,
+        base_lr: float,
+        num_epochs: float,
+        warmup_epochs: float = 0,
+        warmup_lr: float = 3.4e-4
+    ):
         super().__init__(optimizer, num_steps_in_epoch)
         self._base_lr = base_lr
         self._num_epochs = num_epochs
         self._warmup_epochs = warmup_epochs
         self._warmup_lr = warmup_lr
 
     @classmethod
-    def from_config(cls, optimizer: OptimizerType, params: 'LRSchedulerParams'):
+    def from_config(cls, optimizer: OptimizerType, params: "LRSchedulerParams"):
         return cls(optimizer, **params.__dict__)
 
     def stage_step(self, stage_desc: StageDescriptor):
         # do nothing
         pass
 
     def step(self, next_step: Optional[int] = None) -> None:
         super().step(next_step)
-        step_from_epoch_start = self.current_step - (self.current_epoch*(self._num_steps_in_epoch+1))
+        step_from_epoch_start = self.current_step - (self.current_epoch * (self._num_steps_in_epoch + 1))
         if self.current_epoch < self._warmup_epochs and self.current_epoch != -1:
-            warmup_adjust_learning_rate(optimizer=self._optimizer,
-                                                 init_lr=self._base_lr,
-                                                 t_total=self._warmup_epochs * self._num_steps_in_epoch,
-                                                 n_batch=self._num_steps_in_epoch,
-                                                 epoch=self.current_epoch,
-                                                 batch=step_from_epoch_start,
-                                                 warmup_lr=self._warmup_lr)
+            warmup_adjust_learning_rate(
+                optimizer=self._optimizer,
+                init_lr=self._base_lr,
+                t_total=self._warmup_epochs * self._num_steps_in_epoch,
+                n_batch=self._num_steps_in_epoch,
+                epoch=self.current_epoch,
+                batch=step_from_epoch_start,
+                warmup_lr=self._warmup_lr,
+            )
         else:
-            adjust_learning_rate(optimizer=self._optimizer,
-                                          epoch=self.current_epoch - self._warmup_epochs,
-                                          init_lr=self._base_lr,
-                                          epochs=self._num_epochs,
-                                          batch=step_from_epoch_start,
-                                          n_batch=self._num_steps_in_epoch,
-                                          lr_schedule_type='cosine')
+            adjust_learning_rate(
+                optimizer=self._optimizer,
+                epoch=self.current_epoch - self._warmup_epochs,
+                init_lr=self._base_lr,
+                epochs=self._num_epochs,
+                batch=step_from_epoch_start,
+                n_batch=self._num_steps_in_epoch,
+                lr_schedule_type="cosine",
+            )
 
     def get_state(self) -> Dict[str, Any]:
         state_dict = {
-            'num_steps_in_epoch': self._num_steps_in_epoch,
-            'base_lr': self._base_lr,
-            'num_epochs': self._num_epochs,
-            'warmup_epochs': self._warmup_epochs,
-            'warmup_lr': self._warmup_lr
+            "num_steps_in_epoch": self._num_steps_in_epoch,
+            "base_lr": self._base_lr,
+            "num_epochs": self._num_epochs,
+            "warmup_epochs": self._warmup_epochs,
+            "warmup_lr": self._warmup_lr,
         }
         return state_dict
 
+
 class StageLRScheduler(BaseLRScheduler):
     """
     Stage learning rate scheduler. Allows adjustment of the learning rate at a stage transition.
     """
+
     def __init__(self, optimizer: OptimizerType, num_steps_in_epoch: float):
         super().__init__(optimizer, num_steps_in_epoch)
         self._init_lr = None
         self._num_epochs = None
 
     @classmethod
     def from_config(cls, optimizer: OptimizerType, params: LRSchedulerParams):
         return cls(optimizer, params.num_steps_in_epoch)
 
     def stage_step(self, stage_desc: StageDescriptor):
         self.reset(stage_desc.init_lr, stage_desc.epochs_lr)
 
     def step(self, next_step: Optional[int] = None) -> None:
         super().step(next_step)
-        step_from_epoch_start = self.current_step - (self.current_epoch*(self._num_steps_in_epoch+1))
-        adjust_learning_rate(optimizer=self._optimizer,
-                             epoch=self.current_epoch,
-                             init_lr=self._init_lr,
-                             epochs=self._num_epochs,
-                             batch=step_from_epoch_start,
-                             n_batch=self._num_steps_in_epoch,
-                             lr_schedule_type='cosine')
+        step_from_epoch_start = self.current_step - (self.current_epoch * (self._num_steps_in_epoch + 1))
+        adjust_learning_rate(
+            optimizer=self._optimizer,
+            epoch=self.current_epoch,
+            init_lr=self._init_lr,
+            epochs=self._num_epochs,
+            batch=step_from_epoch_start,
+            n_batch=self._num_steps_in_epoch,
+            lr_schedule_type="cosine",
+        )
 
     def get_state(self) -> Dict[str, Any]:
         state_dict = {
-            'num_steps_in_epoch': self._num_steps_in_epoch,
-            'init_lr': self._init_lr,
-            'num_epochs': self._num_epochs
+            "num_steps_in_epoch": self._num_steps_in_epoch,
+            "init_lr": self._init_lr,
+            "num_epochs": self._num_epochs,
         }
         return state_dict
 
     def reset(self, init_lr: float, num_epochs: float):
         """
         Resets the learning rate for the current stage
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,42 +1,35 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from os import path as osp
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
+from typing import Any, Dict, List, Optional, Tuple
 
 from nncf.common.compression import BaseCompressionAlgorithmController as BaseController
 from nncf.common.utils.debug import set_debug_log_dir
 from nncf.config import NNCFConfig
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.model_creation import create_compression_algorithm_builder_from_algo_names
 from nncf.torch.model_creation import synchronize_all_processes_in_distributed_mode
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.utils import is_main_process
 
 
-def create_compressed_model_from_algo_names(nncf_network: NNCFNetwork,
-                                            config: NNCFConfig,
-                                            algo_names: List[str],
-                                            dump_graphs: bool = True) -> Tuple[BaseController, NNCFNetwork]:
+def create_compressed_model_from_algo_names(
+    nncf_network: NNCFNetwork, config: NNCFConfig, algo_names: List[str], dump_graphs: bool = True
+) -> Tuple[BaseController, NNCFNetwork]:
     """
     The main function used to produce a model ready for compression fine-tuning from empty NNCFNetwork,
     a configuration object and a list of compression algorithm names.
 
     :param nncf_network: empty NNCFNetwork, that just wrapped original PyTorch model.
     :param config: A configuration object used to determine the exact compression modifications to be applied
     to the model
@@ -46,36 +39,37 @@
     :return: A controller for the compression algorithm (or algorithms, in which case the controller
     is an instance of CompositeCompressionController) and the model ready for compression parameter training wrapped
     as an object of NNCFNetwork.
     """
     set_debug_log_dir(config.get("log_dir", ""))
 
     if dump_graphs:
-        original_model_graph = nncf_network.get_original_graph()
+        original_model_graph = nncf_network.nncf.get_original_graph()
         original_model_graph.visualize_graph(osp.join(config.get("log_dir", ""), "original_graph.dot"))
 
     builder = create_compression_algorithm_builder_from_algo_names(algo_names, config, should_init=True)
 
     compressed_model = builder.apply_to(nncf_network)
     compression_ctrl = builder.build_controller(compressed_model)
 
     # Required to ensure that the model leaving create_compressed_model has correct compressed graph.
     # In particular, this is currently required for correct functioning of RNNs.
-    compressed_model.rebuild_graph()
+    compressed_model.nncf.rebuild_graph()
 
     if dump_graphs and is_main_process():
-        original_model_graph = compressed_model.get_graph()
+        original_model_graph = compressed_model.nncf.get_graph()
         original_model_graph.visualize_graph(osp.join(config.get("log_dir", ""), "compressed_graph.dot"))
 
     synchronize_all_processes_in_distributed_mode()
     return compression_ctrl, compressed_model
 
 
-def resume_compression_algorithm_builder(compression_state: Dict[str, Any],
-                                         config: Optional[NNCFConfig] = None) -> PTCompressionAlgorithmBuilder:
+def resume_compression_algorithm_builder(
+    compression_state: Dict[str, Any], config: Optional[NNCFConfig] = None
+) -> PTCompressionAlgorithmBuilder:
     """
     Resume compression builder from its state.
 
     :param compression_state: representation of the entire compression state to unambiguously restore
     the compressed model. Includes builder and controller states.
     :param config: A configuration object used to determine the exact compression modifications to be applied
     to the model. NNCFConfig is required for resume from checkpoint, because currently CompressionBuilder takes
@@ -89,18 +83,17 @@
     builder_state = compression_state[BaseController.BUILDER_STATE]
     algo_names = list(builder_state)
     builder = create_compression_algorithm_builder_from_algo_names(algo_names, config, should_init=False)
     builder.load_state(builder_state)
     return builder
 
 
-def resume_compression_from_state(nncf_network: NNCFNetwork,
-                                  compression_state: Dict[str, Any],
-                                  config: Optional[NNCFConfig] = None) \
-    -> Tuple[NNCFNetwork, PTCompressionAlgorithmController]:
+def resume_compression_from_state(
+    nncf_network: NNCFNetwork, compression_state: Dict[str, Any], config: Optional[NNCFConfig] = None
+) -> Tuple[NNCFNetwork, PTCompressionAlgorithmController]:
     """
     Resumes compression model
 
     :param nncf_network: empty NNCFNetwork, that just wrapped original PyTorch model.
     :param compression_state: representation of the entire compression state to unambiguously restore
     the compressed model. Includes builder and controller states.
     :param config: is needed for overriding state or for passing extra structs only, like BNAdaptInitArgs required
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,100 +1,105 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Any
-from typing import Dict
-from typing import List
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Any, Dict, List
 
 from nncf import NNCFConfig
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
 from nncf.config.extractors import get_bn_adapt_algo_kwargs
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_builder import ElasticityBuilder
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
-from nncf.experimental.torch.nas.bootstrapNAS.training.progressive_shrinking_controller import \
-    ProgressiveShrinkingController
+from nncf.experimental.torch.nas.bootstrapNAS.training.progressive_shrinking_controller import (
+    ProgressiveShrinkingController,
+)
 from nncf.experimental.torch.nas.bootstrapNAS.training.scheduler import NASSchedulerParams
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.nncf_network import NNCFNetwork
 
 
 class PSBuilderStateNames:
-    ELASTICITY_BUILDER_STATE = 'elasticity_builder_state'
-    PROGRESSIVITY_OF_ELASTICITY = 'progressivity_of_elasticity'
-    BN_ADAPTATION_PARAMS = 'bn_adaptation_params'
+    ELASTICITY_BUILDER_STATE = "elasticity_builder_state"
+    PROGRESSIVITY_OF_ELASTICITY = "progressivity_of_elasticity"
+    BN_ADAPTATION_PARAMS = "bn_adaptation_params"
 
 
-@PT_COMPRESSION_ALGORITHMS.register('progressive_shrinking')
+@PT_COMPRESSION_ALGORITHMS.register("progressive_shrinking")
 class ProgressiveShrinkingBuilder(PTCompressionAlgorithmBuilder):
     """
     Determines which modifications should be made to the original FP32 model in
     order to train a supernet using Progressive Shrinking procedure from OFA (https://arxiv.org/abs/1908.09791).
     Operates on an NNCFNetwork object wrapping a target PyTorch model (torch.nn.Module).
     """
 
     DEFAULT_PROGRESSIVITY = [ElasticityDim.KERNEL, ElasticityDim.DEPTH, ElasticityDim.WIDTH]
     _state_names = PSBuilderStateNames
 
     def __init__(self, nncf_config: NNCFConfig, should_init: bool = True):
         super().__init__(nncf_config, should_init)
-        self._bn_adapt_params = self._algo_config.get('batchnorm_adaptation', {})
+        self._bn_adapt_params = self._algo_config.get("batchnorm_adaptation", {})
         bn_adapt_algo_kwargs = get_bn_adapt_algo_kwargs(nncf_config, self._bn_adapt_params)
         self._bn_adaptation = BatchnormAdaptationAlgorithm(**bn_adapt_algo_kwargs) if bn_adapt_algo_kwargs else None
 
         default_progressivity = map(lambda x: x.value, self.DEFAULT_PROGRESSIVITY)
-        progressivity_of_elasticity = self._algo_config.get('progressivity_of_elasticity', default_progressivity)
+        progressivity_of_elasticity = self._algo_config.get("progressivity_of_elasticity", default_progressivity)
         self._progressivity_of_elasticity = list(map(ElasticityDim, progressivity_of_elasticity))
         self._elasticity_builder = ElasticityBuilder(self.config, self.should_init)
 
-        self._lr_schedule_config = self._algo_config.get('lr_schedule', {})
+        self._lr_schedule_config = self._algo_config.get("lr_schedule", {})
 
     @staticmethod
-    def check_elasticity_dims_consistency(available_elasticity_dims: List[ElasticityDim],
-                                          progressivity_of_elasticity: List[ElasticityDim]) -> None:
+    def check_elasticity_dims_consistency(
+        available_elasticity_dims: List[ElasticityDim], progressivity_of_elasticity: List[ElasticityDim]
+    ) -> None:
         """
         Verifies that progressivity of elasticity is specified for all available elasticity dimensions.
 
         :param available_elasticity_dims: list of available elasticity dimension
         :param progressivity_of_elasticity: specifies in which order elasticity should be added
         """
         for dim in available_elasticity_dims:
             if dim not in progressivity_of_elasticity:
-                raise ValueError(f'Invalid elasticity dimension {dim} specified as available in `elasticity` section.'
-                                 f' This dimension is not part of the progressivity_of_elasticity='
-                                 f'{progressivity_of_elasticity} which defines order of adding elasticity dimension'
-                                 f' by going from one training stage to another.')
+                raise ValueError(
+                    f"Invalid elasticity dimension {dim} specified as available in `elasticity` section."
+                    f" This dimension is not part of the progressivity_of_elasticity="
+                    f"{progressivity_of_elasticity} which defines order of adding elasticity dimension"
+                    f" by going from one training stage to another."
+                )
 
     def initialize(self, model: NNCFNetwork) -> None:
         """
         Initialize model parameters before training
 
         :param model: The model with additional modifications necessary to enable
             algorithm-specific compression during fine-tuning.
         """
 
     def _get_algo_specific_config_section(self) -> Dict:
-        return self.config.get('bootstrapNAS', {}).get('training', {})
+        return self.config.get("bootstrapNAS", {}).get("training", {})
 
-    def _build_controller(self, model: NNCFNetwork) -> 'ProgressiveShrinkingController':
+    def _build_controller(self, model: NNCFNetwork) -> "ProgressiveShrinkingController":
         elasticity_ctrl = self._elasticity_builder.build_controller(model)
-        schedule_params = NASSchedulerParams.from_config(self._algo_config.get('schedule', {}))
+        schedule_params = NASSchedulerParams.from_config(self._algo_config.get("schedule", {}))
         return ProgressiveShrinkingController(
-            model, elasticity_ctrl, self._bn_adaptation, self._progressivity_of_elasticity, schedule_params,
-            self._lr_schedule_config)
+            model,
+            elasticity_ctrl,
+            self._bn_adaptation,
+            self._progressivity_of_elasticity,
+            schedule_params,
+            self._lr_schedule_config,
+        )
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         available_elasticity_dims = self._elasticity_builder.get_available_elasticity_dims()
         self.check_elasticity_dims_consistency(available_elasticity_dims, self._progressivity_of_elasticity)
         return self._elasticity_builder.get_transformation_layout(target_model)
 
     def _get_state_without_name(self) -> Dict[str, Any]:
@@ -103,25 +108,24 @@
 
         :return: Returns a dictionary with Python data structures
             (dict, list, tuple, str, int, float, True, False, None) that represents state of the object.
         """
         return {
             self._state_names.ELASTICITY_BUILDER_STATE: self._elasticity_builder.get_state(),
             self._state_names.PROGRESSIVITY_OF_ELASTICITY: [d.value for d in self._progressivity_of_elasticity],
-            self._state_names.BN_ADAPTATION_PARAMS: self._bn_adapt_params
+            self._state_names.BN_ADAPTATION_PARAMS: self._bn_adapt_params,
         }
 
     def _load_state_without_name(self, state_without_name: Dict[str, Any]):
         """
         Implementation of load state that takes state without builder name.
 
         :param state_without_name: Output of `_get_state_without_name()` method.
         """
         elasticity_builder_state = state_without_name[self._state_names.ELASTICITY_BUILDER_STATE]
         self._elasticity_builder.load_state(elasticity_builder_state)
         progressivity_of_elasticity = state_without_name[self._state_names.PROGRESSIVITY_OF_ELASTICITY]
         # No conflict resolving with the related config options, parameters are overridden by compression state
         self._progressivity_of_elasticity = [ElasticityDim(dim) for dim in progressivity_of_elasticity]
         self._bn_adapt_params = state_without_name[self._state_names.BN_ADAPTATION_PARAMS]
-        bn_adapt_algo_kwargs = get_bn_adapt_algo_kwargs(self.config,
-                                                        self._bn_adapt_params)
+        bn_adapt_algo_kwargs = get_bn_adapt_algo_kwargs(self.config, self._bn_adapt_params)
         self._bn_adaptation = BatchnormAdaptationAlgorithm(**bn_adapt_algo_kwargs) if bn_adapt_algo_kwargs else None
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,88 +1,86 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import NoReturn
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Any, Dict, List, NoReturn
 
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
-from nncf.common.statistics import NNCFStatistics
 from nncf.common.logging import nncf_logger
-from nncf.experimental.torch.nas.bootstrapNAS.training.lr_scheduler import GlobalLRScheduler
-from nncf.experimental.torch.nas.bootstrapNAS.training.lr_scheduler import StageLRScheduler
-from nncf.experimental.torch.nas.bootstrapNAS.training.scheduler import NASSchedulerParams
-from nncf.torch.algo_selector import ZeroCompressionLoss
+from nncf.common.statistics import NNCFStatistics
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_controller import ElasticityController
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.multi_elasticity_handler import MultiElasticityHandler
 from nncf.experimental.torch.nas.bootstrapNAS.training.base_training import BNASTrainingController
+from nncf.experimental.torch.nas.bootstrapNAS.training.lr_scheduler import GlobalLRScheduler
+from nncf.experimental.torch.nas.bootstrapNAS.training.lr_scheduler import StageLRScheduler
 from nncf.experimental.torch.nas.bootstrapNAS.training.scheduler import BootstrapNASScheduler
+from nncf.experimental.torch.nas.bootstrapNAS.training.scheduler import NASSchedulerParams
 from nncf.experimental.torch.nas.bootstrapNAS.training.stage_descriptor import StageDescriptor
+from nncf.torch.algo_selector import ZeroCompressionLoss
 from nncf.torch.nncf_network import NNCFNetwork
 
 
 class PSControllerStateNames:
-    ELASTICITY_CONTROLLER_STATE = 'elasticity_controller_compression_state'
-    LR_GLOBAL_SCHEDULE_STATE = 'learning_rate_global_schedule_state'
+    ELASTICITY_CONTROLLER_STATE = "elasticity_controller_compression_state"
+    LR_GLOBAL_SCHEDULE_STATE = "learning_rate_global_schedule_state"
 
 
 class ProgressiveShrinkingController(BNASTrainingController):
     """
     Serves as a handle to the additional modules, parameters and hooks inserted
     into the original uncompressed model in order to train a supernet using Progressive Shrinking procedure
     from OFA (https://arxiv.org/abs/1908.09791).
     Hosts entities that are to be used during the training process, such as compression scheduler and
     compression loss.
     """
 
     _ps_state_names = PSControllerStateNames
 
-    def __init__(self, target_model: NNCFNetwork,
-                 elasticity_ctrl: ElasticityController,
-                 bn_adaptation: BatchnormAdaptationAlgorithm,
-                 progressivity_of_elasticity: List[ElasticityDim],
-                 schedule_params: NASSchedulerParams,
-                 lr_schedule_config: Dict[str, Any]):
+    def __init__(
+        self,
+        target_model: NNCFNetwork,
+        elasticity_ctrl: ElasticityController,
+        bn_adaptation: BatchnormAdaptationAlgorithm,
+        progressivity_of_elasticity: List[ElasticityDim],
+        schedule_params: NASSchedulerParams,
+        lr_schedule_config: Dict[str, Any],
+    ):
         super().__init__(target_model)
         self._elasticity_ctrl = elasticity_ctrl
         self._bn_adaptation = bn_adaptation
         self._progressivity_of_elasticity = progressivity_of_elasticity
         self._target_model = target_model
         self._loss = ZeroCompressionLoss(next(target_model.parameters()).device)
         self._available_elasticity_dims = self.multi_elasticity_handler.get_available_elasticity_dims()
         self._lr_schedule_config = lr_schedule_config
-        self._scheduler = BootstrapNASScheduler(self, schedule_params, self._available_elasticity_dims,
-                                                self._progressivity_of_elasticity)
+        self._scheduler = BootstrapNASScheduler(
+            self, schedule_params, self._available_elasticity_dims, self._progressivity_of_elasticity
+        )
         self._sample_rate = 1
 
     def set_training_lr_scheduler_args(self, optimizer, train_iters):
-        params = self._lr_schedule_config.get('params', {})
-        num_epochs = params.get('num_epochs', None)
-        base_lr = params.get('base_lr', None)
+        params = self._lr_schedule_config.get("params", {})
+        num_epochs = params.get("num_epochs", None)
+        base_lr = params.get("base_lr", None)
 
         if base_lr is not None:
             nncf_logger.info("Global LR scheduler in use")
             # Global lr scheduler
             if num_epochs is None:
-                params['num_epochs'] = self.get_total_num_epochs()
+                params["num_epochs"] = self.get_total_num_epochs()
             lr_scheduler = GlobalLRScheduler(optimizer, train_iters, **params)
         else:
             nncf_logger.info("Stage LR scheduler in use")
             lr_scheduler = StageLRScheduler(optimizer, train_iters)
         self._scheduler.lr_scheduler = lr_scheduler
 
     @property
@@ -133,15 +131,15 @@
 
     def step(self) -> None:
         """
         Should be called at the beginning of each training step for activation some Subnet(s).
         """
         if self._scheduler.current_step % self._sample_rate == 0:
             self.multi_elasticity_handler.activate_random_subnet()
-            nncf_logger.debug(f'Active config: {self.multi_elasticity_handler.get_active_config()}')
+            nncf_logger.debug(f"Active config: {self.multi_elasticity_handler.get_active_config()}")
 
     def prepare_for_validation(self) -> None:
         """
         Performs some action on active subnet or supernet before validation. For instance, it can be the batchnorm
         adaptation to achieve the best accuracy on validation.
         """
         if self._bn_adaptation:
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,79 +1,70 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Any, Dict, List, Optional, Tuple
 
-from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.logging import nncf_logger
+from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
 from nncf.experimental.torch.nas.bootstrapNAS.training.base_training import BNASTrainingAlgorithm
 from nncf.experimental.torch.nas.bootstrapNAS.training.lr_scheduler import BaseLRScheduler
 from nncf.experimental.torch.nas.bootstrapNAS.training.stage_descriptor import DEFAULT_STAGE_LR_RATE
 from nncf.experimental.torch.nas.bootstrapNAS.training.stage_descriptor import StageDescriptor
 
 
 class NSParamsStateNames:
-    LIST_STAGE_DESCRIPTIONS = 'list_stage_descriptions'
+    LIST_STAGE_DESCRIPTIONS = "list_stage_descriptions"
 
 
 class NASSchedulerParams:
     _state_names = NSParamsStateNames
 
     def __init__(self, list_stage_descriptions: Optional[List[StageDescriptor]] = None):
         """
         Constructor
 
         :param list_stage_descriptions: List of parameters per each supernet training stage.
         """
         if list_stage_descriptions is None:
             list_stage_descriptions = [
-                StageDescriptor(train_dims=[ElasticityDim.KERNEL],
-                                epochs=1),
-                StageDescriptor(train_dims=[ElasticityDim.KERNEL, ElasticityDim.DEPTH],
-                                epochs=1),
-                StageDescriptor(train_dims=[ElasticityDim.KERNEL, ElasticityDim.DEPTH],
-                                epochs=1,
-                                depth_indicator=2),
-                StageDescriptor(train_dims=[ElasticityDim.KERNEL, ElasticityDim.DEPTH, ElasticityDim.WIDTH],
-                                epochs=1),
-                StageDescriptor(train_dims=[ElasticityDim.KERNEL, ElasticityDim.DEPTH, ElasticityDim.WIDTH],
-                                epochs=1,
-                                width_indicator=2,
-                                reorg_weights=True,
-                                bn_adapt=True)
+                StageDescriptor(train_dims=[ElasticityDim.KERNEL], epochs=1),
+                StageDescriptor(train_dims=[ElasticityDim.KERNEL, ElasticityDim.DEPTH], epochs=1),
+                StageDescriptor(train_dims=[ElasticityDim.KERNEL, ElasticityDim.DEPTH], epochs=1, depth_indicator=2),
+                StageDescriptor(train_dims=[ElasticityDim.KERNEL, ElasticityDim.DEPTH, ElasticityDim.WIDTH], epochs=1),
+                StageDescriptor(
+                    train_dims=[ElasticityDim.KERNEL, ElasticityDim.DEPTH, ElasticityDim.WIDTH],
+                    epochs=1,
+                    width_indicator=2,
+                    reorg_weights=True,
+                    bn_adapt=True,
+                ),
             ]
         self.list_stage_descriptions = list_stage_descriptions
 
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'NASSchedulerParams':
+    def from_config(cls, config: Dict[str, Any]) -> "NASSchedulerParams":
         """
         Creates the object from its config.
         """
         descs_config = config.get(cls._state_names.LIST_STAGE_DESCRIPTIONS)
         descs = None
         if descs_config is not None:
             descs = [StageDescriptor.from_config(stage_desc_config) for stage_desc_config in descs_config]
         return cls(descs)
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'NASSchedulerParams':
+    def from_state(cls, state: Dict[str, Any]) -> "NASSchedulerParams":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         list_stage_descriptions_state = state[cls._state_names.LIST_STAGE_DESCRIPTIONS]
         list_stage_descriptions = [StageDescriptor.from_state(state) for state in list_stage_descriptions_state]
@@ -85,34 +76,38 @@
 
         :return: The compression loss state.
         """
         return {
             self._state_names.LIST_STAGE_DESCRIPTIONS: [desc.get_state() for desc in self.list_stage_descriptions],
         }
 
-    def __eq__(self, other: 'NASSchedulerParams') -> bool:
+    def __eq__(self, other: "NASSchedulerParams") -> bool:
         return self.__dict__ == other.__dict__
 
 
 class BNASSchedulerStateNames:
-    LIST_STAGE_DESCRIPTIONS = 'list_stage_descriptions'
+    LIST_STAGE_DESCRIPTIONS = "list_stage_descriptions"
 
 
 class BootstrapNASScheduler(BaseCompressionScheduler):
     """
     The cornerstone of supernet training within a NAS algorithm. The `step()` and `epoch_step()` methods of the
     compression scheduler must be called in the beginning of each training step and epoch, respectively.
     These methods trigger a subnet activations, elasticity configuration during the training.
     """
+
     _state_names = BNASSchedulerStateNames
 
-    def __init__(self, training_ctrl: BNASTrainingAlgorithm,
-                 params: NASSchedulerParams,
-                 available_elasticity_dims: List[ElasticityDim],
-                 progressivity_of_elasticity: List[ElasticityDim]):
+    def __init__(
+        self,
+        training_ctrl: BNASTrainingAlgorithm,
+        params: NASSchedulerParams,
+        available_elasticity_dims: List[ElasticityDim],
+        progressivity_of_elasticity: List[ElasticityDim],
+    ):
         super().__init__()
         self._training_ctrl = training_ctrl
         self._params = params
         self._available_elasticity_dims = available_elasticity_dims
         self._progressivity_of_elasticity = progressivity_of_elasticity
         self.current_stage_idx = -1
         # Property setter with validation is not used intentionally for the resume case. When the actual list stage
@@ -234,60 +229,65 @@
 
         :return: The compression scheduler state.
         """
         state = super().get_state()
         state[self._state_names.LIST_STAGE_DESCRIPTIONS] = [desc.get_state() for desc in self.list_stage_descriptors]
         return state
 
-    def _validate_elasticity_dims(self,
-                                  available_elasticity_dims: List[ElasticityDim],
-                                  progressivity_of_elasticity: List[ElasticityDim]) -> None:
+    def _validate_elasticity_dims(
+        self, available_elasticity_dims: List[ElasticityDim], progressivity_of_elasticity: List[ElasticityDim]
+    ) -> None:
         last_stage = -1
         first_stage = len(progressivity_of_elasticity)
         for desc in self._list_stage_descriptors:
             high_priority_dim_idx = -1
             low_priority_dim_idx = len(progressivity_of_elasticity)
             stages_covered = []
             for train_dim in desc.train_dims:
                 if train_dim not in available_elasticity_dims:
                     raise ValueError(
                         f"Invalid training elasticity dimension {train_dim} in the scheduler.\n"
                         f"The elasticity for this dimension is not enabled.\n"
                         f"It can be enabled by specifying `available_elasticity_dims` param in the `elasticity` "
                         f"section of config.\n"
-                        f"List of currently available dimensions: {[dim.value for dim in available_elasticity_dims]}")
+                        f"List of currently available dimensions: {[dim.value for dim in available_elasticity_dims]}"
+                    )
                 dim_idx = progressivity_of_elasticity.index(train_dim)
                 if dim_idx not in stages_covered:
                     stages_covered.append(dim_idx)
                 if dim_idx > high_priority_dim_idx:
                     high_priority_dim_idx = dim_idx
                 if dim_idx < low_priority_dim_idx:
                     low_priority_dim_idx = dim_idx
             if high_priority_dim_idx < last_stage or low_priority_dim_idx > first_stage:
                 raise ValueError(
-                    f"stage {progressivity_of_elasticity[high_priority_dim_idx]} violates progressivity of elasticity")
+                    f"stage {progressivity_of_elasticity[high_priority_dim_idx]} violates progressivity of elasticity"
+                )
             for i in range(low_priority_dim_idx, high_priority_dim_idx):
                 if i not in stages_covered and progressivity_of_elasticity[i] in available_elasticity_dims:
                     raise ValueError(
                         f"Missed to call {progressivity_of_elasticity[i]} in {desc.train_dims} which violates "
-                        f"progressivity of elasticity {progressivity_of_elasticity}")
+                        f"progressivity of elasticity {progressivity_of_elasticity}"
+                    )
             last_stage = high_priority_dim_idx
             first_stage = low_priority_dim_idx
 
     def _validate_lr(self):
         for desc in self._list_stage_descriptors:
             # Check if global learning rate has been set
             if desc.init_lr is not None and bool(self._training_ctrl.lr_schedule_config):
                 raise ValueError(
                     f"Global learning rate scheduler is in use. Cannot set stage learning rate: {desc.init_lr}"
                 )
             # Check if stage learning rate has been set
             if desc.init_lr is None and not bool(self._training_ctrl.lr_schedule_config):
                 nncf_logger.warning(
-                    "Stage learning rate in use but init_lr value for stage wasn't set. Using default value of 3.5e-6")
+                    "Stage learning rate in use but init_lr value for stage wasn't set. Using default value of 3.5e-6"
+                )
                 desc.init_lr = DEFAULT_STAGE_LR_RATE
 
             if desc.init_lr is not None and desc.epochs_lr is None:
                 nncf_logger.warning(
                     f"Stage learning rate in use but epochs_lr value for stage wasn't set. "
-                    f"Using number of epochs for stage {desc.epochs}")
+                    f"Using number of epochs for stage {desc.epochs}"
+                )
                 desc.epochs_lr = desc.epochs
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,88 +1,89 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Any
-from typing import Dict
-from typing import List
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Any, Dict, List
 
 from nncf.common.logging import nncf_logger
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_dim import ElasticityDim
 
 DEFAULT_STAGE_LR_RATE = 3.5e-06
 
+
 class SDescriptorParamNames:
-    TRAIN_DIMS = 'train_dims'
-    EPOCHS = 'epochs'
-    REORG_WEIGHTS = 'reorg_weights'
-    WIDTH_INDICATOR = 'width_indicator'
-    DEPTH_INDICATOR = 'depth_indicator'
-    BN_ADAPT = 'bn_adapt'
-    INIT_LR = 'init_lr'
-    EPOCHS_LR = 'epochs_lr'
-    SAMPLE_RATE = 'sample_rate'
+    TRAIN_DIMS = "train_dims"
+    EPOCHS = "epochs"
+    REORG_WEIGHTS = "reorg_weights"
+    WIDTH_INDICATOR = "width_indicator"
+    DEPTH_INDICATOR = "depth_indicator"
+    BN_ADAPT = "bn_adapt"
+    INIT_LR = "init_lr"
+    EPOCHS_LR = "epochs_lr"
+    SAMPLE_RATE = "sample_rate"
 
 
 class StageDescriptor:
     """
     Describes parameters of the training stage. The stage defines active elastic dimension and its parameters.
     """
+
     _state_names = SDescriptorParamNames
 
-    def __init__(self, train_dims: List[ElasticityDim],
-                 epochs: int = 1,
-                 reorg_weights: bool = False,
-                 bn_adapt: bool = False,
-                 depth_indicator: int = 1,
-                 width_indicator: int = 1,
-                 init_lr: float = None,
-                 epochs_lr: int = None,
-                 sample_rate: int = 1):
+    def __init__(
+        self,
+        train_dims: List[ElasticityDim],
+        epochs: int = 1,
+        reorg_weights: bool = False,
+        bn_adapt: bool = False,
+        depth_indicator: int = 1,
+        width_indicator: int = 1,
+        init_lr: float = None,
+        epochs_lr: int = None,
+        sample_rate: int = 1,
+    ):
         self.train_dims = train_dims
         self.epochs = epochs
         self.depth_indicator = depth_indicator
         self.width_indicator = width_indicator
         self.reorg_weights = reorg_weights
         self.bn_adapt = bn_adapt
         self.init_lr = init_lr
         self.epochs_lr = epochs_lr
         self.sample_rate = sample_rate
         if sample_rate <= 0:
             nncf_logger.warning(f"Only positive integers are allowed for sample rate, but sample_rate={sample_rate}.")
             nncf_logger.warning("Setting sample rate to default 1")
             self.sample_rate = 1
 
-    def __eq__(self, other: 'StageDescriptor'):
+    def __eq__(self, other: "StageDescriptor"):
         return self.__dict__ == other.__dict__
 
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'StageDescriptor':
+    def from_config(cls, config: Dict[str, Any]) -> "StageDescriptor":
         """
         Creates the object from its config.
         """
-        train_dims = config.get(cls._state_names.TRAIN_DIMS, ['kernel'])
+        train_dims = config.get(cls._state_names.TRAIN_DIMS, ["kernel"])
         kwargs = {
             cls._state_names.TRAIN_DIMS: [ElasticityDim(dim) for dim in train_dims],
             cls._state_names.EPOCHS: config.get(cls._state_names.EPOCHS, 1),
             cls._state_names.REORG_WEIGHTS: config.get(cls._state_names.REORG_WEIGHTS, False),
             cls._state_names.WIDTH_INDICATOR: config.get(cls._state_names.WIDTH_INDICATOR, 1),
             cls._state_names.DEPTH_INDICATOR: config.get(cls._state_names.DEPTH_INDICATOR, 1),
             cls._state_names.BN_ADAPT: config.get(cls._state_names.BN_ADAPT, False),
             cls._state_names.INIT_LR: config.get(cls._state_names.INIT_LR, None),
             cls._state_names.EPOCHS_LR: config.get(cls._state_names.EPOCHS_LR, None),
-            cls._state_names.SAMPLE_RATE: config.get(cls._state_names.SAMPLE_RATE, 1)
+            cls._state_names.SAMPLE_RATE: config.get(cls._state_names.SAMPLE_RATE, 1),
         }
         return cls(**kwargs)
 
     @classmethod
     def from_state(cls, state: Dict[str, Any]):
         """
         Creates the object from its state.
@@ -98,14 +99,14 @@
         state_dict = {
             self._state_names.TRAIN_DIMS: [dim.value for dim in self.train_dims],
             self._state_names.EPOCHS: self.epochs,
             self._state_names.REORG_WEIGHTS: self.reorg_weights,
             self._state_names.WIDTH_INDICATOR: self.width_indicator,
             self._state_names.DEPTH_INDICATOR: self.depth_indicator,
             self._state_names.BN_ADAPT: self.bn_adapt,
-            self._state_names.SAMPLE_RATE: self.sample_rate
+            self._state_names.SAMPLE_RATE: self.sample_rate,
         }
         if self.init_lr is not None:
-            state_dict['init_lr'] = self.init_lr
+            state_dict["init_lr"] = self.init_lr
         if self.epochs_lr is not None:
-            state_dict['epochs_lr'] = self.epochs_lr
+            state_dict["epochs_lr"] = self.epochs_lr
         return state_dict
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py` & `nncf-2.5.0/nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,89 +1,81 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from pathlib import Path
 from shutil import copyfile
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import Optional
-from typing import Tuple
-from typing import TypeVar
+from typing import Any, Callable, Dict, Optional, Tuple, TypeVar
 
 import torch
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.api.compression import CompressionStage
 from nncf.common.logging import nncf_logger
 from nncf.config.structures import BNAdaptationInitArgs
 from nncf.experimental.torch.nas.bootstrapNAS.elasticity.elasticity_controller import ElasticityController
 from nncf.experimental.torch.nas.bootstrapNAS.training.base_training import BNASTrainingController
-from nncf.experimental.torch.nas.bootstrapNAS.training.model_creator_helpers import \
-    create_compressed_model_from_algo_names
+from nncf.experimental.torch.nas.bootstrapNAS.training.model_creator_helpers import (
+    create_compressed_model_from_algo_names,
+)
 from nncf.experimental.torch.nas.bootstrapNAS.training.model_creator_helpers import resume_compression_from_state
 from nncf.torch.checkpoint_loading import load_state
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.utils import is_main_process
 
-TModel = TypeVar('TModel')
-OptimizerType = TypeVar('OptimizerType')
-LRSchedulerType = TypeVar('LRSchedulerType')
-TensorboardWriterType = TypeVar('TensorboardWriterType')
-DataLoaderType = TypeVar('DataLoaderType')
+TModel = TypeVar("TModel")
+OptimizerType = TypeVar("OptimizerType")
+LRSchedulerType = TypeVar("LRSchedulerType")
+TensorboardWriterType = TypeVar("TensorboardWriterType")
+DataLoaderType = TypeVar("DataLoaderType")
 TrainEpochFnType = Callable[
     [
         DataLoaderType,
         TModel,
         CompressionAlgorithmController,
         int,
         OptimizerType,
-    ], None
-]
-ValFnType = Callable[
-    [
-        TModel,
-        DataLoaderType
     ],
-    Tuple[float, float, float]
+    None,
 ]
+ValFnType = Callable[[TModel, DataLoaderType], Tuple[float, float, float]]
 
 
 class EBTrainAlgoStateNames:
-    MODEL_STATE = 'model_state'
-    EPOCH = 'epoch'
-    SUPERNET_ACC1 = 'acc1'
-    SUPERNET_BEST_ACC1 = 'best_acc1'
-    MIN_SUBNET_ACC1 = 'min_subnet_acc1'
-    MIN_SUBNET_BEST_ACC1 = 'min_subnet_best_acc1'
-    OPTIMIZER = 'optimizer'
-    TRAINING_ALGO_STATE = 'training_algo_state'
+    MODEL_STATE = "model_state"
+    EPOCH = "epoch"
+    SUPERNET_ACC1 = "acc1"
+    SUPERNET_BEST_ACC1 = "best_acc1"
+    MIN_SUBNET_ACC1 = "min_subnet_acc1"
+    MIN_SUBNET_BEST_ACC1 = "min_subnet_best_acc1"
+    OPTIMIZER = "optimizer"
+    TRAINING_ALGO_STATE = "training_algo_state"
 
 
 class EpochBasedTrainingAlgorithm:
     """
     Algorithm for training supernet by using a train function for a single epoch. In contrast, there can be step-based
     algorithm that uses a train function for a single training step.
     """
+
     _state_names = EBTrainAlgoStateNames
 
-    def __init__(self,
-                 nncf_network: NNCFNetwork,
-                 training_ctrl: BNASTrainingController,
-                 checkpoint: Optional[Dict[str, Any]] = None):
+    def __init__(
+        self,
+        nncf_network: NNCFNetwork,
+        training_ctrl: BNASTrainingController,
+        checkpoint: Optional[Dict[str, Any]] = None,
+    ):
         """
         Initializes the training algorithm
 
         :param nncf_network: it's supposed to be a model with elasticity ops,
         controlled by elasticity ctrl, which is owned by training ctrl
         :param training_ctrl: controller of the training algorithm (by default ProgressiveShrinkingController)
         :param checkpoint: data to restore state of the training algorithm
@@ -106,23 +98,25 @@
     @property
     def elasticity_ctrl(self) -> ElasticityController:
         """
         :return: elasticity controller
         """
         return self._training_ctrl.elasticity_controller
 
-    def run(self,
-            train_epoch_fn: TrainEpochFnType,
-            train_loader: DataLoaderType,
-            val_fn: ValFnType,
-            val_loader: DataLoaderType,
-            optimizer: OptimizerType,
-            checkpoint_save_dir: str,
-            tensorboard_writer: Optional[TensorboardWriterType] = None,
-            train_iters: Optional[float] = None) -> Tuple[NNCFNetwork, ElasticityController]:
+    def run(
+        self,
+        train_epoch_fn: TrainEpochFnType,
+        train_loader: DataLoaderType,
+        val_fn: ValFnType,
+        val_loader: DataLoaderType,
+        optimizer: OptimizerType,
+        checkpoint_save_dir: str,
+        tensorboard_writer: Optional[TensorboardWriterType] = None,
+        train_iters: Optional[float] = None,
+    ) -> Tuple[NNCFNetwork, ElasticityController]:
         """
         Implements a training loop for supernet training.
 
         :param train_epoch_fn: a method to fine-tune the model for a single epoch
         :param train_loader: data loader for training
         :param lr_scheduler: scheduler for learning rate
         :param val_fn: a method to evaluate the model on the validation dataset
@@ -142,19 +136,22 @@
 
         self._optimizer = optimizer
 
         log_validation_info = True
         if tensorboard_writer is None:
             try:
                 from torch.utils.tensorboard import SummaryWriter
+
                 tensorboard_writer = SummaryWriter(checkpoint_save_dir)
                 # log compression config to tensorboard
             except ModuleNotFoundError:
-                nncf_logger.warning("Tensorboard installation not found! Install tensorboard Python package "
-                                    "in order for BootstrapNAS tensorboard data to be dumped")
+                nncf_logger.warning(
+                    "Tensorboard installation not found! Install tensorboard Python package "
+                    "in order for BootstrapNAS tensorboard data to be dumped"
+                )
 
         total_num_epochs = self._training_ctrl.get_total_num_epochs()
 
         best_compression_stage = CompressionStage.UNCOMPRESSED
         for epoch in range(self._start_epoch, total_num_epochs):
             self._training_ctrl.scheduler.epoch_step()
 
@@ -167,110 +164,113 @@
 
             compression_stage = self._training_ctrl.compression_stage()
 
             self._training_ctrl.multi_elasticity_handler.activate_minimum_subnet()
             min_subnet_acc1, acc5, loss = self._validate_subnet(val_fn, val_loader)
             if log_validation_info:
                 nncf_logger.info(
-                    f'* Acc@1 {min_subnet_acc1:.3f} Acc@5 {acc5:.3f} '
-                    f'for Minimal SubNet={self._training_ctrl.multi_elasticity_handler.get_active_config()}')
+                    f"* Acc@1 {min_subnet_acc1:.3f} Acc@5 {acc5:.3f} "
+                    f"for Minimal SubNet={self._training_ctrl.multi_elasticity_handler.get_active_config()}"
+                )
             if is_main_process() and log_validation_info:
                 tensorboard_writer.add_scalar("val/min_subnet_loss", loss, len(val_loader) * epoch)
                 tensorboard_writer.add_scalar("val/min_subnet_top1", min_subnet_acc1, len(val_loader) * epoch)
                 tensorboard_writer.add_scalar("val/min_subnet_top5", acc5, len(val_loader) * epoch)
-            min_subnet_best_acc1 = self._define_best_accuracy(min_subnet_acc1, self._min_subnet_best_acc1,
-                                                              compression_stage, best_compression_stage)
+            min_subnet_best_acc1 = self._define_best_accuracy(
+                min_subnet_acc1, self._min_subnet_best_acc1, compression_stage, best_compression_stage
+            )
 
             self._training_ctrl.multi_elasticity_handler.activate_supernet()
             supernet_acc1, acc5, loss = self._validate_subnet(val_fn, val_loader)
             if log_validation_info:
                 nncf_logger.info(
-                    f'* Acc@1 {supernet_acc1:.3f} Acc@5 {acc5:.3f} '
-                    f'of SuperNet={self._training_ctrl.multi_elasticity_handler.get_active_config()}')
+                    f"* Acc@1 {supernet_acc1:.3f} Acc@5 {acc5:.3f} "
+                    f"of SuperNet={self._training_ctrl.multi_elasticity_handler.get_active_config()}"
+                )
 
             if is_main_process() and log_validation_info:
                 tensorboard_writer.add_scalar("val/supernet_loss", loss, len(val_loader) * epoch)
                 tensorboard_writer.add_scalar("val/supernet_top1", supernet_acc1, len(val_loader) * epoch)
                 tensorboard_writer.add_scalar("val/supernet_top5", acc5, len(val_loader) * epoch)
-            supernet_best_acc1 = self._define_best_accuracy(supernet_acc1, self._supernet_best_acc1,
-                                                            compression_stage, best_compression_stage)
+            supernet_best_acc1 = self._define_best_accuracy(
+                supernet_acc1, self._supernet_best_acc1, compression_stage, best_compression_stage
+            )
 
             best_compression_stage = max(compression_stage, best_compression_stage)
 
-            checkpoint_path = Path(checkpoint_save_dir, 'supernet_last.pth')
+            checkpoint_path = Path(checkpoint_save_dir, "supernet_last.pth")
             checkpoint = {
                 self._state_names.EPOCH: epoch + 1,
                 self._state_names.MODEL_STATE: self._model.state_dict(),
                 self._state_names.SUPERNET_BEST_ACC1: supernet_best_acc1,
                 self._state_names.SUPERNET_ACC1: supernet_acc1,
                 self._state_names.MIN_SUBNET_BEST_ACC1: min_subnet_best_acc1,
                 self._state_names.MIN_SUBNET_ACC1: min_subnet_acc1,
                 self._state_names.OPTIMIZER: optimizer.state_dict(),
-                self._state_names.TRAINING_ALGO_STATE: self._training_ctrl.get_compression_state()
+                self._state_names.TRAINING_ALGO_STATE: self._training_ctrl.get_compression_state(),
             }
             torch.save(checkpoint, checkpoint_path)
 
             if compression_stage == CompressionStage.FULLY_COMPRESSED and supernet_best_acc1 == supernet_acc1:
-                best_path = Path(checkpoint_save_dir) / 'supernet_best.pth'
+                best_path = Path(checkpoint_save_dir) / "supernet_best.pth"
                 copyfile(checkpoint_path, best_path)
 
             # Backup elasticity state and model weight to directly restore from it in a separate search sample
-            elasticity_path = Path(checkpoint_save_dir) / 'last_elasticity.pth'
+            elasticity_path = Path(checkpoint_save_dir) / "last_elasticity.pth"
             elasticity_state = self._training_ctrl.elasticity_controller.get_compression_state()
-            model_path = Path(checkpoint_save_dir) / 'last_model_weights.pth'
+            model_path = Path(checkpoint_save_dir) / "last_model_weights.pth"
             model_state = self._model.state_dict()
             torch.save(elasticity_state, elasticity_path)
             torch.save(model_state, model_path)
 
         return self._model, self.elasticity_ctrl
 
     @classmethod
-    def from_config(cls, nncf_network: NNCFNetwork, nncf_config: NNCFConfig) -> 'EpochBasedTrainingAlgorithm':
+    def from_config(cls, nncf_network: NNCFNetwork, nncf_config: NNCFConfig) -> "EpochBasedTrainingAlgorithm":
         """
         Creates the training algorithm from a config by a given empty NNCFNetwork.
 
         :param nncf_network: empty NNCFNetwork
         :param nncf_config: parameters of the training algorithm
         :return: the training algorithm
         """
-        algo_name = nncf_config.get('bootstrapNAS', {}).get('training', {}).get('algorithm', 'progressive_shrinking')
-        training_ctrl, model = create_compressed_model_from_algo_names(nncf_network, nncf_config,
-                                                                       algo_names=[algo_name])
+        algo_name = nncf_config.get("bootstrapNAS", {}).get("training", {}).get("algorithm", "progressive_shrinking")
+        training_ctrl, model = create_compressed_model_from_algo_names(
+            nncf_network, nncf_config, algo_names=[algo_name]
+        )
         return EpochBasedTrainingAlgorithm(model, training_ctrl)
 
     @classmethod
-    def from_checkpoint(cls,
-                        nncf_network: NNCFNetwork,
-                        bn_adapt_args: BNAdaptationInitArgs,
-                        resuming_checkpoint_path: str) -> 'EpochBasedTrainingAlgorithm':
+    def from_checkpoint(
+        cls, nncf_network: NNCFNetwork, bn_adapt_args: BNAdaptationInitArgs, resuming_checkpoint_path: str
+    ) -> "EpochBasedTrainingAlgorithm":
         """
         Resumes the training algorithm from a checkpoint by a given empty NNCFNetwork and BN adaptation arguments only,
         config is not involved.
 
         :param nncf_network: empty NNCFNetwork
         :param bn_adapt_args: arguments for batchnorm statistics adaptation algorithm
         :param resuming_checkpoint_path: path to the resuming checkpoint
         :return: the training algorithm
         """
         if not Path(resuming_checkpoint_path).is_file():
             raise FileNotFoundError("no checkpoint found at '{}'".format(resuming_checkpoint_path))
         nncf_logger.info(f"=> loading checkpoint '{resuming_checkpoint_path}'")
-        checkpoint = torch.load(resuming_checkpoint_path, map_location='cpu')
+        checkpoint = torch.load(resuming_checkpoint_path, map_location="cpu")
 
         training_state = checkpoint[cls._state_names.TRAINING_ALGO_STATE]
         nncf_config = NNCFConfig()
         nncf_config.register_extra_structs([bn_adapt_args])
         model, training_ctrl = resume_compression_from_state(nncf_network, training_state, nncf_config)
         return EpochBasedTrainingAlgorithm(model, training_ctrl, checkpoint)
 
     @staticmethod
-    def _define_best_accuracy(acc1: float,
-                              best_acc1: float,
-                              compression_stage: CompressionStage,
-                              best_compression_stage: CompressionStage) -> float:
+    def _define_best_accuracy(
+        acc1: float, best_acc1: float, compression_stage: CompressionStage, best_compression_stage: CompressionStage
+    ) -> float:
         """
         The best accuracy value should be considered not only by value, but also per each stage.
         Currently, there are two stages in NAS algo only: PARTIALLY_COMPRESSED and FULLY_COMPRESSED.
         It's aligned with Compression Algorithm stages, but ideally number of stages should correspond to number of
         scheduler stages (usually more than two).
         The last means that scheduler is activated the last stage.
         """
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/search_building_blocks/search_blocks.py` & `nncf-2.5.0/nncf/experimental/torch/search_building_blocks/search_blocks.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,46 +1,41 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from enum import Enum
 from functools import cmp_to_key
 from functools import partial
 from operator import itemgetter
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Set
-from typing import Tuple
+from typing import Any, Callable, Dict, List, Optional, Set, Tuple
 
 import networkx as nx
 import torch
 
 from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.experimental.torch.search_building_blocks.search_graph import SearchGraph
 from nncf.experimental.torch.search_building_blocks.search_graph import SearchGraphNode
-from nncf.experimental.torch.search_building_blocks.search_graph import \
-    check_graph_has_no_act_layer_duplication_after_block_removal
-from nncf.experimental.torch.search_building_blocks.search_graph import \
-    check_graph_has_no_duplicate_edges_after_block_removal
-from nncf.experimental.torch.search_building_blocks.search_graph import \
-    check_graph_has_no_hanging_edges_after_block_removal
+from nncf.experimental.torch.search_building_blocks.search_graph import (
+    check_graph_has_no_act_layer_duplication_after_block_removal,
+)
+from nncf.experimental.torch.search_building_blocks.search_graph import (
+    check_graph_has_no_duplicate_edges_after_block_removal,
+)
+from nncf.experimental.torch.search_building_blocks.search_graph import (
+    check_graph_has_no_hanging_edges_after_block_removal,
+)
 from nncf.experimental.torch.search_building_blocks.search_graph import get_num_ops_in_block
 from nncf.experimental.torch.search_building_blocks.search_graph import get_search_graph
 from nncf.torch.dynamic_graph.operation_address import OperationAddress
 from nncf.torch.graph.graph import PTNNCFGraph
 from nncf.torch.graph.operator_metatypes import PTDropoutMetatype
 from nncf.torch.graph.operator_metatypes import PTLinearMetatype
 from nncf.torch.graph.operator_metatypes import PTMatMulMetatype
@@ -59,28 +54,28 @@
     Describes a building block that is uniquely defined by the first skipped node and end nodes.
     """
 
     def __init__(self, first_skipped_node: SearchGraphNode, end_node: SearchGraphNode):
         self.first_skipped_node = first_skipped_node
         self.end_node = end_node
 
-    def __eq__(self, __o: 'PotentialBuildingBlock') -> bool:
+    def __eq__(self, __o: "PotentialBuildingBlock") -> bool:
         return self.first_skipped_node == __o.first_skipped_node and self.end_node == __o.end_node
 
 
 class BuildingBlock:
     """
     Describes a building block that is uniquely defined by the start and end nodes.
     """
 
     def __init__(self, start_node_name: NNCFNodeName, end_node_name: NNCFNodeName):
         self.start_node_name = start_node_name
         self.end_node_name = end_node_name
 
-    def __eq__(self, __o: 'BuildingBlock') -> bool:
+    def __eq__(self, __o: "BuildingBlock") -> bool:
         return self.start_node_name == __o.start_node_name and self.end_node_name == __o.end_node_name
 
     def __repr__(self) -> str:
         return str(self)
 
     def __str__(self) -> str:
         return "[START NODE: {}, END_NODE: {}]".format(self.start_node_name, self.end_node_name)
@@ -88,99 +83,102 @@
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
-        return {
-            'start_node_name': self.start_node_name,
-            'end_node_name': self.end_node_name
-        }
+        return {"start_node_name": self.start_node_name, "end_node_name": self.end_node_name}
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'BuildingBlock':
+    def from_state(cls, state: Dict[str, Any]) -> "BuildingBlock":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return BuildingBlock(**state)
 
 
 class BuildingBlockType(Enum):
     """
     Describes type of building block for transformers-based network.
     `MHSA` type is characterized by the presence 4 FC and 2 MatMul layers.
     `FF` type is characterized by the presence 2 FC layers.
     """
-    MHSA = 'MHSA'
-    FF = 'FF'
-    Unknown = 'unknown'
 
-    def __eq__(self, other: 'BuildingBlockType'):
+    MHSA = "MHSA"
+    FF = "FF"
+    Unknown = "unknown"
+
+    def __eq__(self, other: "BuildingBlockType"):
         return self.__dict__ == other.__dict__
 
     def __hash__(self) -> int:
         return hash(self.name)
 
 
 class EBBlocksStateNames:
-    BASIC_BLOCK = 'basic_block'
-    BLOCK_TYPE = 'block_type'
-    ORDINAL_IDS = 'ordinal_ids'
-    OP_ADDRESSES = 'op_addresses'
+    BASIC_BLOCK = "basic_block"
+    BLOCK_TYPE = "block_type"
+    ORDINAL_IDS = "ordinal_ids"
+    OP_ADDRESSES = "op_addresses"
 
 
 class ExtendedBuildingBlock:
     """
     Provides extended information about building block. In addition to the addresses of boundary operations, it defines
     block type, indexes of start and end node and addresses all operations inside the block.
     """
+
     _state_names = EBBlocksStateNames
 
-    def __init__(self, basic_block: BuildingBlock,
-                 block_type: BuildingBlockType,
-                 ordinal_ids: Optional[List[int]],
-                 op_addresses: Optional[Set[OperationAddress]]):
+    def __init__(
+        self,
+        basic_block: BuildingBlock,
+        block_type: BuildingBlockType,
+        ordinal_ids: Optional[List[int]],
+        op_addresses: Optional[Set[OperationAddress]],
+    ):
         self.basic_block = basic_block
         self.block_type = block_type
         self.ordinal_ids = ordinal_ids
         self.op_addresses = op_addresses
 
     def __str__(self) -> str:
-        return "[START NODE: {}, END_NODE: {}] [{}, {}] #ops={}".format(self.basic_block.start_node_name,
-                                                                        self.basic_block.end_node_name,
-                                                                        *self.ordinal_ids, len(self.op_addresses))
+        return "[START NODE: {}, END_NODE: {}] [{}, {}] #ops={}".format(
+            self.basic_block.start_node_name, self.basic_block.end_node_name, *self.ordinal_ids, len(self.op_addresses)
+        )
 
     @property
     def start_node_name(self):
         return self.basic_block.start_node_name
 
     @property
     def end_node_name(self):
         return self.basic_block.end_node_name
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'ExtendedBuildingBlock':
+    def from_state(cls, state: Dict[str, Any]) -> "ExtendedBuildingBlock":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         bbtype = BuildingBlockType(state[cls._state_names.BLOCK_TYPE])
         bblock = BuildingBlock.from_state(state[cls._state_names.BASIC_BLOCK])
-        op_addresses = {OperationAddress.from_str(op_address_state) for op_address_state in
-                        state[cls._state_names.OP_ADDRESSES]}
+        op_addresses = {
+            OperationAddress.from_str(op_address_state) for op_address_state in state[cls._state_names.OP_ADDRESSES]
+        }
         ordinal_ids = state[cls._state_names.ORDINAL_IDS]
         kwargs = {
             cls._state_names.BLOCK_TYPE: bbtype,
             cls._state_names.BASIC_BLOCK: bblock,
             cls._state_names.OP_ADDRESSES: op_addresses,
-            cls._state_names.ORDINAL_IDS: ordinal_ids
+            cls._state_names.ORDINAL_IDS: ordinal_ids,
         }
         return cls(**kwargs)
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
@@ -188,22 +186,24 @@
         :return: state of the object
         """
         op_addresses = sorted([str(op_address) for op_address in self.op_addresses])
         return {
             self._state_names.BLOCK_TYPE: self.block_type.name,
             self._state_names.BASIC_BLOCK: self.basic_block.get_state(),
             self._state_names.OP_ADDRESSES: op_addresses,
-            self._state_names.ORDINAL_IDS: self.ordinal_ids
+            self._state_names.ORDINAL_IDS: self.ordinal_ids,
         }
 
-    def __eq__(self, other: 'ExtendedBuildingBlock'):
-        return self.block_type == other.block_type and \
-               self.basic_block == other.basic_block and \
-               self.op_addresses == other.op_addresses and \
-               self.ordinal_ids == other.ordinal_ids
+    def __eq__(self, other: "ExtendedBuildingBlock"):
+        return (
+            self.block_type == other.block_type
+            and self.basic_block == other.basic_block
+            and self.op_addresses == other.op_addresses
+            and self.ordinal_ids == other.ordinal_ids
+        )
 
 
 BuildingBlocks = List[BuildingBlock]
 ExtendedBuildingBlocks = List[ExtendedBuildingBlock]
 
 
 def add_node_to_aux_struct(node: SearchGraphNode, shape: List[int], shape_map: ShapeVsNodesMap):
@@ -245,26 +245,29 @@
     if a.first_skipped_node.main_id != b.first_skipped_node.main_id:
         return a.first_skipped_node.main_id - b.first_skipped_node.main_id
     num_ops_a = get_num_ops_in_block(a.first_skipped_node, a.end_node)
     num_ops_b = get_num_ops_in_block(b.first_skipped_node, b.end_node)
     return num_ops_a - num_ops_b
 
 
-def get_building_block_for_original_graph(building_blocks: List[PotentialBuildingBlock],
-                                          orig_graph: PTNNCFGraph) -> ExtendedBuildingBlocks:
+def get_building_block_for_original_graph(
+    building_blocks: List[PotentialBuildingBlock], orig_graph: PTNNCFGraph
+) -> ExtendedBuildingBlocks:
     """
     Restore the original names and ids of the start and end of the block in original graph.
     Very cost expensive function, because of finding all op addresses in the block via nx.all_simple_paths function.
     """
     building_block_in_orig_format = []
     for block in building_blocks:
         id_end = block.end_node.bottom_id
         start_node_id = get_start_node_id(block, orig_graph)
-        block_in_orig_format = BuildingBlock(orig_graph.get_node_key_by_id(start_node_id).split(' ')[-1],
-                                             orig_graph.get_node_key_by_id(id_end).split(' ')[-1])
+        block_in_orig_format = BuildingBlock(
+            orig_graph.get_node_key_by_id(start_node_id).split(" ")[-1],
+            orig_graph.get_node_key_by_id(id_end).split(" ")[-1],
+        )
         ordinal_ids = [start_node_id, id_end]
         op_addresses = get_all_node_op_addresses_in_block(orig_graph, block_in_orig_format)
         block_type = get_type_building_block(op_addresses)
         ext_block = ExtendedBuildingBlock(block_in_orig_format, block_type, ordinal_ids, op_addresses)
         building_block_in_orig_format.append(ext_block)
     return building_block_in_orig_format
 
@@ -280,22 +283,24 @@
     """
     act_input_shape = {}  # key - str(shape), value - set of node_keys
     act_output_shape = {}  # key - str(shape), value - set of node_keys
     for node in search_graph.get_all_nodes():
         next_edges = search_graph.get_next_edges(node.node_key)
         prev_edges = search_graph.get_prev_edges(node.node_key)
         for _, edge_attr in next_edges.items():
-            search_graph.set_node_attr(node.node_key, SearchGraph.ACTIVATION_OUTPUT_SHAPE_ATTR,
-                                       edge_attr[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR])
+            search_graph.set_node_attr(
+                node.node_key, SearchGraph.ACTIVATION_OUTPUT_SHAPE_ATTR, edge_attr[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR]
+            )
             if not node.is_dummy:
                 add_node_to_aux_struct(node, edge_attr[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR], act_output_shape)
             break
         for _, edge_attr in prev_edges.items():
-            search_graph.set_node_attr(node.node_key, SearchGraph.ACTIVATION_OUTPUT_SHAPE_ATTR,
-                                       edge_attr[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR])
+            search_graph.set_node_attr(
+                node.node_key, SearchGraph.ACTIVATION_OUTPUT_SHAPE_ATTR, edge_attr[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR]
+            )
             add_node_to_aux_struct(node, edge_attr[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR], act_input_shape)
             break
     return act_input_shape, act_output_shape
 
 
 def itemgetter_force_tuple(*indexes):
     """
@@ -304,101 +309,109 @@
     """
     getter = itemgetter(*indexes)
     if len(indexes) == 1:
         return lambda seq: (getter(seq),)  # Wrap in a tuple.
     return getter
 
 
-def is_target_block_type(start_node_id: int,
-                         end_node_id: int,
-                         orig_graph: NNCFGraph,
-                         target_block_types: Optional[List[BuildingBlockType]] = None) -> bool:
+def is_target_block_type(
+    start_node_id: int,
+    end_node_id: int,
+    orig_graph: NNCFGraph,
+    target_block_types: Optional[List[BuildingBlockType]] = None,
+) -> bool:
     """
     Returns true if block has a type equal to one of the specified.
     :param start_node_id: bottom index of the starting node in search graph
     :param end_node_id: bottom index of the ending node in search graph
     :param orig_graph: original non-compressed graph.
     :param target_block_types: list of block types to match the type of the given block
     :return: additional info for the building blocks - block type and addresses of all operations inside the block .
     """
     if target_block_types is None:
         return True
-    block_for_ops = BuildingBlock(orig_graph.get_node_key_by_id(start_node_id).split(' ')[-1],
-                                  orig_graph.get_node_key_by_id(end_node_id).split(' ')[-1])
+    block_for_ops = BuildingBlock(
+        orig_graph.get_node_key_by_id(start_node_id).split(" ")[-1],
+        orig_graph.get_node_key_by_id(end_node_id).split(" ")[-1],
+    )
     op_addresses = get_all_node_op_addresses_in_block(orig_graph, block_for_ops)
     block_type = get_type_building_block(op_addresses)
     return block_type in target_block_types
 
 
 class BlockFilteringStrategy(Enum):
     """
     Defines strategy for filtering overlapping blocks.
     KEEP_SMALL - gives a preference to small blocks. It starts from the smallest block and filters all blocks that
     intersect or include it. Then it finds the next smallest block from the remaining ones and repeats the procedure.
     KEEP_SEQUENTIAL - gives a preference to sequential blocks, which follow each other in the model. This strategy
     is helpful for Progressive Shrinking Algorithm.
     """
-    KEEP_SMALL = 'keep_small'
-    KEEP_SEQUENTIAL = 'keep_sequential'
+
+    KEEP_SMALL = "keep_small"
+    KEEP_SEQUENTIAL = "keep_sequential"
 
 
-def get_building_blocks(compressed_model: NNCFNetwork,
-                        max_block_size: int = 50,
-                        min_block_size: int = 5,
-                        block_filter_strategy=BlockFilteringStrategy.KEEP_SEQUENTIAL,
-                        hw_fused_ops: bool = True,
-                        target_block_types: Optional[List[BuildingBlockType]] = None) -> Tuple[ExtendedBuildingBlocks,
-                                                                                               GroupedBlockIDs]:
+def get_building_blocks(
+    compressed_model: NNCFNetwork,
+    max_block_size: int = 50,
+    min_block_size: int = 5,
+    block_filter_strategy=BlockFilteringStrategy.KEEP_SEQUENTIAL,
+    hw_fused_ops: bool = True,
+    target_block_types: Optional[List[BuildingBlockType]] = None,
+) -> Tuple[ExtendedBuildingBlocks, GroupedBlockIDs]:
     """
     This algorithm finds building blocks based on the analysis of the transformed graph.
     A building block is a block that satisfies the following rules:
     - has one input and one output tensors
     - input and output tensors shape are the same
     - removing a block from the graph (that is, the layers included in the block are not executed)
       does not lead to duplication of edges along which the same tensor flows
     - removing a block from the graph (that is, the layers included in the block are not executed)
       does not lead to dangling edges
     - removing a block from the graph (that is, the layers included in the block are not executed)
       does not lead to duplicate activation layers
     """
     if min_block_size > max_block_size:
-        raise AttributeError(f'Minimal value for block size {min_block_size} can not be more than maximum one '
-                             f'{max_block_size}. Change max_block_size or min_block_size.')
-    orig_graph = compressed_model.get_original_graph()  # PTNNCFGraph
+        raise AttributeError(
+            f"Minimal value for block size {min_block_size} can not be more than maximum one "
+            f"{max_block_size}. Change max_block_size or min_block_size."
+        )
+    orig_graph = compressed_model.nncf.get_original_graph()  # PTNNCFGraph
     blocks = get_potential_building_blocks(orig_graph, hw_fused_ops, min_block_size, max_block_size)
     sorted_blocks = sorted(blocks, key=cmp_to_key(compare_for_building_block))
     filtered_building_blocks = remove_duplicates(sorted_blocks)
-    filtered_building_blocks = remove_overlapping_blocks(filtered_building_blocks, block_filter_strategy, orig_graph,
-                                                         target_block_types)
+    filtered_building_blocks = remove_overlapping_blocks(
+        filtered_building_blocks, block_filter_strategy, orig_graph, target_block_types
+    )
     ext_building_blocks = get_building_block_for_original_graph(filtered_building_blocks, orig_graph)
     filtered_basic_blocks = [eb.basic_block for eb in ext_building_blocks]
     group_dependent = get_group_of_dependent_blocks(filtered_basic_blocks)
     return ext_building_blocks, group_dependent
 
 
-def get_potential_building_blocks(orig_graph: NNCFGraph,
-                                  hw_fused_ops: bool,
-                                  min_block_size: int,
-                                  max_block_size: int):
+def get_potential_building_blocks(orig_graph: NNCFGraph, hw_fused_ops: bool, min_block_size: int, max_block_size: int):
     """
     Builds a list of potential building blocks that has the same shapes on input and output and that satisfy some
     heuristic rules.
     :param orig_graph: NNCF graph of the non-compressed model
     :param hw_fused_ops: If True, automatic block search will not relate operations, which are fused on inference,
          into different blocks for skipping.
     :param min_block_size: minimum number of operations that should be in the block. Blocks with smaller number of
     operations are not considered.
     :param max_block_size: maximum number of operations that should be in the block. Blocks with larger number of
     operations are not considered.
     :return: list of potential building blocks
     """
     sgraph = get_search_graph(orig_graph, hw_fused_ops)
-    heuristic_rules = [check_graph_has_no_duplicate_edges_after_block_removal,
-                       check_graph_has_no_act_layer_duplication_after_block_removal,
-                       check_graph_has_no_hanging_edges_after_block_removal]
+    heuristic_rules = [
+        check_graph_has_no_duplicate_edges_after_block_removal,
+        check_graph_has_no_act_layer_duplication_after_block_removal,
+        check_graph_has_no_hanging_edges_after_block_removal,
+    ]
     blocks = []
     act_input_shape, act_output_shape = get_potential_candidate_for_block(sgraph)
     for shape, first_skipped_nodes in act_input_shape.items():
         for first_skipped_node in first_skipped_nodes:
             previous_nodes = sgraph.get_prev_nodes(first_skipped_node.node_key)
             if first_skipped_node.node_type == IgnoredNameOperators or len(previous_nodes) != 1:
                 continue
@@ -415,24 +428,28 @@
                 for rule_fn in heuristic_rules:
                     if not rule_fn(sgraph, first_skipped_node, end_node):
                         all_rules_is_true = False
                         break
                 if all_rules_is_true:
                     blocks.append(PotentialBuildingBlock(first_skipped_node, end_node))
     if len(blocks) > 300:
-        nncf_logger.warning('Number of potential building blocks is too much. The processing time can be high. '
-                            'Shallow the accepted range for the length of building blocks via '
-                            'max_block_size and min_block_size to accelerate the search process.')
+        nncf_logger.warning(
+            "Number of potential building blocks is too much. The processing time can be high. "
+            "Shallow the accepted range for the length of building blocks via "
+            "max_block_size and min_block_size to accelerate the search process."
+        )
     return blocks
 
 
-def remove_overlapping_blocks(building_blocks: List[PotentialBuildingBlock],
-                              block_filter_strategy: BlockFilteringStrategy,
-                              orig_graph: NNCFGraph,
-                              target_block_types: List[BuildingBlockType]) -> List[PotentialBuildingBlock]:
+def remove_overlapping_blocks(
+    building_blocks: List[PotentialBuildingBlock],
+    block_filter_strategy: BlockFilteringStrategy,
+    orig_graph: NNCFGraph,
+    target_block_types: List[BuildingBlockType],
+) -> List[PotentialBuildingBlock]:
     """
     Applies the given filtering strategy to remove overlapping blocks - pairs of blocks that shares common operations.
     :param building_blocks: list of potential building blocks
     :param block_filter_strategy: strategy to remove overlapping blocks
     :param orig_graph: NNCF graph for non-compressed model
     :param target_block_types: list of target block types that shouldn't be removed
     :return: list of non-overlapping building blocks
@@ -443,28 +460,28 @@
     for block in building_blocks:
         start_node_id = get_start_node_id(block, orig_graph)
         id_end = block.end_node.bottom_id
         num_ops_in_block = id_end - start_node_id - 1
         start_ids.append(start_node_id)
         end_ids.append(id_end)
         num_ops_in_blocks.append(num_ops_in_block)
-    is_target_block_type_fn = partial(is_target_block_type, orig_graph=orig_graph,
-                                      target_block_types=target_block_types)
+    is_target_block_type_fn = partial(
+        is_target_block_type, orig_graph=orig_graph, target_block_types=target_block_types
+    )
     get_indexes_of_overlapping_blocks_fn = get_indexes_of_overlapping_blocks_seq
     if block_filter_strategy == BlockFilteringStrategy.KEEP_SMALL:
         get_indexes_of_overlapping_blocks_fn = get_indexes_of_overlapping_blocks_min
     ids_of_overlapping_blocks = get_indexes_of_overlapping_blocks_fn(
         start_ids=start_ids,
         end_ids=end_ids,
         num_ops_in_blocks=num_ops_in_blocks,
-        is_target_block_type_fn=is_target_block_type_fn
+        is_target_block_type_fn=is_target_block_type_fn,
     )
     if ids_of_overlapping_blocks:
-        building_blocks = [block for i, block in enumerate(building_blocks) if
-                           i not in ids_of_overlapping_blocks]
+        building_blocks = [block for i, block in enumerate(building_blocks) if i not in ids_of_overlapping_blocks]
     return building_blocks
 
 
 def get_start_node_id(block: PotentialBuildingBlock, orig_graph: NNCFGraph) -> int:
     """
     Returns id of the starting node of the block - the node right before first skipped node.
     :param block: potential building block
@@ -474,73 +491,77 @@
     id_st = block.first_skipped_node.main_id
     first_skipped_node = orig_graph.get_node_by_id(id_st)
     input_nodes = orig_graph.get_input_nodes()
     start_node_id = id_st
     if first_skipped_node not in input_nodes and not block.first_skipped_node.is_dummy:
         previous_nodes = orig_graph.get_previous_nodes(first_skipped_node)
         num_inputs = len(previous_nodes)
-        assert num_inputs == 1, f'building block should have a single input, but it has {num_inputs} inputs.'
+        assert num_inputs == 1, f"building block should have a single input, but it has {num_inputs} inputs."
         start_node_id = previous_nodes[0].node_id
     return start_node_id
 
 
 def get_indexes_of_overlapping_blocks_seq(
-        start_ids: List[int],
-        end_ids: List[int],
-        num_ops_in_blocks: List[int],
-        is_target_block_type_fn: Optional[Callable[[int, int], bool]] = None) -> Set[int]:
+    start_ids: List[int],
+    end_ids: List[int],
+    num_ops_in_blocks: List[int],
+    is_target_block_type_fn: Optional[Callable[[int, int], bool]] = None,
+) -> Set[int]:
     """
     The function takes coordinates of the building block (start and end ids) and finds indexes of overlapping blocks.
     After only disjoint blocks remain after filtering the found blocks.
     :param start_ids: indexes of start node in the blocks.
     :param end_ids: indexes of end node in the blocks.
     :param num_ops_in_blocks: number of operations in the block.
     :param is_target_block_type_fn: functor that defines whether the block of target type by taking indexes of start
     and end node. Currently, types are ignored in this strategy
     :return: set of indexes of the overlapping blocks
     """
     num_blocks = len(num_ops_in_blocks)
     block_graph = nx.DiGraph()
     for i, (s, e, n) in enumerate(zip(start_ids, end_ids, num_ops_in_blocks)):
-        block_graph.add_edge(s, e, attr={'cost': 4 - n, 'block_id': i})
+        block_graph.add_edge(s, e, attr={"cost": 4 - n, "block_id": i})
 
     result = set(range(num_blocks))
     ids_of_not_overlapping_blocks = set()
     while block_graph.nodes:
-        ids_of_not_overlapping_nodes = nx.dag_longest_path(block_graph, weight='cost')
+        ids_of_not_overlapping_nodes = nx.dag_longest_path(block_graph, weight="cost")
         node_ids_to_remove = set()
         for i in range(len(ids_of_not_overlapping_nodes) - 1):
             data = block_graph.get_edge_data(ids_of_not_overlapping_nodes[i], ids_of_not_overlapping_nodes[i + 1])
-            ids_of_not_overlapping_blocks.add(data['attr']['block_id'])
+            ids_of_not_overlapping_blocks.add(data["attr"]["block_id"])
         for node_id in ids_of_not_overlapping_nodes:
             block_graph.remove_node(node_id)
 
         left_border = ids_of_not_overlapping_nodes[0]
         right_border = ids_of_not_overlapping_nodes[-1]
         for node_id1, node_id2, data in block_graph.edges(data=True):
-            i = data['attr']['block_id']
-            does_intersect_found_block = left_border < start_ids[i] < right_border or \
-                                         left_border < end_ids[i] < right_border
-            does_include_found_block = start_ids[i] <= left_border <= end_ids[i] and \
-                                       start_ids[i] <= right_border <= end_ids[i]
+            i = data["attr"]["block_id"]
+            does_intersect_found_block = (
+                left_border < start_ids[i] < right_border or left_border < end_ids[i] < right_border
+            )
+            does_include_found_block = (
+                start_ids[i] <= left_border <= end_ids[i] and start_ids[i] <= right_border <= end_ids[i]
+            )
             if does_intersect_found_block or does_include_found_block:
                 node_ids_to_remove.add(node_id1)
                 node_ids_to_remove.add(node_id2)
 
         for node_id in node_ids_to_remove:
             block_graph.remove_node(node_id)
 
     return result - ids_of_not_overlapping_blocks
 
 
 def get_indexes_of_overlapping_blocks_min(
-        start_ids: List[int],
-        end_ids: List[int],
-        num_ops_in_blocks: List[int],
-        is_target_block_type_fn: Optional[Callable[[int, int], bool]] = None) -> Set[int]:
+    start_ids: List[int],
+    end_ids: List[int],
+    num_ops_in_blocks: List[int],
+    is_target_block_type_fn: Optional[Callable[[int, int], bool]] = None,
+) -> Set[int]:
     """
     The function takes coordinates of the building block (start and end ids) and finds indexes of overlapping blocks.
     After only disjoint blocks remain after filtering the found blocks.
     :param start_ids: indexes of start node in the blocks.
     :param end_ids: indexes of end node in the blocks.
     :param num_ops_in_blocks: number of operations in the block.
     :param is_target_block_type_fn: functor that defines whether the block of target type by taking indexes of start
@@ -572,18 +593,20 @@
                 result.add(found_block_id)
             if found_block_id in list_of_all_block_indexes:
                 list_of_all_block_indexes.remove(found_block_id)
         left_border = start_ids[found_block_id]
         right_border = end_ids[found_block_id]
         ids_to_remove = []
         for i in list_of_all_block_indexes:
-            does_intersect_found_block = left_border < start_ids[i] < right_border or \
-                                         left_border < end_ids[i] < right_border
-            does_include_found_block = start_ids[i] <= left_border <= end_ids[i] and \
-                                       start_ids[i] <= right_border <= end_ids[i]
+            does_intersect_found_block = (
+                left_border < start_ids[i] < right_border or left_border < end_ids[i] < right_border
+            )
+            does_include_found_block = (
+                start_ids[i] <= left_border <= end_ids[i] and start_ids[i] <= right_border <= end_ids[i]
+            )
             if does_intersect_found_block or does_include_found_block:
                 ids_to_remove.append(i)
                 result.add(i)
         for i in ids_to_remove:
             list_of_all_block_indexes.remove(i)
     return result
 
@@ -610,16 +633,15 @@
             idx += 1
             groups[idx] = []
     groups[idx].append(len(blocks) - 1)
 
     return groups
 
 
-def get_all_node_op_addresses_in_block(graph: NNCFGraph,
-                                       block: BuildingBlock) -> Set[OperationAddress]:
+def get_all_node_op_addresses_in_block(graph: NNCFGraph, block: BuildingBlock) -> Set[OperationAddress]:
     """
     Returns set of operation addresses of all layers included in the block.
 
     :param graph: original non-compressed graph.
     :param block: Building blocks.
     :return: Set of operation addresses for building block.
     """
@@ -630,27 +652,28 @@
             node = graph.get_node_by_key(node_key)
             op_addresses.add(OperationAddress.from_str(node.node_name))
     start_op_address = OperationAddress.from_str(block.start_node_name)
     op_addresses.remove(start_op_address)
     return op_addresses
 
 
-def get_all_modules_in_blocks(compressed_model: NNCFNetwork,
-                              op_adresses_in_blocks: Set[OperationAddress]) -> List[torch.nn.Module]:
+def get_all_modules_in_blocks(
+    compressed_model: NNCFNetwork, op_adresses_in_blocks: Set[OperationAddress]
+) -> List[torch.nn.Module]:
     """
     Returns set of all modules included in the block.
 
     :param compressed_model: Target model.
     :param op_adresses_in_blocks: Set of operation addresses for building block.
     :return: List of module for building block.
     """
     modules = []
     for op_address in op_adresses_in_blocks:
         if op_address.operator_name in NNCF_MODULES_OP_NAMES:
-            modules.append(compressed_model.get_module_by_scope(op_address.scope_in_model))
+            modules.append(compressed_model.nncf.get_module_by_scope(op_address.scope_in_model))
     return modules
 
 
 def get_type_building_block(op_addresses_in_block: Set[OperationAddress]) -> BuildingBlockType:
     """
     Returns type of building block.
     """
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/search_building_blocks/search_graph.py` & `nncf-2.5.0/nncf/experimental/torch/search_building_blocks/search_graph.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,45 +1,40 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import deque
 from copy import deepcopy
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Set
+from typing import Any, Dict, List, Set
 
 import networkx as nx
 
 from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNodeName
 from nncf.common.graph.graph_matching import find_subgraphs_matching_pattern
+from nncf.common.graph.patterns.manager import PatternsManager
+from nncf.common.graph.patterns.manager import TargetDevice
+from nncf.common.utils.backend import BackendType
 from nncf.common.utils.dot_file_rw import write_dot_graph
 from nncf.torch.graph.graph import PTNNCFGraph
 from nncf.torch.graph.operator_metatypes import PTRELUMetatype
-from nncf.torch.hardware.fused_patterns import PT_HW_FUSED_PATTERNS
 
 
 class SearchGraphNode:
     """
     Class describing nodes used in SearchGraph.
     """
 
-    def __init__(self,
-                 node_key: str,
-                 data: Dict):
+    def __init__(self, node_key: str, data: Dict):
         self.node_key = node_key
         self.data = data if data else {}
 
     @property
     def is_merged(self) -> bool:
         """
         Returns value that the node is merged.
@@ -73,25 +68,25 @@
 
     @property
     def main_id(self) -> int:
         """
         Returns the id of node. In case if the node is merged returns id of the first node from merged node list.
         """
         if not self.is_merged:
-            return self.data.get('id')
-        return self.data.get(SearchGraph.MERGED_NODES_NODE_ATTR)[0].get('id')
+            return self.data.get("id")
+        return self.data.get(SearchGraph.MERGED_NODES_NODE_ATTR)[0].get("id")
 
     @property
     def bottom_id(self) -> int:
         """
         Returns the id of node. In case if the node is merged returns id of the last node from merged node list.
         """
         if not self.is_merged:
-            return self.data.get('id')
-        return self.data.get(SearchGraph.MERGED_NODES_NODE_ATTR)[-1].get('id')
+            return self.data.get("id")
+        return self.data.get(SearchGraph.MERGED_NODES_NODE_ATTR)[-1].get("id")
 
     def __str__(self):
         return self.node_key
 
     def __hash__(self):
         return hash(str(self))
 
@@ -103,20 +98,21 @@
 
 
 class SearchGraph:
     """
     A wrapper over the graph, which represents the DNN execution graph transformed
     by pattern matching, merging nodes and inserting auxiliary nodes.
     """
-    ACTIVATION_OUTPUT_SHAPE_ATTR = 'activation_output_shape'
-    IS_MERGED_NODE_ATTR = 'is_merged'
-    IS_DUMMY_NODE_ATTR = 'is_dummy'
-    KEY_NODE_ATTR = 'key'
-    MERGED_NODES_NODE_ATTR = 'merged_nodes'
-    TYPE_NODE_ATTR = 'type'
+
+    ACTIVATION_OUTPUT_SHAPE_ATTR = "activation_output_shape"
+    IS_MERGED_NODE_ATTR = "is_merged"
+    IS_DUMMY_NODE_ATTR = "is_dummy"
+    KEY_NODE_ATTR = "key"
+    MERGED_NODES_NODE_ATTR = "merged_nodes"
+    TYPE_NODE_ATTR = "type"
     DUMMY_POSTFIX = " dummy"
 
     def __init__(self, nx_merged_graph: nx.DiGraph):
         self._nx_graph = nx_merged_graph
         old_merged_graph_nodes = deepcopy(self._nx_graph._node)
         for node_key in old_merged_graph_nodes:
             # pylint: disable=protected-access
@@ -212,44 +208,43 @@
         :return: A user-friendly graph .dot file, making it easier to debug the network and setup
         ignored/target scopes.
         """
         out_graph = nx.DiGraph()
         for node in self.get_all_nodes():
             attrs_node = {}
             if node.is_merged:
-                attrs_node['label'] = f"main: {node.main_id} bottom: {node.bottom_id} {node.node_key}"
+                attrs_node["label"] = f"main: {node.main_id} bottom: {node.bottom_id} {node.node_key}"
             elif node.is_dummy:
-                attrs_node['label'] = f'dummy {node.node_key}'
+                attrs_node["label"] = f"dummy {node.node_key}"
             else:
-                attrs_node['label'] = f"id: {node.node_key}"
+                attrs_node["label"] = f"id: {node.node_key}"
             out_graph.add_node(node.node_key, **attrs_node)
 
         for u, v in self._nx_graph.edges:
             edge = self._nx_graph.edges[u, v]
-            style = 'solid'
+            style = "solid"
             out_graph.add_edge(u, v, label=edge[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR], style=style)
 
-        mapping = {k: v['label'] for k, v in out_graph.nodes.items()}
+        mapping = {k: v["label"] for k, v in out_graph.nodes.items()}
         out_graph = nx.relabel_nodes(out_graph, mapping)
         for node in out_graph.nodes.values():
-            node.pop('label')
+            node.pop("label")
 
         return out_graph
 
     def visualize_graph(self, path: str):
         out_graph = self._get_graph_for_visualization()
         write_dot_graph(out_graph, path)
 
 
 def get_search_graph(original_graph: PTNNCFGraph, hw_fused_ops: bool) -> SearchGraph:
     """
     Returns a transformed representation of the network graph for blocks searching.
     """
-    nx_merged_graph = get_merged_original_graph_with_pattern(original_graph.get_nx_graph_copy(),
-                                                             hw_fused_ops)
+    nx_merged_graph = get_merged_original_graph_with_pattern(original_graph.get_nx_graph_copy(), hw_fused_ops)
     sgraph = SearchGraph(nx_merged_graph)
     return sgraph
 
 
 def get_merged_original_graph_with_pattern(orig_graph: nx.DiGraph, hw_fused_ops: bool) -> nx.DiGraph:
     """
     :param orig_graph: Original graph of model
@@ -257,15 +252,15 @@
     of different skipping block.
     :return: Graph with merged nodes by patterns
     """
     merged_graph = orig_graph
     if not hw_fused_ops:
         return merged_graph
     # pylint: disable=protected-access
-    pattern_fusing_graph = PT_HW_FUSED_PATTERNS.get_full_pattern_graph()
+    pattern_fusing_graph = PatternsManager.get_full_hw_pattern_graph(backend=BackendType.TORCH, device=TargetDevice.ANY)
     matches = find_subgraphs_matching_pattern(orig_graph, pattern_fusing_graph)
     nx.set_node_attributes(merged_graph, False, SearchGraph.IS_DUMMY_NODE_ATTR)
     nx.set_node_attributes(merged_graph, False, SearchGraph.IS_MERGED_NODE_ATTR)
     for match in matches:
         if len(match) == 1:
             continue
         input_node_key = match[0]
@@ -281,15 +276,15 @@
             out_edge_copies_dict[out_edge_key] = deepcopy(merged_graph.edges[out_edge_key])
 
         merged_node_key = ""
         merged_nodes = []
         type_list = []
         for node_key in match:
             attrs = orig_graph.nodes[node_key]
-            merged_node_key += str(attrs['id']) + ' ' + attrs[SearchGraph.TYPE_NODE_ATTR] + '  '
+            merged_node_key += str(attrs["id"]) + " " + attrs[SearchGraph.TYPE_NODE_ATTR] + "  "
             # pylint: disable=protected-access
             merged_nodes.append(orig_graph.nodes[node_key])
             merged_graph.remove_node(node_key)
             type_list.append(attrs[SearchGraph.TYPE_NODE_ATTR])
         merged_node_attrs = {
             SearchGraph.KEY_NODE_ATTR: merged_node_key,
             SearchGraph.IS_MERGED_NODE_ATTR: True,
@@ -302,17 +297,17 @@
         for out_edge_key, out_edge_attrs in out_edge_copies_dict.items():
             merged_graph.add_edge(merged_node_key, out_edge_key[1], **out_edge_attrs)
 
     return merged_graph
 
 
 # pylint:disable=too-many-branches
-def check_graph_has_no_hanging_edges_after_block_removal(graph: SearchGraph,
-                                                         first_skipped_node: SearchGraphNode,
-                                                         end_node: SearchGraphNode) -> bool:
+def check_graph_has_no_hanging_edges_after_block_removal(
+    graph: SearchGraph, first_skipped_node: SearchGraphNode, end_node: SearchGraphNode
+) -> bool:
     """
     The subgraph is traversed starting with the first_skipped_node and ending with the end_node
     to determine that after deleting such a block there are no dangling edges in the graph.
     """
     #            A
     #           / \
     #          /   \
@@ -326,15 +321,15 @@
     #           ...
     #            |
     #         end_node
     start_node = first_skipped_node
     if not first_skipped_node.is_dummy:
         previous_nodes = graph.get_previous_nodes(first_skipped_node)
         num_inputs = len(previous_nodes)
-        assert num_inputs == 1, f'building block should have a single input, but it has {num_inputs} inputs.'
+        assert num_inputs == 1, f"building block should have a single input, but it has {num_inputs} inputs."
         start_node = previous_nodes[0]
     q = deque([start_node])
     addit_nodes = set()
     nodes = []
     potential_end_nodes = []
     while len(q) != 0:
         current_node = q.pop()
@@ -361,17 +356,17 @@
     for node in addit_nodes:
         if node not in nodes:
             return False
     nodes.append(end_node)
     return True
 
 
-def check_graph_has_no_duplicate_edges_after_block_removal(sgraph: SearchGraph,
-                                                           first_skipped_node: SearchGraphNode,
-                                                           end_node: SearchGraphNode) -> bool:
+def check_graph_has_no_duplicate_edges_after_block_removal(
+    sgraph: SearchGraph, first_skipped_node: SearchGraphNode, end_node: SearchGraphNode
+) -> bool:
     """
     This rule ensures that no duplicate edges will be created in the graph after a block is deleted.
     """
     #         A              A
     #        / \            / \
     #       /   \          /   \
     #    block   |  =>     \   /
@@ -392,17 +387,17 @@
         if len(previous_node) != 0 and len(next_end_node) != 0:
             attr = sgraph._nx_graph.get_edge_data(previous_node[0].node_key, next_end_node[0].node_key)
         else:
             attr = None
     return attr is None
 
 
-def check_graph_has_no_act_layer_duplication_after_block_removal(sgraph: SearchGraph,
-                                                                 first_skipped_node: SearchGraphNode,
-                                                                 end_node: SearchGraphNode) -> bool:
+def check_graph_has_no_act_layer_duplication_after_block_removal(
+    sgraph: SearchGraph, first_skipped_node: SearchGraphNode, end_node: SearchGraphNode
+) -> bool:
     """
     This rule ensures that after the block is deleted there will be no duplication of activation layers.
     """
     #         A             A
     #         |             |
     #        relu          relu
     #         |      =>     |
@@ -413,16 +408,18 @@
     previous_nodes = sgraph.get_prev_nodes(first_skipped_node.node_key)
     next_end_node = sgraph.get_next_nodes(end_node.node_key)
     if len(next_end_node) == 0 or len(previous_nodes) == 0:
         return True
     if previous_nodes[0].is_dummy:
         previous_nodes = sgraph.get_prev_nodes(previous_nodes[0].node_key)
 
-    if previous_nodes[0].node_type[-1] in PTRELUMetatype.get_all_aliases() \
-            and next_end_node[0].node_type[0] in PTRELUMetatype.get_all_aliases():
+    if (
+        previous_nodes[0].node_type[-1] in PTRELUMetatype.get_all_aliases()
+        and next_end_node[0].node_type[0] in PTRELUMetatype.get_all_aliases()
+    ):
         return False
     return True
 
 
 def get_num_ops_in_block(first_skipped_node: SearchGraphNode, end_node: SearchGraphNode) -> int:
     """
     Calculates number of operations in the block by using indexes. Indexes should be in execution order.
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/sparsity/movement/algo.py` & `nncf-2.5.0/nncf/experimental/torch/sparsity/movement/algo.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,18 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import inspect
 from copy import deepcopy
 from typing import List
 
 import torch
 import torch.distributed as dist
 
 from nncf import NNCFConfig
@@ -30,16 +29,14 @@
 from nncf.experimental.torch.sparsity.movement.layers import SparseConfig
 from nncf.experimental.torch.sparsity.movement.layers import SparseConfigByScope
 from nncf.experimental.torch.sparsity.movement.layers import SparseStructure
 from nncf.experimental.torch.sparsity.movement.loss import ImportanceLoss
 from nncf.experimental.torch.sparsity.movement.scheduler import MovementPolynomialThresholdScheduler
 from nncf.experimental.torch.sparsity.movement.scheduler import MovementSchedulerParams
 from nncf.experimental.torch.sparsity.movement.structured_mask_handler import StructuredMaskHandler
-from nncf.experimental.torch.sparsity.movement.structured_mask_strategy import STRUCTURED_MASK_STRATEGY
-from nncf.experimental.torch.sparsity.movement.structured_mask_strategy import detect_supported_model_family
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.graph.transformations.commands import TransformationPriority
 from nncf.torch.layers import NNCFLinear
 from nncf.torch.module_operations import UpdateWeightAndBias
@@ -49,105 +46,118 @@
 from nncf.torch.sparsity.base_algo import SparseModuleInfo
 from nncf.torch.sparsity.collector import PTSparseModelStatisticsCollector
 from nncf.torch.utils import get_model_device
 
 SUPPORTED_NNCF_MODULES = [NNCFLinear]
 
 
-@PT_COMPRESSION_ALGORITHMS.register('movement_sparsity')
+@PT_COMPRESSION_ALGORITHMS.register("movement_sparsity")
 class MovementSparsityBuilder(BaseSparsityAlgoBuilder):
     def __init__(self, config, should_init: bool = True):
         super().__init__(config, should_init)
-        configs = self._algo_config.get('sparse_structure_by_scopes', [])
+        configs = self._algo_config.get("sparse_structure_by_scopes", [])
         self._sparse_configs_by_scopes = [SparseConfigByScope.from_config(c) for c in configs]
 
-    def create_weight_sparsifying_operation(self, target_module_node: NNCFNode,
-                                            compression_lr_multiplier: float) -> MovementSparsifier:
+    def create_weight_sparsifying_operation(
+        self, target_module_node: NNCFNode, compression_lr_multiplier: float
+    ) -> MovementSparsifier:
         sparse_cfg = SparseConfig(SparseStructure.FINE)
         node_name = target_module_node.node_name
         matched_scopes = []
         for configs_per_scopes in self._sparse_configs_by_scopes:
             target_scopes = configs_per_scopes.target_scopes
             if matches_any(node_name, target_scopes):
                 sparse_cfg = configs_per_scopes.sparse_config
                 matched_scopes.append(target_scopes)
         if len(matched_scopes) >= 2:
             raise RuntimeError(f'"{node_name}" is matched by multiple items in `sparse_structure_by_scopes`.')
 
-        return MovementSparsifier(target_module_node, sparse_cfg=sparse_cfg, frozen=False,
-                                  compression_lr_multiplier=compression_lr_multiplier,
-                                  layerwise_loss_lambda=0.5)
+        return MovementSparsifier(
+            target_module_node,
+            sparse_cfg=sparse_cfg,
+            frozen=False,
+            compression_lr_multiplier=compression_lr_multiplier,
+            layerwise_loss_lambda=0.5,
+        )
 
     def _sparsify_weights(self, target_model: NNCFNetwork) -> List[PTInsertionCommand]:
         device = get_model_device(target_model)
-        sparsified_module_nodes = target_model.get_weighted_original_graph_nodes(
+        sparsified_module_nodes = target_model.nncf.get_weighted_original_graph_nodes(
             nncf_module_names=[m.__name__ for m in SUPPORTED_NNCF_MODULES]
         )
         insertion_commands = []
         for module_node in sparsified_module_nodes:
             node_name = module_node.node_name
 
             if not self._should_consider_scope(node_name):
-                nncf_logger.info(f'Ignored adding weight sparsifier in scope: {node_name}')
+                nncf_logger.info(f"Ignored adding weight sparsifier in scope: {node_name}")
                 continue
 
-            nncf_logger.debug('Adding weight sparsifier in scope: {node_name}')
+            nncf_logger.debug("Adding weight sparsifier in scope: {node_name}")
             compression_lr_multiplier = self.config.get_redefinable_global_param_value_for_algo(
-                'compression_lr_multiplier', self.name
+                "compression_lr_multiplier", self.name
             )
             sparsifying_operation = self.create_weight_sparsifying_operation(module_node, compression_lr_multiplier)
             hook = UpdateWeightAndBias(sparsifying_operation).to(device)
             insertion_commands.append(
                 PTInsertionCommand(
                     PTTargetPoint(TargetType.PRE_LAYER_OPERATION, target_node_name=node_name),
                     hook,
-                    TransformationPriority.SPARSIFICATION_PRIORITY
+                    TransformationPriority.SPARSIFICATION_PRIORITY,
                 )
             )
-            sparsified_module = target_model.get_containing_module(node_name)
-            self._sparsified_module_info.append(
-                SparseModuleInfo(node_name, sparsified_module, sparsifying_operation)
-            )
+            sparsified_module = target_model.nncf.get_containing_module(node_name)
+            self._sparsified_module_info.append(SparseModuleInfo(node_name, sparsified_module, sparsifying_operation))
 
         if not insertion_commands:
-            raise RuntimeError('No sparsifiable layer found for movement sparsity algorithm.')
+            raise RuntimeError("No sparsifiable layer found for movement sparsity algorithm.")
         return insertion_commands
 
     def _build_controller(self, model: NNCFNetwork) -> PTCompressionAlgorithmController:
         return MovementSparsityController(model, self._sparsified_module_info, self.config)
 
 
-@ADAPTIVE_COMPRESSION_CONTROLLERS.register('pt_movement_sparsity')
+MODEL_FAMILIES = ["bert", "wav2vec2", "swin", "mobilebert", "distilbert", "clip", "vit"]
+
+
+def is_supported_model_family(model: NNCFNetwork) -> None:
+    """
+    Checks whether the model family is supported by movement sparsity to conduct structured masking.
+
+    :param model: The compressed model wrapped by NNCF.
+    """
+    model_pymodules = inspect.getmodule(model).__name__.split(".")
+    is_supported = False
+    if len(model_pymodules) >= 3 and model_pymodules[:2] == ["transformers", "models"]:
+        # the case of input model defined by HuggingFace's transformers
+        model_family = model_pymodules[2]
+        is_supported = model_family in MODEL_FAMILIES
+    return is_supported
+
+
+@ADAPTIVE_COMPRESSION_CONTROLLERS.register("pt_movement_sparsity")
 class MovementSparsityController(BaseSparsityAlgoController):
-    def __init__(self, target_model: NNCFNetwork,
-                 sparsified_module_info: List[SparseModuleInfo],
-                 config: NNCFConfig):
+    def __init__(self, target_model: NNCFNetwork, sparsified_module_info: List[SparseModuleInfo], config: NNCFConfig):
         super().__init__(target_model, sparsified_module_info)
-        algo_config = extract_algo_specific_config(config, 'movement_sparsity')
+        algo_config = extract_algo_specific_config(config, "movement_sparsity")
         sparsify_operations = [m.operand for m in self.sparsified_module_info]
-        params = deepcopy(algo_config.get('params', {}))
+        params = deepcopy(algo_config.get("params", {}))
         self._distributed = False
         self._scheduler_params = MovementSchedulerParams.from_dict(params)
         self._scheduler = MovementPolynomialThresholdScheduler(self, self._scheduler_params)
         self._loss = ImportanceLoss(sparsify_operations)
         self._config = config
 
         if self._scheduler.enable_structured_masking:
-            model_family = detect_supported_model_family(self.model)
-            if model_family not in STRUCTURED_MASK_STRATEGY.registry_dict:
-                supported_model_families = list(STRUCTURED_MASK_STRATEGY.registry_dict.keys())
+            if not is_supported_model_family(self.model):
                 raise RuntimeError(
-                    'You set `enable_structured_masking=True`, but no supported model is detected. '
-                    f'Supported model families: {supported_model_families}.'
+                    "You set `enable_structured_masking=True`, but no supported model is detected. "
+                    f"Supported model families: {MODEL_FAMILIES}."
                 )
-            strategy_cls = STRUCTURED_MASK_STRATEGY.get(model_family)
-            strategy = strategy_cls.from_compressed_model(self.model)
-            self._structured_mask_handler = StructuredMaskHandler(self.model,
-                                                                  self.sparsified_module_info,
-                                                                  strategy)
+            self._structured_mask_handler = StructuredMaskHandler(self.model, self.sparsified_module_info)
 
     @property
     def compression_rate(self) -> float:
         return self.statistics().movement_sparsity.model_statistics.sparsity_level
 
     def reset_independent_structured_mask(self):
         """
@@ -165,27 +175,29 @@
 
     def populate_structured_mask(self):
         """
         Asks the structured mask handler to update structured binary masks in model operands.
         """
         assert self._scheduler.enable_structured_masking
         self._structured_mask_handler.populate_dependent_structured_mask_to_operand()
-        self._structured_mask_handler.report_structured_sparsity(self._config.get('log_dir', '.'))
+        self._structured_mask_handler.report_structured_sparsity(self._config.get("log_dir", "."))
 
     def compression_stage(self) -> CompressionStage:
         if self.scheduler.current_epoch < self._scheduler_params.warmup_start_epoch:
             return CompressionStage.UNCOMPRESSED
         if self.scheduler.current_epoch >= self._scheduler_params.warmup_end_epoch:
             return CompressionStage.FULLY_COMPRESSED
         return CompressionStage.PARTIALLY_COMPRESSED
 
     def distributed(self):
         if not dist.is_initialized():
-            raise KeyError('Could not set distributed mode for the compression algorithm '
-                           'because the default process group has not been initialized.')
+            raise KeyError(
+                "Could not set distributed mode for the compression algorithm "
+                "because the default process group has not been initialized."
+            )
 
         if next(self._model.parameters()).is_cuda:
             state = torch.cuda.get_rng_state()
             if dist.get_backend() == dist.Backend.NCCL:
                 state = state.cuda()
             torch.distributed.broadcast(state, src=0)
             torch.cuda.set_rng_state(state.cpu())
@@ -198,19 +210,19 @@
 
     def freeze(self):
         self._loss.disable()
         for minfo in self.sparsified_module_info:
             minfo.operand.requires_grad_(False)
 
     def statistics(self, quickly_collected_only=False) -> NNCFStatistics:
-        collector = PTSparseModelStatisticsCollector(self.model,
-                                                     self.sparsified_module_info,
-                                                     supports_sparse_bias=True)
+        collector = PTSparseModelStatisticsCollector(self.model, self.sparsified_module_info, supports_sparse_bias=True)
         model_statistics = collector.collect()
 
-        stats = MovementSparsityStatistics(model_statistics,
-                                           self.scheduler.current_importance_threshold,
-                                           self.scheduler.current_importance_regularization_factor)
+        stats = MovementSparsityStatistics(
+            model_statistics,
+            self.scheduler.current_importance_threshold,
+            self.scheduler.current_importance_regularization_factor,
+        )
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('movement_sparsity', stats)
+        nncf_stats.register("movement_sparsity", stats)
         return nncf_stats
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/sparsity/movement/layers.py` & `nncf-2.5.0/nncf/experimental/torch/sparsity/movement/layers.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,20 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import math
 from copy import deepcopy
 from enum import Enum
-import math
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import torch
 from torch import nn
 
 from nncf.common.graph import NNCFNode
 from nncf.experimental.torch.sparsity.movement.functions import binary_mask_by_threshold
@@ -24,85 +22,90 @@
 from nncf.torch.layer_utils import CompressionParameter
 from nncf.torch.sparsity.functions import apply_binary_mask as apply_binary_mask_impl
 from nncf.torch.sparsity.layers import BinaryMask
 from nncf.torch.utils import is_tracing_state
 
 
 class SparseStructure(Enum):
-    FINE = 'fine'
-    BLOCK = 'block'
-    PER_DIM = 'per_dim'
+    FINE = "fine"
+    BLOCK = "block"
+    PER_DIM = "per_dim"
 
 
 class SparseConfig:
     """
     Defines the sparse structure config with required options for a certain supported layer.
     """
 
-    def __init__(self,
-                 mode: SparseStructure,
-                 sparse_factors: Optional[Tuple[int, int]] = None,
-                 sparse_axis: Optional[int] = None):
+    def __init__(
+        self, mode: SparseStructure, sparse_factors: Optional[Tuple[int, int]] = None, sparse_axis: Optional[int] = None
+    ):
         """
         Parses and validates the sparse structure of a certain layer for movement sparsity.
 
         :param mode: The sparse structure mode.
         :param sparse_factors: Block shape to sparsify as a whole in a weight. Required when `mode` is "block".
         :param sparse_axis: The dimension to sparsify in a weight. Required when `mode` is "per_dim".
         """
-        error_prefix = 'Invalid sparse config.'
+        error_prefix = "Invalid sparse config."
         self.sparse_factors = None
         self.sparse_axis = None
         self.mode = mode
         if self.mode == SparseStructure.FINE:
-            if not ((isinstance(sparse_factors, (tuple, list)) and tuple(sparse_factors) == (1, 1)) or
-                    sparse_factors is None):
-                raise ValueError(f'{error_prefix} Fine sparse structure expects `sparse_factors` '
-                                 'to be [1, 1] or unspecified.')
+            if not (
+                (isinstance(sparse_factors, (tuple, list)) and tuple(sparse_factors) == (1, 1))
+                or sparse_factors is None
+            ):
+                raise ValueError(
+                    f"{error_prefix} Fine sparse structure expects `sparse_factors` to be [1, 1] or unspecified."
+                )
             if sparse_axis is not None:
-                raise ValueError(f'{error_prefix} Fine sparse structure does not expect '
-                                 'specified `axis`.')
+                raise ValueError(f"{error_prefix} Fine sparse structure does not expect specified `axis`.")
             self.sparse_factors = (1, 1)
 
         if self.mode == SparseStructure.BLOCK:
             if sparse_factors is None:
-                raise ValueError(f'{error_prefix} Missing `sparse_factors`. Block sparsity '
-                                 'structure expects it specified.')
+                raise ValueError(
+                    f"{error_prefix} Missing `sparse_factors`. Block sparsity structure expects it specified."
+                )
             if not (isinstance(sparse_factors, (tuple, list)) and len(sparse_factors) == 2):
-                raise ValueError(f'{error_prefix} Invalid format of `sparse_factors. '
-                                 'Block sparsity structure expects tuple of two numbers.')
+                raise ValueError(
+                    f"{error_prefix} Invalid format of `sparse_factors. "
+                    "Block sparsity structure expects tuple of two numbers."
+                )
             if sparse_axis is not None:
-                raise ValueError(f'{error_prefix} Block sparse structure does not expect '
-                                 'specified `axis`.')
+                raise ValueError(f"{error_prefix} Block sparse structure does not expect specified `axis`.")
             self.sparse_factors = tuple(sparse_factors)
 
         if self.mode == SparseStructure.PER_DIM:
             if sparse_axis is None:
-                raise ValueError(f'{error_prefix} Missing `axis`. Per-dim sparsity structure '
-                                 'expects it to be specified.')
+                raise ValueError(
+                    f"{error_prefix} Missing `axis`. Per-dim sparsity structure expects it to be specified."
+                )
             if sparse_factors is not None:
-                raise ValueError(f'{error_prefix} Per-dim sparsity structure does not expect '
-                                 'specified `sparse_factors`.')
+                raise ValueError(
+                    f"{error_prefix} Per-dim sparsity structure does not expect specified `sparse_factors`."
+                )
             self.sparse_axis = int(sparse_axis)
 
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'SparseConfig':
+    def from_config(cls, config: Dict[str, Any]) -> "SparseConfig":
         """
         Creates the object from its config.
 
         :param config: A dict that describes the sparse structure.
         """
-        mode_str = config.get('mode', SparseStructure.FINE.value)
+        mode_str = config.get("mode", SparseStructure.FINE.value)
         mode = SparseStructure(mode_str)
-        sparse_factors = config.get('sparse_factors')
-        axis = config.get('axis')
+        sparse_factors = config.get("sparse_factors")
+        axis = config.get("axis")
         return cls(mode, sparse_factors, axis)
 
     def __str__(self) -> str:
-        return f'{self.mode.value, self.sparse_factors}'
+        return f"{self.mode.value, self.sparse_factors}"
 
 
 class SparseConfigByScope:
     """
     Defines an entry for `sparse_structure_by_scopes` in movement sparsity configuration.
     It includes the sparse structure config, and the target scopes it is applied to.
     """
@@ -114,24 +117,24 @@
         :param sparse_config: `SparseConfig` object that describes the sparse structure config.
         :param target_scopes: The scopes to match with this `sparse_config`.
         """
         self.sparse_config = sparse_config
         self.target_scopes = target_scopes
 
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'SparseConfigByScope':
+    def from_config(cls, config: Dict[str, Any]) -> "SparseConfigByScope":
         """
         Creates the object from its representation.
 
         :param config: A dict that describes the sparse structure.
         """
-        error_prefix = f'Invalid sparse structure by scopes {config}.'
-        target_scopes = config.get('target_scopes')
+        error_prefix = f"Invalid sparse structure by scopes {config}."
+        target_scopes = config.get("target_scopes")
         if not target_scopes:
-            raise ValueError(f'{error_prefix} Missing `target_scopes`.')
+            raise ValueError(f"{error_prefix} Missing `target_scopes`.")
         sparse_config = SparseConfig.from_config(config)
         return cls(sparse_config, target_scopes)
 
 
 @COMPRESSION_MODULES.register()
 class MovementSparsifier(nn.Module):
     """
@@ -157,21 +160,22 @@
         """
         super().__init__()
         self.target_module_node = target_module_node
         self.prune_bias = bool(target_module_node.layer_attributes.bias)
         self.frozen = frozen
         self.layerwise_loss_lambda = layerwise_loss_lambda
         self._importance_threshold = -math.inf
-        self._importance_regularization_factor = 0.
+        self._importance_regularization_factor = 0.0
 
         weight_shape: List[int] = target_module_node.layer_attributes.get_weight_shape()
-        assert len(weight_shape) == 2, 'Unsupported module with weight shape not in 2D.'
+        assert len(weight_shape) == 2, "Unsupported module with weight shape not in 2D."
         self.weight_ctx = BinaryMask(weight_shape)
         self.sparse_factors = self._get_sparse_factors(weight_shape, sparse_cfg)
         self.sparse_structure = sparse_cfg.mode
+        self.sparse_cfg = sparse_cfg
 
         weight_importance_shape = self._get_weight_importance_shape(
             weight_shape, self.sparse_factors, self.sparse_structure
         )
         self.weight_importance = CompressionParameter(
             torch.zeros(weight_importance_shape),
             requires_grad=not self.frozen,
@@ -204,16 +208,17 @@
     def importance_regularization_factor(self):
         return self._importance_regularization_factor
 
     @importance_regularization_factor.setter
     def importance_regularization_factor(self, value: float):
         self._importance_regularization_factor = value
 
-    def forward(self, weight: torch.Tensor, bias: Optional[torch.Tensor] = None
-                ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
+    def forward(
+        self, weight: torch.Tensor, bias: Optional[torch.Tensor] = None
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
         if is_tracing_state():
             masked_weight = weight.mul(self.weight_ctx.binary_mask)
             masked_bias = None if bias is None else bias.mul(self.bias_ctx.binary_mask)
         else:
             weight_mask = self._calc_training_binary_mask(is_bias=False)
             masked_weight = apply_binary_mask_impl(weight_mask, weight)
             masked_bias = None
@@ -230,90 +235,93 @@
         """
         Gets the importance score parameter of the operand.
 
         :param is_bias: If true, will return the bias importance. Otherwise will return the weight importance.
         :param expanded: Whether should expand the importance to the same shape as module weight or bias.
         """
         if is_bias and (not self.prune_bias):
-            raise ValueError('The layer to sparsify does not contain bias.')
+            raise ValueError("The layer to sparsify does not contain bias.")
         importance = self.bias_importance if is_bias else self.weight_importance
         if (not expanded) or self.sparse_factors == [1, 1]:
             return importance
         expand_factors = [self.sparse_factors[0]] if is_bias else self.sparse_factors
         for dim, factor in enumerate(expand_factors):
             importance = importance.repeat_interleave(factor, dim=dim)
         return importance
 
     def loss(self) -> torch.Tensor:
-        if self.frozen or self.importance_regularization_factor == 0.:
-            return torch.tensor(0., device=self._get_device())
-        layer_loss = torch.mean(torch.sigmoid(self.weight_importance)) * \
-            self.layerwise_loss_lambda * math.prod(self.sparse_factors)
+        if self.frozen or self.importance_regularization_factor == 0.0:
+            return torch.tensor(0.0, device=self._get_device())
+        layer_loss = (
+            torch.mean(torch.sigmoid(self.weight_importance))
+            * self.layerwise_loss_lambda
+            * math.prod(self.sparse_factors)
+        )
         if self.prune_bias:
-            layer_loss += torch.mean(torch.sigmoid(self.bias_importance)) * \
-                self.layerwise_loss_lambda * float(self.sparse_factors[0])
+            layer_loss += (
+                torch.mean(torch.sigmoid(self.bias_importance))
+                * self.layerwise_loss_lambda
+                * float(self.sparse_factors[0])
+            )
         return layer_loss * self.importance_regularization_factor
 
     def requires_grad_(self, requires_grad: bool = True):
         super().requires_grad_(requires_grad)
         if not requires_grad:
             self.zero_grad(set_to_none=True)  # avoid further unexpected update with Adam optimizer
         self.frozen = not requires_grad
 
     def extra_repr(self) -> str:
-        return f'sparse_structure: {self.sparse_structure.value} {self.sparse_factors}'
+        return f"sparse_structure: {self.sparse_structure.value} {self.sparse_factors}"
 
     def _get_device(self) -> torch.device:
         return self.weight_importance.device
 
     def _calc_training_binary_mask(self, is_bias: bool = False):
         ctx = self.bias_ctx if is_bias else self.weight_ctx
         if (not self.training) or self.frozen:
             return ctx.binary_mask
         mask = binary_mask_by_threshold(
-            input_tensor=self.get_importance(is_bias, expanded=True),
-            threshold=self.importance_threshold
+            input_tensor=self.get_importance(is_bias, expanded=True), threshold=self.importance_threshold
         )
         ctx.binary_mask = mask
         return mask
 
     @staticmethod
-    def _get_weight_importance_shape(weight_shape: List[int],
-                                     sparse_factors: Tuple[int, int],
-                                     sparse_structure: SparseStructure) -> Tuple[int, int]:
+    def _get_weight_importance_shape(
+        weight_shape: List[int], sparse_factors: Tuple[int, int], sparse_structure: SparseStructure
+    ) -> Tuple[int, int]:
         if sparse_structure == SparseStructure.FINE:
             return weight_shape
 
         if sparse_structure == SparseStructure.BLOCK:
             r, c = sparse_factors
             return (weight_shape[0] // r, weight_shape[1] // c)
 
         if sparse_structure == SparseStructure.PER_DIM:
             score_shape = []
             for axis, (dim, factor) in enumerate(zip(weight_shape, sparse_factors)):
-                assert dim % factor == 0, f'{factor} is not a factor of axis {axis} with dim size {dim}.'
+                assert dim % factor == 0, f"{factor} is not a factor of axis {axis} with dim size {dim}."
                 score_shape.append(dim // factor)
             return tuple(score_shape)
 
-        raise RuntimeError('Unknown sparse structure.')
+        raise RuntimeError("Unknown sparse structure.")
 
     @staticmethod
-    def _get_sparse_factors(weight_shape: List[int],
-                            sparse_config: SparseConfig) -> Tuple[int, int]:
+    def _get_sparse_factors(weight_shape: List[int], sparse_config: SparseConfig) -> Tuple[int, int]:
         sparse_factors = sparse_config.sparse_factors
         if sparse_config.mode == SparseStructure.BLOCK:
             r, c = sparse_factors
-            assert weight_shape[0] % r == 0, f'r: {r} is not a factor of dim axis 0.'
-            assert weight_shape[1] % c == 0, f'c: {c} is not a factor of dim axis 1.'
+            assert weight_shape[0] % r == 0, f"r: {r} is not a factor of dim axis 0."
+            assert weight_shape[1] % c == 0, f"c: {c} is not a factor of dim axis 1."
 
         if sparse_config.mode == SparseStructure.PER_DIM:
             if sparse_config.sparse_axis < 0 or sparse_config.sparse_axis >= len(weight_shape):
-                raise ValueError('Invalid axis id {}, axes range is [0, {}]'.format(
-                    sparse_config.sparse_axis,
-                    len(weight_shape))
+                raise ValueError(
+                    "Invalid axis id {}, axes range is [0, {}]".format(sparse_config.sparse_axis, len(weight_shape))
                 )
             sparse_factors = deepcopy(weight_shape)
             sparse_factors[sparse_config.sparse_axis] = 1
             sparse_factors = tuple(sparse_factors)
         return sparse_factors
 
 
@@ -323,14 +331,14 @@
     """
 
     def __init__(self, module: nn.Module):
         # pylint: disable=protected-access
         self.hook = module._register_state_dict_hook(self.hook_fn)
 
     def hook_fn(self, module, state_dict: Dict, prefix: str, local_metadata):
-        state_dict[prefix + 'weight_ctx._binary_mask'] = module.weight_ctx.binary_mask
+        state_dict[prefix + "weight_ctx._binary_mask"] = module.weight_ctx.binary_mask
         if module.prune_bias:
-            state_dict[prefix + 'bias_ctx._binary_mask'] = module.bias_ctx.binary_mask
+            state_dict[prefix + "bias_ctx._binary_mask"] = module.bias_ctx.binary_mask
         return state_dict
 
     def close(self):
         self.hook.remove()
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/sparsity/movement/loss.py` & `nncf-2.5.0/nncf/experimental/torch/sparsity/movement/loss.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import List
 
 import torch
 
 from nncf.experimental.torch.sparsity.movement.layers import MovementSparsifier
 from nncf.torch.compression_method_api import PTCompressionLoss
 
@@ -26,23 +24,23 @@
     def __init__(self, operands: List[MovementSparsifier]):
         """
         Initializes the loss of movement sparsity in its algorithm controller.
 
         :param operands: List of movement sparsity operands for each layer to sparsify.
         """
         super().__init__()
-        assert len(operands) > 0, 'No sparse layers to calculate importance loss.'
+        assert len(operands) > 0, "No sparse layers to calculate importance loss."
         self.operands = operands
         self._disabled = False
 
     def disable(self):
         self._disabled = True
 
     def calculate(self) -> torch.Tensor:
-        loss = torch.tensor(0., device=self._get_device())
+        loss = torch.tensor(0.0, device=self._get_device())
         if not self._disabled:
             for n, operand in enumerate(self.operands):
                 loss = loss * (n / (n + 1)) + operand.loss() / (n + 1)  # avoid overflow
         return loss
 
     def _get_device(self) -> torch.device:
         return next(self.operands[0].parameters()).device
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/sparsity/movement/scheduler.py` & `nncf-2.5.0/nncf/experimental/torch/sparsity/movement/scheduler.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,21 +1,19 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from enum import IntEnum
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import math
+from enum import IntEnum
 from typing import Any, Dict, Optional
 
 import torch
 
 from nncf.common.logging import nncf_logger
 from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.schedulers import PolynomialDecaySchedule
@@ -24,94 +22,101 @@
 from nncf.config.schemata.experimental_schema import MOVEMENT_POWER
 
 
 class MovementSchedulerStage(IntEnum):
     """
     Describes the current stage of training with movement sparsity.
     """
+
     PRE_WARMUP = 0
     IN_WARMUP = 1
     POST_WARMUP = 2
 
 
 class MovementSchedulerParams:
     """
     Stores the params to initialize the scheduler of movement sparsity.
     """
 
-    def __init__(self,
-                 warmup_start_epoch: int,
-                 warmup_end_epoch: int,
-                 importance_regularization_factor: float,
-                 enable_structured_masking: bool = MOVEMENT_ENABLE_STRUCTURED_MASKING,
-                 init_importance_threshold: Optional[float] = None,
-                 final_importance_threshold: float = MOVEMENT_FINAL_IMPORTANCE_THRESHOLD,
-                 power: float = MOVEMENT_POWER,
-                 steps_per_epoch: Optional[int] = None,
-                 ):
+    def __init__(
+        self,
+        warmup_start_epoch: int,
+        warmup_end_epoch: int,
+        importance_regularization_factor: float,
+        enable_structured_masking: bool = MOVEMENT_ENABLE_STRUCTURED_MASKING,
+        init_importance_threshold: Optional[float] = None,
+        final_importance_threshold: float = MOVEMENT_FINAL_IMPORTANCE_THRESHOLD,
+        power: float = MOVEMENT_POWER,
+        steps_per_epoch: Optional[int] = None,
+    ):
         """
         Initializes and validates the params for scheduler.
 
         :param warmup_start_epoch: Index of the starting epoch (inclusive) for warmup stage.
         :param warmup_end_epoch: Index of the end epoch (exclusive) for warmup stage.
         :param importance_regularization_factor: The regularization factor on weight importance scores.
         :param enable_structured_masking: Whether to do structured mask resolution after warmup stage.
         :param init_importance_threshold: The initial value of importance threshold during warmup stage.
         :param final_importance_threshold: The final value of importance threshold during warmup stage.
         :param power: The power value of polynomial decay for threshold update during warmup stage.
         :param steps_per_epoch: Number of training steps in one epoch.
         """
 
         if steps_per_epoch is None and warmup_start_epoch < 1:
-            raise ValueError('`warmup_start_epoch` must be >= 1 to enable the auto calculation of '
-                             '`steps_per_epoch`. Please either change `warmup_start_epoch` to a larger '
-                             'number or specify `steps_per_epoch` in the config.')
+            raise ValueError(
+                "`warmup_start_epoch` must be >= 1 to enable the auto calculation of "
+                "`steps_per_epoch`. Please either change `warmup_start_epoch` to a larger "
+                "number or specify `steps_per_epoch` in the config."
+            )
 
         if warmup_start_epoch < 0 or warmup_end_epoch <= warmup_start_epoch:
-            raise ValueError('Movement sparsity requires 0 <= warmup_start_epoch < warmup_end_epoch.')
+            raise ValueError("Movement sparsity requires 0 <= warmup_start_epoch < warmup_end_epoch.")
 
         if importance_regularization_factor < 0:
-            raise ValueError('`importance_regularization_factor` should not be a negative number.')
+            raise ValueError("`importance_regularization_factor` should not be a negative number.")
 
-        if init_importance_threshold is not None and \
-                init_importance_threshold >= final_importance_threshold:
-            nncf_logger.warning('`init_importance_threshold` is equal to or greater than '
-                                '`final_importance_threshold`. Movement sparsity may not work as expected.')
+        if init_importance_threshold is not None and init_importance_threshold >= final_importance_threshold:
+            nncf_logger.warning(
+                "`init_importance_threshold` is equal to or greater than "
+                "`final_importance_threshold`. Movement sparsity may not work as expected."
+            )
 
         self.warmup_start_epoch = warmup_start_epoch
         self.warmup_end_epoch = warmup_end_epoch
         self.importance_regularization_factor = importance_regularization_factor
         self.enable_structured_masking = enable_structured_masking
         self.init_importance_threshold = init_importance_threshold
         self.final_importance_threshold = final_importance_threshold
         self.power = power
         self.steps_per_epoch = steps_per_epoch
 
     @classmethod
-    def from_dict(cls, params: Dict[str, Any]) -> 'MovementSchedulerParams':
+    def from_dict(cls, params: Dict[str, Any]) -> "MovementSchedulerParams":
         """
         Initialize `MovementSchedulerParams` object from the config in dict format.
 
         :param params: A dict that specifies the parameters of movement sparsity scheduler.
         :return: A `MovementSchedulerParams` object that stores the parameters from `params`.
         """
-        warmup_start_epoch: int = params.get('warmup_start_epoch', None)
-        warmup_end_epoch: int = params.get('warmup_end_epoch', None)
-        importance_regularization_factor: float = params.get('importance_regularization_factor', None)
-        enable_structured_masking: bool = params.get('enable_structured_masking',
-                                                     MOVEMENT_ENABLE_STRUCTURED_MASKING)
-        init_importance_threshold: Optional[float] = params.get('init_importance_threshold', None)
-        final_importance_threshold: float = params.get('final_importance_threshold',
-                                                       MOVEMENT_FINAL_IMPORTANCE_THRESHOLD)
-        power: float = params.get('power', MOVEMENT_POWER)
-        steps_per_epoch: Optional[int] = params.get('steps_per_epoch', None)
+        warmup_start_epoch: int = params.get("warmup_start_epoch", None)
+        warmup_end_epoch: int = params.get("warmup_end_epoch", None)
+        importance_regularization_factor: float = params.get("importance_regularization_factor", None)
+        enable_structured_masking: bool = params.get("enable_structured_masking", MOVEMENT_ENABLE_STRUCTURED_MASKING)
+        init_importance_threshold: Optional[float] = params.get("init_importance_threshold", None)
+        final_importance_threshold: float = params.get(
+            "final_importance_threshold", MOVEMENT_FINAL_IMPORTANCE_THRESHOLD
+        )
+        power: float = params.get("power", MOVEMENT_POWER)
+        steps_per_epoch: Optional[int] = params.get("steps_per_epoch", None)
 
         if None in [warmup_start_epoch, warmup_end_epoch, importance_regularization_factor]:
-            raise ValueError('`warmup_start_epoch`, `warmup_start_epoch` and `importance_regularization_factor` '
-                             'are required in config for Movement Sparsity.')
+            raise ValueError(
+                "`warmup_start_epoch`, `warmup_start_epoch` and `importance_regularization_factor` "
+                "are required in config for Movement Sparsity."
+            )
 
         return cls(
             warmup_start_epoch=warmup_start_epoch,
             warmup_end_epoch=warmup_end_epoch,
             importance_regularization_factor=importance_regularization_factor,
             enable_structured_masking=enable_structured_masking,
             init_importance_threshold=init_importance_threshold,
@@ -130,41 +135,43 @@
     factor per optimizer step. Parameter `steps_per_epoch` should be provided
     in config for the per step calculation. If not provided, the scheduler will
     use the first epoch to calculate `steps_per_epoch` parameter. In this case,
     parameter `warmup_start_epoch` must be equal to or larger than 1, and the
     scheduler will start calculation only after `steps_per_epoch` is calculated.
     """
 
-    def __init__(self, controller: 'MovementSparsityController', params: MovementSchedulerParams):
+    def __init__(self, controller: "MovementSparsityController", params: MovementSchedulerParams):
         """
         Initializes a movement sparsity scheduler with a polynomial decay schedule.
 
         :param controller: Movement sparsity algorithm controller.
         :param params: Parameters for the scheduler.
         """
         super().__init__()
         self._controller = controller
         self._params = params
         self._schedule = PolynomialDecaySchedule(
-            initial_value=0., target_value=1.,
+            initial_value=0.0,
+            target_value=1.0,
             target_epoch=(self._params.warmup_end_epoch - self._params.warmup_start_epoch),
             power=self._params.power,
-            concave=True
+            concave=True,
         )
         self._steps_per_epoch = self._params.steps_per_epoch
         self._init_importance_threshold = self._params.init_importance_threshold
         self._cached_importance_threshold_lambda = None
         self._is_controller_frozen = False
         self._steps_in_current_epoch = 0
         self._should_skip = False
 
     @property
     def current_stage(self) -> MovementSchedulerStage:
-        if self._steps_per_epoch is None or \
-                self.current_step < int(self._params.warmup_start_epoch * self._steps_per_epoch):
+        if self._steps_per_epoch is None or self.current_step < int(
+            self._params.warmup_start_epoch * self._steps_per_epoch
+        ):
             return MovementSchedulerStage.PRE_WARMUP
         if self.current_step < int(self._params.warmup_end_epoch * self._steps_per_epoch):
             return MovementSchedulerStage.IN_WARMUP
         return MovementSchedulerStage.POST_WARMUP
 
     @property
     def current_importance_regularization_factor(self) -> float:
@@ -173,17 +180,17 @@
         stays zero before warmup stage, and gradually increases during warmup, and stays at
         the fixed value after warmup.
 
         :return: The value of importance regularization factor at the current step.
         """
         current_stage = self.current_stage
         if current_stage == MovementSchedulerStage.PRE_WARMUP:
-            return 0.
+            return 0.0
         if current_stage == MovementSchedulerStage.IN_WARMUP:
-            return self._calc_current_scheduled_value(0., self._params.importance_regularization_factor)
+            return self._calc_current_scheduled_value(0.0, self._params.importance_regularization_factor)
         return self._params.importance_regularization_factor
 
     @property
     def current_importance_threshold(self) -> float:
         """
         Calculates the value of importance threshold per the current state. The threshold
         stays `-math.inf` before warmup stage, and gradually increases from `self._init_importance_threshold`
@@ -192,16 +199,17 @@
         :return: The value of importance threshold at the current step.
         """
         current_stage = self.current_stage
         if current_stage == MovementSchedulerStage.PRE_WARMUP:
             return -math.inf
         if current_stage == MovementSchedulerStage.IN_WARMUP:
             assert self._init_importance_threshold is not None
-            return self._calc_current_scheduled_value(self._init_importance_threshold,
-                                                      self._params.final_importance_threshold)
+            return self._calc_current_scheduled_value(
+                self._init_importance_threshold, self._params.final_importance_threshold
+            )
         return self._params.final_importance_threshold
 
     @property
     def enable_structured_masking(self) -> bool:
         return self._params.enable_structured_masking
 
     def epoch_step(self, next_epoch: Optional[int] = None) -> None:
@@ -214,43 +222,46 @@
         self._steps_in_current_epoch += 1
         if self._should_skip:
             return
         self._schedule_controller()
 
     def get_state(self) -> Dict[str, Any]:
         state = super().get_state()
-        state['_init_importance_threshold'] = self._init_importance_threshold
-        state['_steps_per_epoch'] = self._steps_per_epoch
+        state["_init_importance_threshold"] = self._init_importance_threshold
+        state["_steps_per_epoch"] = self._steps_per_epoch
         return state
 
     def load_state(self, state: Dict[str, Any]) -> None:
         super().load_state(state)
-        self._init_importance_threshold = state['_init_importance_threshold']
-        self._steps_per_epoch = state['_steps_per_epoch']
+        self._init_importance_threshold = state["_init_importance_threshold"]
+        self._steps_per_epoch = state["_steps_per_epoch"]
         if self._steps_per_epoch is None:  # It is the first epoch and `steps_per_epoch` not specified
             self._steps_in_current_epoch = self._current_step + 1
             self._should_skip = True
         else:
             self._steps_in_current_epoch = self._current_step % self._steps_per_epoch + 1
 
     def _schedule_controller(self):
         """
         Asks and updates the controller during training steps. It (1) updates the importance threshold
         and importance regularization factor in the operand at each step; (2) freezes the controller
         after warmup; (3) calculates the initial importance threshold if unspecified; (4) conducts
         structured masking if supported.
         """
-        if self._init_importance_threshold is None and \
-                self.current_stage == MovementSchedulerStage.IN_WARMUP:
+        if self._init_importance_threshold is None and self.current_stage == MovementSchedulerStage.IN_WARMUP:
             adaptive_init_threshold = self._calc_init_threshold_from_controller(target_sparsity=0.001)
-            nncf_logger.info('Movement sparsity automatically calculates `init_importance_threshold` as '
-                             f'{adaptive_init_threshold} so that warmup starts from ~0.1% linear layer sparsity.')
+            nncf_logger.info(
+                "Movement sparsity automatically calculates `init_importance_threshold` as "
+                f"{adaptive_init_threshold} so that warmup starts from ~0.1% linear layer sparsity."
+            )
             if adaptive_init_threshold >= self._params.final_importance_threshold:
-                nncf_logger.warning('The auto-calculated `init_importance_threshold` is equal to or greater than '
-                                    '`final_importance_threshold`. Movement sparsity may not work as expected.')
+                nncf_logger.warning(
+                    "The auto-calculated `init_importance_threshold` is equal to or greater than "
+                    "`final_importance_threshold`. Movement sparsity may not work as expected."
+                )
             self._init_importance_threshold = adaptive_init_threshold
         if self.current_stage == MovementSchedulerStage.POST_WARMUP and (not self._is_controller_frozen):
             if self._params.enable_structured_masking:
                 self._controller.reset_independent_structured_mask()
                 self._controller.resolve_structured_mask()
                 self._controller.populate_structured_mask()
             self._controller.freeze()
@@ -264,19 +275,19 @@
         schedule_epoch = schedule_current_step // self._steps_per_epoch
         schedule_step = schedule_current_step % self._steps_per_epoch
         scale = self._schedule(schedule_epoch, schedule_step, self._steps_per_epoch)
         return start_value + scale * (end_value - start_value)
 
     @torch.no_grad()
     def _calc_init_threshold_from_controller(self, target_sparsity: float = 0.001) -> float:
-        # Calculate the k-th smallest value over all importance scores as the initial importance threhsold
+        # Calculate the k-th smallest value over all importance scores as the initial importance threshold
         # so that roughly k weight elements are masked and thus target sparsity is achieved. We conduct this on
         # CPU to (1) limit GPU memory usage; (2) avoid the non-deterministic behavior of `torch.Tensor.kthvalue`
         # on CUDA.
-        assert 0. <= target_sparsity < 1.
+        assert 0.0 <= target_sparsity < 1.0
         importance_tensors = []
         for minfo in self._controller.sparsified_module_info:
             operand = minfo.operand
             weight = operand.get_importance(is_bias=False, expanded=True)
             importance_tensors.append(weight.detach().cpu().view(-1))
             if operand.prune_bias:
                 bias = operand.get_importance(is_bias=True, expanded=True)
@@ -302,15 +313,19 @@
         self._should_skip = False
 
         if self._steps_per_epoch is None and self._steps_in_current_epoch > 0:
             self._steps_per_epoch = self._steps_in_current_epoch
 
         if self._steps_per_epoch is not None and self._steps_in_current_epoch > 0:
             if self._steps_per_epoch != self._steps_in_current_epoch:
-                raise Exception('Actual steps per epoch and steps per epoch from the scheduler '
-                                'parameters are different. Scheduling may be incorrect.')
+                raise Exception(
+                    "Actual steps per epoch and steps per epoch from the scheduler "
+                    "parameters are different. Scheduling may be incorrect."
+                )
 
         if self._steps_per_epoch is None:
             self._should_skip = True
-            nncf_logger.info('Movement sparsity scheduler updates importance threshold and regularization'
-                             'factor per optimizer step, but steps_per_epoch was not set in config. Will '
-                             'measure the actual steps per epoch as signaled by a .epoch_step() call.')
+            nncf_logger.info(
+                "Movement sparsity scheduler updates importance threshold and regularization"
+                "factor per optimizer step, but steps_per_epoch was not set in config. Will "
+                "measure the actual steps per epoch as signaled by a .epoch_step() call."
+            )
```

### Comparing `nncf-2.4.0/nncf/experimental/torch/sparsity/movement/structured_mask_handler.py` & `nncf-2.5.0/nncf/experimental/torch/sparsity/movement/structured_mask_handler.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,58 +1,56 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from functools import reduce
 from pathlib import Path
-from typing import Dict, List, Optional, Tuple
+from typing import List, Optional, Tuple
 
 import pandas as pd
 import torch
 import torch.nn.functional as F
 
 from nncf.common.graph.graph import NNCFNodeName
 from nncf.common.graph.layer_attributes import LinearLayerAttributes
 from nncf.common.logging import nncf_logger
-from nncf.common.scopes import matches_any
-from nncf.experimental.torch.search_building_blocks.search_blocks import BlockFilteringStrategy
-from nncf.experimental.torch.search_building_blocks.search_blocks import BuildingBlockType
-from nncf.experimental.torch.search_building_blocks.search_blocks import get_building_blocks
+from nncf.experimental.common.pruning.nodes_grouping import get_pruning_groups
+from nncf.experimental.common.pruning.nodes_grouping import select_largest_groups
+from nncf.experimental.common.pruning.propagation_data import ProducerInfo
+from nncf.experimental.torch.pruning.operations import PT_EXPERIMENTAL_PRUNING_OPERATOR_METATYPES
 from nncf.experimental.torch.sparsity.movement.layers import MovementSparsifier
-from nncf.experimental.torch.sparsity.movement.structured_mask_strategy import BaseStructuredMaskStrategy
-from nncf.experimental.torch.sparsity.movement.structured_mask_strategy import StructuredMaskRule
+from nncf.experimental.torch.sparsity.movement.layers import SparseStructure
 from nncf.torch.layers import NNCFLinear
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.sparsity.base_algo import SparseModuleInfo
 
 SUPPORTED_NNCF_MODULES = [NNCFLinear]
 EXPECTED_NODE_LAYER_ATTRS = [LinearLayerAttributes]
 
 
 class StructuredMaskContextStatistics:
     """
     Describes details of the resolved structured mask in a supported layer.
     """
 
-    def __init__(self,
-                 weight_shape: Tuple[int, int],
-                 pruned_weight_shape: Tuple[int, int],
-                 bias_shape: Tuple[int],
-                 pruned_bias_shape: Tuple[int],
-                 head_or_channel_id_to_keep: List[int],
-                 module_node_name: NNCFNodeName,
-                 ):
+    def __init__(
+        self,
+        weight_shape: Tuple[int, int],
+        pruned_weight_shape: Tuple[int, int],
+        bias_shape: Tuple[int],
+        pruned_bias_shape: Tuple[int],
+        head_or_channel_id_to_keep: List[int],
+        module_node_name: NNCFNodeName,
+    ):
         """
         Initializes the statistics for the target linear module of a structured mask context.
 
         :param weight_shape: Shape of the original weight in a linear layer.
         :param pruned_weight_shape: Shape of the weight after structured mask resolution,
             discarding the pruned regions.
         :param bias_shape: Shape of the original bias in a linear layer.
@@ -76,227 +74,243 @@
     Context to interact with the operand of a module in movement sparsity.
 
     This context can resolve the independent structured mask from operand, and can refresh the binary
     mask back to operand with dependent structured mask. Serves as an agent for `StructuredMaskHandler`
     to conduct structured mask resolution.
     """
 
-    def __init__(self,
-                 sparsifier_operand: MovementSparsifier,
-                 module_node_name: NNCFNodeName,
-                 grid_size: Tuple[int, int],
-                 prune_by_row: bool,
-                 ):
+    def __init__(
+        self,
+        sparsifier_operand: MovementSparsifier,
+        module_node_name: NNCFNodeName,
+        grid_size: Tuple[int, int],
+        prune_by_row: bool,
+    ):
         """
         Initializes the context of the target module for structured masking.
 
         :param sparsifier_operand: Operand for the target module.
         :param module_node_name: Node name of the target module.
         :param grid_size: The grid shape for resolving the independent structured mask.
         :param prune_by_row: Determines whether to resolve the independent structured mask by row or column.
         """
         self.sparsifier_operand = sparsifier_operand
         self.module_node_name = module_node_name
-        operand_mask: torch.Tensor = sparsifier_operand.weight_ctx.binary_mask   # type: ignore
+        operand_mask: torch.Tensor = sparsifier_operand.weight_ctx.binary_mask  # type: ignore
         self.operand_mask_shape = operand_mask.shape
         self.grid_size = self._resolve_grid_size(grid_size)
-        self.structured_mask_shape = torch.Size(dim // grid for dim, grid in
-                                                zip(self.operand_mask_shape, self.grid_size))
+        self.structured_mask_shape = torch.Size(
+            dim // grid for dim, grid in zip(self.operand_mask_shape, self.grid_size)
+        )
         self.prune_by_row = prune_by_row
         self._independent_structured_mask = None
         self._dependent_structured_mask = None
 
     def __str__(self) -> str:
-        prune_info = 'row prune' if self.prune_by_row else 'column prune'
+        prune_info = "row prune" if self.prune_by_row else "column prune"
         return f'{self.__class__.__name__}({prune_info} by {self.grid_size}, "{self.module_node_name}")'
 
     @property
     def independent_structured_mask(self) -> Optional[torch.Tensor]:
         if self._independent_structured_mask is None:
-            nncf_logger.debug('Independent structured mask has not been calculated. Return None.')
+            nncf_logger.debug("Independent structured mask has not been calculated. Return None.")
         return self._independent_structured_mask
 
     @independent_structured_mask.setter
     @torch.no_grad()
     def independent_structured_mask(self, tensor: torch.Tensor):
         if self.structured_mask_shape != tensor.shape:
-            raise ValueError('Wrong shape about independent structured mask.')
+            raise ValueError("Wrong shape about independent structured mask.")
         if self._independent_structured_mask is None:
             self._independent_structured_mask = tensor.clone()
         else:
             if self._independent_structured_mask.device != tensor.device:
-                nncf_logger.debug(f'Changing independent_structured_mask device to {tensor.device}')
+                nncf_logger.debug(f"Changing independent_structured_mask device to {tensor.device}")
                 self._independent_structured_mask = self._independent_structured_mask.to(tensor.device)
             self._independent_structured_mask.copy_(tensor)
 
     @property
     def dependent_structured_mask(self) -> Optional[torch.Tensor]:
         if self._dependent_structured_mask is None:
-            nncf_logger.debug('Dependent structured mask has not been calculated. Return None.')
+            nncf_logger.debug("Dependent structured mask has not been calculated. Return None.")
         return self._dependent_structured_mask
 
     @dependent_structured_mask.setter
     @torch.no_grad()
     def dependent_structured_mask(self, tensor: torch.Tensor):
         if self.structured_mask_shape != tensor.shape:
-            raise ValueError('Wrong shape about dependent structured mask.')
+            raise ValueError("Wrong shape about dependent structured mask.")
         if self._dependent_structured_mask is None:
             self._dependent_structured_mask = tensor.clone()
         else:
             if self._dependent_structured_mask.device != tensor.device:
-                nncf_logger.debug(f'Changing dependent_structured_mask device to {tensor.device}', )
+                nncf_logger.debug(
+                    f"Changing dependent_structured_mask device to {tensor.device}",
+                )
                 self._dependent_structured_mask = self._dependent_structured_mask.to(tensor.device)
             self._dependent_structured_mask.copy_(tensor)
 
     @torch.no_grad()
     def update_independent_structured_mask_from_operand(self):
         """
         Gets the current unstructured binary mask from operand, resolves it to the independent structured one, and
         stores in `self.independent_structured_mask` for later use in `StructuredMaskHandler`.
         """
         weight_binary_mask = self.sparsifier_operand.weight_ctx.binary_mask.detach().clone()
         mask_by_grid = F.max_pool2d(
-            weight_binary_mask.unsqueeze(0), kernel_size=self.grid_size, stride=self.grid_size).squeeze(0)
+            weight_binary_mask.unsqueeze(0), kernel_size=self.grid_size, stride=self.grid_size
+        ).squeeze(0)
         preserved_cols = mask_by_grid.amax(dim=0)
         preserved_rows = mask_by_grid.amax(dim=1)
 
         if self.sparsifier_operand.prune_bias:
             bias_binary_mask = self.sparsifier_operand.bias_ctx.binary_mask.detach().clone()
             bias_preserved_rows = F.max_pool1d(
-                bias_binary_mask.view(1, -1), kernel_size=self.grid_size[0], stride=self.grid_size[0]).squeeze(0)
+                bias_binary_mask.view(1, -1), kernel_size=self.grid_size[0], stride=self.grid_size[0]
+            ).squeeze(0)
             preserved_rows = bias_preserved_rows.logical_or(preserved_rows)
 
         structured_mask = preserved_rows.unsqueeze(1) * preserved_cols.unsqueeze(0)
         self.independent_structured_mask = structured_mask
         return structured_mask
 
+    def initialize_binary_mask(self):
+        """
+        Initialize binary mask by all ones. The inflated dependent mask will be applied via logical "and"
+        operation to it. It's needed for the case when 1 binary mask shared for 2 groups: in one group operator
+        can be pruned by input channels, i.e. be a consumer of pruning masks, and for another - can be pruned by
+        output channels, i.e. be a producer of pruning masks.
+        Initial  |  Mask 2 last input channels   |  Mask middle output channel    |     Result
+        --------------------------------------------------------------------------------------
+         1111                 1100                             1111                     1100
+         1111    &            1100               &             0000                =    0000
+         1111                 1100                             1111                     1100
+        """
+
+        self.sparsifier_operand.weight_ctx.binary_mask.fill_(1)
+        if self.sparsifier_operand.prune_bias:
+            self.sparsifier_operand.bias_ctx.binary_mask.fill_(1)
+
     def populate_dependent_structured_mask_to_operand(self):
         """
         Updates the actual binary masks in operand with `self.dependent_structured_mask`.
         """
         structured_mask_inflated = self._inflate_structured_mask(self.dependent_structured_mask, self.grid_size)
-        self.sparsifier_operand.weight_ctx.binary_mask = structured_mask_inflated
+        self.sparsifier_operand.weight_ctx.binary_mask *= structured_mask_inflated
         if self.sparsifier_operand.prune_bias:
-            self.sparsifier_operand.bias_ctx.binary_mask = structured_mask_inflated.amax(dim=1)
+            self.sparsifier_operand.bias_ctx.binary_mask *= structured_mask_inflated.amax(dim=1)
 
     def gather_statistics_from_operand(self) -> StructuredMaskContextStatistics:
         """
         Collects the structured mask statistics from the binary masks in operand.
 
         :return: The statistics of the structured mask context.
         """
         node = self.sparsifier_operand.target_module_node
         assert isinstance(node.layer_attributes, tuple(EXPECTED_NODE_LAYER_ATTRS))
         weight_shape: Tuple[int, int] = tuple(node.layer_attributes.get_weight_shape())
-        bias_shape: Tuple[int] = (node.layer_attributes.get_bias_shape(),
-                                  ) if self.sparsifier_operand.prune_bias else (0,)
+        bias_shape: Tuple[int] = (
+            (node.layer_attributes.get_bias_shape(),) if self.sparsifier_operand.prune_bias else (0,)
+        )
 
         pruned_weight_shape = list(weight_shape)
         head_id_to_keep = []
         if self.prune_by_row:
-            pruneable_rows = self.sparsifier_operand.weight_ctx.binary_mask.amax(dim=1)
-            pruned_weight_shape[0] = int(pruneable_rows.count_nonzero().item())
-            kept_row_blocks = F.max_pool1d(pruneable_rows.unsqueeze(0), kernel_size=self.grid_size[0]).squeeze(0)
+            prunable_rows = self.sparsifier_operand.weight_ctx.binary_mask.amax(dim=1)
+            pruned_weight_shape[0] = int(prunable_rows.count_nonzero().item())
+            kept_row_blocks = F.max_pool1d(prunable_rows.unsqueeze(0), kernel_size=self.grid_size[0]).squeeze(0)
             head_id_to_keep = kept_row_blocks.nonzero().view(-1).cpu().numpy().tolist()
         else:
-            pruneable_cols = self.sparsifier_operand.weight_ctx.binary_mask.amax(dim=0)
-            pruned_weight_shape[1] = int(pruneable_cols.count_nonzero().item())
-            kept_col_blocks = F.max_pool1d(pruneable_cols.unsqueeze(0), kernel_size=self.grid_size[1]).squeeze(0)
+            prunable_cols = self.sparsifier_operand.weight_ctx.binary_mask.amax(dim=0)
+            pruned_weight_shape[1] = int(prunable_cols.count_nonzero().item())
+            kept_col_blocks = F.max_pool1d(prunable_cols.unsqueeze(0), kernel_size=self.grid_size[1]).squeeze(0)
             head_id_to_keep = kept_col_blocks.nonzero().view(-1).cpu().numpy().tolist()
 
         pruned_bias_shape = bias_shape
         if self.sparsifier_operand.prune_bias and self.prune_by_row:
             pruned_bias_shape = (int(self.sparsifier_operand.bias_ctx.binary_mask.count_nonzero().item()),)
 
         return StructuredMaskContextStatistics(
             weight_shape=weight_shape,
             pruned_weight_shape=tuple(pruned_weight_shape),
             bias_shape=bias_shape,
             pruned_bias_shape=pruned_bias_shape,
             head_or_channel_id_to_keep=head_id_to_keep,
-            module_node_name=self.module_node_name
+            module_node_name=self.module_node_name,
         )
 
     def _resolve_grid_size(self, grid_size) -> Tuple[int, int]:
         a, b = grid_size
-        return (a if a > 0 else self.operand_mask_shape[0],
-                b if b > 0 else self.operand_mask_shape[1])
+        return (a if a > 0 else self.operand_mask_shape[0], b if b > 0 else self.operand_mask_shape[1])
 
     @staticmethod
     def _inflate_structured_mask(structured_mask: torch.Tensor, grid_size: Tuple[int, int]) -> torch.Tensor:
-        assert len(structured_mask.shape) == len(grid_size), \
-            f'Unmatching dimension with structured_mask in shape {structured_mask.shape} and grid_size in 2D.'
+        assert len(structured_mask.shape) == len(
+            grid_size
+        ), f"Unmatched dimension with structured_mask in shape {structured_mask.shape} and grid_size in 2D."
         inflated_mask = structured_mask.clone()
         for axis, repeat_times in enumerate(grid_size):
             inflated_mask = inflated_mask.repeat_interleave(repeat_times, dim=axis)
         return inflated_mask
 
 
 class StructuredMaskContextGroup:
     """
     Stores together the structured mask contexts that are related to the same building block.
     """
 
-    def __init__(self, group_id: int,
-                 group_type: BuildingBlockType,
-                 structured_mask_contexts: List[StructuredMaskContext]):
+    def __init__(self, group_id: int, structured_mask_contexts: List[StructuredMaskContext]):
         """
         Initializes a group of related structured mask contexts.
 
         :param group_id: The index of the building block.
-        :param group_type: The type of building block that this group belongs to.
         :param structured_mask_contexts: A list of structured mask contexts corresponding
             to the building block.
         """
         self.group_id = group_id
-        self.group_type = group_type
         self.structured_mask_contexts = structured_mask_contexts
 
     def __str__(self) -> str:
         if not self.structured_mask_contexts:
-            ctxes_str = '[]'
+            ctxes_str = "[]"
         else:
-            ctxes = (f'\n\t{ctx}' for ctx in self.structured_mask_contexts)
-            ctxes_str = '[{}\n]'.format(''.join(ctxes))
-        return f'{self.__class__.__name__}[{self.group_id}]({self.group_type}): {ctxes_str}'
+            ctxes = (f"\n\t{ctx}" for ctx in self.structured_mask_contexts)
+            ctxes_str = "[{}\n]".format("".join(ctxes))
+        return f"{self.__class__.__name__}[{self.group_id}]: {ctxes_str}"
 
 
 class StructuredMaskHandler:
     """
     Handler to conduct structured masking on supported models.
 
     This handler gathers sparsifiable layers together as groups according to the building block
     they belong to, e.g., multi-head self-attention or feed-forward network in Transformers.
     Within each group, it refreshes the binary masks from unstructured to structured ones,
     while considering the pruning dependencies across layers. All these operations are conducted
     via the `StructuredMaskContext` of each module that supports structured masking.
     """
 
-    def __init__(self,
-                 compressed_model: NNCFNetwork,
-                 sparsified_module_info_list: List[SparseModuleInfo],
-                 strategy: BaseStructuredMaskStrategy):
+    def __init__(self, compressed_model: NNCFNetwork, sparsified_module_info_list: List[SparseModuleInfo]):
         """
         Initializes the handler for structured masking in movement sparsity.
 
         :param compressed_model: The wrapped compressed model.
         :param sparsified_module_info_list: List of `SparsifiedModuleInfo` in the
             controller of `compressed_model`.
         :param strategy: Strategy of resolving structured masks for `compressed_model`.
         """
-        self.strategy = strategy
-        self.rules_by_group_type = strategy.rules_by_group_type
         self.compressed_model = compressed_model
         self.sparsified_module_info_list = sparsified_module_info_list
         self._structured_mask_ctx_groups = self._create_structured_mask_context_groups(
-            compressed_model, sparsified_module_info_list, self.rules_by_group_type)
+            compressed_model, sparsified_module_info_list
+        )
 
-        nncf_logger.debug('Totally %d structured mask context groups.', len(self._structured_mask_ctx_groups))
+        nncf_logger.debug("Totally %d structured mask context groups.", len(self._structured_mask_ctx_groups))
         for structured_mask_ctx_group in self._structured_mask_ctx_groups:
-            nncf_logger.debug(f'{structured_mask_ctx_group}')
+            nncf_logger.debug(f"{structured_mask_ctx_group}")
 
     def update_independent_structured_mask(self):
         """
         Asks all contexts in `self._structured_mask_ctx_groups` to calculate the independent structured mask.
         """
         for group in self._structured_mask_ctx_groups:
             for ctx in group.structured_mask_contexts:
@@ -304,100 +318,125 @@
 
     def resolve_dependent_structured_mask(self):
         """
         Within each context group, it reads the independent structured masks of related layers and
         resolves the structured masks based on dependency rules defined in `self.rules_by_group_type`.
         """
         for group in self._structured_mask_ctx_groups:
-            group_type = group.group_type
-            if group_type not in self.rules_by_group_type:
-                raise ValueError(f'No structured mask strategy for group_type="{group_type}"')
             ctxes = group.structured_mask_contexts
             row_prune_ctxes = list(filter(lambda ctx: ctx.prune_by_row, ctxes))
             col_prune_ctxes = list(filter(lambda ctx: not ctx.prune_by_row, ctxes))
-            independent_masks = [ctx.independent_structured_mask for ctx in row_prune_ctxes] + \
-                [ctx.independent_structured_mask.t() for ctx in col_prune_ctxes]
+            independent_masks = [ctx.independent_structured_mask for ctx in row_prune_ctxes] + [
+                ctx.independent_structured_mask.t() for ctx in col_prune_ctxes
+            ]
             coarse_mask = reduce(torch.logical_or, independent_masks).float()
             with torch.no_grad():
                 for ctx in row_prune_ctxes:
                     ctx.dependent_structured_mask = coarse_mask
                 for ctx in col_prune_ctxes:
                     ctx.dependent_structured_mask = coarse_mask.t()
 
     def populate_dependent_structured_mask_to_operand(self):
         """
         Asks all contexts in `self._structured_mask_ctx_groups` to update the actual binary masks in operand.
         """
         for group in self._structured_mask_ctx_groups:
             for ctx in group.structured_mask_contexts:
+                ctx.initialize_binary_mask()
+        for group in self._structured_mask_ctx_groups:
+            for ctx in group.structured_mask_contexts:
                 ctx.populate_dependent_structured_mask_to_operand()
 
-    def report_structured_sparsity(self,
-                                   save_dir: str,
-                                   file_name: str = 'structured_sparsity',
-                                   to_csv: bool = True,
-                                   max_num_of_kept_heads_to_report: int = 20) -> pd.DataFrame:
+    def report_structured_sparsity(
+        self,
+        save_dir: str,
+        file_name: str = "structured_sparsity",
+        to_csv: bool = True,
+        max_num_of_kept_heads_to_report: int = 20,
+    ) -> pd.DataFrame:
         """
         Generates a report file that describes the structured mask statistics for each context group.
 
         :param save_dir: The folder to save the report file.
         :param file_name: File name of the report.
         :param to_csv: Whether to dump the report file in csv format.
         :param max_num_of_kept_heads_to_report: The max number of heads or channels to display that are
             preserved after structured masking. Used to avoid showing too many elements in the list.
         :return: The structured mask statistics in `pandas.DataFrame` format.
         """
         df = self._gather_statistics_dataframe(max_num_of_kept_heads_to_report)
         if to_csv:
-            df.to_csv(Path(save_dir, f'{file_name}.csv'))
+            df.to_csv(Path(save_dir, f"{file_name}.csv"))
         return df
 
     def _gather_statistics_dataframe(self, max_num_of_kept_heads_to_report: int = 20) -> pd.DataFrame:
         module_vs_name_map = {module: name for name, module in self.compressed_model.named_modules()}
         entry_list = []
         for group in self._structured_mask_ctx_groups:
-            ctxes = sorted(group.structured_mask_contexts,
-                           key=lambda ctx: ctx.sparsifier_operand.target_module_node.node_id)
+            ctxes = sorted(
+                group.structured_mask_contexts, key=lambda ctx: ctx.sparsifier_operand.target_module_node.node_id
+            )
             for ctx in ctxes:
                 stats = ctx.gather_statistics_from_operand()
-                module = self.compressed_model.get_containing_module(stats.module_node_name)
-                entry = dict(group_id=group.group_id,
-                             type=group.group_type.value,
-                             torch_module=module_vs_name_map[module],
-                             **stats.__dict__)
+                module = self.compressed_model.nncf.get_containing_module(stats.module_node_name)
+                entry = dict(group_id=group.group_id, torch_module=module_vs_name_map[module], **stats.__dict__)
                 if len(stats.head_or_channel_id_to_keep) > max_num_of_kept_heads_to_report:  # avoid long display
-                    entry['head_or_channel_id_to_keep'] = f'[{len(stats.head_or_channel_id_to_keep)} items]'
+                    entry["head_or_channel_id_to_keep"] = f"[{len(stats.head_or_channel_id_to_keep)} items]"
                 entry_list.append(entry)
         return pd.DataFrame(entry_list)
 
     @staticmethod
     def _create_structured_mask_context_groups(
-            compressed_model: NNCFNetwork,
-            sparsified_module_info_list: List[SparseModuleInfo],
-            rules_by_group_type: Dict[BuildingBlockType, List[StructuredMaskRule]],
+        nncf_network: NNCFNetwork, sparsified_module_info_list: List[SparseModuleInfo]
     ) -> List[StructuredMaskContextGroup]:
         module_vs_sparse_module_info_map = {minfo.module: minfo for minfo in sparsified_module_info_list}
-        building_blocks, _ = get_building_blocks(compressed_model,
-                                                 target_block_types=[BuildingBlockType.MHSA, BuildingBlockType.FF],
-                                                 block_filter_strategy=BlockFilteringStrategy.KEEP_SMALL,
-                                                 hw_fused_ops=True)
-        groups = []
-        for group_id, building_block in enumerate(building_blocks):
-            group_type = building_block.block_type
+
+        pruning_producing_types = ["linear"]
+        nncf_graph = nncf_network.get_original_graph()
+        pruning_groups = get_pruning_groups(
+            nncf_graph, PT_EXPERIMENTAL_PRUNING_OPERATOR_METATYPES, pruning_producing_types
+        )
+        pruning_groups = select_largest_groups(pruning_groups)
+        result = []
+        for group_id, group in enumerate(pruning_groups):
             ctxes = []
-            for op_addr in building_block.op_addresses:
-                if op_addr.operator_name in [m.op_func_name for m in SUPPORTED_NNCF_MODULES]:
-                    module = compressed_model.get_module_by_scope(op_addr.scope_in_model)
+            block = group.block
+            extended_producers = group.producers
+            for consumer in group.consumers:
+                if consumer.pruning_dimension is not None:
+                    extended_producers.add(ProducerInfo(consumer.node_id, consumer.pruning_dimension))
+            is_group_matched = True
+            prefix_warning = (
+                "Automatically found structured pruning group does not match the given unstructured "
+                f"pruning structures: \n{group}\n."
+            )
+            for producer_info in extended_producers:
+                nncf_node = nncf_graph.get_node_by_id(producer_info.node_id)
+                module = nncf_network.nncf.get_containing_module(nncf_node.node_name)
+                if module in module_vs_sparse_module_info_map:
+                    # 0 dimension corresponds to row (output channels), 1st dimension - to column (input channels)
+                    prune_by_row = not bool(producer_info.pruning_dimension)
                     minfo = module_vs_sparse_module_info_map[module]
-                    for rule in rules_by_group_type[group_type]:
-                        if matches_any(minfo.module_node_name, rule.keywords):
-                            ctx = StructuredMaskContext(minfo.operand,
-                                                        minfo.module_node_name,
-                                                        rule.prune_grid,
-                                                        rule.prune_by_row)
-                            ctxes.append(ctx)
+                    sparsifier: MovementSparsifier = minfo.operand
+                    if sparsifier.sparse_structure == SparseStructure.PER_DIM:
+                        sparse_axis = sparsifier.sparse_cfg.sparse_axis
+                        if sparse_axis != producer_info.pruning_dimension:
+                            nncf_logger.warning(
+                                f"{prefix_warning}. Unstructured pruning is defined for "
+                                f"{sparse_axis} axis, but structured one - for "
+                                f"{producer_info.pruning_dimension} axis."
+                            )
+                            is_group_matched = False
                             break
-                    else:
-                        raise ValueError('No structured mask rule found for '
-                                         f'[{group_type}]{minfo.module_node_name}.')
-            groups.append(StructuredMaskContextGroup(group_id, group_type, ctxes))
-        return groups
+                    prune_grid = (block.size, -1) if prune_by_row else (-1, block.size)
+                    ctx = StructuredMaskContext(minfo.operand, minfo.module_node_name, prune_grid, prune_by_row)
+                    ctxes.append(ctx)
+                else:
+                    nncf_logger.warning(
+                        f"Automatically found structured pruning group does not match the given "
+                        f"unstructured sparse structures:\n {group}"
+                    )
+                    is_group_matched = False
+                    break
+            if ctxes and is_group_matched:
+                result.append(StructuredMaskContextGroup(group_id, ctxes))
+        return result
```

### Comparing `nncf-2.4.0/nncf/onnx/engine.py` & `nncf-2.5.0/nncf/onnx/engine.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Dict
 
 import numpy as np
 import onnxruntime as rt
 
 from nncf.common.engine import Engine
@@ -22,15 +20,15 @@
 class ONNXEngine(Engine):
     """
     Engine for ONNX backend using ONNXRuntime to infer the model.
     """
 
     def __init__(self, model, **rt_session_options):
         self.input_names = set()
-        rt_session_options['providers'] = ['CPUExecutionProvider']
+        rt_session_options["providers"] = ["CPUExecutionProvider"]
         serialized_model = model.SerializeToString()
         self.sess = rt.InferenceSession(serialized_model, **rt_session_options)
 
         for inp in self.sess.get_inputs():
             self.input_names.add(inp.name)
 
     def infer(self, input_data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
```

### Comparing `nncf-2.4.0/nncf/onnx/graph/metatypes/onnx_metatypes.py` & `nncf-2.5.0/nncf/onnx/graph/metatypes/onnx_metatypes.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,29 +1,28 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List, Type, Optional
 from dataclasses import dataclass
+from typing import List, Optional, Type
 
 import onnx
+
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.operator_metatypes import OperatorMetatypeRegistry
 from nncf.common.hardware.opset import HWConfigOpName
 
-ONNX_OPERATION_METATYPES = OperatorMetatypeRegistry('onnx_operator_metatypes')
+ONNX_OPERATION_METATYPES = OperatorMetatypeRegistry("onnx_operator_metatypes")
 
 
 class ONNXOpMetatype(OperatorMetatype):
     op_names = []  # type: List[str]
     subtypes = []  # type: List[Type[OperatorMetatype]]
 
     @classmethod
@@ -41,16 +40,15 @@
     @classmethod
     def determine_subtype(cls, model: onnx.ModelProto, node: onnx.NodeProto) -> Optional[Type[OperatorMetatype]]:
         matches = []
         for subtype in cls.get_subtypes():
             if subtype.matches(model, node):
                 matches.append(subtype)
         if len(matches) > 1:
-            raise RuntimeError('Multiple subtypes match operator call - '
-                               'cannot determine single subtype.')
+            raise RuntimeError("Multiple subtypes match operator call - cannot determine single subtype.")
         if not matches:
             return None
         return matches[0]
 
 
 @dataclass
 class OpWeightDef:
@@ -59,425 +57,460 @@
 
     :param weight_channel_axis: Axis for weight per-channel quantization, meaning the number of output filters.
     :param weight_port_id: Input port of the node's weight.
     If the value is None the weight_port_id should be determined dynamically.
     :param bias_port_id: Input port of the node's bias.
     If the value is None it means that the Metatype does not have bias.
     """
+
     weight_channel_axis: int
     weight_port_id: Optional[int] = None
     bias_port_id: Optional[int] = None
 
 
 class ONNXOpWithWeightsMetatype(ONNXOpMetatype):
     weight_definitions = None  # type: OpWeightDef
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXDepthwiseConvolutionMetatype(ONNXOpWithWeightsMetatype):
-    name = 'DepthwiseConvOp'
-    op_names = ['Conv']
+    name = "DepthwiseConvOp"
+    op_names = ["Conv"]
     hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]
     weight_definitions = OpWeightDef(weight_channel_axis=0, weight_port_id=1, bias_port_id=2)
+    output_channel_axis = 1
 
     @classmethod
     def matches(cls, model: onnx.ModelProto, node: onnx.NodeProto) -> bool:
         return _is_depthwise_conv(model, node)
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXConvolutionMetatype(ONNXOpWithWeightsMetatype):
-    name = 'ConvOp'
-    op_names = ['Conv']
+    name = "ConvOp"
+    op_names = ["Conv"]
     hw_config_names = [HWConfigOpName.CONVOLUTION]
     weight_definitions = OpWeightDef(weight_channel_axis=0, weight_port_id=1, bias_port_id=2)
+    output_channel_axis = 1
     subtypes = [ONNXDepthwiseConvolutionMetatype]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXConvolutionTransposeMetatype(ONNXOpWithWeightsMetatype):
-    name = 'ConvTransposeOp'
-    op_names = ['ConvTranspose']
+    name = "ConvTransposeOp"
+    op_names = ["ConvTranspose"]
     hw_config_names = [HWConfigOpName.CONVOLUTION]
     weight_definitions = OpWeightDef(weight_channel_axis=1, weight_port_id=1, bias_port_id=2)
+    output_channel_axis = 1
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXLinearMetatype(ONNXOpWithWeightsMetatype):
-    name = 'LinearOp'
-    op_names = ['Gemm']
+    name = "LinearOp"
+    op_names = ["Gemm"]
     hw_config_names = [HWConfigOpName.MATMUL]
     # TODO(kshpv): ticket:95156
     weight_definitions = OpWeightDef(weight_channel_axis=0, weight_port_id=1, bias_port_id=2)
+    output_channel_axis = -1
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXReluMetatype(ONNXOpMetatype):
-    name = 'ReluOp'
-    op_names = ['Relu', 'Clip']
+    name = "ReluOp"
+    op_names = ["Relu", "Clip"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXLeakyReluMetatype(ONNXOpMetatype):
-    name = 'LeakyReluOp'
-    op_names = ['LeakyRelu']
+    name = "LeakyReluOp"
+    op_names = ["LeakyRelu"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXThresholdedReluMetatype(ONNXOpMetatype):
-    name = 'ThresholdedReluOp'
-    op_names = ['ThresholdedRelu']
+    name = "ThresholdedReluOp"
+    op_names = ["ThresholdedRelu"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXEluMetatype(ONNXOpMetatype):
-    name = 'EluOp'
-    op_names = ['Elu']
+    name = "EluOp"
+    op_names = ["Elu"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXPReluMetatype(ONNXOpMetatype):
-    name = 'PReluOp'
-    op_names = ['PRelu']
+    name = "PReluOp"
+    op_names = ["PRelu"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXSigmoidMetatype(ONNXOpMetatype):
-    name = 'SigmoidOp'
-    op_names = ['Sigmoid']
+    name = "SigmoidOp"
+    op_names = ["Sigmoid"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXHardSigmoidMetatype(ONNXOpMetatype):
-    name = 'HardSigmoidOp'
-    op_names = ['HardSigmoid']
+    name = "HardSigmoidOp"
+    op_names = ["HardSigmoid"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXHardSwishMetatype(ONNXOpMetatype):
-    name = 'HardSwishOp'
-    op_names = ['HardSwish']
+    name = "HardSwishOp"
+    op_names = ["HardSwish"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXGlobalAveragePoolMetatype(ONNXOpMetatype):
-    name = 'GlobalAveragePoolOp'
-    op_names = ['GlobalAveragePool']
+    name = "GlobalAveragePoolOp"
+    op_names = ["GlobalAveragePool"]
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXAveragePoolMetatype(ONNXOpMetatype):
-    name = 'AveragePoolOp'
-    op_names = ['AveragePool']
+    name = "AveragePoolOp"
+    op_names = ["AveragePool"]
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXMaxPoolMetatype(ONNXOpMetatype):
-    name = 'MaxPoolOp'
-    op_names = ['MaxPool']
+    name = "MaxPoolOp"
+    op_names = ["MaxPool"]
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXConstantMetatype(ONNXOpMetatype):
-    name = 'ConstantOp'
-    op_names = ['Constant']
+    name = "ConstantOp"
+    op_names = ["Constant"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXAddLayerMetatype(ONNXOpMetatype):
-    name = 'AddOp'
-    op_names = ['Add', 'Sum']
+    name = "AddOp"
+    op_names = ["Add", "Sum"]
     hw_config_names = [HWConfigOpName.ADD]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXSubMetatype(ONNXOpMetatype):
-    name = 'SubOp'
-    op_names = ['Sub']
+    name = "SubOp"
+    op_names = ["Sub"]
     hw_config_names = [HWConfigOpName.SUBTRACT]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXMulLayerMetatype(ONNXOpMetatype):
-    name = 'MulOp'
-    op_names = ['Mul']
+    name = "MulOp"
+    op_names = ["Mul"]
     hw_config_names = [HWConfigOpName.MULTIPLY]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXDivLayerMetatype(ONNXOpMetatype):
-    name = 'DivOp'
-    op_names = ['Div']
+    name = "DivOp"
+    op_names = ["Div"]
     hw_config_names = [HWConfigOpName.DIVIDE]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXConcatLayerMetatype(ONNXOpMetatype):
-    name = 'ConcatOp'
-    op_names = ['Concat']
+    name = "ConcatOp"
+    op_names = ["Concat"]
     hw_config_names = [HWConfigOpName.CONCAT]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXBatchNormMetatype(ONNXOpMetatype):
-    name = 'BatchNormalizationOp'
-    op_names = ['BatchNormalization']
+    name = "BatchNormalizationOp"
+    op_names = ["BatchNormalization"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXResizeMetatype(ONNXOpMetatype):
-    name = 'ResizeOp'
-    op_names = ['Resize']
+    name = "ResizeOp"
+    op_names = ["Resize"]
     hw_config_names = [HWConfigOpName.INTERPOLATE]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXReshapeMetatype(ONNXOpMetatype):
-    name = 'ReshapeOp'
-    op_names = ['Reshape']
+    name = "ReshapeOp"
+    op_names = ["Reshape"]
     hw_config_names = [HWConfigOpName.RESHAPE]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXUpsampleMetatype(ONNXOpMetatype):
-    name = 'UpsampleOp'
-    op_names = ['Upsample']
+    name = "UpsampleOp"
+    op_names = ["Upsample"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXConstantOfShapeMetatype(ONNXOpMetatype):
-    name = 'ConstantOfShapeOp'
-    op_names = ['ConstantOfShape']
+    name = "ConstantOfShapeOp"
+    op_names = ["ConstantOfShape"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXShapeMetatype(ONNXOpMetatype):
-    name = 'ShapeOp'
-    op_names = ['Shape']
+    name = "ShapeOp"
+    op_names = ["Shape"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXExpandMetatype(ONNXOpMetatype):
-    name = 'ExpandOp'
-    op_names = ['Expand']
+    name = "ExpandOp"
+    op_names = ["Expand"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXNonZeroMetatype(ONNXOpMetatype):
-    name = 'NonZeroOp'
-    op_names = ['NonZero']
+    name = "NonZeroOp"
+    op_names = ["NonZero"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXSplitMetatype(ONNXOpMetatype):
-    name = 'SplitOp'
-    op_names = ['Split']
+    name = "SplitOp"
+    op_names = ["Split"]
     hw_config_names = [HWConfigOpName.SPLIT]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXLessMetatype(ONNXOpMetatype):
-    name = 'LessOp'
-    op_names = ['Less']
+    name = "LessOp"
+    op_names = ["Less"]
     hw_config_names = [HWConfigOpName.LESS]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXGreaterMetatype(ONNXOpMetatype):
-    name = 'GreaterOp'
-    op_names = ['Greater']
+    name = "GreaterOp"
+    op_names = ["Greater"]
     hw_config_names = [HWConfigOpName.GREATER]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXEqualMetatype(ONNXOpMetatype):
-    name = 'EqualOp'
-    op_names = ['Equal']
+    name = "EqualOp"
+    op_names = ["Equal"]
     hw_config_names = [HWConfigOpName.EQUAL]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXNotMetatype(ONNXOpMetatype):
-    name = 'NotOp'
-    op_names = ['Not']
+    name = "NotOp"
+    op_names = ["Not"]
     hw_config_names = [HWConfigOpName.LOGICALNOT]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXAndMetatype(ONNXOpMetatype):
-    name = 'AndOp'
-    op_names = ['And']
+    name = "AndOp"
+    op_names = ["And"]
     hw_config_names = [HWConfigOpName.LOGICALAND]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXOrMetatype(ONNXOpMetatype):
-    name = 'OrOp'
-    op_names = ['Or']
+    name = "OrOp"
+    op_names = ["Or"]
     hw_config_names = [HWConfigOpName.LOGICALOR]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXFloorMetatype(ONNXOpMetatype):
-    name = 'FloorOp'
-    op_names = ['Floor']
+    name = "FloorOp"
+    op_names = ["Floor"]
     hw_config_names = [HWConfigOpName.FLOORMOD]
 
 
 @ONNX_OPERATION_METATYPES.register()
+class ONNXPowMetatype(ONNXOpMetatype):
+    name = "PowOp"
+    op_names = ["Pow"]
+    hw_config_names = [HWConfigOpName.POWER]
+
+
+@ONNX_OPERATION_METATYPES.register()
 class ONNXSqrtMetatype(ONNXOpMetatype):
-    name = 'SqrtOp'
-    op_names = ['Sqrt']
+    name = "SqrtOp"
+    op_names = ["Sqrt"]
+    hw_config_names = [HWConfigOpName.POWER]
+
+
+@ONNX_OPERATION_METATYPES.register()
+class ONNXReciprocalMetatype(ONNXOpMetatype):
+    name = "ReciprocalOp"
+    op_names = ["Reciprocal"]
     hw_config_names = [HWConfigOpName.POWER]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXLogMetatype(ONNXOpMetatype):
-    name = 'LogOp'
-    op_names = ['Log']
+    name = "LogOp"
+    op_names = ["Log"]
+
+
+@ONNX_OPERATION_METATYPES.register()
+class ONNXAbsMetatype(ONNXOpMetatype):
+    name = "AbsOp"
+    op_names = ["Abs"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXScatterElementslMetatype(ONNXOpMetatype):
-    name = 'ScatterElementsOp'
-    op_names = ['ScatterElements']
+    name = "ScatterElementsOp"
+    op_names = ["ScatterElements"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXRoiAlignMetatype(ONNXOpMetatype):
-    name = 'RoiAlignOp'
-    op_names = ['RoiAlign']
+    name = "RoiAlignOp"
+    op_names = ["RoiAlign"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXMatMulMetatype(ONNXOpMetatype):
-    name = 'MatMulOp'
-    op_names = ['MatMul']
+    name = "MatMulOp"
+    op_names = ["MatMul"]
     hw_config_names = [HWConfigOpName.MATMUL]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXGatherMetatype(ONNXOpMetatype):
-    name = 'GatherOp'
-    op_names = ['Gather']
+    name = "GatherOp"
+    op_names = ["Gather"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXUnsqueezeMetatype(ONNXOpMetatype):
-    name = 'UnsqueezeOp'
-    op_names = ['Unsqueeze']
+    name = "UnsqueezeOp"
+    op_names = ["Unsqueeze"]
     hw_config_names = [HWConfigOpName.UNSQUEEZE]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXSqueezeMetatype(ONNXOpMetatype):
-    name = 'SqueezeOp'
-    op_names = ['Squeeze']
+    name = "SqueezeOp"
+    op_names = ["Squeeze"]
     hw_config_names = [HWConfigOpName.SQUEEZE]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXNonMaxSuppressionMetatype(ONNXOpMetatype):
-    name = 'NonMaxSuppressionOp'
-    op_names = ['NonMaxSuppression']
+    name = "NonMaxSuppressionOp"
+    op_names = ["NonMaxSuppression"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXCastMetatype(ONNXOpMetatype):
-    name = 'CastOp'
-    op_names = ['Cast']
+    name = "CastOp"
+    op_names = ["Cast"]
     hw_config_names = [HWConfigOpName.SQUEEZE]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXReduceMinMetatype(ONNXOpMetatype):
-    name = 'ReduceMinOp'
-    op_names = ['ReduceMin']
+    name = "ReduceMinOp"
+    op_names = ["ReduceMin"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXReduceMeanMetatype(ONNXOpMetatype):
-    name = 'ReduceMeanOp'
-    op_names = ['ReduceMean']
+    name = "ReduceMeanOp"
+    op_names = ["ReduceMean"]
     hw_config_names = [HWConfigOpName.REDUCEMEAN]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXTopKMetatype(ONNXOpMetatype):
-    name = 'TopKOp'
-    op_names = ['TopK']
+    name = "TopKOp"
+    op_names = ["TopK"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXSliceMetatype(ONNXOpMetatype):
-    name = 'SliceOp'
-    op_names = ['Slice']
+    name = "SliceOp"
+    op_names = ["Slice"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXExpMetatype(ONNXOpMetatype):
-    name = 'ExpOp'
-    op_names = ['Exp']
+    name = "ExpOp"
+    op_names = ["Exp"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXTransposeMetatype(ONNXOpMetatype):
-    name = 'TransposeOp'
-    op_names = ['Transpose']
+    name = "TransposeOp"
+    op_names = ["Transpose"]
     hw_config_names = [HWConfigOpName.TRANSPOSE]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXFlattenMetatype(ONNXOpMetatype):
-    name = 'FlattenOp'
-    op_names = ['Flatten']
+    name = "FlattenOp"
+    op_names = ["Flatten"]
     hw_config_names = [HWConfigOpName.FLATTEN]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXSoftmaxMetatype(ONNXOpMetatype):
-    name = 'SoftmaxOp'
-    op_names = ['Softmax']
+    name = "SoftmaxOp"
+    op_names = ["Softmax"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXPadMetatype(ONNXOpMetatype):
-    name = 'PadOp'
-    op_names = ['Pad']
+    name = "PadOp"
+    op_names = ["Pad"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXIdentityMetatype(ONNXOpMetatype):
-    name = 'IdentityOp'
-    op_names = ['Identity']
+    name = "IdentityOp"
+    op_names = ["Identity"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXQuantizeLinearMetatype(ONNXOpMetatype):
-    name = 'QuantizeLinearOp'
-    op_names = ['QuantizeLinear']
+    name = "QuantizeLinearOp"
+    op_names = ["QuantizeLinear"]
 
 
 @ONNX_OPERATION_METATYPES.register()
 class ONNXDequantizeLinearMetatype(ONNXOpMetatype):
-    name = 'DequantizeLinearOp'
-    op_names = ['DequantizeLinear']
+    name = "DequantizeLinearOp"
+    op_names = ["DequantizeLinear"]
+
+
+@ONNX_OPERATION_METATYPES.register()
+class ONNXDeformableConvolutionMetatype(ONNXOpMetatype):
+    name = "DeformConvOp"
+    op_names = ["DeformConv"]
+
 
+WEIGHT_LAYER_METATYPES = [
+    ONNXConvolutionMetatype,
+    ONNXDepthwiseConvolutionMetatype,
+    ONNXConvolutionTransposeMetatype,
+    ONNXLinearMetatype,
+]
 
-WEIGHT_LAYER_METATYPES = [ONNXConvolutionMetatype,
-                          ONNXDepthwiseConvolutionMetatype,
-                          ONNXConvolutionTransposeMetatype,
-                          ONNXLinearMetatype]
-
-LAYERS_WITH_BIAS_METATYPES = [ONNXConvolutionMetatype,
-                              ONNXDepthwiseConvolutionMetatype,
-                              ONNXConvolutionTransposeMetatype]
+# Contains the operation metatypes for which bias can be applied.
+OPERATIONS_WITH_BIAS_METATYPES = [
+    ONNXConvolutionMetatype,
+    ONNXDepthwiseConvolutionMetatype,
+]
 
 
 def get_operator_metatypes() -> List[Type[OperatorMetatype]]:
     """
     Returns a list of the operator metatypes.
 
     :return: List of operator metatypes .
@@ -495,24 +528,27 @@
 
     :param model: ONNX model to get the node's weight.
     :param node: Convolution node to check whether it is depthwise.
     :return: True if the convolution is depthwise, False - otherwise.
     """
     conv_group = None
     for attribute in node.attribute:
-        if attribute.name == 'group':
+        if attribute.name == "group":
             conv_group = onnx.helper.get_attribute_value(attribute)
     if conv_group is None:
         return False
     weight_tensor_value = None
     initializer_name = node.input[1]
     for init in model.graph.initializer:
         if init.name == initializer_name:
             weight_tensor_value = onnx.numpy_helper.to_array(init)
     if weight_tensor_value is None:
         return False
     conv_out_channels = weight_tensor_value.shape[0]
     conv_in_channels = weight_tensor_value.shape[1] * conv_group
-    if conv_out_channels % conv_in_channels == 0 and conv_out_channels // conv_in_channels > 0 and\
-            conv_group == conv_in_channels:
+    if (
+        conv_out_channels % conv_in_channels == 0
+        and conv_out_channels // conv_in_channels > 0
+        and conv_group == conv_in_channels
+    ):
         return True
     return False
```

### Comparing `nncf-2.4.0/nncf/onnx/graph/model_transformer.py` & `nncf-2.5.0/nncf/onnx/graph/model_transformer.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,372 +1,419 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import List, Optional, Tuple, Set
-
-from copy import deepcopy
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import Counter
-import onnx
+from copy import deepcopy
+from typing import Dict, List, Set, Tuple, Union
+
 import numpy as np
+import onnx
 
-from nncf.common.graph.graph import NNCFGraph
+from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
-from nncf.common.graph.definitions import NNCFGraphNodeType
+from nncf.onnx.graph.node_utils import get_input_edge
 from nncf.onnx.graph.onnx_graph import ONNXGraph
 from nncf.onnx.graph.transformations.commands import ONNXBiasCorrectionCommand
 from nncf.onnx.graph.transformations.commands import ONNXModelExtractionCommand
 from nncf.onnx.graph.transformations.commands import ONNXOutputInsertionCommand
-from nncf.onnx.graph.transformations.commands import ONNXQuantizerInsertionCommand
 from nncf.onnx.graph.transformations.commands import ONNXQDQNodeRemovingCommand
-from nncf.common.factory import NNCFGraphFactory
-from nncf.common.graph.model_transformer import ModelTransformer
+from nncf.onnx.graph.transformations.commands import ONNXQuantizerInsertionCommand
 
 
-# pylint: disable=no-member
 class ONNXModelTransformer(ModelTransformer):
-    QUANTIZER_NAME_PREFIX = 'QuantizeLinear_'
-    DEQUANTIZER_NAME_PREFIX = 'DequantizeLinear_'
-    SCALE_TENSOR_NAME_PREFIX = 'scale_'
-    ZERO_POINT_NAME_PREFIX = 'zero_point_'
+    """
+    Applies transformations upon ONNX model.
+    ModelTransformer should be created once for a particular model,
+    and be used to apply transformations to the provided model.
+    """
+
+    QUANTIZER_NAME_PREFIX = "QuantizeLinear_"
+    DEQUANTIZER_NAME_PREFIX = "DequantizeLinear_"
+    SCALE_TENSOR_NAME_PREFIX = "scale_"
+    ZERO_POINT_NAME_PREFIX = "zero_point_"
 
     def __init__(self, model: onnx.ModelProto):
         super().__init__(model)
-        self._model = deepcopy(model)
-        self._nncf_graph = NNCFGraphFactory.create(self._model)
+        self.onnx_model_extractor = onnx.utils.Extractor(self._model)
+
+    def _get_target_edge(
+        self,
+        port_id: int,
+        node_name: str,
+        transform_type: TargetType,
+        onnx_graph: ONNXGraph,
+        input_edges_mapping: Dict[str, str],
+    ) -> str:
+        """
+        Returns edge name corresponding to the node with a name equal to node_name, port_id and transform_type.
+
+        :param port_id: Edge number of port.
+        :param node_name: Node name.
+        :param transform_type: Type of transformation.
+        :param onnx_graph: ONNXGraph.
+        :param input_edges_mapping: Mapping between NNCF Input nodes and
+            the following ONNX nodes and corresponding input port id.
+        :return: Target edge name.
+        """
+        if transform_type in [TargetType.PRE_LAYER_OPERATION, TargetType.OPERATION_WITH_WEIGHTS]:
+            return onnx_graph.get_node_edge_names(node_name)["input"][port_id]
+        if node_name in input_edges_mapping:  # ADD INPUT NODE CASE
+            return get_input_edge(node_name, input_edges_mapping, onnx_graph)
+        return onnx_graph.get_node_edge_names(node_name)["output"][port_id]
 
     def transform(self, transformation_layout: TransformationLayout) -> onnx.ModelProto:
         """
-        Applies transformations by type-callback on the model
+        Applies transformations to the model using an out-of-place approach.
+        The transformations do not affect the original model, and a new model
+        is returned with the transformations applied. If there are no transformations,
+        returns a new instance of the original model.
 
-        :param transformations: lisf of the TransformationCommand transformations
+        :param transformation_layout: Transformation commands.
+        :return: The new instance of a model with applied transformations.
         """
         quantizer_insert_transformations = []
         output_insert_transformations = []
         bias_correction_transformations = []
         qdq_node_removing_transformations = []
         model_extraction_transformation = None
-
         transformations = transformation_layout.transformations
-
+        # No transformation applied
+        if not transformations:
+            return deepcopy(self._model)
         for transformation in transformations:
             if isinstance(transformation, ONNXQuantizerInsertionCommand):
                 quantizer_insert_transformations.append(transformation)
             elif isinstance(transformation, ONNXOutputInsertionCommand):
                 output_insert_transformations.append(transformation)
             elif isinstance(transformation, ONNXBiasCorrectionCommand):
                 bias_correction_transformations.append(transformation)
             elif isinstance(transformation, ONNXModelExtractionCommand):
                 model_extraction_transformation = transformation
             elif isinstance(transformation, ONNXQDQNodeRemovingCommand):
                 qdq_node_removing_transformations.append(transformation)
-
-        if quantizer_insert_transformations:
-            self._apply_quantizer_insertion_transformations(quantizer_insert_transformations)
+        # Inplace transformations, using deepcopy of model
+        if quantizer_insert_transformations or bias_correction_transformations or qdq_node_removing_transformations:
+            model = deepcopy(self._model)
+            if quantizer_insert_transformations:
+                model = self._apply_quantizer_insertion_transformations(model, quantizer_insert_transformations)
+            if bias_correction_transformations:
+                model = self._apply_bias_correction_transformations(model, bias_correction_transformations)
+            if qdq_node_removing_transformations:
+                model = self._apply_qdq_node_removing_transformations(model, qdq_node_removing_transformations)
+        # Transformations that create new model
         if output_insert_transformations:
-            self._apply_output_insertion_transformations(output_insert_transformations)
-        if bias_correction_transformations:
-            self._apply_bias_correction_transformations(bias_correction_transformations)
+            model = self._apply_output_insertion_transformations(output_insert_transformations)
         if model_extraction_transformation:
-            self._model = self._apply_model_extraction_transformation(model_extraction_transformation)
-        if qdq_node_removing_transformations:
-            self._apply_qdq_node_removing_transformation(qdq_node_removing_transformations)
-
-        return self._model
+            model = self._apply_model_extraction_transformation(model_extraction_transformation)
+        return model
 
-    def _apply_output_insertion_transformations(self, transformations: List[ONNXOutputInsertionCommand]) -> None:
+    def _apply_output_insertion_transformations(
+        self, transformations: List[ONNXOutputInsertionCommand]
+    ) -> onnx.ModelProto:
         """
-        Applies incoming transformations to the model
+        Returns a new model with extra outputs provided by transformations.
 
-        :param transformations: list of the ONNXOutputInsertionCommand transformations
+        :param transformations: ONNXOutputInsertionCommand transformations.
+        :return: New model with inserted outputs.
         """
         onnx_graph = ONNXGraph(self._model)
-        nncf_graph = NNCFGraphFactory.create(self._model)
-        model_outputs = [output.name for output in onnx_graph.get_model_outputs()]
-        extra_model_outputs = self._get_extra_model_outputs(nncf_graph,
-                                                            onnx_graph,
-                                                            transformations)
-
-        model_with_intermediate_outputs = self._insert_outputs(self._model,
-                                                               outputs=[*extra_model_outputs,
-                                                                        *model_outputs])
-        self._model = model_with_intermediate_outputs
-
-    def _get_extra_model_outputs(self,
-                                 nncf_graph: NNCFGraph,
-                                 onnx_graph: ONNXGraph,
-                                 transformations: List[ONNXOutputInsertionCommand]) -> Set[str]:
-        """
-        Collects extra model outputs based on transformations
-
-        :param nncf_graph: NNCFGraph
-        :param onnx_graph: ONNXGraph
-        :param transformations: lisf of the ONNXOutputInsertionCommand
-        :return: list of the output names
-        """
-        extra_model_outputs = set()
-        input_edge_names = []
-
+        model_outputs = set(output.name for output in onnx_graph.get_model_outputs())
         for transformation in transformations:
+            port_id = transformation.target_point.port_id
             node_name = transformation.target_point.target_node_name
-            if NNCFGraphNodeType.INPUT_NODE in node_name:
-                nncf_node_name = nncf_graph.get_node_by_name(transformation.target_point.target_node_name)
-                onnx_nodes_after_input_node = [edge.to_node for edge in nncf_graph.get_output_edges(nncf_node_name)]
-                for onnx_node_name in onnx_nodes_after_input_node:
-                    input_edge_names.append(onnx_graph.get_node_edge_names(onnx_node_name.node_name)['input'][0])
-                extra_model_outputs.update(input_edge_names)
-                input_edge_names = []
-            else:
-                if transformation.target_point.type == TargetType.POST_LAYER_OPERATION:
-                    edge_name = onnx_graph.get_node_edge_names(node_name)['output'][
-                        transformation.target_point.port_id]
-                elif transformation.target_point.type == TargetType.PRE_LAYER_OPERATION:
-                    edge_name = onnx_graph.get_node_edge_names(node_name)['input'][
-                        transformation.target_point.port_id]
-                else:
-                    raise RuntimeError
-                extra_model_outputs.add(edge_name)
-            extra_model_outputs.update(input_edge_names)
-        return extra_model_outputs
-
-    def _insert_outputs(self, model: onnx.ModelProto, outputs: List[str] = None) -> onnx.ModelProto:
-        """
-        Takes a model and adds outputs based on the list of edge names to collect data from
-
-        :param model: *ONNX* model
-        :param outputs: edge names to collect data from
-        :return: modified model
+            transform_type = transformation.target_point.type
+            input_edges_mapping = transformation.input_edges_mapping
+            target_edge_name = self._get_target_edge(
+                port_id, node_name, transform_type, onnx_graph, input_edges_mapping
+            )
+            model_outputs.add(target_edge_name)
+
+        return ONNXModelTransformer._insert_outputs(self._model, outputs=model_outputs)
+
+    @staticmethod
+    def _insert_outputs(model: onnx.ModelProto, outputs: Union[List[str], Set[str]]) -> onnx.ModelProto:
+        """
+        Creates a new model as a copy of provided model with additional outputs.
+
+        :param model: Model of which copy will be created.
+        :param outputs: Edge names to use as outputs.
+        :return: New model with inserted outputs.
         """
-        if outputs is None:
-            raise RuntimeError("Parameter outputs cannot be None.")
         onnx_graph = ONNXGraph(model)
-        var_out = []
-        for out in outputs:
-            # shape should be None; if you place not None, some models will have inference problems (e.g. Mask RCNN)
-            type_proto = onnx.helper.make_tensor_type_proto(onnx_graph.get_edge_dtype(out),
-                                                            shape=None)
-            value_info = onnx.helper.make_value_info(
-                name=out, type_proto=type_proto)
-            var_out.append(value_info)
-
-        graph = onnx.helper.make_graph(nodes=model.graph.node,
-                                       name=model.graph.name,
-                                       inputs=model.graph.input,
-                                       outputs=var_out,
-                                       initializer=model.graph.initializer,
-                                       value_info=model.graph.value_info)
-        onnx_model = onnx.helper.make_model(graph,
-                                            ir_version=model.ir_version,
-                                            producer_name=model.producer_name,
-                                            producer_version=model.producer_version,
-                                            domain=model.domain,
-                                            model_version=model.model_version,
-                                            doc_string=model.doc_string)
-        if len(model.metadata_props) > 0:
+        model_outputs = []
+        for output in outputs:
+            edge = onnx_graph.get_edge(output)
+            onnx_dtype = ONNXGraph.get_edge_dtype(edge)
+            type_proto = onnx.helper.make_tensor_type_proto(onnx_dtype, shape=None)
+            model_outputs.append(onnx.helper.make_value_info(name=output, type_proto=type_proto))
+
+        graph = onnx.helper.make_graph(
+            nodes=model.graph.node,
+            name=model.graph.name,
+            inputs=model.graph.input,
+            outputs=model_outputs,
+            initializer=model.graph.initializer,
+            value_info=model.graph.value_info,
+        )
+        new_model = onnx.helper.make_model(
+            graph,
+            ir_version=model.ir_version,
+            producer_name=model.producer_name,
+            producer_version=model.producer_version,
+            domain=model.domain,
+            model_version=model.model_version,
+            doc_string=model.doc_string,
+        )
+        if model.metadata_props:
             values = {p.key: p.value for p in model.metadata_props}
-            onnx.helper.set_model_props(onnx_model, values)
-
-        if len(onnx_model.graph.input) != len(model.graph.input):
-            raise RuntimeError("Input mismatch {} != {}".format(
-                len(onnx_model.input), len(model.input)))
-        # fix opset import
-        del onnx_model.opset_import[:]
+            onnx.helper.set_model_props(new_model, values)
+        del new_model.opset_import[:]
         for oimp in model.opset_import:
-            op_set = onnx_model.opset_import.add()
+            op_set = new_model.opset_import.add()
             op_set.domain = oimp.domain
             op_set.version = oimp.version
-        return onnx_model
+        return new_model
 
     def _apply_quantizer_insertion_transformations(
-            self,
-            transformations: List[ONNXQuantizerInsertionCommand]) -> None:
+        self, model: onnx.ModelProto, transformations: List[ONNXQuantizerInsertionCommand]
+    ) -> onnx.ModelProto:
         """
-        Applies transformations on the model
+        Creates a new model as a deepcopy of provided model and inserts QuantizeLinear-DequantizeLinear nodes pair.
 
-        :param transformations: lisf of the TransformationCommand transformations
+        :param model: Model to apply transformations.
+        :param transformations: QuantizeLinear-DequantizeLinear nodes pair insertion transformation commands.
+        :return: New model with inserted QuantizeLinear-DequantizeLinear nodes pairs.
         """
         self._added_target_edges = Counter()
-        onnx_graph = ONNXGraph(self._model)
         for transformation in transformations:
-            self._insert_quantizer_dequantizer(transformation, onnx_graph)
-
-    def _get_target_edge_name(self, transformation: ONNXQuantizerInsertionCommand, onnx_graph: ONNXGraph) -> \
-            Optional[str]:
-        target_edge_name = None
-        if transformation.target_point.type == TargetType.OPERATION_WITH_WEIGHTS:
-            target_edge_name = onnx_graph.get_node_edge_names(transformation.target_point.target_node_name)['input'][
-                transformation.target_point.port_id]
-        elif transformation.target_point.type == TargetType.PRE_LAYER_OPERATION:
-            target_edge_name = onnx_graph.get_node_edge_names(transformation.target_point.target_node_name)['input'][
-                transformation.target_point.port_id]
-        elif transformation.target_point.type == TargetType.POST_LAYER_OPERATION:
-            if NNCFGraphNodeType.INPUT_NODE in transformation.target_point.target_node_name:  # ADD INPUT NODE CASE
-                nncf_node_name = self._nncf_graph.get_node_by_name(transformation.target_point.target_node_name)
-                onnx_nodes_after_input_node = [edge.to_node for edge in
-                                               self._nncf_graph.get_output_edges(nncf_node_name)]
-                for onnx_node_name in onnx_nodes_after_input_node:
-                    target_edge_name = onnx_graph.get_node_edge_names(onnx_node_name.node_name)['input'][
-                        transformation.target_point.port_id]
-                    break
-            else:
-                target_edge_name = onnx_graph.get_node_edge_names(transformation.target_point.target_node_name)[
-                    'output'][transformation.target_point.port_id]
-        else:
-            raise RuntimeError(
-                'Could not find the edge corresponding to node {}'.format(
-                    transformation.target_point.target_node_name))
-        self._added_target_edges[target_edge_name] += 1
-        return target_edge_name
+            model = self._insert_quantizer_dequantizer(model, transformation)
+        return model
 
-    def _get_quantize_dequantize_nodes(self, transformation: ONNXQuantizerInsertionCommand, target_edge_name: str) -> \
-            Tuple[onnx.NodeProto, onnx.NodeProto]:
+    def _get_quantize_dequantize_nodes(
+        self, transformation: ONNXQuantizerInsertionCommand, target_edge_name: str
+    ) -> Tuple[onnx.NodeProto, onnx.NodeProto]:
+        """
+        Returns QuantizeLinear-DequantizeLinear nodes pair, based on the transformation parameters and
+        inserted onto edge with name target_edge_name.
+
+        :param transformation: QuantizeLinear-DequantizeLinear insertion transformation,
+        from which quantization axis is obtained.
+        :param target_edge_name: Edge name on which QuantizeLinear-DequantizeLinear nodes pair should be placed.
+        :return: QuantizeLinear-DequantizeLinear nodes pair.
+        """
         axis = transformation.quantizer_parameters.axis
 
         cnt = self._added_target_edges[target_edge_name]
 
         input_target_edge = target_edge_name
-        q_target_edge_name = target_edge_name + '_' + str(cnt)
+        q_target_edge_name = target_edge_name + "_" + str(cnt)
         quantizer_name = ONNXModelTransformer.QUANTIZER_NAME_PREFIX + q_target_edge_name
         dequantizer_name = ONNXModelTransformer.DEQUANTIZER_NAME_PREFIX + q_target_edge_name
         scale_tensor_name = ONNXModelTransformer.SCALE_TENSOR_NAME_PREFIX + q_target_edge_name
         zero_point_tensor_name = ONNXModelTransformer.ZERO_POINT_NAME_PREFIX + q_target_edge_name
 
         quantizer = onnx.helper.make_node(
             name=quantizer_name,
-            op_type='QuantizeLinear',
+            op_type="QuantizeLinear",
             inputs=[input_target_edge, scale_tensor_name, zero_point_tensor_name],
-            outputs=['q_output_' + q_target_edge_name],
-            axis=axis
+            outputs=["q_output_" + q_target_edge_name],
+            axis=axis,
         )
 
         dequantizer = onnx.helper.make_node(
             name=dequantizer_name,
-            op_type='DequantizeLinear',
-            inputs=['q_output_' + q_target_edge_name, scale_tensor_name, zero_point_tensor_name],
-            outputs=['dq_output_' + q_target_edge_name],
+            op_type="DequantizeLinear",
+            inputs=["q_output_" + q_target_edge_name, scale_tensor_name, zero_point_tensor_name],
+            outputs=["dq_output_" + q_target_edge_name],
             axis=axis,
         )
 
         return quantizer, dequantizer
 
-    def _get_scale_zero_point_tensors(self, transformation: ONNXQuantizerInsertionCommand, quantizer: onnx.NodeProto,
-                                      dequantizer: onnx.NodeProto) -> Tuple[onnx.TensorProto, onnx.TensorProto]:
+    @staticmethod
+    def _get_scale_zero_point_tensors(
+        transformation: ONNXQuantizerInsertionCommand, quantizer: onnx.NodeProto, dequantizer: onnx.NodeProto
+    ) -> Tuple[onnx.TensorProto, onnx.TensorProto]:
+        """
+        Returns scale and zero point of QuantizeLinear-DequantizeLinear nodes pair.
+
+        :param transformation: QuantizeLinear-DequantizeLinear insertion transformation,
+        from which scale and zero point values are obtained.
+        :param quantizer: QuantizeLinear node.
+        :param dequantizer: DequantizeLinear node.
+        :return: Scale and zero point tensors.
+        """
         scale = transformation.quantizer_parameters.scale
         zero_point = transformation.quantizer_parameters.zero_point
         tensor_type = transformation.quantizer_parameters.tensor_type
 
         per_channel = scale.ndim > 0
         dims = scale.shape if per_channel else []
-        onnx_scale = [scale.tolist()] if not per_channel else scale.tolist()
-        onnx_zero_point = [zero_point.tolist()] if not per_channel else zero_point.tolist()
+        onnx_scale = [scale.tolist()] if not per_channel else scale
+        onnx_zero_point = [zero_point.tolist()] if not per_channel else zero_point
         if tensor_type == np.uint8:
             onnx_tensor_type = onnx.TensorProto.UINT8
         elif tensor_type == np.int8:
             onnx_tensor_type = onnx.TensorProto.INT8
         else:
-            raise RuntimeError('Incorrect tensor type.')
+            raise RuntimeError(f"Incorrect tensor type - {tensor_type}.")
         assert quantizer.input[1] == dequantizer.input[1] and quantizer.input[2] == dequantizer.input[2]
         scale_tensor_name = quantizer.input[1]
         zero_point_tensor_name = quantizer.input[2]
         onnx_scale_tensor = onnx.helper.make_tensor(scale_tensor_name, onnx.TensorProto.FLOAT, dims, onnx_scale)
-        onnx_zero_point_tensor = onnx.helper.make_tensor(zero_point_tensor_name, onnx_tensor_type, dims,
-                                                         onnx_zero_point)
+        onnx_zero_point_tensor = onnx.helper.make_tensor(
+            zero_point_tensor_name, onnx_tensor_type, dims, onnx_zero_point
+        )
         return onnx_scale_tensor, onnx_zero_point_tensor
 
-    def _insert_quantizer_dequantizer(self, transformation: ONNXQuantizerInsertionCommand,
-                                      onnx_graph: ONNXGraph) -> None:
-        target_edge_name = self._get_target_edge_name(transformation, onnx_graph)
+    def _get_quantizer_dequantizer_edge_name(
+        self, transformation: ONNXQuantizerInsertionCommand, onnx_graph: ONNXGraph
+    ) -> str:
+        """
+        Returns an edge name on which QuantizeLinear-DequantizeLinear nodes pair has to be inserted.
+
+        :param transformation: QuantizeLinear-DequantizeLinear insertion transformation.
+        :param onnx_graph: ONNXGraph.
+        :return: Edge name to insert QuantizeLinear-DequantizeLinear nodes pair.
+        """
+        port_id = transformation.target_point.port_id
+        node_name = transformation.target_point.target_node_name
+        transform_type = transformation.target_point.type
+        input_edges_mapping = transformation.input_edges_mapping
+        target_edge_name = self._get_target_edge(port_id, node_name, transform_type, onnx_graph, input_edges_mapping)
+        self._added_target_edges[target_edge_name] += 1
+        return target_edge_name
+
+    def _insert_quantizer_dequantizer(
+        self, model: onnx.ModelProto, transformation: ONNXQuantizerInsertionCommand
+    ) -> onnx.ModelProto:
+        """
+        Inserts QuantizeLinear-DequantizeLinear nodes pair.
+
+        :param model: Model to insert new nodes.
+        :param transformation: QuantizeLinear-DequantizeLinear insertion transformation.
+        :return: Updated model with inserted QuantizeLinear-DequantizeLinear pair.
+        """
+        onnx_graph = ONNXGraph(model)
+        target_edge_name = self._get_quantizer_dequantizer_edge_name(transformation, onnx_graph)
         quantizer, dequantizer = self._get_quantize_dequantize_nodes(transformation, target_edge_name)
-        onnx_scale_tensor, onnx_zero_point_tensor = self._get_scale_zero_point_tensors(transformation, quantizer,
-                                                                                       dequantizer)
+        onnx_scale_tensor, onnx_zero_point_tensor = ONNXModelTransformer._get_scale_zero_point_tensors(
+            transformation, quantizer, dequantizer
+        )
 
         # If several nodes on one edge
         input_nodes = []
         input_nodes.extend(onnx_graph.get_nodes_by_input(target_edge_name))
         if not input_nodes:
             raise RuntimeError(
-                f'Can not add the quantizer to the {target_edge_name} edge. This edge does not have end node.')
+                f"Can not add the quantizer to the {target_edge_name} edge. This edge does not have end node."
+            )
 
         if transformation.target_point.type == TargetType.PRE_LAYER_OPERATION:
             # If we need to change only target nodes input
             target_node = onnx_graph.get_node_by_name(transformation.target_point.target_node_name)
             for i, inp in enumerate(target_node.input):
                 if inp == target_edge_name:
                     target_node.input[i] = dequantizer.output[0]
         else:
             for node in input_nodes:
                 for i, inp in enumerate(node.input):
                     if inp == target_edge_name:
                         node.input[i] = dequantizer.output[0]
 
-        onnx_scale_value_info = onnx.helper.make_tensor_value_info(onnx_scale_tensor.name, onnx_scale_tensor.data_type,
-                                                                   onnx_scale_tensor.dims)
-        onnx_zero_point_info = onnx.helper.make_tensor_value_info(onnx_zero_point_tensor.name,
-                                                                  onnx_zero_point_tensor.data_type,
-                                                                  onnx_zero_point_tensor.dims)
-        self._model.graph.initializer.extend([onnx_scale_tensor, onnx_zero_point_tensor])
-        self._model.graph.value_info.extend([onnx_scale_value_info, onnx_zero_point_info])
+        onnx_scale_value_info = onnx.helper.make_tensor_value_info(
+            onnx_scale_tensor.name, onnx_scale_tensor.data_type, onnx_scale_tensor.dims
+        )
+        onnx_zero_point_info = onnx.helper.make_tensor_value_info(
+            onnx_zero_point_tensor.name, onnx_zero_point_tensor.data_type, onnx_zero_point_tensor.dims
+        )
+        model.graph.initializer.extend([onnx_scale_tensor, onnx_zero_point_tensor])
+        model.graph.value_info.extend([onnx_scale_value_info, onnx_zero_point_info])
         insert_index = onnx_graph.get_node_index(input_nodes[0].name)
-        self._model.graph.node.insert(insert_index, quantizer)
-        self._model.graph.node.insert(insert_index + 1, dequantizer)
-
-    def _apply_bias_correction_transformations(self, transformations: List[ONNXBiasCorrectionCommand]) -> None:
+        model.graph.node.insert(insert_index, quantizer)
+        model.graph.node.insert(insert_index + 1, dequantizer)
+        return model
+
+    def _apply_bias_correction_transformations(
+        self, model: onnx.ModelProto, transformations: List[ONNXBiasCorrectionCommand]
+    ) -> onnx.ModelProto:
+        """
+        Creates a copy of original model and applies bias correction transformations on the model.
+
+        :param model: Model to apply transformations.
+        :param transformations: Bias correction transformations.
+        :return: Copy of original model with updated biases.
         """
-        Applies bias correction transformations on the model
-
-        :param transformations: lisf of the bias correction transformations
-        """
-        onnx_graph = ONNXGraph(self._model)
+        onnx_graph = ONNXGraph(model)
         for transformation in transformations:
             bias_tensor_position = transformation.target_point.port_id
             node_name = transformation.target_point.target_node_name
             onnx_node = onnx_graph.get_node_by_name(node_name)
             bias_initializer_name = onnx_node.input[bias_tensor_position]
             bias_initializer = onnx_graph.get_initializer(bias_initializer_name)
 
-            new_bias_tensor = onnx.numpy_helper.from_array(transformation.bias_value,
-                                                           bias_initializer_name)
+            new_bias_tensor = onnx.numpy_helper.from_array(transformation.bias_value, bias_initializer_name)
             bias_initializer.CopyFrom(new_bias_tensor)
+        return model
 
     def _apply_model_extraction_transformation(self, transformation: ONNXModelExtractionCommand) -> onnx.ModelProto:
         """
-        Extracts sub-model from the original based on the inputs and outputs names
-
-        :param transformation: model extraction transformation
-        """
-        onnx_model_exctactor = onnx.utils.Extractor(self._model)
-        return onnx_model_exctactor.extract_model(transformation.inputs, transformation.outputs)
+        Returns a new model that is a sub-model from the original between provided inputs and outputs.
 
-    def _apply_qdq_node_removing_transformation(self, transformations: List[ONNXQDQNodeRemovingCommand]) -> None:
+        :param transformation: Model extraction transformation.
+        :return: Extracted sub-model.
         """
-        Removes the layers from the model.
+        onnx_graph = ONNXGraph(self._model)
 
-        :param transformations: lisf of the node removing transformations.
+        input_tensor_names = []
+        for input_node_name in transformation.inputs:
+            input_onnx_node = onnx_graph.get_node_by_name(input_node_name)
+            input_tensor_names.append(input_onnx_node.input[0])
+
+        output_tensor_names = []
+        for output_node_name in transformation.outputs:
+            output_onnx_node = onnx_graph.get_node_by_name(output_node_name)
+            output_tensor_names.append(output_onnx_node.output[0])
+
+        if not output_tensor_names:
+            output_tensor_names = [n.name for n in onnx_graph.get_model_outputs()]
+
+        return self.onnx_model_extractor.extract_model(input_tensor_names, output_tensor_names)
+
+    def _apply_qdq_node_removing_transformations(
+        self, model: onnx.ModelProto, transformations: List[ONNXQDQNodeRemovingCommand]
+    ) -> onnx.ModelProto:
+        """
+        Returns a copy of original model with removed nodes.
+
+        :param model: Model to apply transformations.
+        :param transformations: Nodes removing transformations.
+        :return: Model with removed nodes.
         """
-        onnx_graph = ONNXGraph(self._model)
+        onnx_graph = ONNXGraph(model)
         for transformation in transformations:
             node = onnx_graph.get_node_by_name(transformation.target_point.target_node_name)
 
             node_children = onnx_graph.get_children(node)
             for node_child in node_children:
                 for input_id, input_obj in enumerate(node_child.input):
                     if input_obj == node.output[0]:
                         node_child.input[input_id] = node.input[0]
 
-            initializers = {i.name: i for i in self._model.graph.initializer}
-            value_infos = {i.name: i for i in self._model.graph.value_info}
+            initializers = {i.name: i for i in model.graph.initializer}
+            value_infos = {i.name: i for i in model.graph.value_info}
             for initializer_name in node.input:
                 if initializer_name in initializers:
-                    self._model.graph.initializer.remove(initializers[initializer_name])
+                    model.graph.initializer.remove(initializers[initializer_name])
                 if initializer_name in value_infos:
-                    self._model.graph.value_info.remove(value_infos[initializer_name])
+                    model.graph.value_info.remove(value_infos[initializer_name])
 
-            self._model.graph.node.remove(node)
+            model.graph.node.remove(node)
+        return model
```

### Comparing `nncf-2.4.0/nncf/onnx/graph/nncf_graph_builder.py` & `nncf-2.5.0/nncf/onnx/graph/nncf_graph_builder.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,254 +1,225 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Union, List
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import Counter
+from typing import List, Optional, Tuple
+
 import onnx
-from onnx import ModelProto
 
 from nncf.common.graph import NNCFGraph
-from nncf.common.graph.definitions import NNCFGraphNodeType
-from nncf.common.graph.layer_attributes import BaseLayerAttributes, Dtype
 from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
 from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME
+from nncf.common.graph.definitions import NNCFGraphNodeType
+from nncf.common.graph.layer_attributes import BaseLayerAttributes
+from nncf.common.graph.layer_attributes import Dtype
 from nncf.common.graph.operator_metatypes import InputNoopMetatype
 from nncf.common.graph.operator_metatypes import OutputNoopMetatype
-from nncf.common.graph.operator_metatypes import UnknownMetatype
-from nncf.common.logging import nncf_logger
-
-from nncf.onnx.graph.onnx_graph import ONNXGraph
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNX_OPERATION_METATYPES
 from nncf.onnx.graph.metatypes.onnx_metatypes import WEIGHT_LAYER_METATYPES
+from nncf.onnx.graph.onnx_graph import ONNXGraph
 
 
 class GraphConverter:
     """
     Builds the NNCFGraph from an ONNX model.
     """
 
-    DEFAULT_TENSOR_SHAPE = [1]
-
     @staticmethod
     def _replace_empty_node_name(model: onnx.ModelProto) -> onnx.ModelProto:
         """
         Sets a unique name to every node in 'model' with empty name field.
         NNCFGraph expects every node to have a unique name.
 
         :param model: ONNX model.
         :return: ONNX model with filled nodes.
         """
         for i, node in enumerate(model.graph.node):
-            if node.name == '':
-                node.name = node.op_type + '_nncf_' + str(i)
+            if node.name == "":
+                node.name = node.op_type + "_nncf_" + str(i)
 
         name_counter = Counter([node.name for node in model.graph.node])
 
         if max(name_counter.values()) > 1:
             raise RuntimeError(
                 f"Nodes {[(name, cnt) for name, cnt in name_counter.items() if cnt > 1]} "
                 "(name, counts) occurred more than once. "
-                "NNCF expects every node to have a unique name.")
+                "NNCF expects every node to have a unique name."
+            )
 
         return model
 
     @staticmethod
-    def _get_tensor_shape(onnx_graph: onnx.GraphProto, tensor: Union[str, onnx.ValueInfoProto]) -> List[int]:
-        """
-        Returns the shape of the 'tensor'.
-        :param onnx_graph: Graph, in which 'tensor' is been seeking.
-        :param tensor: Could be a name of tensor or ONNX internal tensor type.
-        :return: the 'tensor' shape.
-        """
-        try:
-            if isinstance(tensor, str):
-                tensor_shape = onnx_graph.get_edge_shape(tensor)
-            elif isinstance(tensor, onnx.ValueInfoProto):
-                tensor_shape = ONNXGraph.get_tensor_shape(tensor)
-        except RuntimeError as err:
-            # This exception raised because ONNX format allows to not have shape field.
-            # Model example - effecienet-v2, mobilenet_v2.
-            # In fact, the quantization algorithm doesn't utilize tensor shape information.
-            # So, if there is no shape, the DEFAULT_TENSOR_SHAPE is used.
-            nncf_logger.debug(err)
-            nncf_logger.debug('The default tensor shape will be set.')
-            tensor_shape = GraphConverter.DEFAULT_TENSOR_SHAPE
-        return tensor_shape
-
-    @staticmethod
-    def _add_nncf_input_nodes(onnx_graph: onnx.GraphProto, nncf_graph: NNCFGraph) -> None:
+    def _add_nncf_input_nodes(onnx_graph: ONNXGraph, nncf_graph: NNCFGraph) -> None:
         """
         Adds special NNCF Input nodes to NNCFGraph.
         For all the ONNX model inputs, the special NNCF Input node is placed and then corresponding edges are added.
         :param onnx_graph: ONNXGraph, which helps to get information about the ONNX model.
         :param nncf_graph: NNCFGraph, in which the new nodes will be added.
         :return: None.
         """
         for i, _input in enumerate(onnx_graph.get_model_inputs()):
             input_name = _input.name
-            layer_attributes = ONNXExtendedLayerAttributes([input_name], [input_name])
-            input_node = nncf_graph.add_nncf_node(node_name=MODEL_INPUT_OP_NAME + '_' + str(i),
-                                                  node_type=NNCFGraphNodeType.INPUT_NODE,
-                                                  node_metatype=InputNoopMetatype,
-                                                  layer_attributes=layer_attributes)
+            input_node = nncf_graph.add_nncf_node(
+                node_name=MODEL_INPUT_OP_NAME + "_" + str(i),
+                node_type=NNCFGraphNodeType.INPUT_NODE,
+                node_metatype=InputNoopMetatype,
+            )
             to_nodes = onnx_graph.get_nodes_by_input(input_name)
 
             input_node_node_id = input_node.node_id
-            input_shape = GraphConverter._get_tensor_shape(onnx_graph, input_name)
-            onnx_dtype = onnx_graph.get_edge_dtype_name(input_name)
+            edge = onnx_graph.get_edge(input_name)
+            input_shape = ONNXGraph.get_edge_shape(edge)
+            onnx_dtype = ONNXGraph.get_edge_dtype(edge)
             nncf_dtype = GraphConverter.convert_onnx_dtype_to_nncf_dtype(onnx_dtype)
             output_port_id = 0
             for node in to_nodes:
                 to_node_id = nncf_graph.get_node_by_name(node.name).node_id
                 input_port_id = ONNXGraph.get_input_port_id_for_node_after_input(input_name, node)
                 nncf_graph.add_edge_between_nncf_nodes(
                     from_node_id=input_node_node_id,
                     to_node_id=to_node_id,
                     tensor_shape=input_shape,
                     input_port_id=input_port_id,
                     output_port_id=output_port_id,
-                    dtype=nncf_dtype
+                    dtype=nncf_dtype,
                 )
                 output_port_id += 1
 
     @staticmethod
-    def _add_nncf_output_nodes(onnx_graph: onnx.GraphProto, nncf_graph: NNCFGraph) -> None:
+    def _add_nncf_output_nodes(onnx_graph: ONNXGraph, nncf_graph: NNCFGraph) -> None:
         """
         Adds special NNCF Output nodes to NNCFGraph.
         For all the ONNX model outputs, the special NNCF Output node is placed and then corresponding edges are added.
         :param onnx_graph: ONNXGraph, which helps to get information about the ONNX model.
         :param nncf_graph: NNCFGraph, in which the new nodes will be added.
         :return: None.
         """
         for i, _output in enumerate(onnx_graph.get_model_outputs()):
             output_name = _output.name
-            layer_attributes = ONNXExtendedLayerAttributes([output_name], [output_name])
-            output_node = nncf_graph.add_nncf_node(node_name=MODEL_OUTPUT_OP_NAME + '_' + str(i),
-                                                   node_type=NNCFGraphNodeType.OUTPUT_NODE,
-                                                   node_metatype=OutputNoopMetatype,
-                                                   layer_attributes=layer_attributes)
+            output_node = nncf_graph.add_nncf_node(
+                node_name=MODEL_OUTPUT_OP_NAME + "_" + str(i),
+                node_type=NNCFGraphNodeType.OUTPUT_NODE,
+                node_metatype=OutputNoopMetatype,
+            )
             from_nodes = onnx_graph.get_nodes_by_output(output_name)
 
             output_node_node_id = output_node.node_id
-            output_shape = GraphConverter._get_tensor_shape(onnx_graph, output_name)
-            onnx_dtype = onnx_graph.get_edge_dtype_name(output_name)
+            edge = onnx_graph.get_edge(output_name)
+            output_shape = ONNXGraph.get_edge_shape(edge)
+            onnx_dtype = ONNXGraph.get_edge_dtype(edge)
             nncf_dtype = GraphConverter.convert_onnx_dtype_to_nncf_dtype(onnx_dtype)
             input_port_id = 0
             for node in from_nodes:
                 from_node_id = nncf_graph.get_node_by_name(node.name).node_id
                 output_port_id = ONNXGraph.get_output_port_id_for_node_before_output(output_name, node)
                 nncf_graph.add_edge_between_nncf_nodes(
                     from_node_id=from_node_id,
                     to_node_id=output_node_node_id,
                     tensor_shape=output_shape,
                     input_port_id=input_port_id,
                     output_port_id=output_port_id,
-                    dtype=nncf_dtype
+                    dtype=nncf_dtype,
                 )
                 input_port_id += 1
 
     @staticmethod
-    def convert_onnx_dtype_to_nncf_dtype(onnx_dtype: str) -> Dtype:
+    def convert_onnx_dtype_to_nncf_dtype(onnx_dtype: int) -> Dtype:
+        """
+        Converts the data type from the ONNX domain to the NNCF domain.
+
+        :param np_dtype: ONNX data type.
+        :return: NNCF data type.
         """
-        Converts the primitive types from the ONNX domain to the NNCF domain.
-        :param onnx_dtype: ONNX primitive typename.
-        :return: NNCF primitive type.
-        """
-        conversion_map = {
-            "FLOAT": "float",
-            "FLOAT16": "float",
-            "BFLOAT16": "float",
-            "DOUBLE": "float",
-        }
-        return Dtype(conversion_map.get(onnx_dtype, 'int'))
+        return Dtype.FLOAT if onnx_dtype == int(onnx.TensorProto.FLOAT) else Dtype.INTEGER
 
     @staticmethod
-    def create_nncf_graph(onnx_model: ModelProto) -> NNCFGraph:
+    def create_nncf_graph(onnx_model: onnx.ModelProto) -> NNCFGraph:
         """
         Creates NNCFGraph from 'onnx_model'.
         Initially, ONNXGraph is built. All nodes from onnx_model which have valid metatype are added to NNCFGraph.
         Then, corresponding edges are added to the NNCFGraph with shape, type, output and input port ids.
         In the last step, special NNCF Input and Output nodes are added.
         :param onnx_model: ONNX model.
         :return: NNCFGraph.
         """
         onnx_model = GraphConverter._replace_empty_node_name(onnx_model)
         nncf_graph = NNCFGraph()
         onnx_graph = ONNXGraph(onnx_model)
         for node in onnx_graph.get_all_nodes():
             metatype = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name(node.op_type)
-            if metatype is not UnknownMetatype:
-                if metatype.get_subtypes():
-                    subtype = metatype.determine_subtype(onnx_model, node)
-                    if subtype is not None:
-                        metatype = subtype
-            layer_attributes = ONNXExtendedLayerAttributes(node.input, node.output)
-            is_shared, layer_name = None, None
+            if metatype.get_subtypes():
+                subtype = metatype.determine_subtype(onnx_model, node)
+                if subtype is not None:
+                    metatype = subtype
+
             if metatype in WEIGHT_LAYER_METATYPES:
                 is_shared = onnx_graph.is_node_shared(node)
-                layer_name = onnx_graph.get_node_layer_name(node)
-            nncf_graph.add_nncf_node(node_name=node.name,
-                                     node_type=node.op_type,
-                                     node_metatype=metatype,
-                                     layer_attributes=layer_attributes,
-                                     layer_name=layer_name,
-                                     is_shared=is_shared)
+                weight_edge_name = onnx_graph.get_weight_tensor_edge(node)
+                edge = onnx_graph.get_edge(weight_edge_name)
+                weight_shape = ONNXGraph.get_edge_shape(edge)
+                layer_attributes = ONNXExtendedLayerAttributes(node.input, node.output, weight_shape)
+            else:
+                is_shared, weight_edge_name, layer_attributes = None, None, None
+            nncf_graph.add_nncf_node(
+                node_name=node.name,
+                node_type=node.op_type,
+                node_metatype=metatype,
+                layer_attributes=layer_attributes,
+                layer_name=weight_edge_name,
+                is_shared=is_shared,
+            )
         for output_node in onnx_graph.get_all_nodes():
-            output_edges = onnx_graph.get_node_edge_names(output_node.name)['output']
+            output_edges = onnx_graph.get_node_edge_names(output_node.name)["output"]
             for output_edge in output_edges:
-                tensor_shape = GraphConverter._get_tensor_shape(onnx_graph, output_edge)
-
-                output_node_id = nncf_graph.get_node_by_name(output_node.name).node_id
-                try:
-                    onnx_dtype = onnx_graph.get_edge_dtype_name(output_edge)
-                except RuntimeError:
-                    # If the edge was not added during inference of ONNX model,
-                    # we do not add it to NNCFGraph.
-                    # Particularly, BatchNorm exported in Training mode has unused outputs edges:
-                    # mean, var, saved_mean, saved_var.
+                edge = onnx_graph.get_edge(output_edge)
+                if edge is None:
+                    # If the edge is None it means that the edge was not added during shape inference of ONNX model.
+                    # BatchNorm exported in Training mode has unused outputs edges: mean, var, saved_mean, saved_var.
+                    # NNCFGraph should not contain such edges.
                     continue
+                tensor_shape = ONNXGraph.get_edge_shape(edge)
+                onnx_dtype = ONNXGraph.get_edge_dtype(edge)
                 nncf_dtype = GraphConverter.convert_onnx_dtype_to_nncf_dtype(onnx_dtype)
-
+                output_node_id = nncf_graph.get_node_by_name(output_node.name).node_id
                 input_nodes = onnx_graph.get_nodes_by_input(output_edge)
-                if not input_nodes:
-                    # if this node is output
-                    continue
                 for input_node in input_nodes:
                     port_ids = ONNXGraph.get_port_ids_between_nodes(output_node, input_node)
-                    input_port_id = port_ids['input_port_id']
-                    output_port_id = port_ids['output_port_id']
+                    input_port_id = port_ids["input_port_id"]
+                    output_port_id = port_ids["output_port_id"]
                     in_node_id = nncf_graph.get_node_by_name(input_node.name).node_id
                     nncf_graph.add_edge_between_nncf_nodes(
                         from_node_id=output_node_id,
                         to_node_id=in_node_id,
                         tensor_shape=tensor_shape,
                         input_port_id=input_port_id,
                         output_port_id=output_port_id,
-                        dtype=Dtype(nncf_dtype)
+                        dtype=Dtype(nncf_dtype),
                     )
         GraphConverter._add_nncf_input_nodes(onnx_graph, nncf_graph)
         GraphConverter._add_nncf_output_nodes(onnx_graph, nncf_graph)
         return nncf_graph
 
 
 class ONNXExtendedLayerAttributes(BaseLayerAttributes):
     """
     This class stores extended attributes of modules/layers for the algorithms.
     """
 
-    def __init__(self, input_tensor_names, output_tensor_names):
-        """
-        :param input_tensor_names: List of the input tensor/edge names of the module/layer
-        :param output_tensor_names: List of the output tensor/edge names of the module/layer
+    def __init__(
+        self, input_tensor_names: List[str], output_tensor_names: List[str], weight_shape: Optional[Tuple[int]] = None
+    ):
+        """
+        :param input_tensor_names: List of the input tensor/edge names of the module/layer.
+        :param output_tensor_names: List of the output tensor/edge names of the module/layer.
+        :param weight_shape: Shape of a weight shape of the module/layer.
         """
         self.input_tensor_names = input_tensor_names
         self.output_tensor_names = output_tensor_names
+        self.weight_shape = weight_shape
```

### Comparing `nncf-2.4.0/nncf/onnx/graph/onnx_graph.py` & `nncf-2.5.0/nncf/onnx/graph/onnx_graph.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,57 +1,49 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Callable, Dict, List, Optional, Tuple
+from typing import Callable, Dict, List, Optional, Tuple, Union
 
-from collections import deque
-import onnx
-from onnx import numpy_helper  # pylint: disable=no-name-in-module
 import numpy as np
+import onnx
+from onnx import numpy_helper
 
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNX_OPERATION_METATYPES
-from nncf.onnx.graph.metatypes.onnx_metatypes import OpWeightDef
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXIdentityMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXReshapeMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXQuantizeLinearMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import WEIGHT_LAYER_METATYPES
+from nncf.onnx.graph.metatypes.onnx_metatypes import OpWeightDef
 
 
-# pylint: disable=no-member
 # pylint: disable=too-many-public-methods
-
 class ONNXGraph:
     """
     The class provides the interface to get the necessary information from ONNX model.
     """
 
     def __init__(self, onnx_model: onnx.ModelProto):
         self.onnx_model = onnx_model
-        self._node_name_to_node = None  # type: Dict[str, onnx.onnx.NodeProto]
-        self._activations_tensor_name_to_value_info = None  # type: Dict[str, onnx.onnx.ValueInfoProto]
+        self._node_name_to_node = None  # type: Dict[str, onnx.NodeProto]
+        self._edge_name_to_value_info = None  # type: Dict[str, onnx.ValueInfoProto]
 
-    def _update_activation_tensors(self, do_shape_inference: bool = False) -> None:
-        if do_shape_inference:
-            self.onnx_model = onnx.shape_inference.infer_shapes(self.onnx_model)
-        self._activations_tensor_name_to_value_info = {tensor.name: tensor for tensor in
-                                                       self.onnx_model.graph.value_info}
-        model_inputs_name_to_value_info = {tensor.name: tensor for tensor in self.onnx_model.graph.input}
-        model_outputs_name_to_value_info = {tensor.name: tensor for tensor in self.onnx_model.graph.output}
-        self._activations_tensor_name_to_value_info.update(model_inputs_name_to_value_info)
-        self._activations_tensor_name_to_value_info.update(model_outputs_name_to_value_info)
+    def _update_edges(self) -> None:
+        self.onnx_model = onnx.shape_inference.infer_shapes(self.onnx_model)
+        value_infos = [
+            *self.onnx_model.graph.value_info,
+            *self.onnx_model.graph.input,
+            *self.onnx_model.graph.output,
+            *self.onnx_model.graph.initializer,
+        ]
+        self._edge_name_to_value_info = {tensor.name: tensor for tensor in value_infos}
 
     def _update_node_names(self) -> None:
         self._node_name_to_node = {n.name: n for n in self.onnx_model.graph.node}
 
     def get_all_nodes(self) -> List[onnx.NodeProto]:
         """
         Returns model nodes in the original order.
@@ -69,14 +61,26 @@
         :param node_name: Name of the node.
         :return: None if the node with the specified name exists - otherwise returns the node.
         """
         if self._node_name_to_node is None:
             self._update_node_names()
         return self._node_name_to_node[node_name] if node_name in self._node_name_to_node else None
 
+    def get_edge(self, edge_name: str) -> Optional[onnx.ValueInfoProto]:
+        """
+        Returns edge by its name or None if the model has no such edge.
+        If self._edge_name_to_value_info is not initialized runs an initialization.
+
+        :param edge_name: Name of edge.
+        :return: Edge.
+        """
+        if self._edge_name_to_value_info is None:
+            self._update_edges()
+        return self._edge_name_to_value_info.get(edge_name, None)
+
     def get_model_inputs(self) -> List[onnx.ValueInfoProto]:
         """
         Returns all model inputs.
 
         :return: Model Inputs.
         """
         inputs = []
@@ -110,16 +114,17 @@
         Returns all nodes that have input with the name 'input_name'.
 
         :param input_name: The name of input edge.
         :return: Nodes with corresponding input.
         """
         return self._get_nodes_by_lambda(input_name, lambda node: node.input)
 
-    def _get_nodes_by_lambda(self, name: str, func: Callable[[onnx.NodeProto], List[onnx.NodeProto]]) -> List[
-        onnx.NodeProto]:
+    def _get_nodes_by_lambda(
+        self, name: str, func: Callable[[onnx.NodeProto], List[onnx.NodeProto]]
+    ) -> List[onnx.NodeProto]:
         output = []
         for node in self.get_all_nodes():
             if name in func(node):
                 output.append(node)
         return output
 
     def get_node_edge_names(self, node_name: str) -> Dict[str, List[str]]:
@@ -129,64 +134,66 @@
         :param node_name: The name of the node.
         :return: Dict with two keys: 'input' and 'output',
         which are corresponding to input and output edges accordingly.
         """
         if self._node_name_to_node is None:
             self._update_node_names()
         if node_name in self._node_name_to_node:
-            return {'input': list(self._node_name_to_node[node_name].input),
-                    'output': list(self._node_name_to_node[node_name].output)}
-        raise RuntimeError('There is no node with the name {}'.format(node_name))
+            return {
+                "input": list(self._node_name_to_node[node_name].input),
+                "output": list(self._node_name_to_node[node_name].output),
+            }
+        raise RuntimeError("There is no node with the name {}".format(node_name))
 
     @staticmethod
     def get_input_port_id_for_node_after_input(input_name: str, to_node: onnx.NodeProto) -> int:
         """
         Returns input_port_id for 'to_node' connected with the model input with the name 'input_name'.
 
         :param input_name: Name of the ONNX model Input.
         :param to_node: Node, which has input edge with 'input_name' name.
         :return: input port number for 'to_node', which is connected to 'input_name'.
         """
         for input_port_id, port in enumerate(to_node.input):
             if port == input_name:
                 return input_port_id
-        raise RuntimeError(f'The node {to_node} does not have input edge with the name {input_name}')
+        raise RuntimeError(f"The node {to_node} does not have input edge with the name {input_name}")
 
     @staticmethod
     def get_output_port_id_for_node_before_output(output_name: str, from_node: onnx.NodeProto) -> int:
         """
         Returns output_port_id for 'from_node' connected with the model output with the name 'output_name'.
 
         :param output_name: Name of the ONNX model Output.
         :param from_node: Node, which has output edge with 'output_name' name.
         :return: output port number for 'from_node', which is connected to 'output_name'.
         """
         for output_port_id, port in enumerate(from_node.output):
             if port == output_name:
                 return output_port_id
-        raise RuntimeError(f'The node {from_node} does not have output edge with the name {output_name}')
+        raise RuntimeError(f"The node {from_node} does not have output edge with the name {output_name}")
 
     @staticmethod
     def get_port_ids_between_nodes(from_node: onnx.NodeProto, to_node: onnx.NodeProto) -> Dict[str, int]:
         """
         Returns input_port_id and output_port_id between 'from_node' and 'to_node'.
 
         :param from_node: Node, whose output is connected to 'to_node' node.
         :param to_node: Node, whose input is connected to 'from_node' node.
         :return: Dict{'input_port_id': input port id, 'output_port_id': output port id}
         """
-        output = {'input_port_id': None, 'output_port_id': None}
+        output = {"input_port_id": None, "output_port_id": None}
         for port_id, port in enumerate(to_node.input):
             if port in from_node.output:
-                output['input_port_id'] = port_id
+                output["input_port_id"] = port_id
         for port_id, port in enumerate(from_node.output):
             if port in to_node.input:
-                output['output_port_id'] = port_id
-        if output['output_port_id'] is None or output['input_port_id'] is None:
-            raise RuntimeError(f'The nodes {from_node.name} and {to_node.name} do not have edges between.')
+                output["output_port_id"] = port_id
+        if output["output_port_id"] is None or output["input_port_id"] is None:
+            raise RuntimeError(f"The nodes {from_node.name} and {to_node.name} do not have edges between.")
         return output
 
     def get_nodes_by_type(self, node_type: str) -> List[onnx.NodeProto]:
         """
         Returns all nodes in the model that have type equal to 'node_type'.
 
         :param node_type: Type of the nodes.
@@ -194,93 +201,62 @@
         """
         output = []
         for node in self.get_all_nodes():
             if str(node.op_type) == node_type:
                 output.append(node)
         return output
 
-    def get_weight_tensor(self, node: onnx.NodeProto) -> Tuple[str, np.ndarray]:
-        # TODO(kshpv): ticket: 101211
-        """
-        Returns node's weight tensor name and its value.
-
-        :param node: Node, in which the weight tensor finally applied.
-        :return: Weight tensor name and its value.
-        """
-        weight_port_id = self.get_weight_port_id(node)
-        weight_input = self.get_node_edge_names(node.name)['input'][weight_port_id]
-        if self.has_initializer(weight_input):
-            return self.get_initializer(weight_input).name, self.get_initializers_value(weight_input)
-        parent_node_on_weight_port = self.get_nodes_by_output(weight_input)[0]
-        nodes = deque([parent_node_on_weight_port])
-        while nodes:
-            current_node = nodes.popleft()
-            node_parents = self.get_parents(current_node)
-            nodes.extendleft(node_parents)
-            metatype = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name(current_node.op_type)
-            if metatype in [ONNXIdentityMetatype, ONNXQuantizeLinearMetatype]:
-                if self.has_initializer(current_node.input[0]):
-                    return self._get_tensor_from_zero_input(current_node)
-                continue
-            if metatype == ONNXReshapeMetatype:
-                return self._get_weight_tensor_with_reshape(current_node)
-            if metatype in WEIGHT_LAYER_METATYPES:
-                weight_port_id = self.get_weight_port_id(current_node)
-                weight_input = self.get_node_edge_names(current_node.name)['input'][weight_port_id]
-                return self.get_initializer(weight_input).name, self.get_initializers_value(weight_input)
-        raise RuntimeError('Could not find the weight tensor of the node')
-
     @staticmethod
     def _get_weight_definitions(node: onnx.NodeProto) -> OpWeightDef:
         """
         Returns the weight_definitions of the node's metatype.
 
         :param node: Node from which weight definition is obtained.
         :return: weight definition of the node.
         """
         metatype = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name(node.op_type)
         if metatype in WEIGHT_LAYER_METATYPES:
             return metatype.weight_definitions
-        raise RuntimeError(f'The metatype {metatype} does not belong to a list of metatypes with a weight tensor.')
+        raise RuntimeError(f"The metatype {metatype} does not belong to a list of metatypes with a weight tensor.")
 
     def get_weight_port_id(self, node: onnx.NodeProto) -> int:
         """
         Returns input port id, where a weight tensor should output.
 
         :param node: Node, for which input port id is returned,
         :return: input port id, where a weight tensor should output.
         """
         weight_definitions = self._get_weight_definitions(node)
         if weight_definitions.weight_port_id is not None:
             return weight_definitions.weight_port_id
-        raise RuntimeError(f'The metatype {node} does not have weight_port_id attribute')
+        raise RuntimeError(f"The metatype {node} does not have weight_port_id attribute")
 
     def get_weight_channel_axis(self, node: onnx.NodeProto) -> int:
         """
         Returns a channel axis for weight per-channel quantization.
 
         :param node: Node, for which weight per-channel axis id is returned,
         :return: Channel axis for per-channel quantization.
         """
         weight_definitions = self._get_weight_definitions(node)
         if weight_definitions.weight_channel_axis is not None:
             return weight_definitions.weight_channel_axis
-        raise RuntimeError(f'The node {node} does not have weight_channel_axis attribute')
+        raise RuntimeError(f"The node {node} does not have weight_channel_axis attribute")
 
     def get_bias_tensor_port_id(self, node: onnx.NodeProto) -> int:
         """
         Returns input port id, where a bias tensor should output.
 
         :param node: Node, for which input port id is returned,
         :return: input port id, where a weight bias should output.
         """
         weight_definitions = self._get_weight_definitions(node)
         if weight_definitions.bias_port_id is not None:
             return weight_definitions.bias_port_id
-        raise RuntimeError(f'The node {node} does not have bias_port_id attribute')
+        raise RuntimeError(f"The node {node} does not have bias_port_id attribute")
 
     def _get_weight_tensor_with_reshape(self, node: onnx.NodeProto) -> Tuple[str, np.ndarray]:
         """
         Returns node's weight tensor name and its value in the case when reshape node is placed after the weight.
         The returned weight tensor will be reshaped according to a shape attribute of the reshape node.
 
         :param node: Reshape node, whose input is weight tensor.
@@ -321,15 +297,15 @@
         :param initializer_name: Name of the tensor.
         :return: The value of the tensor.
         """
         for init in self.onnx_model.graph.initializer:
             if init.name == initializer_name:
                 tensor = numpy_helper.to_array(init)
                 return tensor
-        raise RuntimeError('There is no initializer with the name {}'.format(initializer_name))
+        raise RuntimeError("There is no initializer with the name {}".format(initializer_name))
 
     def has_initializer(self, initializer_name: str) -> bool:
         """
         Returns True whether the model has the initializer with the name equals to initializer_name.
 
         :param initializer_name: Name of the initializer.
         :return: True if the model has such initializer, False - otherwise.
@@ -345,90 +321,54 @@
 
         :param initializer_name: Name of the Initializer.
         :return: The Initializer.
         """
         for init in self.onnx_model.graph.initializer:
             if init.name == initializer_name:
                 return init
-        raise RuntimeError('There is no initializer with the name {}'.format(initializer_name))
+        raise RuntimeError("There is no initializer with the name {}".format(initializer_name))
 
     @staticmethod
-    def get_tensor_shape(tensor: onnx.ValueInfoProto) -> List[int]:
+    def get_edge_shape(edge: Union[onnx.ValueInfoProto, onnx.TensorProto]) -> List[int]:
         """
-        Returns 'tensor' shape.
+        Returns edge shape.
 
-        :param tensor: The tensor.
+        :param edge: The edge.
         :return: Shape of the Tensor.
         """
-        tensor_type = tensor.type.tensor_type
+        if isinstance(edge, onnx.TensorProto):
+            return list(edge.dims)
+        tensor_type = edge.type.tensor_type
         shape = []
         if tensor_type.HasField("shape"):
             for d in tensor_type.shape.dim:
                 if d.HasField("dim_value"):
                     dim_value = d.dim_value
                     if isinstance(dim_value, int):
                         shape.append(dim_value)
                     else:
-                        raise RuntimeError(f'The tensor {tensor.name} has non integer shape.')
+                        return shape
                 elif d.HasField("dim_param"):
-                    # flexible shape
-                    # make manually 1
-                    shape.append(1)
+                    # flexible shape  make manually -1
+                    shape.append(-1)
                 else:
-                    raise RuntimeError(f'The tensor {tensor.name} does not have dim_value field.')
-        else:
-            raise RuntimeError(f'The tensor {tensor.name} does not have shape field')
+                    return shape
         return shape
 
-    def get_edge_shape(self, edge_name: str) -> List[int]:
-        """
-        Returns a shape of the edge with the name 'edge_name'.
-        If the activations tensors were not filled in self._activations_tensor_name_to_value_info, it updates them.
-        If after updating of the self._activations_tensor_name_to_value_info, there is still no such tensor,
-        do shape inference of the model.
-
-        :param edge_name: The name of the edge.
-        :return: Shape of the tensor on that edge.
-        """
-        if self._activations_tensor_name_to_value_info is None:
-            self._update_activation_tensors()
-        if edge_name in self._activations_tensor_name_to_value_info:
-            return ONNXGraph.get_tensor_shape(self._activations_tensor_name_to_value_info[edge_name])
-        self._update_activation_tensors(do_shape_inference=True)
-        if edge_name in self._activations_tensor_name_to_value_info:
-            return ONNXGraph.get_tensor_shape(self._activations_tensor_name_to_value_info[edge_name])
-        raise RuntimeError('There is no edge with the name {}'.format(edge_name))
-
-    def get_edge_dtype(self, edge_name: str) -> int:
-        """
-        Returns the data type of the edge with the name 'edge_name'.
-        If the activations tensors were not filled in self._activations_tensor_name_to_value_info, it updates them.
-        If after updating of the self._activations_tensor_name_to_value_info, there is still no such tensor,
-        do shape inference of the model.
-
-        :param edge_name: The name of the edge.
-        :return: Shape of the tensor on that edge.
-        """
-        if self._activations_tensor_name_to_value_info is None:
-            self._update_activation_tensors()
-        if edge_name in self._activations_tensor_name_to_value_info:
-            return self._activations_tensor_name_to_value_info[edge_name].type.tensor_type.elem_type
-        self._update_activation_tensors(do_shape_inference=True)
-        if edge_name in self._activations_tensor_name_to_value_info:
-            return self._activations_tensor_name_to_value_info[edge_name].type.tensor_type.elem_type
-        raise RuntimeError('There is no edge with the name {}'.format(edge_name))
-
-    def get_edge_dtype_name(self, edge_name: str) -> str:
+    @staticmethod
+    def get_edge_dtype(edge: Union[onnx.ValueInfoProto, onnx.TensorProto]) -> int:
         """
-        Returns the name of datatype of the edge with the name 'edge_name'.
+        Returns the data type of the edge.
 
-        :param edge_name: The name of the edge.
-        :return: The Name of the datatype.
+        :param edge: The edge.
+        :return: Data type of the edge.
         """
-        return onnx.TensorProto.DataType.Name(self.get_edge_dtype(edge_name))
+        if isinstance(edge, onnx.ValueInfoProto):
+            return edge.type.tensor_type.elem_type
+        return edge.data_type
 
     def get_parents(self, node: onnx.NodeProto) -> List[onnx.NodeProto]:
         """
         Returns parents of the node.
 
         :param node: The child node.
         :return: All children nodes.
@@ -442,32 +382,33 @@
         """
         Returns children of the node.
 
         :param node: The parent node.
         :return: All children nodes.
         """
         output = []
-        node_edges = self.get_node_edge_names(node.name)['output']
+        node_edges = self.get_node_edge_names(node.name)["output"]
         for node_edge in node_edges:
             output.extend(self.get_nodes_by_input(node_edge))
         return output
 
-    def is_node_shared(self, node: onnx.NodeProto) -> bool:
+    def get_weight_tensor_edge(self, node: onnx.NodeProto) -> str:
         """
-        Returns whether the node share a weight.
+        Returns weight edge name.
 
-        :param node: Node.
-        :return: True whether node shares a weight - otherwise False.
+        :param node: Node with weight tensor.
+        :return: Weight edge name.
         """
-        weight_tensor_name, _ = self.get_weight_tensor(node)
-        nodes = self.get_nodes_by_input(weight_tensor_name)
-        return len(nodes) > 1
+        weight_port_id = self.get_weight_port_id(node)
+        weight_tensor_edge = self.get_node_edge_names(node.name)["input"][weight_port_id]
+        return weight_tensor_edge
 
-    def get_node_layer_name(self, node: onnx.NodeProto) -> Optional[str]:
+    def is_node_shared(self, node: onnx.NodeProto) -> bool:
         """
-        Returns name of a weight tensor if it exists.
+        Returns whether the node share a weight.
 
         :param node: Node.
-        :return: Name of a weight tensor or None if the node does not have a weight.
+        :return: True whether node shares a weight - otherwise False.
         """
-        weight_tensor_name, _ = self.get_weight_tensor(node)
-        return weight_tensor_name
+        weight_tensor_edge = self.get_weight_tensor_edge(node)
+        nodes = self.get_nodes_by_input(weight_tensor_edge)
+        return len(nodes) > 1
```

### Comparing `nncf-2.4.0/nncf/onnx/graph/transformations/commands.py` & `nncf-2.5.0/nncf/openvino/graph/transformations/commands.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,121 +1,147 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
+
 import numpy as np
 
-from nncf.common.graph.transformations.commands import Command, TransformationCommand
-from nncf.common.graph.transformations.commands import TransformationType
-from nncf.common.graph.transformations.commands import TargetType
+from nncf.common.graph.transformations.commands import Command
 from nncf.common.graph.transformations.commands import TargetPoint
-from nncf.onnx.quantization.quantizer_parameters import ONNXQuantizerLayerParameters
+from nncf.common.graph.transformations.commands import TargetType
+from nncf.common.graph.transformations.commands import TransformationCommand
+from nncf.common.graph.transformations.commands import TransformationType
+from nncf.openvino.graph.node_utils import InplaceInsertionFnType
+from nncf.quantization.fake_quantize import FakeQuantizeParameters
 
 
-class ONNXTargetPoint(TargetPoint):
+class OVTargetPoint(TargetPoint):
     def __init__(self, target_type: TargetType, target_node_name: str, port_id: int):
         super().__init__(target_type)
         self.target_node_name = target_node_name
         self.port_id = port_id
 
-    def __eq__(self, other: 'ONNXTargetPoint') -> bool:
-        return isinstance(other, ONNXTargetPoint) and \
-               self.type == other.type and self.target_node_name == other.target_node_name and \
-               self.port_id == other.port_id
+    def __eq__(self, other: "OVTargetPoint") -> bool:
+        return (
+            isinstance(other, OVTargetPoint)
+            and self.type == other.type
+            and self.target_node_name == other.target_node_name
+            and self.port_id == other.port_id
+        )
 
     def __hash__(self) -> int:
         return hash((self.target_node_name, self.port_id, self._target_type))
 
-    def __lt__(self, other: 'ONNXTargetPoint') -> bool:
-        # The ONNXTargetPoint should have the way to compare.
-        # NNCF has to be able returning the Quantization Target Points in the deterministic way.
-        # MinMaxQuantizationAlgorithm returns the sorted Set of such ONNXTargetPoints.
-        params = ['_target_type', 'target_node_name', 'port_id']
-        for param in params:
-            if self.__getattribute__(param) < other.__getattribute__(param):
-                return True
-            if self.__getattribute__(param) > other.__getattribute__(param):
-                return False
-        return False
-
 
-class ONNXInsertionCommand(TransformationCommand):
-    def __init__(self, target_point: ONNXTargetPoint):
+class OVInsertionCommand(TransformationCommand):
+    def __init__(self, target_point: OVTargetPoint):
         super().__init__(TransformationType.INSERT, target_point)
 
-    def union(self, other: 'TransformationCommand') -> 'TransformationCommand':
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
         # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
         raise NotImplementedError()
 
 
-class ONNXQuantizerInsertionCommand(ONNXInsertionCommand):
-    def __init__(self, target_point: ONNXTargetPoint, quantizer_parameters: ONNXQuantizerLayerParameters):
+class OVOutputInsertionCommand(OVInsertionCommand):
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
+        # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
+        raise NotImplementedError()
+
+
+class OVInplaceFnInsertionCommand(OVInsertionCommand):
+    def __init__(self, target_point: OVTargetPoint, inplace_op_fn: InplaceInsertionFnType, fn_output_port_id: int):
         super().__init__(target_point)
-        self.quantizer_parameters = quantizer_parameters
+        self.inplace_op_fn = inplace_op_fn
+        self.fn_output_port_id = fn_output_port_id
+
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
+        # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
+        raise NotImplementedError()
+
+
+class OVFQNodeRemovingCommand(TransformationCommand):
+    """
+    Removes FakeQuantize nodes from the model.
+    """
 
-    def union(self, other: 'TransformationCommand') -> 'TransformationCommand':
+    def __init__(self, target_point: OVTargetPoint):
+        """
+        :param target_point: The TargetPoint instance for the layer that contains information for removing.
+        """
+        super().__init__(TransformationType.REMOVE, target_point)
+
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
         # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
         raise NotImplementedError()
 
 
-class ONNXOutputInsertionCommand(ONNXInsertionCommand):
-    def union(self, other: 'TransformationCommand') -> 'TransformationCommand':
+class OVQuantizerInsertionCommand(OVInsertionCommand):
+    def __init__(self, target_point: OVTargetPoint, quantizer_parameters: FakeQuantizeParameters):
+        super().__init__(target_point)
+        self.quantizer_parameters = quantizer_parameters
+
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
         # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
         raise NotImplementedError()
 
-class ONNXBiasCorrectionCommand(TransformationCommand):
+
+class OVBiasCorrectionCommand(TransformationCommand):
     """
     Corrects bias value in the model based on the input value.
     """
-    def __init__(self, target_point: ONNXTargetPoint, bias_value: np.ndarray):
+
+    def __init__(self, target_point: OVTargetPoint, bias_value: np.ndarray):
         """
         :param target_point: The TargetPoint instance for the correction that contains layer's information.
         :param bias_value: The bias shift value (numpy format) that will be added to the original bias value.
         """
         super().__init__(TransformationType.CHANGE, target_point)
         self.bias_value = bias_value
 
-    def union(self, other: 'TransformationCommand') -> 'TransformationCommand':
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
         # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
         raise NotImplementedError()
 
-class ONNXModelExtractionCommand(Command):
+
+class OVWeightUpdateCommand(TransformationCommand):
     """
-    Extracts sub-graph based on the sub-model input and output names.
+    Updates weight value in the model.
     """
-    def __init__(self, inputs: List[str], outputs: List[str]):
+
+    def __init__(self, target_point: OVTargetPoint, weight_value: np.ndarray):
         """
-        :param inputs: List of the input names that denote the sub-graph beggining.
-        :param outputs: List of the output names that denote the sub-graph ending.
+        :param target_point: Target point.
+        :param weight_value: New weight value.
         """
-        super().__init__(TransformationType.EXTRACT)
-        self.inputs = inputs
-        self.outputs = outputs
+        super().__init__(TransformationType.CHANGE, target_point)
+        self.weight_value = weight_value
 
-    def union(self, other: 'Command') -> 'Command':
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
         # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
         raise NotImplementedError()
 
-class ONNXQDQNodeRemovingCommand(TransformationCommand):
+
+class OVModelExtractionCommand(Command):
     """
-    Removes Quantizer or Dequantizer nodes from the model.
+    Extracts sub-graph based on the sub-model input and output names.
     """
 
-    def __init__(self, target_point: ONNXTargetPoint):
+    def __init__(self, inputs: List[str], outputs: List[str]):
         """
-        :param target_point: The TargetPoint instance for the layer that contains information for removing.
+        :param inputs: List of the input names that denote the sub-graph beggining.
+        :param outputs: List of the output names that denote the sub-graph ending.
         """
-        super().__init__(TransformationType.REMOVE, target_point)
+        super().__init__(TransformationType.EXTRACT)
+        self.inputs = inputs
+        self.outputs = outputs
 
-    def union(self, other: 'TransformationCommand') -> 'TransformationCommand':
+    def union(self, other: "Command") -> "Command":
         # Have a look at nncf/torch/graph/transformations/commands/PTInsertionCommand
         raise NotImplementedError()
```

### Comparing `nncf-2.4.0/nncf/onnx/hardware/config.py` & `nncf-2.5.0/nncf/tensorflow/hardware/config.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,24 +1,21 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List
-from typing import Type
+from typing import List, Type
 
 from nncf.common.graph import OperatorMetatype
 from nncf.common.hardware.config import HWConfig
-from nncf.onnx.graph.metatypes.onnx_metatypes import get_operator_metatypes
+from nncf.tensorflow.graph.metatypes.common import get_operator_metatypes
 
 
-class ONNXHWConfig(HWConfig):
+class TFHWConfig(HWConfig):
     def _get_available_operator_metatypes_for_matching(self) -> List[Type[OperatorMetatype]]:
         return get_operator_metatypes()
```

### Comparing `nncf-2.4.0/nncf/onnx/hardware/pattern_operations.py` & `nncf-2.5.0/nncf/onnx/hardware/pattern_operations.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,69 +1,79 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+from nncf.common.graph.patterns import GraphPattern
 from nncf.common.graph.patterns import merge_two_types_of_operations
-from nncf.common.graph.graph_matching import GraphPattern
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXAddLayerMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXBatchNormMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXConvolutionMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXDepthwiseConvolutionMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXConvolutionTransposeMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXLinearMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXMatMulMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXBatchNormMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXReluMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXLeakyReluMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXThresholdedReluMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXDeformableConvolutionMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXDepthwiseConvolutionMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXDivLayerMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXEluMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXPReluMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXSigmoidMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXHardSigmoidMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXHardSwishMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXAddLayerMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXLeakyReluMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXLinearMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXMatMulMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXMulLayerMetatype
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXDivLayerMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXPReluMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXReluMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXSigmoidMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXSubMetatype
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXThresholdedReluMetatype
 
-LINEAR_OPERATIONS = {GraphPattern.METATYPE_ATTR: [ONNXConvolutionMetatype,
-                                                  ONNXDepthwiseConvolutionMetatype,
-                                                  ONNXConvolutionTransposeMetatype,
-                                                  ONNXLinearMetatype,
-                                                  ONNXMatMulMetatype
-                                                  ],
-                     GraphPattern.LABEL_ATTR: 'LINEAR'}
-
-BATCH_NORMALIZATION_OPERATIONS = {GraphPattern.METATYPE_ATTR: [ONNXBatchNormMetatype],
-                                  GraphPattern.LABEL_ATTR: 'BATCH_NORMALIZATION'}
-
-RELU_OPERATIONS = {GraphPattern.METATYPE_ATTR: [ONNXReluMetatype,
-                                                ONNXLeakyReluMetatype,
-                                                ONNXThresholdedReluMetatype
-                                                ],
-                   GraphPattern.LABEL_ATTR: 'RELU'}
-
-NON_RELU_ACTIVATIONS_OPERATIONS = {GraphPattern.METATYPE_ATTR: [ONNXEluMetatype,
-                                                                ONNXPReluMetatype,
-                                                                ONNXSigmoidMetatype,
-                                                                ONNXHardSigmoidMetatype,
-                                                                ONNXHardSwishMetatype
-                                                                ],
-                                   GraphPattern.LABEL_ATTR: 'NON_RELU_ACTIVATIONS'}
-
-ATOMIC_ACTIVATIONS_OPERATIONS = merge_two_types_of_operations(RELU_OPERATIONS,
-                                                              NON_RELU_ACTIVATIONS_OPERATIONS,
-                                                              'ATOMIC_ACTIVATIONS')
-
-ARITHMETIC_OPERATIONS = {GraphPattern.METATYPE_ATTR: [ONNXAddLayerMetatype,
-                                                      ONNXSubMetatype,
-                                                      ONNXMulLayerMetatype,
-                                                      ONNXDivLayerMetatype,
-                                                      ],
-                         GraphPattern.LABEL_ATTR: 'ARITHMETIC'}
+LINEAR_OPERATIONS = {
+    GraphPattern.METATYPE_ATTR: [
+        ONNXConvolutionMetatype,
+        ONNXDepthwiseConvolutionMetatype,
+        ONNXConvolutionTransposeMetatype,
+        ONNXDeformableConvolutionMetatype,
+        ONNXLinearMetatype,
+        ONNXMatMulMetatype,
+    ],
+    GraphPattern.LABEL_ATTR: "LINEAR",
+}
+
+BATCH_NORMALIZATION_OPERATIONS = {
+    GraphPattern.METATYPE_ATTR: [ONNXBatchNormMetatype],
+    GraphPattern.LABEL_ATTR: "BATCH_NORMALIZATION",
+}
+
+RELU_OPERATIONS = {
+    GraphPattern.METATYPE_ATTR: [ONNXReluMetatype, ONNXLeakyReluMetatype, ONNXThresholdedReluMetatype],
+    GraphPattern.LABEL_ATTR: "RELU",
+}
+
+NON_RELU_ACTIVATIONS_OPERATIONS = {
+    GraphPattern.METATYPE_ATTR: [
+        ONNXEluMetatype,
+        ONNXPReluMetatype,
+        ONNXSigmoidMetatype,
+        ONNXHardSigmoidMetatype,
+        ONNXHardSwishMetatype,
+    ],
+    GraphPattern.LABEL_ATTR: "NON_RELU_ACTIVATIONS",
+}
+
+ATOMIC_ACTIVATIONS_OPERATIONS = merge_two_types_of_operations(
+    RELU_OPERATIONS, NON_RELU_ACTIVATIONS_OPERATIONS, "ATOMIC_ACTIVATIONS"
+)
+
+ARITHMETIC_OPERATIONS = {
+    GraphPattern.METATYPE_ATTR: [
+        ONNXAddLayerMetatype,
+        ONNXSubMetatype,
+        ONNXMulLayerMetatype,
+        ONNXDivLayerMetatype,
+    ],
+    GraphPattern.LABEL_ATTR: "ARITHMETIC",
+}
```

### Comparing `nncf-2.4.0/nncf/onnx/quantization/quantize.py` & `nncf-2.5.0/nncf/onnx/quantization/quantize_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,63 +1,70 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Optional
 
 import onnx
 
+from nncf.common.logging.logger import nncf_logger
+from nncf.common.quantization.structs import QuantizationPreset
 from nncf.data import Dataset
-from nncf.quantization.telemetry_extractors import CompressionStartedWithQuantizeApi
-from nncf.parameters import convert_ignored_scope_to_list
-from nncf.parameters import IgnoredScope
 from nncf.parameters import ModelType
 from nncf.parameters import TargetDevice
-from nncf.common.quantization.structs import QuantizationPreset
+from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters
 from nncf.quantization.algorithms.post_training.algorithm import PostTrainingQuantization
-from nncf.quantization.algorithms.post_training.algorithm  import PostTrainingQuantizationParameters
+from nncf.quantization.telemetry_extractors import CompressionStartedWithQuantizeApi
+from nncf.scopes import IgnoredScope
 from nncf.telemetry import tracked_function
 from nncf.telemetry.events import NNCF_ONNX_CATEGORY
 
 
 @tracked_function(NNCF_ONNX_CATEGORY, [CompressionStartedWithQuantizeApi(), "target_device", "preset"])
-def quantize_impl(model: onnx.ModelProto,
-                  calibration_dataset: Dataset,
-                  preset: QuantizationPreset,
-                  target_device: TargetDevice,
-                  subset_size: int,
-                  fast_bias_correction: bool,
-                  model_type: Optional[ModelType] = None,
-                  ignored_scope: Optional[IgnoredScope] = None) -> onnx.ModelProto:
+def quantize_impl(
+    model: onnx.ModelProto,
+    calibration_dataset: Dataset,
+    preset: QuantizationPreset,
+    target_device: TargetDevice,
+    subset_size: int,
+    fast_bias_correction: bool,
+    model_type: Optional[ModelType] = None,
+    ignored_scope: Optional[IgnoredScope] = None,
+    advanced_parameters: Optional[AdvancedQuantizationParameters] = None,
+) -> onnx.ModelProto:
     """
     Implementation of the `quantize()` method for the ONNX backend.
     """
-    if model_type is not None:
-        raise ValueError(f'model_type={model_type} is not supported')
-    if ignored_scope is not None and ignored_scope.types is not None:
-        raise RuntimeError('Quantization algorithm from the ONNX backend '
-                           'does not support operation types in the ignored '
-                           'scopes yet')
     if target_device == TargetDevice.CPU_SPR:
-        raise RuntimeError('target_device == CPU_SPR is not supported.')
+        raise RuntimeError("target_device == CPU_SPR is not supported.")
+    if model.opset_import[0].version < 10:
+        raise RuntimeError("ONNX models with opset version < 10 do not support quantization.")
+    if model.opset_import[0].version < 13:
+        nncf_logger.warning(
+            "ONNX models with 10 < opset version < 13 do not support per-channel quantization."
+            " Per-tensor quantization will be applied."
+        )
+        if advanced_parameters is None:
+            advanced_parameters = AdvancedQuantizationParameters()
+        advanced_parameters.weights_quantization_params.per_channel = False
+        advanced_parameters.activations_quantization_params.per_channel = False
 
-    quantization_parameters = PostTrainingQuantizationParameters(
+    quantization_algorithm = PostTrainingQuantization(
         preset=preset,
         target_device=target_device,
-        number_samples=subset_size,
-        ignored_scopes=convert_ignored_scope_to_list(ignored_scope),
-        fast_bias_correction=fast_bias_correction
+        subset_size=subset_size,
+        ignored_scope=ignored_scope,
+        fast_bias_correction=fast_bias_correction,
+        model_type=model_type,
+        advanced_parameters=advanced_parameters,
     )
 
-    quantization_algorithm = PostTrainingQuantization(quantization_parameters)
     quantized_model = quantization_algorithm.apply(model, dataset=calibration_dataset)
 
     return quantized_model
```

### Comparing `nncf-2.4.0/nncf/onnx/quantization/quantizer_parameters.py` & `nncf-2.5.0/nncf/onnx/quantization/quantizer_parameters.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,139 +1,105 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Optional, Tuple
 from dataclasses import dataclass
+from typing import Optional, Tuple
+
 import numpy as np
-from nncf.common.quantization.structs import QuantizationMode
-from nncf.common.quantization.structs import QuantizerConfig
-from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
+
+from nncf.quantization.fake_quantize import FakeQuantizeParameters
 
 
 @dataclass
 class ONNXQuantizerLayerParameters:
     """
     Class handles Quantizer layer attributes.
 
     :param scale: Quantizer scale.
     :param zero_point: Quantizer zero point.
-    :param mode: Quantizer mode. Could be Symmetric or Asymmetric.
-    :param axis: Axis for per-channel quantization. Should be none in case of per-tensor.
     :param tensor_type: Signed or Unsigned tensor type.
+    :param axis: Axis for per-channel quantization. Should be none in case of per-tensor.
     """
+
     scale: np.ndarray
     zero_point: np.ndarray
-    mode: QuantizationMode
+    tensor_type: np.dtype
     axis: Optional[int] = None
-    tensor_type: Optional[np.dtype] = None
 
 
-def get_level_low_level_high(tensor_type: np.dtype, num_bits: int) -> Tuple[int, int]:
+def convert_fq_params_to_onnx_params(
+    parameters: FakeQuantizeParameters, num_bits: int, tensor_type: np.dtype, axis: Optional[int] = None
+) -> ONNXQuantizerLayerParameters:
+    """
+    Converts common FakeQuantizeParameters to ONNXQuantizerLayerParameters.
+
+    :param parameters: FakeQuantizeParameters representation.
+    :param num_bits: Number of quantizer bits.
+    :param tensor_type: Value type of the tensor. Could be INT8 or UINT8.
+    :param axis: Axis for per-channel quantization. Should be none in case of per-tensor.
+    :return: Quantizer layer attributes.
+    """
+    if num_bits != 8:
+        raise ValueError("Can only export to INT8/UIN8 8 bits ONNX Quantize/Dequantize pairs.")
+
+    levels = parameters.levels
+    if levels not in [255, 256]:
+        raise ValueError("Can only export to INT8/UIN8 256-level ONNX Quantize/Dequantize pairs.")
+
+    input_low, input_high = parameters.input_low, parameters.input_high
+    output_low, output_high = parameters.output_low, parameters.output_high
+    if not np.allclose(input_high, output_high) or not np.allclose(input_low, output_low):
+        raise ValueError(
+            "ONNX Quantize/Dequantize pairs only support input_high == output_high and input_low == output_low."
+        )
+
+    level_low, level_high = get_level_low_level_high(tensor_type)
+    narrow_range = levels == 2**num_bits - 1
+    scale, zero_point = calculate_scale_zero_point(input_low, input_high, level_low, level_high, narrow_range)
+    return ONNXQuantizerLayerParameters(scale, zero_point, tensor_type, axis)
+
+
+def get_level_low_level_high(tensor_type: np.dtype) -> Tuple[int, int]:
     """
     Returns the minimum and maximum level for the quantizer.
     In ONNX opset Q/DequantizeLinear-13 uses only two levels: [-128, 127] and [0, 255].
 
     :param tensor_type: Value type of the tensor. Could be INT8 or UINT8.
-    :param num_bits: Number of quantizer bits.
     :return: Minimum level and maximum level of the quantizer.
     """
-    if tensor_type == np.uint8:
-        return 0, 2 ** num_bits - 1
-    return - (2 ** num_bits) // 2, (2 ** num_bits) // 2 - 1
+    return (0, 255) if tensor_type == np.uint8 else (-128, 127)
 
 
-def calculate_scale_zero_point(max_val: np.ndarray, min_val: np.ndarray, level_low: int, level_high: int,
-                               mode: QuantizationMode, tensor_type: np.dtype) -> Tuple[np.ndarray, np.ndarray]:
+def calculate_scale_zero_point(
+    input_low: np.ndarray, input_high: np.ndarray, level_low: int, level_high: int, narrow_range: bool
+) -> Tuple[np.ndarray, np.ndarray]:
     """
     Calculates Quantizer/Dequantizer layer scale level.
-    Returns scale and zero_point values foe the quantizer.
+    Returns scale and zero_point values for the quantizer.
 
-    :param max_val: The maximum value of the input tensor.
-    :param min_val: The minimum value of the input tensor.
-    :param level_low: The minimum level of quantizer quants.
-    :param level_high: The maximum level of quantizer quants.
-    :param mode: Symmetric or Asymmetric mode.
-    :param tensor_type: Value type of the tensor. Could be INT8 or UINT8.
+    :param input_low: The minimum limit for an input value based on collected statistics.
+    :param input_high: The maximum limit for an input value based on collected statistics.
+    :param level_low: The minimum level in the integer range to quantize.
+        The default is "0" for an unsigned range, and "-2^(bit-1)" for a signed one .
+    :param level_high: The maximum level in the integer range to quantize.
+        The default is "2^bits-1" for an unsigned range, and "2^(bit-1)-1" for a signed one.
+    :param narrow_range: True if the range of quantized values is narrowed as compared to the
+        naive case, False otherwise.
     :return: Scale and Zero point values.
     """
-    scale, zero_point = None, None
-    if mode == QuantizationMode.SYMMETRIC:
-        input_abs_max = np.maximum(np.abs(max_val), np.abs(min_val))
-        max_val = input_abs_max
-        min_val = np.zeros_like(input_abs_max) if tensor_type == np.uint8 else -input_abs_max
-        scale = np.array((max_val - min_val) / float(level_high - level_low))
-        zero_point = np.zeros_like(scale, dtype=np.int32)
-    elif mode == QuantizationMode.ASYMMETRIC:
-        scale = np.array((max_val - min_val) / float(level_high - level_low))
-        zero_point = np.round(level_low - min_val / scale).astype(np.int32)
-
-        level_low *= np.ones_like(zero_point, dtype=np.int32)
-        level_high *= np.ones_like(zero_point, dtype=np.int32)
-
-        zero_point = np.maximum(zero_point, level_low)
-        zero_point = np.minimum(zero_point, level_high)
+    levels = level_high - level_low if narrow_range else level_high - level_low + 1
+    scale = np.array((input_high - input_low) / (levels - 1))
+    expected_level_low = level_low + 1 if narrow_range else level_low
+    zero_point = expected_level_low - np.round(input_low / scale)
+    zero_point = np.minimum(np.maximum(zero_point.astype(np.int32), level_low), level_high)
+    scale = np.array(np.squeeze(scale).astype(np.float32))
+    zero_point = np.array(np.squeeze(zero_point))
 
     return scale, zero_point
-
-
-def calculate_weight_quantizer_parameters(weight_tensor: np.ndarray, quantizer_config: QuantizerConfig,
-                                          axis: Optional[int]) -> ONNXQuantizerLayerParameters:
-    """
-    Calculates Quantizer/Dequantizer layer attributes for weight quantizer such as scale, zero_points and
-    quantization mode: symmetric, asymmetric.
-
-    :param weight_tensor: Weight tensor to calculate quantizer attributes.
-    :param quantizer_config: Config of Quantizer.
-    :param axis: In per-channel case - the axis for the quantization. In per-tensor - ignored.
-    :return: Parameters of Quantizer.
-    """
-    if quantizer_config.per_channel:
-        assert axis is not None
-        axes = list(range(len(weight_tensor.shape)))
-        axes.pop(axis)
-        axes = tuple(axes)
-    else:
-        axes = None
-    # The weight is restricted to have only signed range.
-    tensor_type = np.int8
-    if quantizer_config.signedness_to_force is not None and not quantizer_config.signedness_to_force:
-        raise ValueError('The HW expects to have signed quantization of weights, '
-                         'while the quantizer configuration for weights contains signedness_to_force=False.')
-    input_high = np.amax(weight_tensor, axis=axes)
-    input_low = np.amin(weight_tensor, axis=axes)
-    level_low, level_high = get_level_low_level_high(tensor_type, quantizer_config.num_bits)
-    scales, zero_points = calculate_scale_zero_point(input_high, input_low, level_low, level_high,
-                                                     quantizer_config.mode, tensor_type)
-    return ONNXQuantizerLayerParameters(scales, zero_points, quantizer_config.mode, axis, tensor_type)
-
-
-def calculate_activation_quantizer_parameters(statistics: MinMaxTensorStatistic,
-                                              quantizer_config: QuantizerConfig,
-                                              axis: Optional[int] = None) -> ONNXQuantizerLayerParameters:
-    """
-    Calculates Quantizer/Dequantizer layer attributes for activation quantizer such as scale, zero_points and
-    quantization mode: symmetric, asymmetric.
-
-    :param statistics: Collected statistics for the quantized insertion.
-    :param quantizer_config: Config of the quantization configuration.
-    :param axis: Axis of the quantization. None in a per-tensor quantization case.
-    :return: Parameters of the quantizer/dequantizer layers.
-    """
-    input_low = np.array(statistics.min_values)
-    input_high = np.array(statistics.max_values)
-    tensor_type = np.uint8 if np.all(input_low >= 0) else np.int8
-    if quantizer_config.signedness_to_force is not None:
-        tensor_type = np.int8 if quantizer_config.signedness_to_force else np.uint8
-    level_low, level_high = get_level_low_level_high(tensor_type, quantizer_config.num_bits)
-    scales, zero_points = calculate_scale_zero_point(input_high, input_low, level_low, level_high,
-                                                     quantizer_config.mode, tensor_type)
-    return ONNXQuantizerLayerParameters(scales, zero_points, quantizer_config.mode, axis, tensor_type)
```

### Comparing `nncf-2.4.0/nncf/onnx/statistics/statistics.py` & `nncf-2.5.0/nncf/openvino/statistics/statistics.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,36 +1,34 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import numpy as np
 
-from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
-from nncf.common.tensor_statistics.statistics import MeanTensorStatistic
 from nncf.common.tensor_statistics.statistics import BatchTensorStatistic
+from nncf.common.tensor_statistics.statistics import MeanTensorStatistic
+from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
 
 
-class ONNXMinMaxTensorStatistic(MinMaxTensorStatistic):
+class OVMinMaxTensorStatistic(MinMaxTensorStatistic):
     @staticmethod
     def tensor_eq(tensor1: np.ndarray, tensor2: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.allclose(tensor1, tensor2, rtol=rtol))
 
 
-class ONNXMeanTensorStatistic(MeanTensorStatistic):
+class OVMeanTensorStatistic(MeanTensorStatistic):
     @staticmethod
     def tensor_eq(tensor: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.all(tensor, rtol=rtol))
 
 
-class ONNXBatchTensorStatistic(BatchTensorStatistic):
+class OVBatchTensorStatistic(BatchTensorStatistic):
     @staticmethod
     def tensor_eq(tensor: np.ndarray, rtol=1e-6) -> bool:
         return bool(np.all(tensor, rtol=rtol))
```

### Comparing `nncf-2.4.0/nncf/onnx/tensor.py` & `nncf-2.5.0/nncf/onnx/tensor.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import numpy as np
 
 from nncf.common.tensor import NNCFTensor
 
 
 class ONNXNNCFTensor(NNCFTensor):
@@ -22,8 +20,8 @@
     """
 
     def __init__(self, tensor: np.ndarray):
         super().__init__(tensor)
 
     @property
     def device(self):
-        return 'CPU'
+        return "CPU"
```

### Comparing `nncf-2.4.0/nncf/openvino/engine.py` & `nncf-2.5.0/nncf/openvino/pot/engine.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,29 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Optional
-from typing import Callable
-from typing import Iterable
-from typing import Any
-from typing import List
-from typing import Dict
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Any, Callable, Dict, Iterable, List, Optional
 
-from openvino.tools import pot
 import openvino.runtime as ov
+from openvino.tools import pot
 
 from nncf.api.compression import TModel
 from nncf.data import Dataset
 from nncf.data.dataset import DataProvider
 
 
 # TODO(andrey-churkin): This class should be removed after refactoring of the OVEngine class.
@@ -33,29 +26,29 @@
     Dummy implementation of the pot.Metric abstract class. It is used for compatibility
     with the POT API. All docstring for the methods can be
     found [here](https://docs.openvino.ai/latest/pot_compression_api_README.html#metric).
     """
 
     def __init__(self):
         self._avg_value = None
-        self.name = 'original_metric'
+        self.name = "original_metric"
 
     @property
     def avg_value(self):
         return {self.name: self._avg_value}
 
     @avg_value.setter
     def avg_value(self, value):
         self._avg_value = value
 
     def get_attributes(self):
         attributes = {
             self.name: {
-                'direction': 'higher-better',
-                'type': self.name,
+                "direction": "higher-better",
+                "type": self.name,
             }
         }
         return attributes
 
     def reset(self):
         self.avg_value = None
 
@@ -95,109 +88,104 @@
 
         length = 0
         for _ in iterable:
             length = length + 1
         return length
 
 
-def calc_per_sample_metrics(compiled_model: ov.CompiledModel,
-                            val_func: Callable[[ov.CompiledModel, Iterable[Any]], float],
-                            dataset: Dataset,
-                            subset_indices: List[int]) -> List[Dict[str, Any]]:
+def calc_per_sample_metrics(
+    compiled_model: ov.CompiledModel,
+    val_func: Callable[[ov.CompiledModel, Iterable[Any]], float],
+    dataset: Dataset,
+    subset_indices: List[int],
+) -> List[Dict[str, Any]]:
     per_sample_metrics = []
     for inputs in dataset.get_data(subset_indices):
         value = val_func(compiled_model, [inputs])
-        per_sample_metrics.append({
-            'sample_id': len(per_sample_metrics),
-            'metric_name': 'original_metric',
-            'result': value
-        })
+        per_sample_metrics.append(
+            {"sample_id": len(per_sample_metrics), "metric_name": "original_metric", "result": value}
+        )
     return per_sample_metrics
 
 
 # TODO(andrey-churkin): This class should be refactored. We will be able to do that when
 # we will have POT code in NNCF.
 class OVEngine(pot.IEEngine):
     """
     Implementation of the engine for OpenVINO backend.
 
     All docstring for the methods can be found
     [here](https://docs.openvino.ai/latest/pot_compression_api_README.html#engine).
     """
 
-    def __init__(self,
-                 config,
-                 calibration_dataset: Dataset,
-                 validation_dataset: Dataset,
-                 validation_fn: Optional[Callable[[TModel, Iterable[Any]], float]] = None,
-                 use_original_metric: bool = False):
+    def __init__(
+        self,
+        config,
+        calibration_dataset: Dataset,
+        validation_dataset: Dataset,
+        validation_fn: Optional[Callable[[TModel, Iterable[Any]], float]] = None,
+        use_original_metric: bool = False,
+    ):
         metric = DummyMetric() if validation_fn else None
         super().__init__(config, DatasetWrapper(validation_dataset.get_inference_data()), metric)
         self._calibration_dataset = calibration_dataset  # TODO(andrey-churkin): Not used now.
         self._validation_dataset = validation_dataset
         self._validation_fn = validation_fn
         self.use_original_metric = use_original_metric
+        self.statistics_collection = False
 
-    def predict(self,
-                stats_layout=None,
-                sampler=None,
-                stat_aliases=None,
-                metric_per_sample=False,
-                print_progress=False):
-
+    def predict(
+        self, stats_layout=None, sampler=None, stat_aliases=None, metric_per_sample=False, print_progress=False
+    ):
         if self._model is None:
-            raise Exception('Model was not set in Engine class')
+            raise Exception("Model was not set in Engine class")
 
         subset_indices = None
         if sampler:
-            subset_indices = sorted(getattr(sampler, '_subset_indices'))
+            subset_indices = sorted(getattr(sampler, "_subset_indices"))
 
         is_full_dataset = subset_indices is None or len(subset_indices) == len(self.data_loader)
-        if self._validation_fn and (is_full_dataset or self.use_original_metric):
+        if self._validation_fn and (is_full_dataset or self.use_original_metric) and not self.statistics_collection:
             compiled_model = self._ie.compile_model(model=self._model, device_name=self._device)
-            self._metric.avg_value = self._validation_fn(compiled_model,
-                                                         self._validation_dataset.get_data(subset_indices))
+            self._metric.avg_value = self._validation_fn(
+                compiled_model, self._validation_dataset.get_data(subset_indices)
+            )
             if not metric_per_sample and stats_layout is None:
                 metrics = self._metric.avg_value
                 self._reset()
                 return metrics, {}
 
         if self.use_original_metric and metric_per_sample:
             self._per_sample_metrics = self.calculate_per_sample_metrics(subset_indices)
             if stats_layout is None:
                 metrics = self._metric.avg_value
-                metrics = (sorted(self._per_sample_metrics, key=lambda i: i['sample_id']), metrics)
+                metrics = (sorted(self._per_sample_metrics, key=lambda i: i["sample_id"]), metrics)
                 self._reset()
                 return metrics, {}
 
         dataset_wrapper = DatasetWrapper(self._validation_dataset.get_inference_data(subset_indices))
         return super().predict(stats_layout, dataset_wrapper, stat_aliases, metric_per_sample, print_progress)
 
     def calculate_per_sample_metrics(self, subset_indices: List[int]):
         if subset_indices is None:
             subset_indices = list(range(len(self.data_loader)))
 
         if self.use_original_metric:
             compiled_model = self._ie.compile_model(model=self._model, device_name=self._device)
-            per_sample_metrics = calc_per_sample_metrics(compiled_model,
-                                                         self._validation_fn,
-                                                         self._validation_dataset,
-                                                         subset_indices)
+            per_sample_metrics = calc_per_sample_metrics(
+                compiled_model, self._validation_fn, self._validation_dataset, subset_indices
+            )
         else:
             dataset_wrapper = DatasetWrapper(self._validation_dataset.get_inference_data(subset_indices))
             ans = super().predict(None, dataset_wrapper, None, True, None)
-            (per_sample_metrics, _), *_  = ans
+            (per_sample_metrics, _), *_ = ans
         return per_sample_metrics
 
     def _update_metrics(self, output, annotations, need_metrics_per_sample: bool = False):
         if need_metrics_per_sample and not self.use_original_metric:
             sample_id, _ = annotations[0]
-            self._per_sample_metrics.append({
-                    'sample_id': sample_id,
-                    'result': output[0],
-                    'metric_name': 'nmse'
-            })
+            self._per_sample_metrics.append({"sample_id": sample_id, "result": output[0], "metric_name": "nmse"})
 
     @staticmethod
     def _process_batch(batch):
         index, input_data = batch
         return [(index, None)], [input_data], None
```

### Comparing `nncf-2.4.0/nncf/openvino/quantization/accuracy_aware.py` & `nncf-2.5.0/nncf/openvino/pot/quantization/accuracy_aware.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,24 +1,21 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import List
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from functools import partial
+from typing import List
 
 import numpy as np
-
 from openvino.tools import pot
 
 
 # TODO(andrey-churkin): Should be removed after OpenVINO release.
 # pylint: disable=E1101
 class NMSEBasedAccuracyAware(pot.AccuracyAwareCommon):
     """
@@ -29,56 +26,66 @@
     When it is not possible NMSE metric is used to select ranking subset and
     calculate node importance.
     """
 
     def __init__(self, config, engine):
         super().__init__(config, engine)
         if not engine.use_original_metric:
-            self._metrics_config['original_metric'].persample.clear()
-            self._metrics_config['original_metric'].persample.update(
+            self._metrics_config["original_metric"].persample.clear()
+            self._metrics_config["original_metric"].persample.update(
                 {
-                    "name": 'nmse',
-                    "type": 'nmse',
+                    "name": "nmse",
+                    "type": "nmse",
                     "is_special": True,
                     "comparator": lambda a: -a,
                     "sort_fn": partial(
-                        pot.algorithms.quantization.accuracy_aware_common.utils.sort_by_logit_distance,
-                        distance='nmse')
+                        pot.algorithms.quantization.accuracy_aware_common.utils.sort_by_logit_distance, distance="nmse"
+                    ),
                 }
             )
-            self._metrics_config['original_metric'].ranking.clear()
-            self._metrics_config['original_metric'].ranking.update(
+            self._metrics_config["original_metric"].ranking.clear()
+            self._metrics_config["original_metric"].ranking.update(
                 {
                     "name": "nmse",
                     "type": "nmse",
                     "is_special": True,
                     "comparator": lambda a: -a,
-                    "sort_fn": partial(pot.algorithms.quantization.accuracy_aware_common.utils.sort_by_logit_distance,
-                                       distance='nmse')
+                    "sort_fn": partial(
+                        pot.algorithms.quantization.accuracy_aware_common.utils.sort_by_logit_distance, distance="nmse"
+                    ),
                 }
             )
+        self._need_intermediate_model = False
 
     def _get_score(self, model, ranking_subset: List[int], metric_name: str) -> float:
         if self._engine.use_original_metric:
             score = super()._get_score(model, ranking_subset, metric_name)
         else:
             ranking_metric = self._metrics_config[metric_name].ranking
-            original_outputs = [
-                (i, self._original_per_sample_metrics[ranking_metric.name][i]) for i in ranking_subset
-            ]
+            original_outputs = [(i, self._original_per_sample_metrics[ranking_metric.name][i]) for i in ranking_subset]
             _, original_outputs = zip(*sorted(original_outputs, key=lambda x: x[0]))
 
             self._engine.set_model(model)
             per_sample_metrics = self._engine.calculate_per_sample_metrics(ranking_subset)
-            quantized_outputs = [y['result'] for y in per_sample_metrics]
+            quantized_outputs = [y["result"] for y in per_sample_metrics]
 
             nmse_distance = lambda u, v: np.dot(u - v, u - v) / np.dot(u, u)
             distance_between_samples = [
                 nmse_distance(ui.flatten(), vi.flatten()) for ui, vi in zip(original_outputs, quantized_outputs)
             ]
             score = np.mean(distance_between_samples).item()
             score = ranking_metric.comparator(score)
         return score
 
     def _calculate_per_sample_metrics(self, model, subset_indices):
         self._engine.set_model(model)
         return self._engine.calculate_per_sample_metrics(subset_indices)
+
+    def _quantize_model(self, model):
+        self._engine.statistics_collection = True
+        quantized_model = super()._quantize_model(model)
+        self._engine.statistics_collection = False
+        return quantized_model
+
+    def _save_intermediate_model(self, model):
+        if self._need_intermediate_model:
+            super()._save_intermediate_model(model)
```

### Comparing `nncf-2.4.0/nncf/openvino/quantization/quantize.py` & `nncf-2.5.0/nncf/torch/compression_method_api.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,207 +1,232 @@
+#
+#  Copyright (c) 2019-2023 Intel Corporation
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#       http://www.apache.org/licenses/LICENSE-2.0
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+#
+
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+@package docstring
+This package defines the API for the NNCF compression methods so that the user could
+extend the existing algorithms.
 """
+from abc import abstractmethod
+from typing import Any, Dict, List, Tuple, TypeVar
 
-import logging
-import tempfile
-from pathlib import Path
-from typing import Dict
-from typing import Optional
-from typing import Callable
-from typing import Iterable
-from typing import Any
-
-import openvino.runtime as ov
-from openvino.tools import pot
-
-from nncf.data import Dataset
-from nncf.quantization.telemetry_extractors import CompressionStartedWithQuantizeApi
-from nncf.parameters import IgnoredScope
-from nncf.parameters import ModelType
-from nncf.parameters import TargetDevice
-from nncf.common.quantization.structs import QuantizationPreset
-from nncf.common.logging import nncf_logger
-from nncf.openvino.engine import OVEngine
-from nncf.openvino.quantization.accuracy_aware import NMSEBasedAccuracyAware
-from nncf.telemetry import tracked_function
-from nncf.telemetry.events import NNCF_OV_CATEGORY
-
-
-def _convert_openvino_model_to_compressed_model(model: ov.Model,
-                                                target_device: str) -> pot.graph.nx_model.CompressedModel:
-    """
-    Serializes the provided OpenVINO model and loads the model in the POT representation.
-
-    :param model: The OpenVINO model.
-    :param target_device: The target device.
-    :return: The POT representation of the provided model.
-    """
-    with tempfile.TemporaryDirectory(dir=tempfile.gettempdir()) as tmp_dir:
-        xml_path = str(Path(tmp_dir) / 'model.xml')
-        bin_path = str(Path(tmp_dir) / 'model.bin')
-        ov.serialize(model, xml_path, bin_path)
-        model_config = {
-            'model_name': 'model',
-            'model': xml_path,
-            'weights': bin_path,
-        }
-        pot_model = pot.load_model(model_config, target_device)
-
-    return pot_model
-
-
-def _convert_compressed_model_to_openvino_model(model: pot.graph.nx_model.CompressedModel) -> ov.Model:
-    """
-    Saves the provided POT compressed model and loads it as `openvino.runtime.Model` object.
-
-    :param model: The POT compressed model.
-    :return: The `openvino.runtime.Model`  object which represents the provided model.
-    """
-    with tempfile.TemporaryDirectory(dir=tempfile.gettempdir()) as tmp_dir:
-        paths = pot.save_model(model, save_path=tmp_dir, model_name='model')
-        xml_path = paths[0]['model']
-        bin_path = paths[0]['weights']
-        ie = ov.Core()
-        ov_model = ie.read_model(xml_path, bin_path)
-    return ov_model
-
-
-def _create_ignored_scope_config(ignored_scope: Optional[IgnoredScope]) -> Dict:
-    """
-    Maps the content of `IgnoredScope` class to the `ignored` section of POT config.
+import torch
+from torch import nn
 
-    :param ignored_scope: The ignored scope
-    :return: A POT ignored scope configuration as dict
-    """
-    if ignored_scope is None:
+from nncf.api.compression import CompressionLoss
+from nncf.common.compression import BaseCompressionAlgorithmBuilder
+from nncf.common.compression import BaseCompressionAlgorithmController
+from nncf.common.graph import NNCFNodeName
+from nncf.common.logging import nncf_logger
+from nncf.common.scopes import check_scopes_in_graph
+from nncf.common.scopes import should_consider_scope
+from nncf.config import NNCFConfig
+from nncf.torch.graph.transformations.layout import PTTransformationLayout
+from nncf.torch.layers import NNCF_MODULES_DICT
+from nncf.torch.layers import NNCF_WRAPPED_USER_MODULES_DICT
+from nncf.torch.nncf_network import NNCFNetwork
+from nncf.torch.nncf_network import PTModelTransformer
+
+TModel = TypeVar("TModel")
+
+DOMAIN_CUSTOM_OPS_NAME = "org.openvinotoolkit"
+
+
+class PTCompressionLoss(nn.Module, CompressionLoss):
+    """
+    Used to calculate additional loss to be added to the base loss during the
+    training process. It uses the model graph to measure variables and activations
+    values of the layers during the loss construction. For example, the $L_0$-based
+    sparsity algorithm calculates the number of non-zero weights in convolutional
+    and fully-connected layers to construct the loss function.
+    """
+
+    def calculate(self) -> torch.Tensor:
+        """
+        Calculates the compression loss value.
+
+        :return: The compression loss value.
+        """
+        return torch.zeros([])
+
+    def forward(self) -> torch.Tensor:
+        """
+        Overriding  forward function of the base nn.Module class
+
+        :return: The compression loss value.
+        """
+        return self.calculate()
+
+    def load_state(self, state: Dict[str, Any]) -> None:
+        """
+        Loads the compression loss state.
+
+        :param state: Output of `get_state()` method.
+        """
+
+    def get_state(self) -> None:
+        """
+        Returns the compression loss state.
+
+        :return: The compression loss state.
+        """
+
+
+class PTCompressionAlgorithmController(BaseCompressionAlgorithmController):
+    """
+    Serves as a handle to the additional modules, parameters and hooks inserted
+    into the original uncompressed model in order to enable algorithm-specific compression.
+    Hosts entities that are to be used during the training process, such as compression scheduler and
+    compression loss.
+    """
+
+    def distributed(self):
+        """
+        Should be called when distributed training with multiple training processes
+        is going to be used (i.e. after the model is wrapped with DistributedDataParallel).
+        Any special preparations for the algorithm to properly support distributed training
+        should be made inside this function.
+        """
+
+    def prepare_for_export(self) -> None:
+        # For Torch models no need to call strip_model
+        pass
+
+
+class PTCompressionAlgorithmBuilder(BaseCompressionAlgorithmBuilder):
+    """
+    Determines which modifications should be made to the original FP32 model in
+    order to enable algorithm-specific compression during fine-tuning. Operates
+    on an NNCFNetwork object wrapping a target PyTorch model (torch.nn.Module).
+    """
+
+    def __init__(self, config: NNCFConfig, should_init: bool = True):
+        """
+        Arguments:
+          `config` - a dictionary that contains parameters of compression method
+          `should_init` - if False, trainable parameter initialization will be skipped during building
+        """
+        super().__init__(config, should_init)
+        self.compressed_nncf_module_names = self._nncf_module_types_to_compress()
+
+    def apply_to(self, model: NNCFNetwork) -> NNCFNetwork:
+        transformer = PTModelTransformer(model)
+        transformation_layout = self.get_transformation_layout(model)
+        transformed_model = transformer.transform(transformation_layout)
+
+        if self.should_init:
+            self.initialize(transformed_model)
+
+        return transformed_model
+
+    def get_transformation_layout(self, model: NNCFNetwork) -> PTTransformationLayout:
+        """
+        Applies algorithm-specific modifications to the model. Hooks to be executed during model
+        forward operation may be registered using NNCFNetwork command insertion methods. Additional
+        compression modules that are expected to be saved along with the network via torch.save should also be
+        registered and added to the model here.
+        :param model: An instance of NNCFNetwork for the algorithm to be applied to.
+        :return: NNCFNetwork with algorithm-specific modifications applied
+        """
+        check_scopes_in_graph(model.nncf.get_original_graph(), self.ignored_scopes, self.target_scopes)
+
+        layout = self._get_transformation_layout(model)
+        self._handle_frozen_layers(model)
+        return layout
+
+    @abstractmethod
+    def _build_controller(self, model: TModel) -> PTCompressionAlgorithmController:
+        """
+        Simple implementation of building controller without setting builder state and loading controller's one.
+
+        :param model: The model with additional modifications necessary to enable
+            algorithm-specific compression during fine-tuning.
+        :return: The instance of the `BaseCompressionAlgorithmController`.
+        """
+
+    def build_controller(self, model: TModel) -> PTCompressionAlgorithmController:
+        """
+        Builds `PTCompressionAlgorithmController` to handle the additional modules,
+        parameters, and hooks inserted into the model to enable algorithm-specific
+        compression. Registers the built controller in the model's NNCFNetworkInterface.
+
+        :param model: The model with additional modifications necessary to enable
+            algorithm-specific compression during fine-tuning.
+        :return: The instance of the `PTCompressionAlgorithmController`.
+        """
+        ctrl = self._build_controller(model)
+        if not isinstance(ctrl, PTCompressionAlgorithmController):
+            raise RuntimeError(
+                "Internal error: builder must create controller inherited from "
+                "`PTCompressionAlgorithmController` class"
+            )
+        ctrl.set_builder_state_with_name(self.name, self.get_state())
+        return ctrl
+
+    def _get_state_without_name(self) -> Dict[str, Any]:
+        """
+        Implementation of get_state that returns state without builder name.
+
+        :return: Returns a dictionary with Python data structures
+            (dict, list, tuple, str, int, float, True, False, None) that represents state of the object.
+        """
         return {}
 
-    ignored = {}
-    if ignored_scope.names is not None:
-        ignored['scope'] = ignored_scope.names
-    if ignored_scope.patterns is not None:
-        raise RuntimeError('Quantization algorithm from the OpenVINO backend '
-                           'does not support regular expressions in the ignored '
-                           'scopes yet')
-    if ignored_scope.types is not None:
-        ignored['operations'] = [{'type': type} for type in ignored_scope.types]
-    return ignored
-
-
-@tracked_function(NNCF_OV_CATEGORY, [CompressionStartedWithQuantizeApi(), "target_device", "preset"])
-def quantize_impl(model: ov.Model,
-                  calibration_dataset: Dataset,
-                  preset: QuantizationPreset,
-                  target_device: TargetDevice,
-                  subset_size: int,
-                  fast_bias_correction: bool,
-                  model_type: Optional[ModelType] = None,
-                  ignored_scope: Optional[IgnoredScope] = None) -> ov.Model:
-    """
-    Implementation of the `quantize()` method for the OpenVINO backend.
-    """
-    pot.utils.logger.init_logger(
-        level=logging.getLevelName(nncf_logger.getEffectiveLevel())
-    )
-
-    algorithms = [
-        {
-            'name': 'DefaultQuantization',
-            'params': {
-                'target_device': target_device.value,
-                'preset': preset.value,
-                'stat_subset_size': subset_size,
-                'use_fast_bias': fast_bias_correction,
-                'model_type': None if model_type is None else model_type.value,
-                'ignored': _create_ignored_scope_config(ignored_scope)
-            }
-        }
-    ]
-
-    pot_model = _convert_openvino_model_to_compressed_model(model, target_device)
-
-    engine_config = {
-        'device': 'CPU',
-        'stat_requests_number': 2,
-        'eval_requests_number': 2,
-    }
-
-    engine = OVEngine(engine_config, calibration_dataset, calibration_dataset)
-    pipeline = pot.create_pipeline(algorithms, engine)
-    compressed_model = pipeline.run(pot_model)
-    pot.compress_model_weights(compressed_model)
-    quantized_model = _convert_compressed_model_to_openvino_model(compressed_model)
-    return quantized_model
-
-
-def quantize_with_accuracy_control_impl(model: ov.Model,
-                                        calibration_dataset: Dataset,
-                                        validation_dataset: Dataset,
-                                        validation_fn: Callable[[ov.CompiledModel, Iterable[Any]], float],
-                                        max_drop: float = 0.01,
-                                        preset: QuantizationPreset = QuantizationPreset.PERFORMANCE,
-                                        target_device: TargetDevice = TargetDevice.ANY,
-                                        subset_size: int = 300,
-                                        fast_bias_correction: bool = True,
-                                        model_type: Optional[ModelType] = None,
-                                        ignored_scope: Optional[IgnoredScope] = None) -> ov.Model:
-    """
-    Implementation of the `quantize_with_accuracy_control()` method for the OpenVINO backend.
-    """
-    pot.utils.logger.init_logger(
-        level=logging.getLevelName(nncf_logger.getEffectiveLevel())
-    )
-    pot_model = _convert_openvino_model_to_compressed_model(model, target_device)
-
-    engine_config = {
-        'device': 'CPU',
-        'stat_requests_number': 1,
-        'eval_requests_number': 1,
-    }
-
-    # Check whether it is possible to calculate the metric for one data item.
-    # pylint: disable=W0703
-    use_original_metric = True
-    try:
-        ie = ov.Core()
-        compiled_model = ie.compile_model(model, device_name='CPU')
-        _ = validation_fn(compiled_model, validation_dataset.get_data(indices=[0]))
-    except Exception:
-        use_original_metric = False
-    compression_algorithms = pot.algorithms.algorithm_selector.COMPRESSION_ALGORITHMS
-    if 'NMSEBasedAccuracyAware' not in compression_algorithms.registry_dict:
-        compression_algorithms.register('NMSEBasedAccuracyAware')(NMSEBasedAccuracyAware)
-
-    algorithms = [
-        {
-            'name': 'NMSEBasedAccuracyAware',
-            'params': {
-                'target_device': target_device.value,
-                'stat_subset_size': subset_size,
-                'maximal_drop': max_drop,
-                'metric_subset_ratio': 0.5,
-                'preset': preset.value,
-                'use_fast_bias': fast_bias_correction,
-                'model_type': None if model_type is None else model_type.value,
-                'ignored': _create_ignored_scope_config(ignored_scope),
-            }
-        }
-    ]
-
-    engine = OVEngine(engine_config, calibration_dataset, validation_dataset, validation_fn, use_original_metric)
-    pipeline = pot.create_pipeline(algorithms, engine)
-    compressed_model = pipeline.run(pot_model)
-    pot.compress_model_weights(compressed_model)
-
-    quantized_model = _convert_compressed_model_to_openvino_model(compressed_model)
-
-    return quantized_model
+    def _load_state_without_name(self, state_without_name: Dict[str, Any]):
+        """
+        Implementation of load state that takes state without builder name.
+
+        :param state_without_name: Output of `_get_state_without_name()` method.
+        """
+
+    def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
+        raise NotImplementedError()
+
+    def _handle_frozen_layers(self, target_model: NNCFNetwork):
+        scopes_of_frozen_layers = []
+        for weighted_node in target_model.nncf.get_weighted_original_graph_nodes():
+            if not weighted_node.layer_attributes.weight_requires_grad:
+                if self._should_consider_scope(weighted_node.node_name):
+                    scopes_of_frozen_layers.append(weighted_node.node_name)
+        scopes_to_print = "\n".join(scopes_of_frozen_layers)
+        if len(scopes_of_frozen_layers) > 0:
+            is_allowed, reason = self._are_frozen_layers_allowed()
+            if is_allowed:
+                nncf_logger.warning(
+                    f"{reason}, compressing them without tuning weights.\n" f"Frozen layers:\n" f"{scopes_to_print}"
+                )
+            else:
+                raise RuntimeError(
+                    f"{reason}.\n"
+                    f"Please unfreeze them or put into the Ignored Scope.\n"
+                    f"Frozen Layers:\n"
+                    f"{scopes_to_print}"
+                )
+
+    def _should_consider_scope(self, node_name: NNCFNodeName) -> bool:
+        return should_consider_scope(node_name, self.ignored_scopes, self.target_scopes)
+
+    def _nncf_module_types_to_compress(self) -> List[str]:
+        """
+        Return list of NNCF module types which should be compressed by specific algorithm.
+        As name of algorithm used the value set by decorator @Registry.register() or default one.
+        :return: List of names of modules
+        """
+        filtered_nncf_module_names_list = []
+        for module_cls in list(NNCF_MODULES_DICT) + list(NNCF_WRAPPED_USER_MODULES_DICT.values()):
+            if self.name not in module_cls.ignored_algorithms:
+                filtered_nncf_module_names_list.append(module_cls.__name__)
+        return filtered_nncf_module_names_list
+
+    def _are_frozen_layers_allowed(self) -> Tuple[bool, str]:
+        algo_name = self.name.replace("_", " ")
+        return False, f"Frozen layers are not allowed for {algo_name}"
```

### Comparing `nncf-2.4.0/nncf/quantization/__init__.py` & `nncf-2.5.0/nncf/quantization/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,14 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Post-training quantization APIs."""
 from nncf.common.quantization.structs import QuantizationPreset
-from nncf.quantization.quantize import quantize
-from nncf.quantization.quantize import quantize_with_accuracy_control
+from nncf.quantization.quantize_model import quantize
+from nncf.quantization.quantize_model import quantize_with_accuracy_control
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/algorithm.py` & `nncf-2.5.0/nncf/quantization/algorithms/algorithm.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,30 +1,27 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
+from typing import Dict, Optional, TypeVar
 
-from typing import TypeVar, Dict, Optional
-
+from nncf import Dataset
 from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
 from nncf.common.utils.backend import BackendType
-from nncf import Dataset
 
-TModel = TypeVar('TModel')
+TModel = TypeVar("TModel")
 
 
 class AlgorithmParameters(ABC):
     """
     Base class for Post-Training algorithm parameters.
     """
 
@@ -39,39 +36,40 @@
     def available_backends(self) -> Dict[str, BackendType]:
         """
         Returns dictionary of the avaliable backends for the algorithm
 
         :return: Dict of backends supported by the algorithm
         """
 
-    def apply(self,
-              model: TModel,
-              statistic_points: Optional[StatisticPointsContainer] = None,
-              dataset: Optional[Dataset] = None) -> TModel:
+    def apply(
+        self,
+        model: TModel,
+        statistic_points: Optional[StatisticPointsContainer] = None,
+        dataset: Optional[Dataset] = None,
+    ) -> TModel:
         """
         Checks that statistic point exists, sets model into transformer
         and applies the algorithm to the model.
         :param model: model for applying algorithm
         :param engine: engine for the model execution
         :param statistic_points: StatisticPointsContainer
         :return: model after algorithm
         """
         if statistic_points is None:
             return self._apply(model, statistic_points=None, dataset=dataset)
         _statistic_points = self.get_statistic_points(model)
         for edge_name in _statistic_points.keys():
             if statistic_points.get(edge_name) is None:
-                raise RuntimeError(f'No statistics collected for the layer {edge_name}')
+                raise RuntimeError(f"No statistics collected for the layer {edge_name}")
         return self._apply(model, statistic_points)
 
     @abstractmethod
-    def _apply(self,
-               model: TModel,
-               statistic_points: StatisticPointsContainer,
-               dataset: Optional[Dataset] = None) -> TModel:
+    def _apply(
+        self, model: TModel, statistic_points: StatisticPointsContainer, dataset: Optional[Dataset] = None
+    ) -> TModel:
         """
         Applies the algorithm to the model.
         """
 
     @abstractmethod
     def get_statistic_points(self, model: TModel) -> StatisticPointsContainer:
         """
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/bias_correction/algorithm.py` & `nncf-2.5.0/nncf/quantization/algorithms/bias_correction/algorithm.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,70 +1,45 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict, List, TypeVar, Union, Optional, Tuple
+from collections import deque
+from typing import Any, Dict, List, Optional, TypeVar
 
 import numpy as np
-from copy import deepcopy
-from collections import deque
+from tqdm import tqdm
 
-from functools import partial
 from nncf import Dataset
 from nncf import nncf_logger
+from nncf.common.factory import EngineFactory
+from nncf.common.factory import ModelTransformerFactory
+from nncf.common.factory import NNCFGraphFactory
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
-from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TargetType
+from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.layout import TransformationLayout
+from nncf.common.tensor_statistics.statistic_point import StatisticPoint
+from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
 from nncf.common.utils.backend import BackendType
+from nncf.common.utils.backend import copy_model
 from nncf.common.utils.backend import get_backend
-from nncf.quantization.algorithms.algorithm import AlgorithmParameters
 from nncf.quantization.algorithms.algorithm import Algorithm
 from nncf.quantization.algorithms.bias_correction.backend import ALGO_BACKENDS
-from nncf.common.factory import NNCFGraphFactory
-from nncf.common.factory import EngineFactory
-from nncf.common.tensor_statistics.statistic_point import StatisticPoint
-from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
-
-TModel = TypeVar('TModel')
-
-
-class BiasCorrectionParameters(AlgorithmParameters):
-    """
-    Parameters of BiasCorrection algorithm
-
-    :param number_samples: Number of samples for statistics collection.
-    :param threshold: Magnitude threshold that regulates application of shift.
-    """
 
-    def __init__(self, number_samples: int = 100, threshold: float = 1000) -> None:
-        """
-        :param number_samples: The number of samples for the statistics collection.
-            This statistic uses for the further calculation of the bias shift.
-        :param threshold: The magnitude threshold regulates the application of the shift.
-            Magnitude calculates as the maximum of the absolute ratio of the shift to the original bias value.
-            If the calculated value is less than the threshold, the shift will apply to the bias.
-        """
-        self.number_samples = number_samples
-        self.threshold = threshold
+TModel = TypeVar("TModel")
 
-    def to_json(self) -> Dict[str, Union[str, float, int]]:
-        """
-        Serialize all BiasCorrection parameters to JSON.
-        """
+BIAS_CORRECTION_THRESHOLD = 1000
 
 
 class BiasCorrection(Algorithm):
 
     """
     Post-training BiasCorrection algorithm implementation
 
@@ -81,260 +56,292 @@
         weight quantizer-dequantizer pair or fake quantize node, and some other layers;
         - then we correct the original bias by the difference (shift) between floating-point and quantized outputs in
         the sub-graph and the model without quantizer-dequantizer pair or fake quantize node.
         - at the next step, we collect the new statistics for the next layer (that would be corrected) from
         the sub-graph with the updated bias value on the current step;
         - after the new statistics were collected, we drops the unnecessary statistics to reduce memory consumption;
         - in the end, we correct all needed biases in the original model.
-
-    :param number_samples: The number of the samples for the statistics collection.
-    :param threshold: The magnitude threshold that regulates the application of the shift.
-    :param nncf_graph: NNCFGraph class for the algorithm.
     """
 
-    def __init__(self, parameters: BiasCorrectionParameters) -> None:
-        """
-        :param parameters: The instance of the BiasCorrectionParameters.
+    def __init__(
+        self,
+        subset_size: int = 100,
+        threshold: float = BIAS_CORRECTION_THRESHOLD,
+        apply_for_all_nodes: bool = False,
+        inplace_statistics: bool = True,
+        backend_params: Optional[Dict[str, Any]] = None,
+    ):
+        """
+        :param subset_size: Size of a subset for the statistics collection,
+            defaults to 100.
+        :param threshold: The magnitude threshold that regulates the application of the
+            shift. Magnitude calculates as the maximum of the absolute ratio of the
+            shift to the original bias value. If the calculated value is less than the
+            threshold, the shift will apply to the bias, defaults to 1000.
+        :param apply_for_all_nodes: If True, then the bias correction be applied to all
+            quantized nodes, if the node has no bias then a bias node will be inserted,
+            and if False, then the bias correction will only be applied to quantized
+            nodes that have a bias.
+        :param inplace_statistics: Defines wheather to calculate quantizers statistics
+            by backend graph operations or by default Python implementation, defaults
+            to True.
+        :param backend_params: Backend specific parameters.
         """
         super().__init__()
-        self.number_samples = max(np.int(parameters.number_samples * 0.2), 1)
-        self.threshold = parameters.threshold
+        self.subset_size = subset_size
+        self.threshold = threshold
+        self.apply_for_all_nodes = apply_for_all_nodes
+        self.inplace_statistics = inplace_statistics
+        self.backend_params = backend_params
         self.nncf_graph = None
         self._backend_entity = None
         self._collected_stat_inputs = set()
         self._fp_inputs = {}
 
+        if self.apply_for_all_nodes:
+            raise RuntimeError("BiasCorrection algorithm does not support apply_for_all_nodes=True yet")
+
     @property
     def available_backends(self) -> Dict[str, BackendType]:
         return ALGO_BACKENDS.registry_dict
 
     def _set_backend_entity(self, model: TModel) -> None:
         """
         Creates a helper class with a backed-specific logic of the algorithm.
 
         :param model: Backend-specific input model.
         """
         model_backend = get_backend(model)
         if model_backend == BackendType.ONNX:
-            from nncf.quantization.algorithms.bias_correction.onnx_backend import \
-                ONNXBiasCorrectionAlgoBackend
-            self._backend_entity = ONNXBiasCorrectionAlgoBackend()
-        else:
-            raise RuntimeError('Cannot return backend-specific entity'
-                               'because {} is not supported!'.format(model_backend))
+            from nncf.quantization.algorithms.bias_correction.onnx_backend import ONNXBiasCorrectionAlgoBackend
 
-    def _apply(self,
-               model: TModel,
-               statistic_points: Optional[StatisticPointsContainer] = None,
-               dataset: Optional[Dataset] = None) -> TModel:
+            self._backend_entity = ONNXBiasCorrectionAlgoBackend()
+        elif model_backend == BackendType.OPENVINO:
+            from nncf.quantization.algorithms.bias_correction.openvino_backend import OVBiasCorrectionAlgoBackend
 
+            self._backend_entity = OVBiasCorrectionAlgoBackend()
+        else:
+            raise RuntimeError(
+                "Cannot return backend-specific entity because {} is not supported!".format(model_backend)
+            )
+
+    def _apply(
+        self,
+        model: TModel,
+        statistic_points: Optional[StatisticPointsContainer] = None,
+        dataset: Optional[Dataset] = None,
+    ) -> TModel:
         self._set_backend_entity(model)
         main_transformations_layout = TransformationLayout()
-        main_model_transformer = self._backend_entity.model_transformer(model)
+        main_model_transformer = ModelTransformerFactory.create(model)
 
-        model_copy = deepcopy(model)
+        model_copy = copy_model(model)
         model_copy = self._remove_fq_from_inputs(model_copy)
         nncf_graph = NNCFGraphFactory.create(model_copy)
-        subgraphs_data = self._fill_subgraphs_data(nncf_graph)
-
-        for node_name, subgraph_data in subgraphs_data.items():
-            node = nncf_graph.get_node_by_name(node_name)
 
-            if not self._backend_entity.is_node_with_bias(node):
-                nncf_logger.debug(f'Skipping node {node_name} because there is no bias')
-                continue
-            if not self._backend_entity.is_quantized_weights(node, model):
-                nncf_logger.debug(f'Skipping node {node_name} because weights was not quantized')
-                continue
+        nodes_with_bias = []
+        for node in nncf_graph.topological_sort():
+            if self._backend_entity.is_node_with_bias(node, nncf_graph) and self._backend_entity.is_quantized_weights(
+                node, nncf_graph
+            ):
+                nodes_with_bias.append(node)
+        subgraphs_data = [self._get_subgraph_data_for_node(node, nncf_graph) for node in nodes_with_bias]
+
+        for position, (node, subgraph_data) in tqdm(
+            list(enumerate(zip(nodes_with_bias, subgraphs_data))), desc="Biases correction"
+        ):
+            node_name = node.node_name
 
             # We do not make an additional copy of the model because
             # the model transformer (that uses during sub-graph extraction) already does this internally when creating.
-            model_copy_subgraph = self._prepare_subgraph(node, model_copy, subgraph_data)
+            model_copy_subgraph = self._prepare_subgraph(node, model_copy, nncf_graph, subgraph_data)
 
-            feed_dicts = self._create_feed_dicts(nncf_graph, subgraph_data, statistic_points)
+            feed_dicts = self._create_feed_dicts(model_copy_subgraph, subgraph_data, statistic_points)
 
             bias_shift = self._compute_bias_shift(node, model_copy_subgraph, feed_dicts, statistic_points)
 
-            current_bias = self._backend_entity.get_bias_value(model, node)
+            current_bias = self._backend_entity.get_bias_value(node, model, nncf_graph)
+
+            channel_axis = node.metatype.output_channel_axis
+            if current_bias.ndim > 1:
+                channel_axis = range(current_bias.ndim)[channel_axis]
+                axes = [i for i in range(current_bias.ndim) if i != channel_axis]
+                bias_shift = np.expand_dims(bias_shift, axes)
+
             updated_bias = current_bias + bias_shift
             magnitude = self._get_bias_shift_magnitude(current_bias, updated_bias)
 
             if magnitude < self.threshold:
-                nncf_logger.debug(f'{node_name} bias would be changed. Magnitude: {magnitude}')
-                bias_port_id = self._backend_entity.get_bias_port_id(model, node)
-                target_point = self._backend_entity.target_point(TargetType.LAYER,
-                                                                 node.node_name,
-                                                                 bias_port_id)
-                bias_correction_command = self._backend_entity.bias_correction_command(target_point,
-                                                                                       updated_bias)
+                nncf_logger.debug(f"{node_name} bias would be changed. Magnitude: {magnitude}")
+                bias_correction_command = self._backend_entity.create_bias_correction_command(
+                    node, updated_bias, nncf_graph
+                )
                 model_copy_subgraph = self._correct_bias(model_copy_subgraph, bias_correction_command)
                 model_copy = self._correct_bias(model_copy, bias_correction_command)
                 main_transformations_layout.register(bias_correction_command)
             else:
-                nncf_logger.debug(f'{node_name} bias skipped by threshold. Magnitude: {magnitude}')
+                nncf_logger.debug(f"{node_name} bias skipped by threshold. Magnitude: {magnitude}")
 
             self._collect_new_stats(nncf_graph, model_copy_subgraph, feed_dicts, subgraph_data)
-            self._remove_unnecessary_stats(node_name, subgraphs_data)
+            self._remove_unnecessary_stats(position, subgraphs_data)
         return main_model_transformer.transform(main_transformations_layout)
 
     def _remove_fq_from_inputs(self, model: TModel) -> TModel:
         """
         This model removes the activation Fake Quantize nodes (or Quantize-Dequantize pairs) from the model.
         It's needed for the further bias shift calculation that relates on quantized weights.
 
         :param model: Backend-specific model.
         :return: Backend-specific model without activation Fake Quantize nodes (or Quantize-Dequantize pairs).
         """
         transformation_layout = TransformationLayout()
-        skip_types = []
         nncf_graph = NNCFGraphFactory.create(model)
 
-        model_transformer = self._backend_entity.model_transformer(model)
-        for skip_type in self._backend_entity.quantizer_types:
-            skip_types.extend(skip_type.op_names)
+        model_transformer = ModelTransformerFactory.create(model)
 
         seen_nodes = []
         nodes_queue = deque(nncf_graph.get_input_nodes())
         while nodes_queue:
             current_node = nodes_queue.popleft()
             current_node_name = current_node.node_name
 
             if current_node_name in seen_nodes:
                 continue
 
             seen_nodes.append(current_node_name)
-            if current_node.node_type in skip_types:
-                target_point = self._backend_entity.target_point(
-                    TargetType.LAYER, current_node_name, 0)
+            if current_node.metatype in self._backend_entity.quantizer_types:
+                target_point = self._backend_entity.target_point(TargetType.LAYER, current_node_name, 0)
                 command = self._backend_entity.node_removing_command(target_point)
                 transformation_layout.register(command)
             nodes_queue.extend(nncf_graph.get_next_nodes(current_node))
 
         return model_transformer.transform(transformation_layout)
 
-    def _fill_subgraphs_data(self, nncf_graph: NNCFGraph) -> Dict[str, Dict]:
-        """
-        This method collects necessary data for the further optimized subgraph inference
-        that reduces algorithm execution time.
-
-        :param nncf_graph: NNCFGraph instance.
-        :return: A dictionary with the node name as key and needed data for the subgraph building.
-        """
-        subgraphs_data = {}
-
-        for node in nncf_graph.topological_sort():
-            if node.node_type not in self._backend_entity.layers_with_bias_metatypes:
-                continue
-            input_node_names, stat_node_names = self._get_subgraph_data_for_node(node, nncf_graph)
-            subgraphs_data[node.node_name] = {'input_node_names': input_node_names,
-                                              'stat_node_names': stat_node_names}
-        return subgraphs_data
-
-    def _get_subgraph_data_for_node(self, node: NNCFNode, nncf_graph: NNCFGraph) -> Tuple[set, set]:
+    def _get_subgraph_data_for_node(self, node: NNCFNode, nncf_graph: NNCFGraph) -> Dict[str, List[str]]:
         """
         This method collects necessary data for the specified node and its subgraph.
         This data contains the nodes (NNCFNode) for the subgraph building
         and statistics collection (for the next step).
 
         :param node: NNCFNode instance. This is the main node that with bias that would be corrected (or not).
         :param nncf_graph: NNCFGraph instance for graph analysis.
-        :return: A tuple with the set of the nodes for the subgraph input and statistics collection.
+        :return: A dict with the list of the nodes for the subgraph input and statistics collection.
         """
-        stats_nodes = set()
-        input_nodes = set()
+        stats_nodes, input_nodes, output_nodes = [], [], []
 
         def traverse_to_layers_with_bias(node, output):
-            if node.node_type in self._backend_entity.layers_with_bias_metatypes:
+            if node in output:
+                return True, output
+            if self._backend_entity.is_node_with_bias(node, nncf_graph):
                 output.append(node)
                 self._collected_stat_inputs.add(node.node_name)
+                activation_input = nncf_graph.get_input_edges(node)[0].from_node
+
+                output_nodes.append(activation_input)
                 return True, output
             return False, output
 
         def traverse_to_input_layers(node, output):
+            if node in output + input_nodes:
+                return True, output
             if node.node_name in self._collected_stat_inputs and node not in stats_nodes:
                 output.append(node)
                 return True, output
             return False, output
 
         for next_node in nncf_graph.get_next_nodes(node):
-            stats_nodes.update(nncf_graph.traverse_graph(next_node, traverse_to_layers_with_bias))
+            stats_nodes.extend(nncf_graph.traverse_graph(next_node, traverse_to_layers_with_bias))
 
         stats_nodes = stats_nodes if stats_nodes else nncf_graph.get_next_nodes(node)
         for stat_node in stats_nodes:
-            input_nodes.update(nncf_graph.traverse_graph(stat_node, traverse_to_input_layers, traverse_forward=False))
+            input_nodes.extend(nncf_graph.traverse_graph(stat_node, traverse_to_input_layers, traverse_forward=False))
 
-        return [input_node.node_name for input_node in input_nodes], [stat_node.node_name for stat_node in stats_nodes]
+        output_nodes = output_nodes if output_nodes else stats_nodes
+        subgraph_data = {
+            "input_node_names": [input_node.node_name for input_node in input_nodes],
+            "output_node_names": [n.node_name for n in output_nodes],
+            "statistic_node_names": [stat_node.node_name for stat_node in stats_nodes],
+        }
 
-    def _prepare_subgraph(self, node: NNCFNode, model: TModel, subgraph_data: Dict) -> TModel:
+        return subgraph_data
+
+    def _prepare_subgraph(self, node: NNCFNode, model: TModel, nncf_graph: NNCFGraph, subgraph_data: Dict) -> TModel:
         """
         This method prepares the subgraph from the model for the further inference.
 
         :param node: NNCFNode instance for the current layer.
         :param model: Backend-specifig model instance.
+        :param nncf_graph: Instance of NNCFGraph.
         :param subgraph_data: A dictionary with the layers for the graph building.
         :return: Backend-specific subgraph extracted from the model.
         """
-        input_node_names, statistic_node_names = subgraph_data['input_node_names'], subgraph_data['stat_node_names']
-        extracted_model = self._backend_entity.extract_model(model, input_node_names, statistic_node_names)
+        input_node_names, output_node_names = subgraph_data["input_node_names"], subgraph_data["output_node_names"]
+        extracted_model = self.extract_model(model, input_node_names, output_node_names)
 
         transformation_layout = TransformationLayout()
-        model_transformer = self._backend_entity.model_transformer(extracted_model)
-        _, output_port_id = self._backend_entity.get_activation_port_ids_for_bias_node(model, node)
-        statistic_point = self._backend_entity.target_point(TargetType.POST_LAYER_OPERATION,
-                                                            node.node_name,
-                                                            output_port_id)
-        output_insertion_command = self._backend_entity.output_insertion_command(statistic_point)
+        model_transformer = ModelTransformerFactory.create(extracted_model)
+        _, output_port_id = self._backend_entity.get_activation_port_ids_for_bias_node(node)
+        statistic_point = self._backend_entity.target_point(
+            TargetType.POST_LAYER_OPERATION, node.node_name, output_port_id
+        )
+        output_insertion_command = self._backend_entity.output_insertion_command(nncf_graph, statistic_point)
         transformation_layout.register(output_insertion_command)
         return model_transformer.transform(transformation_layout)
 
-    def _create_feed_dicts(self,
-                           nncf_subgraph: NNCFGraph,
-                           subgraph_data: Dict,
-                           statistic_points: StatisticPointsContainer) -> List[Dict]:
+    def _create_feed_dicts(
+        self, model: TModel, subgraph_data: Dict, statistic_points: StatisticPointsContainer
+    ) -> List[Dict]:
         """
         Creates the list of the dictionaries that contains the input data for the model exection.
 
-        :param nncf_subgraph: NNCFGraph instance.
+        :param model: TModel instance.
         :param subgraph_data: A dictionary with the necessary data for current node.
         :param statistic_points: StatisticPointsContainer instance.
         :return: List of the dictionaries with the input data.
         """
         feed_dicts = []
-        for stat_id in range(self.number_samples):
+        statistics_size = self.subset_size
+        statistics_per_input = {}
+
+        for input_node_name in subgraph_data["input_node_names"]:
+            input_tensor_name = self._backend_entity.get_input_name(model, input_node_name)
+            input_fp = self._get_fp_inputs(statistic_points, input_node_name)
+            statistics_per_input[input_tensor_name] = input_fp
+            statistics_size = min(statistics_size, len(input_fp))
+
+        for stat_id in range(statistics_size):
             feed_dict = {}
-            for input_node_name in subgraph_data['input_node_names']:
-                input_node = nncf_subgraph.get_node_by_name(input_node_name)
-                input_tensor_names, _ = self._backend_entity.get_tensor_names(input_node)
-                input_fp = self._get_fp_inputs(statistic_points, input_node_name)
-                feed_dict[input_tensor_names[0]] = np.mean(input_fp[stat_id], axis=0, keepdims=True)
+            for input_node_name in subgraph_data["input_node_names"]:
+                input_tensor_name = self._backend_entity.get_input_name(model, input_node_name)
+                feed_dict[input_tensor_name] = np.mean(
+                    statistics_per_input[input_tensor_name][stat_id], axis=0, keepdims=True
+                )
             feed_dicts.append(feed_dict)
         return feed_dicts
 
-    def _compute_bias_shift(self,
-                            node: NNCFNode,
-                            model: TModel,
-                            feed_dicts: List,
-                            statistic_points: StatisticPointsContainer) -> np.ndarray:
+    def _compute_bias_shift(
+        self, node: NNCFNode, model: TModel, feed_dicts: List, statistic_points: StatisticPointsContainer
+    ) -> np.ndarray:
         """
         Computes bias shift that will be used for the futher bias correction.
 
         :param node: NNCFNode instance, current layer.
         :param model: Backend-specific model.
         :param feed_dicts: List of dictionaries with the input data for model execition.
         :param statistic_points: StatisticPointsContainer instance.
         :return: Calculated bias shift value.
         """
         output_fp = self._get_fp_outputs(statistic_points, node.node_name)
-        output_tensor_names = self._backend_entity.get_output_names(model, node.node_name)
+        output_tensor_name = self._backend_entity.get_output_name(model, node.node_name)
         engine = EngineFactory.create(model)
-        channel_axis = self._backend_entity.channel_axis_by_types[node.node_type]
+        channel_axis = node.metatype.output_channel_axis
         q_outputs = []
         for feed_dict in feed_dicts:
             q_output = engine.infer(feed_dict)
-            q_output = self._backend_entity.process_model_output(q_output, output_tensor_names[0])
+            q_output = self._backend_entity.process_model_output(q_output, output_tensor_name)
             q_outputs.append(self._backend_entity.tensor_processor.mean_per_channel(q_output, channel_axis).tensor)
         q_output = np.mean(q_outputs, axis=0)
         return output_fp - q_output
 
     @staticmethod
     def _get_bias_shift_magnitude(current_bias_value: np.ndarray, updated_bias_value: np.ndarray) -> float:
         """
@@ -353,15 +360,15 @@
         """
         Returns the model (which can be represended as subgraph) with the updated bias value for the current layer.
 
         :param model: Backend-specific model.
         :param bias_correction_command: TransformationCommand instance for the bias correction.
         :return: Backend-specific model, but with the updated bias value.
         """
-        model_transformer = self._backend_entity.model_transformer(model)
+        model_transformer = ModelTransformerFactory.create(model)
         transformation_layout = TransformationLayout()
         transformation_layout.register(bias_correction_command)
         return model_transformer.transform(transformation_layout)
 
     def _collect_new_stats(self, nncf_graph: NNCFGraph, model: TModel, feed_dicts: List, subgraph_data: Dict) -> None:
         """
         Updates the self._fp_inputs with the new statistics for the next layers
@@ -371,153 +378,179 @@
         :param model: Backend-specific subgraph.
         :param feed_dicts: List of dictionaries with the input data for the subgraph.
         :param subgraph_data: A dictionary with the needed list of the statistic nodes that will be updated.
         """
         engine = EngineFactory.create(model)
         for feed_dict in feed_dicts:
             new_q_output = engine.infer(feed_dict)
-            for stat_node_name in subgraph_data['stat_node_names']:
-                stat_node = nncf_graph.get_node_by_name(stat_node_name)
-                input_tensor_names, _ = self._backend_entity.get_tensor_names(stat_node)
+            output_data = zip(subgraph_data["statistic_node_names"], subgraph_data["output_node_names"])
+            for stat_node_name, output_node_name in output_data:
+                output_tensor_name = self._backend_entity.get_output_name(model, output_node_name)
                 if stat_node_name not in self._fp_inputs:
                     self._fp_inputs[stat_node_name] = []
-                self._fp_inputs[stat_node_name].append(new_q_output[input_tensor_names[0]])
+                self._fp_inputs[stat_node_name].append(new_q_output[output_tensor_name])
 
-    def _remove_unnecessary_stats(self, node_name: str, subgraphs_data: Dict[str, Dict]) -> None:
+    def _remove_unnecessary_stats(self, position: int, subgraphs_data: Dict[str, Dict]) -> None:
         """
         Removes unnecessary statistics that were collected before to reduce the memory usage.
 
-        :param node_name: Current name of the node that was corrected.
+        :param position: Zero-based position of the current node that was corrected.
         :param subgraphs_data: A dictionary of the data (input & statistic node names) that
             uses for the sub-graphs creation.
         """
-        needed_stats_list = self._get_current_stats_list(node_name, subgraphs_data)
-        node_inputs_name = subgraphs_data[node_name]['input_node_names']
+        # Collects list of the statistics that needed for the future layers.
+        needed_stats_list = []
+        for i in range(position + 1, len(subgraphs_data)):
+            needed_stats_list.extend(subgraphs_data[i]["input_node_names"])
+
+        node_inputs_name = subgraphs_data[position]["input_node_names"]
         for node_input_name in node_inputs_name:
             if node_input_name not in needed_stats_list and node_input_name in self._fp_inputs:
-                nncf_logger.debug(f'Dropped {node_input_name}')
+                nncf_logger.debug(f"Dropped {node_input_name}")
                 self._fp_inputs[node_input_name] = []
 
-    def _get_current_stats_list(self, current_node_name: str, subgraphs_data: Dict[str, Dict]) -> List[str]:
-        """
-        Collects list of the statistics that needed for the future layers.
-
-        :param node_name: Current name of the node that was corrected.
-        :param subgraphs_data: A dictionary of the data (input & statistic node names) that
-            uses for the sub-graphs creation.
-        :return: The list of the layer names.
-        """
-        stat_nodes_list = []
-        trigger = False
-        for node_name in subgraphs_data:
-            if node_name == current_node_name:
-                trigger = True
-                continue
-            if trigger:
-                for stat_node_name in subgraphs_data[node_name]['input_node_names']:
-                    stat_nodes_list.append(stat_node_name)
-        return stat_nodes_list
-
     def _get_fp_inputs(self, statistic_points: StatisticPointsContainer, node_name: str) -> np.ndarray:
         """
         Makes out pre-layer needed data from the floating-point collected statistics.
 
         :param statistic_points: Filled StatisticPointsContainer.
         :param node_name: Name of the current layer.
         :return: Collected mean tensor data and shape for the further bias calculation.
         """
+
         def input_filter_func(point):
-            return BiasCorrection in point.algorithm_to_tensor_collectors and \
-                point.target_point.type == TargetType.PRE_LAYER_OPERATION
+            return (
+                BiasCorrection in point.algorithm_to_tensor_collectors
+                and point.target_point.type == TargetType.PRE_LAYER_OPERATION
+            )
 
         if node_name in self._fp_inputs:
             return self._fp_inputs[node_name]
 
         input_fp = []
-        for tensor_collector in statistic_points.get_algo_statistics_for_node(node_name,
-                                                                              input_filter_func,
-                                                                              BiasCorrection):
+        for tensor_collector in statistic_points.get_algo_statistics_for_node(
+            node_name, input_filter_func, BiasCorrection
+        ):
             input_fp.extend(tensor_collector.get_statistics().values)
         self._fp_inputs[node_name] = input_fp
         return self._fp_inputs[node_name]
 
     def _get_fp_outputs(self, statistic_points: StatisticPointsContainer, node_name: str) -> np.ndarray:
         """
         Makes out post-layer needed data from the floating-point collected statistics.
 
         :param statistic_points: Filled StatisticPointsContainer.
         :param node_name: Name of the current layer.
         :return: Collected mean tensor data for the further bias calculation.
         """
 
         def output_filter_func(point):
-            return BiasCorrection in point.algorithm_to_tensor_collectors and \
-                point.target_point.type == TargetType.POST_LAYER_OPERATION
+            return (
+                BiasCorrection in point.algorithm_to_tensor_collectors
+                and point.target_point.type == TargetType.POST_LAYER_OPERATION
+            )
 
         output_fp = []
-        for tensor_collector in statistic_points.get_algo_statistics_for_node(node_name,
-                                                                              output_filter_func,
-                                                                              BiasCorrection):
+        for tensor_collector in statistic_points.get_algo_statistics_for_node(
+            node_name, output_filter_func, BiasCorrection
+        ):
             output_fp.extend(tensor_collector.get_statistics().mean_values)
         return np.array(output_fp)
 
     def get_statistic_points(self, model: TModel) -> StatisticPointsContainer:
         self._set_backend_entity(model)
-        nncf_graph = NNCFGraphFactory.create(model) if self.nncf_graph is None else self.nncf_graph
+        model_copy = self._remove_fq_from_inputs(copy_model(model))
+        nncf_graph = NNCFGraphFactory.create(model_copy) if self.nncf_graph is None else self.nncf_graph
         statistic_container = StatisticPointsContainer()
 
-        biased_nodes = []
-        for node in nncf_graph.topological_sort():
-            if node.node_type in self._backend_entity.layers_with_bias_metatypes:
-                biased_nodes.append(node)
-
+        nodes_with_bias = [
+            node for node in nncf_graph.topological_sort() if self._backend_entity.is_node_with_bias(node, nncf_graph)
+        ]
         model_inputs = nncf_graph.get_input_nodes()
         biased_after_input_nodes = self._get_biased_after_input_nodes(nncf_graph, model_inputs)
 
-        for biased_node in biased_nodes:
-            biased_node_name = biased_node.node_name
-            channel_axis = self._backend_entity.channel_axis_by_types[biased_node.node_type]
-            input_port_id, output_port_id = self._backend_entity.get_activation_port_ids_for_bias_node(model,
-                                                                                                       biased_node)
-            if biased_node_name in biased_after_input_nodes:
-                self._collected_stat_inputs.add(biased_node_name)
-                statistic_point = self._backend_entity.target_point(TargetType.PRE_LAYER_OPERATION,
-                                                                    biased_node_name,
-                                                                    input_port_id)
-                stat_collector = self._backend_entity.batch_statistic_collector(num_samples=self.number_samples)
-                statistic_container.add_statistic_point(StatisticPoint(target_point=statistic_point,
-                                                                       tensor_collector=stat_collector,
-                                                                       algorithm=BiasCorrection))
-            statistic_point = self._backend_entity.target_point(TargetType.POST_LAYER_OPERATION,
-                                                                biased_node_name,
-                                                                output_port_id)
-            stat_collector = self._backend_entity.mean_statistic_collector(reduction_shape=channel_axis,
-                                                                           num_samples=self.number_samples)
-            statistic_container.add_statistic_point(StatisticPoint(target_point=statistic_point,
-                                                                   tensor_collector=stat_collector,
-                                                                   algorithm=BiasCorrection))
+        for node in nodes_with_bias:
+            node_name = node.node_name
+            channel_axis = node.metatype.output_channel_axis
+            input_port_id, output_port_id = self._backend_entity.get_activation_port_ids_for_bias_node(node)
+            if node_name in biased_after_input_nodes:
+                self._collected_stat_inputs.add(node_name)
+                statistic_point = self._backend_entity.target_point(
+                    TargetType.PRE_LAYER_OPERATION, node_name, input_port_id
+                )
+                stat_collector = self._backend_entity.batch_statistic_collector(
+                    num_samples=self.subset_size, inplace=self.inplace_statistics
+                )
+                statistic_container.add_statistic_point(
+                    StatisticPoint(
+                        target_point=statistic_point, tensor_collector=stat_collector, algorithm=BiasCorrection
+                    )
+                )
+            statistic_point = self._backend_entity.target_point(
+                TargetType.POST_LAYER_OPERATION, node_name, output_port_id
+            )
+            stat_collector = self._backend_entity.mean_statistic_collector(
+                reduction_shape=channel_axis, num_samples=self.subset_size, inplace=self.inplace_statistics
+            )
+            statistic_container.add_statistic_point(
+                StatisticPoint(target_point=statistic_point, tensor_collector=stat_collector, algorithm=BiasCorrection)
+            )
+
+        for input_node in model_inputs:
+            for next_input_node in nncf_graph.get_next_nodes(input_node):
+                self._collected_stat_inputs.add(next_input_node.node_name)
+                statistic_point = self._backend_entity.target_point(
+                    TargetType.PRE_LAYER_OPERATION, next_input_node.node_name, port_id=0
+                )
+                stat_collector = self._backend_entity.batch_statistic_collector(
+                    num_samples=self.subset_size, inplace=self.inplace_statistics
+                )
+                statistic_container.add_statistic_point(
+                    StatisticPoint(
+                        target_point=statistic_point, tensor_collector=stat_collector, algorithm=BiasCorrection
+                    )
+                )
 
         return statistic_container
 
     def _get_biased_after_input_nodes(self, nncf_graph: NNCFGraph, model_inputs: List[NNCFNode]) -> Dict[str, str]:
         """
         This method finds and returns the first nodes with the bias in the model that follows after the input nodes.
 
         :param nncf_graph: NNCFGraph instance.
         :param model_inputs: List of the model inputs as NNCFNodes.
         :return: A dictionary with the names of the nodes with bias as keys and their input node names as values.
         """
-        def traverse_to_biased(node, output, biased_op_types):
-            if node.node_type in biased_op_types:
+
+        def traverse_to_biased(node, output):
+            if node in output:
+                return True, output
+            if self._backend_entity.is_node_with_bias(node, nncf_graph):
                 output.append(node)
                 return True, output
             return False, output
 
         biased_after_param_nodes = {}
 
-        traverse_fn = partial(traverse_to_biased, biased_op_types=self._backend_entity.layers_with_bias_metatypes)
         for model_input in model_inputs:
-            biased_nodes = nncf_graph.traverse_graph(model_input, traverse_fn)
-            for biased_node in biased_nodes:
-                activation_input = self._backend_entity.get_node_through_quantizer(biased_node, nncf_graph)
-                biased_after_param_nodes[biased_node.node_name] = activation_input.node_name
+            nodes_with_bias = nncf_graph.traverse_graph(model_input, traverse_to_biased)
+            for node in nodes_with_bias:
+                activation_input = nncf_graph.get_input_edges(node)[0].from_node
+                biased_after_param_nodes[node.node_name] = activation_input.node_name
         return biased_after_param_nodes
+
+    def extract_model(self, model: TModel, input_node_names: List[str], output_node_names: List[str]) -> TModel:
+        """
+        Returns the backend-specific model that bounded by the specified input & output layers.
+
+        :param model: Backend-specific model.
+        :param input_node_names: List with the input node names.
+        :param output_node_names: List with the output node names.
+        :return: Extracted backend-specific model.
+        """
+        transformation_layout = TransformationLayout()
+        model_transformer = ModelTransformerFactory.create(model)
+        model_extraction_command = self._backend_entity.model_extraction_command(
+            set(input_node_names), set(output_node_names)
+        )
+        transformation_layout.register(model_extraction_command)
+        return model_transformer.transform(transformation_layout)
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/bias_correction/backend.py` & `nncf-2.5.0/nncf/quantization/algorithms/bias_correction/backend.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,234 +1,209 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
-from typing import List, Tuple, TypeVar, Optional
+from typing import List, Optional, Tuple, TypeVar
 
 import numpy as np
+
+from nncf.common.graph import NNCFGraph
+from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetPoint
-from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TargetType
+from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.tensor import NNCFTensor
-from nncf.common.graph import NNCFGraph
-from nncf.common.graph import NNCFNode
-from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.common.tensor_statistics.collectors import ReductionShape
+from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.common.utils.registry import Registry
-from nncf.common.graph.model_transformer import ModelTransformer
 
-TModel = TypeVar('TModel')
-OutputType = TypeVar('OutputType')
-ALGO_BACKENDS = Registry('algo_backends')
+TModel = TypeVar("TModel")
+OutputType = TypeVar("OutputType")
+ALGO_BACKENDS = Registry("algo_backends")
 
 
-#pylint:disable=too-many-public-methods
+# pylint:disable=too-many-public-methods
 class BiasCorrectionAlgoBackend(ABC):
-
     @property
     @abstractmethod
-    def layers_with_bias_metatypes(self):
+    def tensor_processor(self):
         """
-        Property for the backend-specific metatypes with bias.
+        Returns backend-specific instance of the NNCFCollectorTensorProcessor.
         """
 
     @property
     @abstractmethod
-    def channel_axis_by_types(self):
+    def quantizer_types(self):
         """
-        Property for the backend-specific info about channels placement in the layout.
+        Returns backend-specific list of the quantizer metatypes.
         """
 
-    @property
+    @staticmethod
     @abstractmethod
-    def tensor_processor(self):
+    def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> TargetPoint:
         """
-        Returns backend-specific instance of the NNCFCollectorTensorProcessor.
+        Returns backend-specific target point.
+
+        :param target_type: Type of the location that should be modified.
+        :param target_node_name: Name of the located node.
+        :param port_id: id of the port for the statistics disctribution.
+        :return: Backend-specific TargetPoint.
         """
 
-    @property
+    @staticmethod
     @abstractmethod
-    def quantizer_types(self):
+    def create_bias_correction_command(node: NNCFNode, bias_value: np.ndarray) -> TransformationCommand:
         """
-        Returns backend-specific list of the quantizer metatypes.
+        Creates backend-specific command to update bias value.
+
+        :param node: The node for which bias should be updated.
+        :param bias_value: New value for the bias.
+        :return: Backend-specific command to update bias value.
         """
 
     @staticmethod
     @abstractmethod
-    def model_transformer(model: TModel) -> ModelTransformer:
+    def model_extraction_command(inputs: List[str], outputs: List[str]) -> TransformationCommand:
         """
-        Returns backend-specific ModelTransformer instance.
+        Returns backend-specific command to extract sub-model based on input & output names.
 
-        :param model: Backend-specific model to create ModelTransformer.
-        :return: ModelTransformer instance.
+        :param inputs: List of the input names for sub-model beggining.
+        :param outputs: List of the output names for sub-model end.
+        :return: Backend-specific TransformationCommand for the model extraction.
         """
 
     @staticmethod
     @abstractmethod
-    def target_point(target_type: TargetType, target_node_name: str, port_id: str = None) -> TargetPoint:
+    def output_insertion_command(nncf_graph: NNCFGraph, target_point: TargetPoint) -> TransformationCommand:
         """
-        Returns backend-specific target point.
+        Returns backend-specific command that inserts output.
 
-        :param target_type: Type of the location that should be modified.
-        :param target_node_name: Name of the located node.
-        :param port_id: id of the port for the statistics disctribution.
-        :return: Backend-specific TargetPoint.
+        :param nncf_graph: NNCFGraph instance.
+        :param target_point: TargetPoint instance.
+        :return: Backend-specific command that inserts output.
         """
 
     @staticmethod
     @abstractmethod
-    def bias_correction_command(target_point: TargetPoint,
-                                bias_value: np.ndarray,
-                                threshold: float) -> TransformationCommand:
+    def node_removing_command(target_point: TargetPoint) -> TransformationCommand:
         """
-        Returns backend-specific bias correction command.
+        Returns backend-specific command that removes node.
 
-        :param target_point: Target location for the correction.
-        :param bias_value: New value for the bias.
-        :param threshold: Parametrized threshold for the shift magnitude comparison.
-        :return: Backend-specific TransformationCommand for the bias correction.
+        :param target_point: TargetPoint instance.
+        :return: Backend-specific command that remove node.
         """
 
     @staticmethod
     @abstractmethod
-    def mean_statistic_collector(reduction_shape: ReductionShape,
-                                 num_samples: Optional[int] = None,
-                                 window_size: Optional[int] = None) -> TensorStatisticCollectorBase:
+    def mean_statistic_collector(
+        reduction_shape: ReductionShape,
+        inplace: bool,
+        num_samples: Optional[int] = None,
+        window_size: Optional[int] = None,
+    ) -> TensorStatisticCollectorBase:
         """
         Returns backend-specific mean statistic collector.
 
         :param reduction_shape: Channel axis for the statistics aggregation.
+        :param inplace: Whether to calculate statistic inplace or not.
         :param num_samples: Maximum number of samples to collect.
         :param window_size: The maximum size of the samples queue.
         :return: Backend-specific TensorStatisticCollectorBase for the statistics calculation.
         """
 
     @staticmethod
     @abstractmethod
-    def batch_statistic_collector(num_samples: int = None) -> TensorStatisticCollectorBase:
+    def batch_statistic_collector(inplace: bool, num_samples: int = None) -> TensorStatisticCollectorBase:
         """
         Returns backend-specific batch statistic collector.
 
+        :param inplace: Whether to calculate statistic inplace or not.
         :param num_samples: Maximum number of samples to collect.
         :return: Backend-specific TensorStatisticCollectorBase for the statistics calculation.
         """
 
     @staticmethod
     @abstractmethod
-    def get_tensor_names(node: NNCFNode) -> Tuple[List[str], List[str]]:
-        """
-        Returns tuple of the lists with the input & output tensor names respectively.
-
-        :param node: NNCFNode with the layer_attributes.
-        :return: Tuple of the lists with the names.
-        """
-
-    @staticmethod
-    @abstractmethod
     def process_model_output(raw_data: OutputType, output_name: str) -> NNCFTensor:
         """
         Returns backend-specific processed output from the model.
 
         :param raw_data: Backend-specific output from the model.
         :param output_name: Name of the output layer or tensor name.
         :return: Processed output as NNCFTensor.
         """
 
     @staticmethod
     @abstractmethod
-    def get_node_through_quantizer(node: NNCFNode, nncf_graph: NNCFGraph) -> NNCFNode:
-        """
-        Returns activation node, but not quanitzers.
-
-        :param node: NNCFNode instance.
-        :param nncf_graph: NNCFGraph instance.
-        :return: NNCFNode activation node.
-        """
-
-    @staticmethod
-    @abstractmethod
-    def get_activation_port_ids_for_bias_node(model: TModel, node: NNCFNode) -> Tuple[int, int]:
+    def get_activation_port_ids_for_bias_node(node: NNCFNode) -> Tuple[int, int]:
         """
         Returns Input Port ID and Output Port ID corresponding to activation input and output edges for
         the node.
         Supports only nodes that could have bias value.
 
-        :param model: Backend-specific model.
         :param node: Node of NNCFGraph with bias value.
         """
 
     @staticmethod
     @abstractmethod
-    def get_bias_value(model: TModel, node: NNCFNode) -> np.ndarray:
+    def get_bias_value(node: NNCFNode, model: TModel, nncf_graph: NNCFGraph) -> np.ndarray:
         """
         Returns bias value in the NumPy format of provided node.
 
-        :param model: Backend-specific model for the initializer finding.
         :param node: Node of NNCFGraph with bias value.
+        :param model: Backend-specific model for the initializer finding.
+        :param nncf_graph: NNCFGraph instance with the node.
         :return: Bias value in the NumPy format.
         """
 
     @staticmethod
     @abstractmethod
-    def get_bias_port_id(model: TModel, node: NNCFNode) -> int:
-        """
-        Returns bias Port ID corresponding to the node.
-
-        :param model: Backend-specific model.
-        :param node: Node of NNCFGraph with bias value.
-        :return: Port ID corresponding to bias.
-        """
-
-    @staticmethod
-    @abstractmethod
-    def get_output_names(model: TModel, node_name: str) -> List[str]:
+    def get_input_name(model: TModel, node_name: str) -> str:
         """
-        Returns list of backend-specific port names.
+        Returns input tensor name for the specific node.
 
-        :param model: Backend-specific model.
+        :param model: Backend-specific model for the initializer finding.
         :param node_name: Name of the backend-specific node.
-        :return: List of the tensor names.
+        :return: Input tensor name.
         """
 
     @staticmethod
     @abstractmethod
-    def extract_model(model: TModel, input_node_names: List[str], output_node_names: List[str]) -> TModel:
+    def get_output_name(model: TModel, node_name: str) -> str:
         """
-        Returns the backend-specific model that bounded by the specified input & output layers.
+        Returns output tensor name for the specific node.
 
         :param model: Backend-specific model.
-        :param input_node_names: List with the input node names.
-        :param output_node_names: List with the output node names.
-        :return: Extracted backend-specific model.
+        :param node_name: Name of the backend-specific node.
+        :return: Output tensor name.
         """
 
     @staticmethod
     @abstractmethod
-    def is_quantized_weights(node: NNCFNode, model: TModel) -> bool:
+    def is_quantized_weights(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
         """
         Checks whether the node is quantized or not.
 
         :param node: NNCFNode to check.
-        :param model: Backend-specific model.
+        :param nncf_graph: NNCFGraph instance with the node.
         :return: boolean indicating whether the node has a quantized weights or not.
         """
 
     @staticmethod
     @abstractmethod
-    def is_node_with_bias(node: NNCFNode) -> bool:
+    def is_node_with_bias(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
         """
         Checks whether the node has a bias or not.
 
         :param node: NNCFNode with the attributes.
+        :param nncf_graph: NNCFGraph instance with the node.
         :return: Boolean indicating whether the node has a bias or not.
         """
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/bias_correction/onnx_backend.py` & `nncf-2.5.0/nncf/quantization/algorithms/bias_correction/onnx_backend.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,194 +1,125 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Dict, List, Optional, Tuple
 
-from typing import Dict, Tuple, List, Optional
-import onnx
 import numpy as np
+import onnx
+
+from nncf.common.graph import NNCFGraph
+from nncf.common.graph import NNCFNode
+from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.tensor_statistics.collectors import ReductionShape
 from nncf.common.utils.backend import BackendType
-from nncf.common.graph import NNCFNode
-from nncf.common.graph import NNCFGraph
-from nncf.common.graph.transformations.layout import TransformationLayout
-from nncf.common.graph.operator_metatypes import OperatorMetatype
-
-from nncf.onnx.graph.metatypes.onnx_metatypes import LAYERS_WITH_BIAS_METATYPES
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNX_OPERATION_METATYPES
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXDequantizeLinearMetatype
-from nncf.onnx.graph.model_transformer import ONNXModelTransformer
+from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXQuantizeLinearMetatype
+from nncf.onnx.graph.node_utils import get_bias_value
+from nncf.onnx.graph.node_utils import is_node_with_bias
+from nncf.onnx.graph.onnx_graph import ONNXGraph
+from nncf.onnx.graph.transformations.command_creation import create_bias_correction_command
 from nncf.onnx.graph.transformations.commands import ONNXBiasCorrectionCommand
 from nncf.onnx.graph.transformations.commands import ONNXModelExtractionCommand
-from nncf.onnx.graph.transformations.commands import ONNXQDQNodeRemovingCommand
 from nncf.onnx.graph.transformations.commands import ONNXOutputInsertionCommand
+from nncf.onnx.graph.transformations.commands import ONNXQDQNodeRemovingCommand
 from nncf.onnx.graph.transformations.commands import ONNXTargetPoint
-from nncf.onnx.statistics.collectors import ONNXMeanStatisticCollector
 from nncf.onnx.statistics.collectors import ONNXBatchStatisticCollector
+from nncf.onnx.statistics.collectors import ONNXMeanStatisticCollector
 from nncf.onnx.statistics.collectors import ONNXNNCFCollectorTensorProcessor
 from nncf.onnx.tensor import ONNXNNCFTensor
 from nncf.quantization.algorithms.bias_correction.backend import ALGO_BACKENDS
 from nncf.quantization.algorithms.bias_correction.backend import BiasCorrectionAlgoBackend
-from nncf.onnx.graph.onnx_graph import ONNXGraph
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXIdentityMetatype
 
 
-#pylint:disable=too-many-public-methods
+# pylint:disable=too-many-public-methods
 @ALGO_BACKENDS.register(BackendType.ONNX)
 class ONNXBiasCorrectionAlgoBackend(BiasCorrectionAlgoBackend):
-
-    @property
-    def layers_with_bias_metatypes(self) -> List[str]:
-        biased_op_types = []
-        for metatype in LAYERS_WITH_BIAS_METATYPES:
-            biased_op_types.extend(metatype.op_names)
-        return biased_op_types
-
-    @property
-    def channel_axis_by_types(self) -> Dict[str, int]:
-        return {'Conv': 1, 'Gemm': -1, 'ConvTranspose': 1}
-
     @property
     def tensor_processor(self) -> ONNXNNCFCollectorTensorProcessor:
         return ONNXNNCFCollectorTensorProcessor()
 
     @property
     def quantizer_types(self) -> List[OperatorMetatype]:
-        return [ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name('QuantizeLinear'),
-                ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name('DequantizeLinear')]
+        return [ONNXQuantizeLinearMetatype, ONNXDequantizeLinearMetatype]
 
     @staticmethod
-    def model_transformer(model: onnx.ModelProto) -> ONNXModelTransformer:
-        return ONNXModelTransformer(model)
-
-    @staticmethod
-    def target_point(target_type: TargetType,
-                     target_node_name: str = None,
-                     port_id: str = None) -> ONNXTargetPoint:
+    def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> ONNXTargetPoint:
         return ONNXTargetPoint(target_type, target_node_name, port_id)
 
     @staticmethod
-    def bias_correction_command(target_point: ONNXTargetPoint,
-                                bias_value: np.ndarray) -> ONNXBiasCorrectionCommand:
-        return ONNXBiasCorrectionCommand(target_point, bias_value)
-
-    @staticmethod
-    def output_insertion_command(target_point: ONNXTargetPoint) -> ONNXOutputInsertionCommand:
-        return ONNXOutputInsertionCommand(target_point)
+    def create_bias_correction_command(
+        node: NNCFNode, bias_value: np.ndarray, nncf_graph: NNCFGraph
+    ) -> ONNXBiasCorrectionCommand:
+        return create_bias_correction_command(node, bias_value)
+
+    @staticmethod
+    def model_extraction_command(inputs: List[str], outputs: List[str]) -> ONNXModelExtractionCommand:
+        return ONNXModelExtractionCommand(inputs, outputs)
+
+    @staticmethod
+    def output_insertion_command(nncf_graph: NNCFGraph, target_point: ONNXTargetPoint) -> ONNXOutputInsertionCommand:
+        nncf_input_node_next_nodes = {}
+        for input_node in nncf_graph.get_input_nodes():
+            next_nodes = nncf_graph.get_next_nodes(input_node)
+            nncf_input_node_next_nodes[input_node.node_name] = [node.node_name for node in next_nodes]
+        return ONNXOutputInsertionCommand(target_point, nncf_input_node_next_nodes)
 
     @staticmethod
     def node_removing_command(target_point: ONNXTargetPoint) -> ONNXQDQNodeRemovingCommand:
-        return ONNXQDQNodeRemovingCommand(target_point=target_point)
+        return ONNXQDQNodeRemovingCommand(target_point)
 
     @staticmethod
-    def mean_statistic_collector(reduction_shape: ReductionShape,
-                                 num_samples: Optional[int] = None,
-                                 window_size: Optional[int] = None) -> ONNXMeanStatisticCollector:
-        return ONNXMeanStatisticCollector(reduction_shape,  num_samples, window_size)
+    def mean_statistic_collector(
+        reduction_shape: ReductionShape,
+        inplace: bool,
+        num_samples: Optional[int] = None,
+        window_size: Optional[int] = None,
+    ) -> ONNXMeanStatisticCollector:
+        return ONNXMeanStatisticCollector(reduction_shape, num_samples, window_size)
 
     @staticmethod
-    def batch_statistic_collector(num_samples: int = None) -> ONNXMeanStatisticCollector:
+    def batch_statistic_collector(inplace: bool, num_samples: int = None) -> ONNXMeanStatisticCollector:
         return ONNXBatchStatisticCollector(num_samples)
 
     @staticmethod
-    def get_tensor_names(node: NNCFNode):
-        return node.layer_attributes.input_tensor_names, \
-            node.layer_attributes.output_tensor_names
-
-    @staticmethod
     def process_model_output(raw_data: Dict, output_name: str) -> ONNXNNCFTensor:
         return ONNXNNCFTensor(raw_data[output_name])
 
     @staticmethod
-    def get_node_through_quantizer(node: NNCFNode, nncf_graph: NNCFGraph) -> NNCFNode:
-        activation_input_port = 0
-        quantizer_type = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name('QuantizeLinear')
-        dequantizer_type = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name('DequantizeLinear')
-        skip_types = dequantizer_type.op_names + quantizer_type.op_names
-        previous_node = nncf_graph.get_previous_nodes(node)[activation_input_port]
-        while previous_node.node_type in skip_types:
-            previous_node = nncf_graph.get_previous_nodes(previous_node)[activation_input_port]
-        return previous_node
-
-    @staticmethod
-    def get_activation_port_ids_for_bias_node(model: onnx.ModelProto, node: NNCFNode) -> Tuple[int, int]:
+    def get_activation_port_ids_for_bias_node(node: NNCFNode) -> Tuple[int, int]:
         return 0, 0
 
     @staticmethod
-    def get_bias_value(model: onnx.ModelProto, node: NNCFNode) -> np.ndarray:
-        onnx_graph = ONNXGraph(model)
-        onnx_node = onnx_graph.get_node_by_name(node.node_name)
-        bias_port_id = onnx_graph.get_bias_tensor_port_id(onnx_node)
-        bias_input_name = onnx_node.input[bias_port_id]
-        if onnx_graph.has_initializer(bias_input_name):
-            return onnx_graph.get_initializers_value(bias_input_name)
-        node = onnx_graph.get_nodes_by_output(bias_input_name)[0]
-        metatype = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name(node.op_type)
-        if metatype == ONNXIdentityMetatype:
-            return onnx_graph.get_initializers_value(node.input[0])
-        raise RuntimeError('Could not find the bias value of the node')
+    def get_bias_value(node: NNCFNode, model: onnx.ModelProto, nncf_graph: NNCFGraph) -> np.ndarray:
+        return get_bias_value(node, model)
 
     @staticmethod
-    def get_bias_port_id(model: onnx.ModelProto, node: NNCFNode) -> int:
+    def get_input_name(model: onnx.ModelProto, node_name: str) -> str:
         onnx_graph = ONNXGraph(model)
-        onnx_node = onnx_graph.get_node_by_name(node.node_name)
-        return onnx_graph.get_bias_tensor_port_id(onnx_node)
+        node = onnx_graph.get_node_by_name(node_name)
+        return node.input[0]
 
     @staticmethod
-    def get_output_names(model: onnx.ModelProto, node_name: str) -> List[str]:
+    def get_output_name(model: onnx.ModelProto, node_name: str) -> List[str]:
         onnx_graph = ONNXGraph(model)
         node = onnx_graph.get_node_by_name(node_name)
-        return node.output
+        return node.output[0]
 
     @staticmethod
-    def extract_model(model: onnx.ModelProto,
-                      input_node_names: List[str],
-                      output_node_names: List[str]) -> onnx.ModelProto:
-        onnx_graph = ONNXGraph(model)
-
-        input_tensor_names = []
-        for input_node_name in input_node_names:
-            input_onnx_node = onnx_graph.get_node_by_name(input_node_name)
-            input_tensor_names.append(input_onnx_node.input[0])
-
-        output_tensor_names = []
-        for output_node_name in output_node_names:
-            output_onnx_node = onnx_graph.get_node_by_name(output_node_name)
-            output_tensor_names.append(output_onnx_node.input[0])
-
-        if not output_node_names:
-            output_tensor_names = [n.name for n in onnx_graph.get_model_outputs()]
-
-        transformation_layout = TransformationLayout()
-        model_transformer = ONNXModelTransformer(model)
-        transformation_layout.register(ONNXModelExtractionCommand(set(input_tensor_names), set(output_tensor_names)))
-        return model_transformer.transform(transformation_layout)
+    def is_quantized_weights(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
+        input_nodes = [edge.from_node for edge in nncf_graph.get_input_edges(node)]
+        weight_port_id = node.metatype.weight_definitions.weight_port_id
+        weight_node = input_nodes[weight_port_id]
+        return weight_node.metatype == ONNXDequantizeLinearMetatype
 
     @staticmethod
-    def is_quantized_weights(node: NNCFNode, model: onnx.ModelProto) -> bool:
-        onnx_graph = ONNXGraph(model)
-        onnx_node = onnx_graph.get_node_by_name(node.node_name)
-        # We assume that the weight is on the first-index
-        weight_port_id = onnx_graph.get_weight_port_id(onnx_node)
-        input_edge_names = onnx_graph.get_node_edge_names(node.node_name)['input']
-        nodes_after_weight = onnx_graph.get_nodes_by_output(input_edge_names[weight_port_id])
-        if not nodes_after_weight:
-            return False
-        # We assume that there is only one node after weight
-        assert len(nodes_after_weight) == 1
-        weight_dequantizer = nodes_after_weight[0]
-        metatype = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name(weight_dequantizer.op_type)
-        return metatype == ONNXDequantizeLinearMetatype
-
-    @staticmethod
-    def is_node_with_bias(node: NNCFNode) -> bool:
-        input_tensor_names = node.layer_attributes.input_tensor_names
-        return len(input_tensor_names) > 2
+    def is_node_with_bias(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
+        return is_node_with_bias(node)
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/definitions.py` & `nncf-2.5.0/nncf/tensorflow/utils/node.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from enum import Enum
+import re
 
+from nncf.tensorflow.graph.utils import get_original_name_and_instance_idx
 
-class Granularity(Enum):
-    PERTENSOR = 'pertensor'
-    PERCHANNEL = 'perchannel'
 
-
-class RangeType(Enum):
-    MINMAX = 'min_max'
-    MEAN_MINMAX = 'mean_min_max'
+def is_ignored(node_name, ignored_scopes):
+    original_name, _ = get_original_name_and_instance_idx(node_name)
+    return any(
+        re.fullmatch(ignored.replace("{re}", ""), original_name)
+        if ignored.startswith("{re}")
+        else ignored == original_name
+        for ignored in ignored_scopes
+    )
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/fast_bias_correction/algorithm.py` & `nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/algorithm.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,327 +1,338 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict, Tuple, List, TypeVar, Optional
+from typing import Any, Dict, List, Optional, Tuple, TypeVar
 
 import numpy as np
+from tqdm import tqdm
+
 from nncf import Dataset
-from nncf.common.tensor import NNCFTensor
-from nncf.common.logging import nncf_logger
+from nncf.common.factory import EngineFactory
+from nncf.common.factory import ModelTransformerFactory
+from nncf.common.factory import NNCFGraphFactory
+from nncf.common.graph.model_transformer import ModelTransformer
+from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.layout import TransformationLayout
+from nncf.common.logging import nncf_logger
+from nncf.common.tensor import NNCFTensor
+from nncf.common.tensor_statistics.statistic_point import StatisticPoint
+from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.backend import get_backend
-from nncf.common.graph.transformations.commands import TargetPoint
-from nncf.quantization.algorithms.algorithm import AlgorithmParameters
 from nncf.quantization.algorithms.algorithm import Algorithm
 from nncf.quantization.algorithms.fast_bias_correction.backend import ALGO_BACKENDS
-from nncf.common.factory import NNCFGraphFactory
-from nncf.common.factory import EngineFactory
-from nncf.common.tensor_statistics.statistic_point import StatisticPoint
-from nncf.common.tensor_statistics.statistic_point import StatisticPointsContainer
-
-TModel = TypeVar('TModel')
 
+TModel = TypeVar("TModel")
 
-class FastBiasCorrectionParameters(AlgorithmParameters):
-    """
-    Parameters of FastBiasCorrection algorithm
-
-    :param number_samples: The number of the samples for the statistics collection.
-    :param threshold: The magnitude threshold that regulates the application of the shift.
-    """
-
-    def __init__(self, number_samples: int = 100, threshold: float = 2.0) -> None:
-        """
-        :param number_samples: The number of the samples for the statistics collection.
-            This statistics uses for the further calculation of the bias shift.
-        :param threshold: The magnitude threshold that regulates the application of the shift.
-            Magnitude calculates as the maximum of the absolute ratio of the shift to the original bias value.
-            If the calculated value less than threshold, shift will apply to the bias.
-        """
-        self.number_samples = number_samples
-        self.threshold = threshold
+FAST_BIAS_CORRECTION_THRESHOLD = 2
 
 
 class FastBiasCorrection(Algorithm):
     """
-    Post-training FastBiasCorrection algorithm implementation
+    Post-training FastBiasCorrection algorithm implementation.
 
     The main purpose of this algorithm to reduce quantization error
     via correction the bias of the Convolutions, FullyConnected, etc. layers.
     The algorithm pipeline is very simple:
         - we collects floating-point statistics from the corresponding model for the layers with bias;
         - then we gets the quantized model and try to reduce it's error by correction of the bias;
         - the shift calculates using the sub-graph that consists of the correction layer and
         weight quantizer-dequantizer pair or fake quantize node;
         - the floating-point statistics uses as input for
         the sub-graph and further quantization output calculation;
         - in the end we corrects the original bias by the difference (shift)
         between floating-point and quantized outputs.
-
-    :param number_samples: The number of the samples for the statistics collection.
-    :param threshold: The magnitude threshold that regulates the application of the shift.
-    :param nncf_graph: NNCFGraph class for the algorithm.
     """
 
-    def __init__(self, parameters: FastBiasCorrectionParameters) -> None:
-        """
-        :param parameters: The instance of the FastBiasCorrectionParameters.
+    def __init__(
+        self,
+        subset_size: int = 100,
+        threshold: float = FAST_BIAS_CORRECTION_THRESHOLD,
+        apply_for_all_nodes: bool = False,
+        inplace_statistics: bool = True,
+        backend_params: Optional[Dict[str, Any]] = None,
+    ):
+        """
+        :param subset_size: Size of a subset for the statistics collection,
+            defaults to 100.
+        :param threshold: The magnitude threshold that regulates the application of the
+            shift. Magnitude calculates as the maximum of the absolute ratio of the
+            shift to the original bias value. If the calculated value is less than the
+            threshold, the shift will apply to the bias, defaults to 2.
+        :param apply_for_all_nodes: If True, then the bias correction be applied to all
+            quantized nodes, if the node has no bias then a bias node will be inserted,
+            and if False, then the bias correction will only be applied to quantized
+            nodes that have a bias.
+        :param inplace_statistics: Defines wheather to calculate quantizers statistics
+            by backend graph operations or by default Python implementation, defaults
+            to True.
+        :param backend_params: Backend specific parameters.
         """
         super().__init__()
-        self.number_samples = parameters.number_samples
-        self.threshold = parameters.threshold
+        self.subset_size = subset_size
+        self.threshold = threshold
+        self.apply_for_all_nodes = apply_for_all_nodes
+        self.inplace_statistics = inplace_statistics
+        self.backend_params = backend_params
         self.nncf_graph = None
         self._backend_entity = None
 
+        if self.apply_for_all_nodes:
+            raise RuntimeError("FastBiasCorrection algorithm does not support apply_for_all_nodes=True yet")
+
     @property
     def available_backends(self) -> Dict[str, BackendType]:
         return ALGO_BACKENDS.registry_dict
 
     def _set_backend_entity(self, model: TModel) -> None:
         """
-        Creates a helper class with a backed-specific logic of the algorithm
+        Creates a helper class with a backed-specific logic of the algorithm.
 
-        :param model: backend-specific input model
+        :param model: Backend-specific input model.
         """
         model_backend = get_backend(model)
         if model_backend == BackendType.ONNX:
-            from nncf.quantization.algorithms.fast_bias_correction.onnx_backend import \
-                ONNXFBCAlgoBackend
-            self._backend_entity = ONNXFBCAlgoBackend()
-        else:
-            raise RuntimeError('Cannot return backend-specific entity'
-                               'because {} is not supported!'.format(model_backend))
+            from nncf.quantization.algorithms.fast_bias_correction.onnx_backend import ONNXFastBiasCorrectionAlgoBackend
 
-    def _apply(self,
-               model: TModel,
-               statistic_points: Optional[StatisticPointsContainer] = None,
-               dataset: Optional[Dataset] = None) -> TModel:
+            self._backend_entity = ONNXFastBiasCorrectionAlgoBackend()
+        elif model_backend == BackendType.OPENVINO:
+            from nncf.quantization.algorithms.fast_bias_correction.openvino_backend import (
+                OVFastBiasCorrectionAlgoBackend,
+            )
+
+            self._backend_entity = OVFastBiasCorrectionAlgoBackend()
+        else:
+            raise RuntimeError(
+                "Cannot return backend-specific entity because {} is not supported!".format(model_backend)
+            )
+
+    def _apply(
+        self,
+        model: TModel,
+        statistic_points: Optional[StatisticPointsContainer] = None,
+        dataset: Optional[Dataset] = None,
+    ) -> TModel:
         self._set_backend_entity(model)
 
-        model_transformer = self._backend_entity.model_transformer(model)
-        transformation_layout = TransformationLayout()
         nncf_graph = NNCFGraphFactory.create(model)
-
-        layers_with_bias_types = self._backend_entity.layers_with_bias_metatypes
-        biased_nodes = nncf_graph.get_nodes_by_metatypes(layers_with_bias_types)
-
-        for node in biased_nodes:
+        node_and_bias_value = (
+            (node, self._backend_entity.get_bias_value(node, nncf_graph, model))
+            for node in nncf_graph.get_all_nodes()
+            if self._backend_entity.is_node_with_bias(node, nncf_graph)
+        )
+        model_transformer = ModelTransformerFactory.create(model)
+        # Fill `node_and_new_bias_value` list. It is a correspondence between nodes
+        # for which we should update bias and new bias values.
+        node_and_new_bias_value = []
+        for node, bias_value in tqdm(list(node_and_bias_value), desc="Biases correction"):
             node_name = node.node_name
 
-            if not self._backend_entity.is_node_with_bias(node):
-                nncf_logger.debug(f'Skipping node {node_name} because there is no bias')
-                continue
-            if not self._backend_entity.is_quantized_weights(node, model):
-                nncf_logger.debug(f'Skipping node {node_name} because weights were not quantized')
+            if not self._backend_entity.is_quantized_weights(node, nncf_graph):
+                nncf_logger.debug(f"Skipping node {node_name} because weights were not quantized")
                 continue
 
             input_fp, input_shape = self._get_fp_inputs(statistic_points, node_name)
             output_fp = self._get_fp_outputs(statistic_points, node_name)
 
-            input_tensor_names, output_tensor_names = self._backend_entity.get_tensor_names(node)
-            input_name = input_tensor_names[0]
-            output_name = output_tensor_names[0]
-
-            extracted_model = self._extract_submodel(model, [input_name], [output_name])
-
-            channel_axis = self._backend_entity.channel_axis_by_types[node.node_type]
-            input_blob = self._create_input_data(input_shape,
-                                                 input_fp,
-                                                 input_name)
+            extracted_model = self._extract_submodel(model_transformer, node_name)
+
+            sub_input_name, sub_output_name = self._backend_entity.get_sub_input_output_names(extracted_model)
+
+            channel_axis = node.metatype.output_channel_axis
+            if bias_value.ndim > 1:
+                # Make index positive
+                channel_axis = range(bias_value.ndim)[channel_axis]
+            input_blob = self._create_input_data(input_shape, input_fp, sub_input_name, channel_axis)
             bias_shift = self._get_bias_shift(
                 model=extracted_model,
                 input_blob=input_blob,
                 channel_axis=channel_axis,
                 output_fp=output_fp,
-                output_name=output_name)
+                output_name=sub_output_name,
+            )
 
-            current_bias = self._backend_entity.get_bias_value(model, node)
-            updated_bias = current_bias + bias_shift
-            magnitude = self._get_bias_shift_magnitude(current_bias, updated_bias)
+            if bias_value.ndim > 1:
+                axes = [i for i in range(bias_value.ndim) if i != channel_axis]
+                bias_shift = np.expand_dims(bias_shift, axes)
+
+            updated_bias = bias_value + bias_shift
+            magnitude = self._get_bias_shift_magnitude(bias_value, updated_bias)
 
             if magnitude < self.threshold:
-                nncf_logger.debug(f'{node_name} bias would be changed')
-                bias_port_id = self._backend_entity.get_bias_port_id(model, node)
-                target_point = self._backend_entity.target_point(TargetType.LAYER,
-                                                                 node.node_name,
-                                                                 bias_port_id)
-                bias_correction_command = self._backend_entity.bias_correction_command(target_point,
-                                                                                       updated_bias)
-                transformation_layout.register(bias_correction_command)
+                nncf_logger.debug(f"{node_name} bias would be changed")
+                node_and_new_bias_value.append((node, updated_bias))
             else:
-                nncf_logger.debug(f'{node_name} bias skipped by threshold')
+                nncf_logger.debug(f"{node_name} bias skipped by threshold. Magnitude: {magnitude}")
+
+        # Create commands of bias correction and apply them to the model.
+        transformation_layout = TransformationLayout()
+        for node, bias_value in node_and_new_bias_value:
+            transformation_layout.register(
+                self._backend_entity.create_bias_correction_command(node, bias_value, nncf_graph)
+            )
+        transformed_model = model_transformer.transform(transformation_layout)
 
-        quantized_model = model_transformer.transform(transformation_layout)
-        return quantized_model
+        return transformed_model
 
     def _get_fp_inputs(self, statistic_points: StatisticPointsContainer, node_name: str) -> Tuple[List, List]:
         """
-        Makes out per-layer needed data from the floating-point collected statistics
+        Makes out per-layer needed data from the floating-point collected statistics.
 
-        :param statistic_points: filled StatisticPointsContainer
-        :param node_name: name of the current layer
-        :return: collected mean tensor data and shape for the further bias calculation
+        :param statistic_points: Filled StatisticPointsContainer.
+        :param node_name: Name of the current layer.
+        :return: Collected mean tensor data and shape for the further bias calculation.
         """
 
         def input_filter_func(point):
-            return FastBiasCorrection in point.algorithm_to_tensor_collectors and \
-                   point.target_point.type == TargetType.PRE_LAYER_OPERATION
+            return (
+                FastBiasCorrection in point.algorithm_to_tensor_collectors
+                and point.target_point.type == TargetType.PRE_LAYER_OPERATION
+            )
 
         input_fp = []
         input_shape = []
         for tensor_collector in statistic_points.get_algo_statistics_for_node(
-                node_name,
-                input_filter_func,
-                FastBiasCorrection):
-            input_fp.extend(tensor_collector.get_statistics().mean_values)
-            input_shape.extend(tensor_collector.get_statistics().shape)
+            node_name, input_filter_func, FastBiasCorrection
+        ):
+            statistics = tensor_collector.get_statistics()
+            input_fp.extend(statistics.mean_values)
+            input_shape.extend(statistics.shape)
         return input_fp, input_shape
 
     def _get_fp_outputs(self, statistic_points: StatisticPointsContainer, node_name: str) -> List[np.ndarray]:
         """
-        Makes out per-layer needed data from the floating-point collected statistics
+        Makes out per-layer needed data from the floating-point collected statistics.
 
-        :param statistic_points: filled StatisticPointsContainer
-        :param node_name: name of the current layer
-        :return: collected mean tensor data for the further bias calculation
+        :param statistic_points: Filled StatisticPointsContainer.
+        :param node_name: Name of the current layer.
+        :return: Collected mean tensor data for the further bias calculation.
         """
 
         def output_filter_func(point):
-            return FastBiasCorrection in point.algorithm_to_tensor_collectors and \
-                   point.target_point.type == TargetType.POST_LAYER_OPERATION
+            return (
+                FastBiasCorrection in point.algorithm_to_tensor_collectors
+                and point.target_point.type == TargetType.POST_LAYER_OPERATION
+            )
 
         output_fp = []
         for tensor_collector in statistic_points.get_algo_statistics_for_node(
-                node_name,
-                output_filter_func,
-                FastBiasCorrection):
+            node_name, output_filter_func, FastBiasCorrection
+        ):
             output_fp.extend(tensor_collector.get_statistics().mean_values)
         return output_fp
 
-    def _extract_submodel(self,
-                          model: TModel,
-                          input_names: List[str],
-                          output_names: List[str]) -> TModel:
-        """
-        Extracts sub-model from the original based on the input & output tensor names
-
-        :param model: backend-specific model
-        :param input_names: list of the input names
-        :param output_names: list of the output names
-        :return: backend-specific sub-model
-        """
-        model_transformer = self._backend_entity.model_transformer(model)
-        model_extraction_command = self._backend_entity.model_extraction_command(input_names,
-                                                                                 output_names)
+    def _extract_submodel(self, model_transformer: ModelTransformer, node_name: str) -> TModel:
+        """
+        Extracts sub-model using backend-specific ModelTransformer.
+
+        :param model_transformer: Backend-specific ModelTransformer.
+        :param node_name: Name of the node that should be a center of the sub-model.
+        :return: Backend-specific sub-model.
+        """
+        model_extraction_command = self._backend_entity.model_extraction_command([node_name], [node_name])
         me_transformation_layout = TransformationLayout()
         me_transformation_layout.register(model_extraction_command)
         extracted_model = model_transformer.transform(me_transformation_layout)
         return extracted_model
 
     def _add_statistic_point(self, container: StatisticPointsContainer, point: TargetPoint, axis: int) -> None:
         """
-        Adds specific statistic point
+        Adds specific statistic point.
 
-        :param container: StatisticPointsContainer
-        :param point: TargetPoint for statistic collection
-        :param axis: channel axis for the statistics calculation
-        """
-        stat_collector = self._backend_entity.mean_statistic_collector(reduction_shape=axis,
-                                                                       num_samples=self.number_samples)
-        container.add_statistic_point(StatisticPoint(target_point=point,
-                                                     tensor_collector=stat_collector,
-                                                     algorithm=FastBiasCorrection))
-
-    def _create_input_data(self,
-                           input_shape: Tuple[int],
-                           input_fp: List[np.ndarray],
-                           input_name: str) -> Dict[str, NNCFTensor]:
-        """
-        Creates input blob for the bias shift calculation
-
-        :param input_shape: input shape for the blob
-        :param input_fp: input data for the blob
-        :param input_name: name for the output dict
-        :return: dictionary of the blob by input name
+        :param container: StatisticPointsContainer instance.
+        :param point: TargetPoint for statistic collection.
+        :param axis: Channel axis for the statistics calculation.
+        """
+        stat_collector = self._backend_entity.mean_statistic_collector(
+            reduction_shape=axis, num_samples=self.subset_size, inplace=self.inplace_statistics
+        )
+        container.add_statistic_point(
+            StatisticPoint(target_point=point, tensor_collector=stat_collector, algorithm=FastBiasCorrection)
+        )
+
+    def _create_input_data(
+        self, input_shape: Tuple[int], input_fp: List[np.ndarray], input_name: str, channel_axis: int
+    ) -> Dict[str, NNCFTensor]:
+        """
+        Creates input blob for the bias shift calculation.
+        :param input_shape: Input shape for the blob.
+        :param input_fp: Input data for the blob.
+        :param input_name: Name for the output dictionary.
+        :param channel_axis: Axis to fill the blob with provided data.
+        :return: The dictionary of the blob by input name.
         """
-        input_blob = self._backend_entity.create_blob(input_shape, input_fp)
+        input_blob = self._backend_entity.create_blob(input_shape, input_fp, channel_axis)
         input_data = {input_name: input_blob}
         return input_data
 
-    def _get_bias_shift(self,
-                        model: TModel,
-                        input_blob: Dict[str, NNCFTensor],
-                        channel_axis: Tuple[int],
-                        output_fp: List[np.ndarray],
-                        output_name: str) -> np.ndarray:
-        """
-        Calculates bias shift for the further corretion
-
-        :param engine: backend-specific engine instance for the model execution
-        :param model: backend-specific sub-model for the execution
-        :param input_blob: input data for the execution
-        :param channel_axis: channel axes for the raw data aggregation
-        :param output_fp: output data for the shift calculation
-        :param output_name: name of the output tensor for the data collection
-        :return: calculated bias shift
+    def _get_bias_shift(
+        self,
+        model: TModel,
+        input_blob: Dict[str, NNCFTensor],
+        channel_axis: Tuple[int],
+        output_fp: List[np.ndarray],
+        output_name: str,
+    ) -> np.ndarray:
+        """
+        Calculates updated bias.
+
+        :param engine: Backend-specific engine instance for the model execution.
+        :param model: Backend-specific sub-model for the execution.
+        :param input_blob: Input data for the execution.
+        :param channel_axis: Channel axis for the raw data aggregation.
+        :param output_fp: Output data for the shift calculation.
+        :param output_name: Name of the output tensor for the data collection.
+        :return: Calculated bias shift.
         """
         engine = EngineFactory.create(model)
         raw_output = engine.infer(input_blob)
         q_outputs = self._backend_entity.process_model_output(raw_output, output_name)
         q_outputs = self._backend_entity.tensor_processor.mean_per_channel(q_outputs, channel_axis).tensor
         bias_shift = np.array(output_fp) - q_outputs
         return bias_shift
 
     @staticmethod
     def _get_bias_shift_magnitude(current_bias_value: np.ndarray, updated_bias_value: np.ndarray) -> float:
         """
-        Calculates bias shift magnitude based on the current and updated values
+        Calculates bias shift magnitude based on the current and updated values.
 
-        :param current_bias_value: the original bias value
-        :param updated_bias_value: the updated bias value
-        :return: magnitude between original and updated bias values
+        :param current_bias_value: The original bias value.
+        :param updated_bias_value: The updated bias value.
+        :return: Magnitude between original and updated bias values.
         """
         bias_shift_magnitude = np.inf
         if np.count_nonzero(current_bias_value == 0) == 0:
             bias_shift_magnitude = np.max(np.abs((updated_bias_value - current_bias_value) / current_bias_value))
         return bias_shift_magnitude
 
     def get_statistic_points(self, model: TModel) -> StatisticPointsContainer:
         self._set_backend_entity(model)
         nncf_graph = NNCFGraphFactory.create(model) if self.nncf_graph is None else self.nncf_graph
-        layers_with_bias_types = self._backend_entity.layers_with_bias_metatypes
-        biased_nodes = nncf_graph.get_nodes_by_metatypes(layers_with_bias_types)
+        nodes_with_bias = [
+            node for node in nncf_graph.get_all_nodes() if self._backend_entity.is_node_with_bias(node, nncf_graph)
+        ]
 
         statistic_container = StatisticPointsContainer()
+        for node in nodes_with_bias:
+            input_port_id, output_port_id = self._backend_entity.get_activation_port_ids_for_bias_node(node)
+            pre_layer_statistic_point = self._backend_entity.target_point(
+                TargetType.PRE_LAYER_OPERATION, node.node_name, input_port_id
+            )
+            post_layer_statistic_point = self._backend_entity.target_point(
+                TargetType.POST_LAYER_OPERATION, node.node_name, output_port_id
+            )
+            channel_axis = node.metatype.output_channel_axis
 
-        for node in biased_nodes:
-            if not self._backend_entity.is_node_with_bias(node):
-                continue
-            input_port_id, output_port_id = self._backend_entity.get_activation_port_ids_for_bias_node(model, node)
-            pre_layer_statistic_point = self._backend_entity.target_point(TargetType.PRE_LAYER_OPERATION,
-                                                                          node.node_name,
-                                                                          input_port_id)
-            post_layer_statistic_point = self._backend_entity.target_point(TargetType.POST_LAYER_OPERATION,
-                                                                           node.node_name,
-                                                                           output_port_id)
-            channel_axis = self._backend_entity.channel_axis_by_types[node.node_type]
-
-            self._add_statistic_point(statistic_container,
-                                      pre_layer_statistic_point,
-                                      channel_axis)
-            self._add_statistic_point(statistic_container,
-                                      post_layer_statistic_point,
-                                      channel_axis)
+            self._add_statistic_point(statistic_container, pre_layer_statistic_point, channel_axis)
+            self._add_statistic_point(statistic_container, post_layer_statistic_point, channel_axis)
 
         return statistic_container
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/fast_bias_correction/backend.py` & `nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/backend.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,194 +1,161 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
-from typing import List, Tuple, TypeVar, Optional
+from typing import List, Optional, Tuple, TypeVar
 
 import numpy as np
+
+from nncf.common.graph import NNCFGraph
+from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetPoint
-from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TargetType
+from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.tensor import NNCFTensor
-from nncf.common.graph import NNCFNode
-from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.common.tensor_statistics.collectors import ReductionShape
+from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.common.utils.registry import Registry
-from nncf.common.graph.model_transformer import ModelTransformer
-
-TModel = TypeVar('TModel')
-OutputType = TypeVar('OutputType')
-ALGO_BACKENDS = Registry('algo_backends')
 
+TModel = TypeVar("TModel")
+OutputType = TypeVar("OutputType")
+ALGO_BACKENDS = Registry("algo_backends")
 
-class FBCAlgoBackend(ABC):
 
+class FastBiasCorrectionAlgoBackend(ABC):
     @property
     @abstractmethod
     def operation_metatypes(self):
         """
         Property for the backend-specific metatypes.
         """
 
     @property
     @abstractmethod
-    def layers_with_bias_metatypes(self):
-        """
-        Property for the backend-specific metatypes with bias.
-        """
-
-    @property
-    @abstractmethod
-    def channel_axis_by_types(self):
-        """
-        Property for the backend-specific info about channels placement in the layout.
-        """
-
-    @property
-    @abstractmethod
     def tensor_processor(self):
         """
         Returns backend-specific instance of the NNCFCollectorTensorProcessor.
         """
 
     @staticmethod
     @abstractmethod
-    def model_transformer(model: TModel) -> ModelTransformer:
-        """
-        Returns backend-specific ModelTransformer instance.
-
-        :param model: Backend-specific model to create ModelTransformer.
-        :return: ModelTransformer instance.
-        """
-
-    @staticmethod
-    @abstractmethod
     def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> TargetPoint:
         """
         Returns backend-specific target point.
 
         :param target_type: Type of the location that should be modified.
         :param target_node_name: Name of the located node.
         :param port_id: Port ID of the tensor for the statistics distribution.
         :return: Backend-specific TargetPoint.
         """
 
     @staticmethod
     @abstractmethod
-    def bias_correction_command(target_point: TargetPoint,
-                                bias_value: np.ndarray,
-                                threshold: float) -> TransformationCommand:
+    def create_bias_correction_command(node: NNCFNode, bias_value: np.ndarray, nncf_graph: NNCFGraph):
         """
-        Returns backend-specific bias correction command.
+        Creates backend-specific command to update bias value.
 
-        :param target_point: Target location for the correction.
+        :param node: The node for which bias should be updated.
         :param bias_value: New value for the bias.
-        :param threshold: Parametrized threshold for the shift magnitude comparison.
-        :return: Backend-specific TransformationCommand for the bias correction.
+        :param nncf_graph: NNCFGraph instance that contains the node.
+        :return: Backend-specific command to update bias value.
         """
 
     @staticmethod
     @abstractmethod
     def model_extraction_command(inputs: List[str], outputs: List[str]) -> TransformationCommand:
         """
-        Returns backend-specific bias correction.
+        Returns backend-specific command to extract sub-model based on input & output names.
 
         :param inputs: List of the input names for sub-model beggining.
         :param outputs: List of the output names for sub-model end.
         :return: Backend-specific TransformationCommand for the model extraction.
         """
 
     @staticmethod
     @abstractmethod
-    def mean_statistic_collector(reduction_shape: ReductionShape,
-                                 num_samples: Optional[int] = None,
-                                 window_size: Optional[int] = None) -> TensorStatisticCollectorBase:
+    def mean_statistic_collector(
+        reduction_shape: ReductionShape,
+        inplace: bool,
+        num_samples: Optional[int] = None,
+        window_size: Optional[int] = None,
+    ) -> TensorStatisticCollectorBase:
         """
         Returns backend-specific mean statistic collector.
 
         :param reduction_shape: Channel axes for the statistics aggregation.
+        :param inplace: Whether to calculate statistic inplace or not.
         :param num_samples: Maximum number of samples to collect.
         :param window_size: The maximum size of the samples queue.
         :return: Backend-specific TensorStatisticCollectorBase for the statistics calculation.
         """
 
     @staticmethod
     @abstractmethod
-    def get_tensor_names(node: NNCFNode) -> Tuple[List[str], List[str]]:
+    def get_sub_input_output_names(subgraph: TModel) -> Tuple[str, str]:
         """
-        Returns tuple of the lists with the input & output tensor names respectively.
+        Returns tuple of the subgraph's the input & output tensor names respectively.
 
-        :param node: NNCFNode with the layer_attributes.
-        :return: Tuple of the lists with the names.
+        :param node: NNCFNode instance.
+        :return: Tuple of the names.
         """
 
     @staticmethod
     @abstractmethod
-    def create_blob(shape: Tuple[int], data: List[float]) -> np.ndarray:
+    def create_blob(shape: Tuple[int], data: List[np.ndarray], channel_axis: int) -> np.ndarray:
         """
         Creates the backend-specific (because of layout) blob.
 
         :param shape: Shape of the blob.
         :param data: Data to fill the blob.
+        :param channel_axis: Axis to fill the blob with provided data.
         :return: np.ndarray blob.
         """
 
     @staticmethod
     @abstractmethod
-    def get_bias_value(model: TModel, node: NNCFNode) -> np.ndarray:
+    def get_bias_value(node: NNCFNode, nncf_graph: NNCFGraph, model: TModel) -> np.ndarray:
         """
         Returns bias value in the NumPy format of provided node.
 
-        :param model: Backend-specific model for the initializer finding.
         :param node: Node of NNCFGraph with bias value.
+        :param nncf_graph: NNCFGraph instance.
+        :param model: Backend-specific model for the initializer finding.
         :return: Bias value in the NumPy format.
         """
 
     @staticmethod
     @abstractmethod
-    def get_bias_port_id(model: TModel, node: NNCFNode) -> int:
-        """
-        Returns bias Port ID corresponding to the node.
-
-        :param model: Backend-specific model.
-        :param node: Node of NNCFGraph with bias value.
-        :return: Port ID corresponding to bias.
-        """
-
-    @staticmethod
-    @abstractmethod
-    def get_activation_port_ids_for_bias_node(model: TModel, node: NNCFNode) -> Tuple[int, int]:
+    def get_activation_port_ids_for_bias_node(node: NNCFNode) -> Tuple[int, int]:
         """
         Returns Input Port ID and Output Port ID corresponding to activation input and output edges for
         the node.
         Supports only nodes that could have bias value.
 
-        :param model: Backend-specific model.
         :param node: Node of NNCFGraph with bias value.
+        :param model: Backend-specific model.
         """
 
     @staticmethod
     @abstractmethod
-    def is_quantized_weights(node: NNCFNode, model: TModel) -> bool:
+    def is_quantized_weights(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
         """
         Checks whether the node is quantized or not.
 
         :param node: NNCFNode to check.
-        :param model: Backend-specific model.
+        :param nncf_graph: NNCFGraph instance.
         :return: boolean indicating whether the node has a quantized weights or not
         """
 
     @staticmethod
     @abstractmethod
     def process_model_output(raw_data: OutputType, output_name: str) -> NNCFTensor:
         """
@@ -197,14 +164,15 @@
         :param raw_data: Backend-specific output from the model.
         :param output_name: Name of the output layer or tensor name.
         :return: Processed output as NNCFTensor.
         """
 
     @staticmethod
     @abstractmethod
-    def is_node_with_bias(node: NNCFNode) -> bool:
+    def is_node_with_bias(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
         """
         Checks whether the node has a bias or not.
 
         :param node: NNCFNode with the attributes.
+        :param nncf_graph: NNCFGraph that contains node.
         :return: Boolean indicating whether the node has a bias or not.
         """
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py` & `nncf-2.5.0/nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,143 +1,105 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Dict, List, Optional, Tuple
 
-from typing import Dict, Tuple, List, Optional
-import onnx
 import numpy as np
+import onnx
+
+from nncf.common.graph import NNCFGraph
+from nncf.common.graph import NNCFNode
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.tensor_statistics.collectors import ReductionShape
 from nncf.common.utils.backend import BackendType
 from nncf.common.utils.registry import Registry
-from nncf.common.graph import NNCFNode
-
-from nncf.onnx.graph.metatypes.onnx_metatypes import LAYERS_WITH_BIAS_METATYPES
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNX_OPERATION_METATYPES
-from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXIdentityMetatype
 from nncf.onnx.graph.metatypes.onnx_metatypes import ONNXDequantizeLinearMetatype
-from nncf.onnx.graph.model_transformer import ONNXModelTransformer
+from nncf.onnx.graph.node_utils import get_bias_value
+from nncf.onnx.graph.node_utils import is_node_with_bias
+from nncf.onnx.graph.transformations.command_creation import create_bias_correction_command
 from nncf.onnx.graph.transformations.commands import ONNXBiasCorrectionCommand
 from nncf.onnx.graph.transformations.commands import ONNXModelExtractionCommand
 from nncf.onnx.graph.transformations.commands import ONNXTargetPoint
 from nncf.onnx.statistics.collectors import ONNXMeanStatisticCollector
 from nncf.onnx.statistics.collectors import ONNXNNCFCollectorTensorProcessor
 from nncf.onnx.tensor import ONNXNNCFTensor
 from nncf.quantization.algorithms.fast_bias_correction.backend import ALGO_BACKENDS
-from nncf.quantization.algorithms.fast_bias_correction.backend import FBCAlgoBackend
-from nncf.onnx.graph.onnx_graph import ONNXGraph
+from nncf.quantization.algorithms.fast_bias_correction.backend import FastBiasCorrectionAlgoBackend
 
 
 @ALGO_BACKENDS.register(BackendType.ONNX)
-class ONNXFBCAlgoBackend(FBCAlgoBackend):
-
+class ONNXFastBiasCorrectionAlgoBackend(FastBiasCorrectionAlgoBackend):
     @property
     def operation_metatypes(self) -> Registry:
         return ONNX_OPERATION_METATYPES
 
     @property
-    def layers_with_bias_metatypes(self) -> Registry:
-        return LAYERS_WITH_BIAS_METATYPES
-
-    @property
-    def channel_axis_by_types(self) -> Dict[str, int]:
-        return {'Conv': 1, 'Gemm': -1, 'ConvTranspose': 1}
-
-    @property
     def tensor_processor(self) -> ONNXNNCFCollectorTensorProcessor:
         return ONNXNNCFCollectorTensorProcessor()
 
     @staticmethod
-    def model_transformer(model: onnx.ModelProto) -> ONNXModelTransformer:
-        return ONNXModelTransformer(model)
-
-    @staticmethod
-    def target_point(target_type: TargetType,
-                     target_node_name: str,
-                     port_id: int) -> ONNXTargetPoint:
+    def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> ONNXTargetPoint:
         return ONNXTargetPoint(target_type, target_node_name, port_id)
 
     @staticmethod
-    def bias_correction_command(target_point: ONNXTargetPoint,
-                                bias_value: np.ndarray) -> ONNXBiasCorrectionCommand:
-        return ONNXBiasCorrectionCommand(target_point, bias_value)
+    def create_bias_correction_command(
+        node: NNCFNode, bias_value: np.ndarray, nncf_graph: NNCFGraph
+    ) -> ONNXBiasCorrectionCommand:
+        return create_bias_correction_command(node, bias_value)
 
     @staticmethod
     def model_extraction_command(inputs: List[str], outputs: List[str]) -> ONNXModelExtractionCommand:
         return ONNXModelExtractionCommand(inputs, outputs)
 
     @staticmethod
-    def mean_statistic_collector(reduction_shape: ReductionShape,
-                                 num_samples: Optional[int] = None,
-                                 window_size: Optional[int] = None) -> ONNXMeanStatisticCollector:
+    def mean_statistic_collector(
+        reduction_shape: ReductionShape,
+        inplace: bool,
+        num_samples: Optional[int] = None,
+        window_size: Optional[int] = None,
+    ) -> ONNXMeanStatisticCollector:
         return ONNXMeanStatisticCollector(reduction_shape, num_samples, window_size)
 
     @staticmethod
-    def get_tensor_names(node: NNCFNode):
-        return node.layer_attributes.input_tensor_names, \
-               node.layer_attributes.output_tensor_names
+    def get_sub_input_output_names(subgraph: onnx.ModelProto) -> Tuple[str, str]:
+        return subgraph.graph.input[0].name, subgraph.graph.output[0].name
 
     @staticmethod
-    def create_blob(shape: Tuple[int], data: List[float]) -> np.ndarray:
+    def create_blob(shape: Tuple[int], data: List[np.ndarray], channel_axis: int) -> np.ndarray:
         blob = np.zeros(shape)
-        for i, value in enumerate(data):
-            blob[:, i] = value
-        blob = blob.astype(np.float32)
+        for j, idx in enumerate(np.ndindex(blob.shape[channel_axis])):
+            index = tuple(slice(None) if i != channel_axis else idx for i in range(blob.ndim))
+            blob[index] = data[j]
+        blob = blob.astype(data[0].dtype)
         return blob
 
     @staticmethod
-    def get_bias_value(model: onnx.ModelProto, node: NNCFNode) -> np.ndarray:
-        onnx_graph = ONNXGraph(model)
-        onnx_node = onnx_graph.get_node_by_name(node.node_name)
-        bias_port_id = onnx_graph.get_bias_tensor_port_id(onnx_node)
-        bias_input_name = onnx_node.input[bias_port_id]
-        if onnx_graph.has_initializer(bias_input_name):
-            return onnx_graph.get_initializers_value(bias_input_name)
-        node = onnx_graph.get_nodes_by_output(bias_input_name)[0]
-        metatype = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name(node.op_type)
-        if metatype == ONNXIdentityMetatype:
-            return onnx_graph.get_initializers_value(node.input[0])
-        raise RuntimeError('Could not find the bias value of the node')
+    def get_bias_value(node: NNCFNode, nncf_graph: NNCFGraph, model: onnx.ModelProto) -> np.ndarray:
+        return get_bias_value(node, model)
 
     @staticmethod
-    def get_activation_port_ids_for_bias_node(model: onnx.ModelProto, node: NNCFNode) -> Tuple[int, int]:
+    def get_activation_port_ids_for_bias_node(node: NNCFNode) -> Tuple[int, int]:
         return 0, 0
 
     @staticmethod
-    def get_bias_port_id(model: onnx.ModelProto, node: NNCFNode) -> int:
-        onnx_graph = ONNXGraph(model)
-        onnx_node = onnx_graph.get_node_by_name(node.node_name)
-        return onnx_graph.get_bias_tensor_port_id(onnx_node)
-
-    @staticmethod
     def process_model_output(raw_data: Dict, output_name: str) -> ONNXNNCFTensor:
         return ONNXNNCFTensor(raw_data[output_name])
 
     @staticmethod
-    def is_quantized_weights(node: NNCFNode, model: onnx.ModelProto) -> bool:
-        onnx_graph = ONNXGraph(model)
-        onnx_node = onnx_graph.get_node_by_name(node.node_name)
-        # We assume that the weight is on the first-index
-        weight_port_id = onnx_graph.get_weight_port_id(onnx_node)
-        input_edge_names = onnx_graph.get_node_edge_names(node.node_name)['input']
-        nodes_after_weight = onnx_graph.get_nodes_by_output(input_edge_names[weight_port_id])
-        if not nodes_after_weight:
-            return False
-        # We assume that there is only one node after weight
-        assert len(nodes_after_weight) == 1
-        weight_dequantizer = nodes_after_weight[0]
-        metatype = ONNX_OPERATION_METATYPES.get_operator_metatype_by_op_name(weight_dequantizer.op_type)
-        return metatype == ONNXDequantizeLinearMetatype
-
-    @staticmethod
-    def is_node_with_bias(node: NNCFNode) -> bool:
-        input_tensor_names = node.layer_attributes.input_tensor_names
-        return len(input_tensor_names) > 2
+    def is_quantized_weights(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
+        input_nodes = [edge.from_node for edge in nncf_graph.get_input_edges(node)]
+        weight_port_id = node.metatype.weight_definitions.weight_port_id
+        weight_node = input_nodes[weight_port_id]
+        return weight_node.metatype == ONNXDequantizeLinearMetatype
+
+    @staticmethod
+    def is_node_with_bias(node: NNCFNode, nncf_graph: NNCFGraph) -> bool:
+        return is_node_with_bias(node)
```

### Comparing `nncf-2.4.0/nncf/quantization/algorithms/min_max/backend.py` & `nncf-2.5.0/nncf/quantization/algorithms/min_max/backend.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,88 +1,98 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from abc import ABC
 from abc import abstractmethod
-from typing import Dict, TypeVar, Tuple, List
+from typing import Dict, List, Optional, Set, TypeVar
 
-import numpy as np
+from nncf.common.graph.graph import NNCFGraph
 from nncf.common.graph.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import OperatorMetatype
-from nncf.common.graph.patterns import HWFusedPatterns
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.hardware.config import HWConfig
-from nncf.common.tensor_statistics.collectors import ReductionShape
+from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
 from nncf.common.utils.registry import Registry
-from nncf.common.quantization.structs import QuantizerConfig
-from nncf.common.graph.model_transformer import ModelTransformer
+from nncf.parameters import ModelType
+from nncf.parameters import TargetDevice
+from nncf.quantization.fake_quantize import FakeQuantizeParameters
+from nncf.quantization.range_estimator import RangeEstimatorParameters
+from nncf.scopes import IgnoredScope
 
-TModel = TypeVar('TModel')
-ALGO_BACKENDS = Registry('algo_backends')
+TModel = TypeVar("TModel")
+ALGO_BACKENDS = Registry("algo_backends")
 
 
 class MinMaxAlgoBackend(ABC):
-
     @property
     @abstractmethod
-    def layers_with_weights_metatypes(self) -> List[OperatorMetatype]:
+    def mat_mul_metatype(self) -> OperatorMetatype:
         """
-        Property for the backend-specific metatypes with weights.
+        Property for the backend-specific MatMul metatype.
         """
 
     @property
     @abstractmethod
     def post_processing_metatypes(self) -> List[OperatorMetatype]:
         """
         Property for the backend-specific post-processing metatypes (NonMaximumSupression, TopK, etc.).
         """
 
     @property
     @abstractmethod
-    def hw_fused_patterns(self) -> HWFusedPatterns:
+    def shapeof_metatypes(self) -> List[OperatorMetatype]:
         """
-        Property for the hardware & backend-specific layers patterns.
+        Property for the backend-specific ShapeOf metatypes.
         """
 
     @property
     @abstractmethod
-    def hw_config(self) -> HWConfig:
+    def conv_metatype(self) -> List[OperatorMetatype]:
         """
-        Property for the hardware backend-specific configuration.
+        Property for the backend-specific Convolution metatypes.
         """
 
     @property
     @abstractmethod
-    def quant_trait_op_dict(self) -> Dict[int, OperatorMetatype]:
+    def overflow_fix_metatypes(self) -> List[OperatorMetatype]:
         """
-        Property for the backend-specific dictionary that contains QuantizationTrait-specific metatypes.
+        Property for the backend-specific metatypes for which overflow_fix is applicable.
         """
 
-    @staticmethod
+    @property
     @abstractmethod
-    def model_transformer(model: TModel) -> ModelTransformer:
+    def read_variable_metatypes(self) -> List[OperatorMetatype]:
+        """
+        Property for the backend-specific metatypes that also can be interpreted as inputs (ReadValue).
         """
-        Returns backend-specific ModelTransformer instance.
 
-        :param model: Backend-specific model to create ModelTransformer.
-        :return: ModelTransformer instance.
+    @property
+    @abstractmethod
+    def hw_config(self) -> HWConfig:
+        """
+        Property for the hardware backend-specific configuration.
+        """
+
+    @property
+    @abstractmethod
+    def quant_trait_op_dict(self) -> Dict[int, OperatorMetatype]:
+        """
+        Property for the backend-specific dictionary that contains QuantizationTrait-specific metatypes.
         """
 
     @staticmethod
     @abstractmethod
     def target_point(target_type: TargetType, target_node_name: str, port_id: int) -> TargetPoint:
         """
         Returns backend-specific target point.
@@ -91,96 +101,123 @@
         :param target_node_name: Name of the located node.
         :param port_id: Port ID of the tensor for the statistics distribution.
         :return: Backend-specific TargetPoint.
         """
 
     @staticmethod
     @abstractmethod
-    def create_activation_quantizer_insertion_command(target_point: TargetPoint,
-                                                      quantizer_config: QuantizerConfig,
-                                                      statistics: MinMaxTensorStatistic) -> TransformationCommand:
+    def create_activation_quantizer_insertion_command(
+        nncf_graph: NNCFGraph,
+        target_point: TargetPoint,
+        quantizer_config: QuantizerConfig,
+        parameters: FakeQuantizeParameters,
+    ) -> TransformationCommand:
         """
         Returns backend-specific quantizer insertion command.
 
+        :param nncf_graph: NNCFGraph to get input/output shapes for the target point.
         :param target_point: Target location for the correction.
         :param quantizer_config: QuantizerConfig instance for the current layer.
-        :param statistics: MinMaxTensorStatistic to calculate activation quantization parameters.
+        :param parameters: FakeQuantizeParameters to calculate activation quantization parameters.
         :return: Backend-specific TransformationCommand for the quantizer insertion operation.
         """
 
     @staticmethod
     @abstractmethod
-    def create_weight_quantizer_insertion_command(target_point: TargetPoint,
-                                                  quantizer_config: QuantizerConfig,
-                                                  weight_tensor: np.ndarray,
-                                                  node: NNCFNode) -> TransformationCommand:
+    def create_weight_quantizer_insertion_command(
+        nncf_graph: NNCFGraph,
+        target_point: TargetPoint,
+        quantizer_config: QuantizerConfig,
+        parameters: FakeQuantizeParameters,
+    ) -> TransformationCommand:
         """
         Returns backend-specific quantizer insertion command.
 
+        :param nncf_graph: NNCFGraph to get input/output shapes for the target point.
         :param target_point: Target location for the correction.
         :param quantizer_config: QuantizerConfig instance for the current layer.
-        :param weight_tensor: weight tensor to calculate weight quantization parameters.
-        :param node: NNCFNode with the attributes.
+        :param parameters: FakeQuantizeParameters to calculate activation quantization parameters.
         :return: Backend-specific TransformationCommand for the quantizer insertion operation.
         """
 
     @staticmethod
     @abstractmethod
-    def minmax_statistic_collector(use_abs_max: bool,
-                                   reduction_shape: ReductionShape,
-                                   num_samples: int = None) -> TensorStatisticCollectorBase:
+    def unify_statistics(statistics: List[MinMaxTensorStatistic]) -> MinMaxTensorStatistic:
         """
-        Returns backend-specific min max statistic collector.
+        Returns backend-specific unified statistics.
 
-        :param use_abs_max: Whether to use absolute maximum value or not.
-        :param reduction_shape: Channel axes for the statistics aggregation.
-        :param num_samples: Maximum number of samples to collect.
-        :return: Backend-specific TensorStatisticCollectorBase for the statistics calculation.
+        :param statistics: List of MinMaxTensorStatistic instances.
+        :return: Unified MinMaxTensorStatistic value.
         """
 
     @staticmethod
     @abstractmethod
-    def mean_minmax_statistic_collector(use_per_sample_stats: bool,
-                                        use_abs_max: bool,
-                                        reduction_shape: ReductionShape,
-                                        num_samples: int = None,
-                                        window_size: int = None) -> TensorStatisticCollectorBase:
+    def get_statistic_collector(
+        range_estimator_params: RangeEstimatorParameters,
+        nncf_graph: NNCFGraph,
+        target_point: TargetPoint,
+        quantizer_config: QuantizerConfig,
+        inplace: bool,
+        num_samples: int = None,
+    ) -> TensorStatisticCollectorBase:
         """
-        Returns backend-specific min max statistic collector.
+        Returns backend-specific statistic collector.
 
-        :param use_abs_max: Whether to use absolute maximum value or not.
-        :param reduction_shape: Channel axes for the statistics aggregation.
+        :param range_estimator_params: Parameters that specify estimators types.
+        :param nncf_graph: NNCFGraph to get input/output shapes for the target point.
+        :param target_point: Target location for the correction.
+        :param quantizer_config: QuantizerConfig instance for the current layer.
+        :param inplace: Whether to calculate statistic inplace or not.
         :param num_samples: Maximum number of samples to collect.
-        :param window_size: The maximum size of the samples queue.
         :return: Backend-specific TensorStatisticCollectorBase for the statistics calculation.
         """
 
     @staticmethod
     @abstractmethod
-    def get_weight_tensor(model: TModel, target_point: TargetPoint) -> Tuple[str, np.ndarray]:
+    def get_weight_tensor_port_ids(node: NNCFNode) -> List[Optional[int]]:
+        """
+        Returns node's input port indices with weight tensors.
+
+        :param node: NNCFNode to find its weight input port indices.
+        :return: Weights input port indices.
         """
-        Returns node's weight tensor name and its value.
 
-        :param model: Backend-specific model for the initializer finding.
-        :param target_point: Backend-specific TargetPoint to find its weight.
-        :return: Weight tensor name and its value.
+    @staticmethod
+    def get_weight_name(nncf_graph: NNCFGraph, target_point: TargetPoint) -> str:
+        """
+        Returns node's weight name corresponding to port ID.
+
+        :param nncf_graph: NNCFGraph instance.
+        :param target_point: The TargetPoint instance that contains layer's information.
+        :return: Weight name.
         """
 
     @staticmethod
-    def get_weight_tensor_port_id(model: TModel, node: NNCFNode) -> int:
+    def should_quantize_weight(weight_name: str, quantized_weight_names: Set[str]) -> bool:
+        """
+        Return True if weight should be quantized.
+
+        :param weight_name: Weight name.
+        :param quantized_weight_names: Set containing already quantized weight names.
+        :return: A boolean value specifying whether a weight should be quantized.
+        """
+
+    @staticmethod
+    @abstractmethod
+    def get_ignored_scope(model_type: ModelType, device: TargetDevice) -> IgnoredScope:
         """
-        Returns node's weight tensor input port ID.
+        Returns ignores scope based on a model type and device parameters.
 
-        :param node: NNCFNode to find its weight input port ID.
-        :return: The input port ID of the weight.
+        :param model_type: Model type parameter.
+        :param device: Target device.
+        :return: Instance of ignored scope.
         """
 
     @staticmethod
     @abstractmethod
-    def get_weight_config(config: QuantizerConfig, model: TModel) -> QuantizerConfig:
+    def get_weight_nodes(nncf_graph: NNCFGraph) -> List[NNCFNode]:
         """
-        Returns backend-specific configuration based on the input model attributes.
+        Returns nodes that have weights.
 
-        :param config: Base QuantizerConfig from the algo.
-        :param model: Backend-specific model instance.
-        :return: The updated QuantizerConfig.
+        :param nncf_graph: Instance of NNCFGraph.
+        :return: All nodes with weights.
         """
```

### Comparing `nncf-2.4.0/nncf/quantization/quantize.py` & `nncf-2.5.0/nncf/tensorflow/quantization/quantize_model.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,133 +1,181 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Optional
-from typing import Iterable
-from typing import Callable
-from typing import Any
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from nncf.api.compression import TModel
+from typing import Any, Dict, Optional
+
+import tensorflow as tf
+
+from nncf.common.initialization.dataloader import NNCFDataLoader
 from nncf.common.quantization.structs import QuantizationPreset
-from nncf.common.utils.backend import BackendType
-from nncf.common.utils.backend import get_backend
+from nncf.config import NNCFConfig
+from nncf.config.structures import BNAdaptationInitArgs
+from nncf.config.structures import QuantizationRangeInitArgs
 from nncf.data import Dataset
-from nncf.parameters import IgnoredScope
+from nncf.data.dataset import DataProvider
 from nncf.parameters import ModelType
 from nncf.parameters import TargetDevice
+from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters
+from nncf.quantization.advanced_parameters import convert_advanced_parameters_to_dict
+from nncf.scopes import IgnoredScope
+from nncf.scopes import convert_ignored_scope_to_list
+from nncf.tensorflow.helpers.model_creation import create_compressed_model
+
+DEFAULT_RANGE_TYPE = "mean_min_max"
+
+
+# TODO(alexsu52): It is a workaround and should be removed.
+class CalibrarionDataLoader(NNCFDataLoader):
+    """
+    This class wraps the nncf.Dataset.
+
+    This is required for proper initialization of certain compression algorithms.
+    """
+
+    def __init__(self, dataset: Dataset):
+        self._dataset = dataset
+
+    @property
+    def batch_size(self) -> int:
+        data_source = getattr(self._dataset, "_data_source")
+
+        if not hasattr(data_source, "_batch_size"):
+            return 1
+        batch_size = getattr(data_source, "_batch_size")
+        try:
+            if isinstance(batch_size, tf.Tensor):
+                batch_size = batch_size.numpy()
+            batch_size = int(batch_size)
+        except:  # pylint: disable=W0702
+            batch_size = 1
+        return batch_size
+
+    def __iter__(self):
+        def transform_fn(data_item):
+            return data_item, None
+
+        return iter(DataProvider(self._dataset.get_inference_data(), transform_fn))
 
 
-def quantize(model: TModel,
-             calibration_dataset: Dataset,
-             preset: QuantizationPreset = QuantizationPreset.PERFORMANCE,
-             target_device: TargetDevice = TargetDevice.ANY,
-             subset_size: int = 300,
-             fast_bias_correction: bool = True,
-             model_type: Optional[ModelType] = None,
-             ignored_scope: Optional[IgnoredScope] = None) -> TModel:
-    """
-    Applies post-training quantization algorithm to provided model.
-
-    :param model: A model to be quantized.
-    :param calibration_dataset: A representative dataset for the
-        calibration process.
+def _get_default_quantization_config(preset: QuantizationPreset, subset_size: int) -> Dict[str, Any]:
+    """
+    Returns the default quantization config
+
     :param preset: A preset that controls the quantization mode
         (symmetric and asymmetric). It can take the following values:
         - `performance`: Symmetric quantization of weights and activations.
         - `mixed`: Symmetric quantization of weights and asymmetric
           quantization of activations.
-    :param target_device: A target device the specificity of which will be taken
-        into account while compressing in order to obtain the best performance
-        for this type of device.
     :param subset_size: Size of a subset to calculate activations
         statistics used for quantization.
-    :param fast_bias_correction: Setting this option to `False` enables a different
-        bias correction method which is more accurate, in general, and takes
-        more time but requires less memory.
-    :param model_type: Model type is needed to specify additional patterns
-        in the model. Supported only `transformer` now.
-    :param ignored_scope: An ignored scope that defined the list of model control
-        flow graph nodes to be ignored during quantization.
-    :return: The quantized model.
+    :return: The default quantization config.
+    """
+    return {
+        "algorithm": "quantization",
+        "preset": preset.value,
+        "initializer": {
+            "range": {"num_init_samples": subset_size, "type": DEFAULT_RANGE_TYPE},
+            "batchnorm_adaptation": {"num_bn_adaptation_samples": subset_size},
+        },
+        "overflow_fix": "first_layer_only",
+    }
+
+
+def _create_nncf_config(
+    preset: QuantizationPreset,
+    target_device: TargetDevice,
+    subset_size: int,
+    ignored_scope: Optional[IgnoredScope],
+    advanced_parameters: Optional[AdvancedQuantizationParameters],
+) -> NNCFConfig:
     """
-    backend = get_backend(model)
-    if backend == BackendType.OPENVINO:
-        from nncf.openvino.quantization.quantize import quantize_impl
-        return quantize_impl(model, calibration_dataset, preset, target_device, subset_size,
-                             fast_bias_correction, model_type, ignored_scope)
-
-    if backend == BackendType.ONNX:
-        from nncf.onnx.quantization.quantize import quantize_impl
-        return quantize_impl(model, calibration_dataset, preset, target_device, subset_size,
-                             fast_bias_correction, model_type, ignored_scope)
-
-    if backend == BackendType.TENSORFLOW:
-        from nncf.tensorflow.quantization.quantize import quantize_impl
-        return quantize_impl(model, calibration_dataset, preset, target_device, subset_size,
-                             fast_bias_correction, model_type, ignored_scope)
-
-    if backend == BackendType.TORCH:
-        from nncf.torch.quantization.quantize import quantize_impl
-        return quantize_impl(model, calibration_dataset, preset, target_device, subset_size,
-                             fast_bias_correction, model_type, ignored_scope)
-
-    raise RuntimeError(f'Unsupported type of backend: {backend}')
-
-
-def quantize_with_accuracy_control(model: ModelType,
-                                   calibration_dataset: Dataset,
-                                   validation_dataset: Dataset,
-                                   validation_fn: Callable[[Any, Iterable[Any]], float],
-                                   max_drop: float = 0.01,
-                                   preset: QuantizationPreset = QuantizationPreset.PERFORMANCE,
-                                   target_device: TargetDevice = TargetDevice.ANY,
-                                   subset_size: int = 300,
-                                   fast_bias_correction: bool = True,
-                                   model_type: Optional[ModelType] = None,
-                                   ignored_scope: Optional[IgnoredScope] = None) -> ModelType:
-    """
-    Applies post-training quantization algorithm with accuracy control to provided model.
-
-    :param model: A model to be quantized.
-    :param calibration_dataset: A representative dataset for the calibration process.
-    :param validation_dataset: A dataset for the validation process.
-    :param validation_fn: A validation function to validate the model. It should take
-        two argumets:
-        - `model`: model to be validate.
-        - `validation_dataset`: dataset that provides data items to
-              validate the provided model.
-        The function should return the value of the metric with the following meaning:
-        A higher value corresponds to better performance of the model.
-    :param max_drop: The maximum absolute accuracy drop that should be achieved after the quantization.
-    :param preset: A preset that controls the quantization mode.
+    Creates the NNCFConfig for the quantization algorithm.
+
+    :param preset: A preset that controls the quantization mode
+        (symmetric and asymmetric). It can take the following values:
+        - `performance`: Symmetric quantization of weights and activations.
+        - `mixed`: Symmetric quantization of weights and asymmetric
+          quantization of activations.
     :param target_device: A target device the specificity of which will be taken
         into account while compressing in order to obtain the best performance
         for this type of device.
     :param subset_size: Size of a subset to calculate activations
         statistics used for quantization.
-    :param fast_bias_correction: Setting this option to `False` enables a different
-        bias correction method which is more accurate, in general, and takes
-        more time but requires less memory.
-    :param model_type: Model type is needed to specify additional patterns
-        in the model. Supported only `transformer` now.
-    :param ignored_scope: An ignored scope that defined the list of model control
+    :param ignored_scope:  An ignored scope that defined the list of model control
         flow graph nodes to be ignored during quantization.
-    :return: The quantized model.
+    :param advanced_parameters: Advanced quantization parameters for
+        fine-tuning the quantization algorithm.
+    :return: NNCFConfig for the quantization algorithm.
     """
-    backend = get_backend(model)
-    if backend == BackendType.OPENVINO:
-        from nncf.openvino.quantization.quantize import quantize_with_accuracy_control_impl
-        return quantize_with_accuracy_control_impl(model, calibration_dataset, validation_dataset, validation_fn,
-                                                   max_drop, preset, target_device, subset_size,
-                                                   fast_bias_correction, model_type, ignored_scope)
+    compression_config = _get_default_quantization_config(preset, subset_size)
+
+    if ignored_scope is not None:
+        _ignored_scope = convert_ignored_scope_to_list(ignored_scope)
+        if "ignored_scopes" in compression_config:
+            compression_config["ignored_scopes"].extend(_ignored_scope)
+        else:
+            compression_config["ignored_scopes"] = _ignored_scope
+
+    if advanced_parameters is not None:
+        advanced_config = convert_advanced_parameters_to_dict(advanced_parameters)
+
+        ranges = advanced_config.get("initializer", {}).get("range")
+        if ranges is not None:
+            for rconfig in ranges:
+                rconfig["num_init_samples"] = subset_size
+                if "type" not in rconfig:
+                    rconfig["type"] = DEFAULT_RANGE_TYPE
+
+        compression_config.update(advanced_config)
+
+    return NNCFConfig({"target_device": target_device.value, "compression": compression_config})
+
+
+def quantize_impl(
+    model: tf.Module,
+    calibration_dataset: Dataset,
+    preset: QuantizationPreset,
+    target_device: TargetDevice,
+    subset_size: int,
+    fast_bias_correction: bool,
+    model_type: Optional[ModelType] = None,
+    ignored_scope: Optional[IgnoredScope] = None,
+    advanced_parameters: Optional[AdvancedQuantizationParameters] = None,
+) -> tf.Module:
+    """
+    Implementation of the `quantize()` method for the TensorFlow backend.
+    """
+    if model_type is not None:
+        raise ValueError(f"model_type={model_type} is not supported")
+    if fast_bias_correction is False:
+        raise ValueError(f"fast_bias_correction={fast_bias_correction} is not supported")
+    if ignored_scope is not None and ignored_scope.types:
+        raise RuntimeError(
+            "Quantization algorithm form the TensorFlow backend "
+            "does not support operation types in the ignored "
+            "scopes yet"
+        )
+    if target_device == TargetDevice.CPU_SPR:
+        raise RuntimeError("target_device == CPU_SPR is not supported.")
+
+    nncf_config = _create_nncf_config(preset, target_device, subset_size, ignored_scope, advanced_parameters)
+
+    calibration_data_loader = CalibrarionDataLoader(calibration_dataset)
+    nncf_config.register_extra_structs(
+        [
+            QuantizationRangeInitArgs(data_loader=calibration_data_loader),
+            BNAdaptationInitArgs(data_loader=calibration_data_loader),
+        ]
+    )
+
+    compression_ctrl, compressed_model = create_compressed_model(model=model, config=nncf_config)
+    stripped_model = compression_ctrl.strip_model(compressed_model)
 
-    raise RuntimeError(f'Unsupported type of backend: {backend}')
+    return stripped_model
```

### Comparing `nncf-2.4.0/nncf/quantization/telemetry_extractors.py` & `nncf-2.5.0/nncf/quantization/telemetry_extractors.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,23 +1,25 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Any
 
 from nncf.telemetry.extractors import CollectedEvent
 from nncf.telemetry.extractors import TelemetryExtractor
 
 
 class CompressionStartedWithQuantizeApi(TelemetryExtractor):
     def extract(self, _: Any) -> CollectedEvent:
-        return CollectedEvent(name="compression_started",
-                              data="quantize_api")
+        return CollectedEvent(name="compression_started", data="quantize_api")
+
+
+class CompressionStartedWithQuantizeWithAccuracyControlApi(TelemetryExtractor):
+    def extract(self, _: Any) -> CollectedEvent:
+        return CollectedEvent(name="compression_started", data="quantize_with_accuracy_control_api")
```

### Comparing `nncf-2.4.0/nncf/telemetry/__init__.py` & `nncf-2.5.0/nncf/telemetry/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,14 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from nncf.telemetry.wrapper import telemetry
 from nncf.telemetry.decorator import tracked_function
 from nncf.telemetry.extractors import TelemetryExtractor
-
+from nncf.telemetry.wrapper import telemetry
```

### Comparing `nncf-2.4.0/nncf/telemetry/decorator.py` & `nncf-2.5.0/nncf/telemetry/decorator.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,37 +1,35 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import functools
 import inspect
-from typing import Callable
-from typing import List, Union
+from typing import Callable, List, Union
 
 from nncf.telemetry.events import get_current_category
 from nncf.telemetry.events import telemetry_category
-from nncf.telemetry.wrapper import telemetry
 from nncf.telemetry.extractors import TelemetryExtractor
 from nncf.telemetry.extractors import VerbatimTelemetryExtractor
+from nncf.telemetry.wrapper import telemetry
 
 
 class tracked_function:
     """
     Decorator to add telemetry events to a given function execution. The call to a decorated function
     will in general result in a telemetry session being created and a set of events being sent before
     function execution. The category of the session and events will be determined by parameters to the decorator.
     """
+
     def __init__(self, category: str = None, extractors: List[Union[str, TelemetryExtractor]] = None):
         """
         :param category: A category to be attributed to the events. If set to None, no events will be sent.
         :param extractors: Add argument names in this list as string values to send an event with an "action" equal to
             the argument name and "label" equal to the argument value before the function start.
             The argument must be either a string or a dictionary of strings. If that is not the case, instead of
             argument names you can specify an object of a customized `TelemetryExtractor` subclass; use the same
@@ -42,14 +40,15 @@
             self._collectors = [VerbatimTelemetryExtractor(x) if isinstance(x, str) else x for x in extractors]
         else:
             self._collectors = []
 
     def __call__(self, fn: Callable) -> Callable:
         fn_signature = inspect.signature(fn)
 
+        @functools.wraps(fn)
         def wrapped(*args, **kwargs):
             bound_args = fn_signature.bind(*args, **kwargs)
             bound_args.apply_defaults()
             events = []  # type: CollectedEvent
             for collector in self._collectors:
                 argname = collector.argname
                 argvalue = bound_args.arguments[argname] if argname is not None else None
@@ -58,20 +57,21 @@
 
             previous_category = get_current_category()
             with telemetry_category(self._category) as category:
                 if category is not None:
                     if category != previous_category:
                         telemetry.start_session(self._category)
                     for event in events:
-                        telemetry.send_event(event_category=category,
-                                                 event_action=event.name,
-                                                 event_label=event.data,
-                                                 event_value=event.int_data)
+                        telemetry.send_event(
+                            event_category=category,
+                            event_action=event.name,
+                            event_label=event.data,
+                            event_value=event.int_data,
+                        )
 
                 retval = fn(*args, **kwargs)
 
                 if category is not None and category != previous_category:
                     telemetry.end_session(self._category)
             return retval
 
         return wrapped
-
```

### Comparing `nncf-2.4.0/nncf/telemetry/events.py` & `nncf-2.5.0/nncf/telemetry/events.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,30 +1,25 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from contextlib import contextmanager
-from typing import List
 from typing import Optional
 
-from nncf.api.compression import CompressionAlgorithmBuilder
-from nncf.common.composite_compression import CompositeCompressionAlgorithmBuilder
-
-NNCF_TF_CATEGORY = 'nncf_tf'
-NNCF_PT_CATEGORY = 'nncf_pt'
-NNCF_ONNX_CATEGORY = 'nncf_onnx'
-NNCF_OV_CATEGORY = 'nncf_ov'
+NNCF_TF_CATEGORY = "nncf_tf"
+NNCF_PT_CATEGORY = "nncf_pt"
+NNCF_ONNX_CATEGORY = "nncf_onnx"
+NNCF_OV_CATEGORY = "nncf_ov"
 
 CURRENT_CATEGORY = None
 
 
 def _set_current_category(category: str):
     global CURRENT_CATEGORY
     CURRENT_CATEGORY = category
```

### Comparing `nncf-2.4.0/nncf/telemetry/extractors.py` & `nncf-2.5.0/nncf/telemetry/extractors.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,61 +1,58 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from abc import ABC
 from abc import abstractmethod
 from dataclasses import dataclass
 from enum import Enum
-from typing import Any
-from typing import Optional
-from typing import Union
+from typing import Any, Optional, Union
 
 SerializableData = Union[str, Enum]
 
 
 @dataclass
 class CollectedEvent:
     """
     name: The name for the event, will be sent as `event_action`
     data: Optional - the data associated with the event. Must be a string - serialize to string if it is not.
     int_data: Optional - the integer data associated with the event. Must be a positive integer.
     """
+
     name: str
     data: SerializableData = None  # GA limitations
     int_data: int = None
 
 
 class TelemetryExtractor(ABC):
     """
     Interface for custom telemetry extractors, to be used with the `nncf.telemetry.tracked_function` decorator.
     """
+
     def __init__(self, argname: Optional[str] = None):
         self._argname = argname
 
     @property
     def argname(self) -> Optional[str]:
         return self._argname
 
     @abstractmethod
     def extract(self, argvalue: Any) -> CollectedEvent:
         """
         Implement this method to prepare the telemetry event data from the tracked function's argument value
         passed via `argvalue`.
-         """
+        """
         pass
 
 
 class VerbatimTelemetryExtractor(TelemetryExtractor):
     def extract(self, argvalue: SerializableData) -> CollectedEvent:
         if isinstance(argvalue, Enum):
             argvalue = str(argvalue.value)
-        return CollectedEvent(name=self._argname,
-                              data=argvalue)
+        return CollectedEvent(name=self._argname, data=argvalue)
```

### Comparing `nncf-2.4.0/nncf/telemetry/wrapper.py` & `nncf-2.5.0/nncf/telemetry/wrapper.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,119 +1,122 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import functools
 import os
 import sys
 from abc import ABC
 from abc import abstractmethod
 from typing import Callable
 from unittest.mock import MagicMock
 
 from nncf import __version__
 from nncf.common.logging import nncf_logger
 from nncf.definitions import NNCF_CI_ENV_VAR_NAME
 from nncf.definitions import NNCF_DEV_ENV_VAR_NAME
 
-
 NNCFTelemetryStub = MagicMock
 
+
 class ITelemetry(ABC):
     # For recommendations on proper usage of categories, actions, labels and values, see:
     # https://support.google.com/analytics/answer/1033068
 
     @abstractmethod
     def start_session(self, category: str, **kwargs):
         """
         Sends a message about starting of a new session.
 
         :param kwargs: additional parameters
         :param category: the application code
         :return: None
         """
-        pass
 
     @abstractmethod
-    def send_event(self, event_category: str, event_action: str, event_label: str, event_value: int = 1,
-                   force_send=False, **kwargs):
+    def send_event(
+        self, event_category: str, event_action: str, event_label: str, event_value: int = 1, force_send=False, **kwargs
+    ):
         """
         Send single event.
 
         :param event_category: category of the event
         :param event_action: action of the event
         :param event_label: the label associated with the action
         :param event_value: the integer value corresponding to this label
         :param force_send: forces to send event ignoring the consent value
         :param kwargs: additional parameters
         :return: None
         """
-        pass
 
     @abstractmethod
     def end_session(self, category: str, **kwargs):
         """
         Sends a message about ending of the current session.
 
         :param kwargs: additional parameters
         :param category: the application code
         :return: None
         """
-        pass
 
 
 def skip_if_raised(func: Callable[..., None]) -> Callable[..., None]:
     """
     For the calls on the decorated function the execution will continue from the statement after the function
     even if an exception was triggered inside the function. The function to be wrapped must return nothing or None.
     """
+
+    @functools.wraps(func)
     def wrapped(*args, **kwargs):
         try:
             func()
+        # pylint:disable=broad-except
         except Exception as e:
             nncf_logger.debug(f"Skipped calling {func.__name__} - internally triggered exception {e}")
+
     return wrapped
 
 
 class NNCFTelemetry(ITelemetry):
-    MEASUREMENT_ID = 'UA-17808594-29'
+    MEASUREMENT_ID = "UA-17808594-29"
 
     def __init__(self):
         try:
-            self._impl = Telemetry(app_name='nncf', app_version=__version__, tid=self.MEASUREMENT_ID)
+            self._impl = Telemetry(app_name="nncf", app_version=__version__, tid=self.MEASUREMENT_ID)
+        # pylint:disable=broad-except
         except Exception as e:
             nncf_logger.debug(f"Failed to instantiate telemetry object: exception {e}")
 
     @skip_if_raised
     def start_session(self, category: str, **kwargs):
         self._impl.start_session(category, **kwargs)
 
     @skip_if_raised
-    def send_event(self, event_category: str, event_action: str, event_label: str, event_value: int = 1,
-                   force_send=False, **kwargs):
+    def send_event(
+        self, event_category: str, event_action: str, event_label: str, event_value: int = 1, force_send=False, **kwargs
+    ):
         self._impl.send_event(event_category, event_action, event_label, event_value, force_send, **kwargs)
 
     @skip_if_raised
     def end_session(self, category: str, **kwargs):
         self._impl.end_session(category, **kwargs)
 
 
 try:
     from openvino_telemetry import Telemetry
+
     telemetry = NNCFTelemetry()
 except ImportError:
     nncf_logger.debug("openvino_telemetry package not found. No telemetry will be sent.")
     telemetry = NNCFTelemetryStub()
 
 # Currently the easiest way to disable telemetry in tests. Will break telemetry if pytest is used in NNCF package code.
 _IS_IN_PYTEST_CONTEXT = "pytest" in sys.modules
 
-if os.getenv(NNCF_CI_ENV_VAR_NAME) or os.getenv(NNCF_DEV_ENV_VAR_NAME) \
-        or _IS_IN_PYTEST_CONTEXT:
+if os.getenv(NNCF_CI_ENV_VAR_NAME) or os.getenv(NNCF_DEV_ENV_VAR_NAME) or _IS_IN_PYTEST_CONTEXT:
     telemetry = NNCFTelemetryStub()
```

### Comparing `nncf-2.4.0/nncf/tensorflow/accuracy_aware_training/keras_model_utils.py` & `nncf-2.5.0/nncf/tensorflow/accuracy_aware_training/keras_model_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,38 +1,51 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import copy
 
 import tensorflow as tf
 
 from nncf.common.accuracy_aware_training import create_accuracy_aware_training_loop
+from nncf.config.structures import ModelEvaluationArgs
 from nncf.tensorflow import tf_internals
 
 
-def accuracy_aware_fit(cls_instance, train_dataset, compression_ctrl,
-                       nncf_config, callbacks, initial_epoch, uncompressed_model_accuracy,
-                       steps_per_epoch=None, batch_size=None, tensorboard_writer=None,
-                       log_dir=None, validation_data=None, validation_steps=None,
-                       result_dict_to_val_metric_fn=None, **kwargs):
+def accuracy_aware_fit(
+    cls_instance,
+    train_dataset,
+    compression_ctrl,
+    nncf_config,
+    callbacks,
+    initial_epoch,
+    steps_per_epoch=None,
+    batch_size=None,
+    tensorboard_writer=None,
+    log_dir=None,
+    uncompressed_model_accuracy=None,
+    validation_data=None,
+    validation_steps=None,
+    result_dict_to_val_metric_fn=None,
+    **kwargs,
+):
     if result_dict_to_val_metric_fn is None:
         result_dict_to_val_metric_fn = lambda metric: metric
 
-    with cls_instance.distribute_strategy.scope(), \
-        tf_internals.keras_engine.training_utils.RespectCompiledTrainableState(cls_instance):
+    # pylint:disable=line-too-long
+    with cls_instance.distribute_strategy.scope(), tf_internals.keras_engine.training_utils.RespectCompiledTrainableState(
+        cls_instance
+    ):
         # pylint: disable=protected-access
         data_handler = tf_internals.keras_engine.data_adapter.DataHandler(
             x=train_dataset,
             y=None,
             sample_weight=None,
             batch_size=batch_size,
             steps_per_epoch=steps_per_epoch,
@@ -40,85 +53,85 @@
             epochs=1,
             shuffle=True,
             class_weight=None,
             max_queue_size=10,
             workers=1,
             use_multiprocessing=False,
             model=cls_instance,
-            steps_per_execution=cls_instance._steps_per_execution)
+            steps_per_execution=cls_instance._steps_per_execution,
+        )
 
         if not isinstance(callbacks, tf.keras.callbacks.CallbackList):
             callbacks = tf.keras.callbacks.CallbackList(
                 callbacks,
                 add_history=True,
                 model=cls_instance,
                 epochs=1,
                 verbose=1,
                 add_progbar=True,
-                steps=data_handler.inferred_steps
-                )
+                steps=data_handler.inferred_steps,
+            )
 
     def train_epoch_fn(compression_ctrl, model, epoch, **kwargs):
         model.reset_metrics()
 
         if model.train_function is None:
             model.train_function = model.make_train_function()
         _, iterator = next(data_handler.enumerate_epochs())
 
         callbacks.on_epoch_begin(epoch)
         with data_handler.catch_stop_iteration():
             for step in data_handler.steps():
-                with tf.profiler.experimental.Trace(
-                    'train',
-                    epoch_num=epoch,
-                    step_num=step,
-                    batch_size=None,
-                    _r=1):
+                with tf.profiler.experimental.Trace("train", epoch_num=epoch, step_num=step, batch_size=None, _r=1):
                     callbacks.on_train_batch_begin(step)
                     tmp_logs = model.train_function(iterator)
                     if data_handler.should_sync:
                         tf_internals.eager_context.async_wait()
                     logs = tmp_logs
                     end_step = step + data_handler.step_increment
                     callbacks.on_train_batch_end(end_step, logs)
                     if model.stop_training:
                         break
 
         if logs is None:
-            raise ValueError('Expect x to be a non-empty array or dataset.')
+            raise ValueError("Expect x to be a non-empty array or dataset.")
         epoch_logs = copy.copy(logs)
         callbacks.on_epoch_end(epoch, epoch_logs)
 
     if validation_data is None:
         validation_data = train_dataset
 
     def validate_fn(model, epoch=None):
-        val_x, val_y, val_sample_weight = (
-            tf.keras.utils.unpack_x_y_sample_weight(validation_data))
+        val_x, val_y, val_sample_weight = tf.keras.utils.unpack_x_y_sample_weight(validation_data)
         val_logs = model.evaluate(
             x=val_x,
             y=val_y,
             sample_weight=val_sample_weight,
             batch_size=None,
             steps=validation_steps,
             callbacks=callbacks,
-            return_dict=True)
+            return_dict=True,
+        )
         return result_dict_to_val_metric_fn(val_logs)
 
     current_optimizer = copy.copy(compression_ctrl.model.optimizer)
 
     def configure_optimizers_fn():
         optimizer = copy.copy(current_optimizer)
         return optimizer, None
 
+    nncf_config.register_extra_structs([ModelEvaluationArgs(eval_fn=validate_fn)])
     callbacks.on_train_begin()
-    cls_instance.original_model_accuracy = uncompressed_model_accuracy
-    acc_aware_training_loop = create_accuracy_aware_training_loop(nncf_config, compression_ctrl)
-    cls_instance = acc_aware_training_loop.run(cls_instance,
-                                               train_epoch_fn=train_epoch_fn,
-                                               validate_fn=validate_fn,
-                                               configure_optimizers_fn=configure_optimizers_fn,
-                                               tensorboard_writer=tensorboard_writer,
-                                               log_dir=log_dir)
+    acc_aware_training_loop = create_accuracy_aware_training_loop(
+        nncf_config, compression_ctrl, uncompressed_model_accuracy
+    )
+    cls_instance = acc_aware_training_loop.run(
+        cls_instance,
+        train_epoch_fn=train_epoch_fn,
+        validate_fn=validate_fn,
+        configure_optimizers_fn=configure_optimizers_fn,
+        tensorboard_writer=tensorboard_writer,
+        log_dir=log_dir,
+    )
     callbacks.on_train_end()
 
     return acc_aware_training_loop.statistics
```

### Comparing `nncf-2.4.0/nncf/tensorflow/accuracy_aware_training/runner.py` & `nncf-2.5.0/nncf/tensorflow/accuracy_aware_training/runner.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import os.path as osp
 
 import tensorflow as tf
 import tensorflow_addons as tfa
 from tensorflow.keras.optimizers import schedules  # pylint:disable=no-name-in-module
 
@@ -23,19 +21,14 @@
 
 
 class TFAccuracyAwareTrainingRunner(BaseAccuracyAwareTrainingRunner):
     """
     The Training Runner implementation for TensorFlow training code.
     """
 
-    def retrieve_uncompressed_model_accuracy(self, model):
-        if not hasattr(model, 'original_model_accuracy'):
-            raise RuntimeError('Original model does not contain the pre-calculated reference metric value')
-        self.uncompressed_model_accuracy = model.original_model_accuracy
-
     def validate(self, model):
         self.current_val_metric_value = self._validate_fn(model, epoch=self.cumulative_epoch_count)
         is_best = (not self.is_higher_metric_better) != (self.current_val_metric_value > self.best_val_metric_value)
         if is_best:
             self.best_val_metric_value = self.current_val_metric_value
         return self.current_val_metric_value
 
@@ -54,59 +47,74 @@
                 optimizer.learning_rate = scheduler
                 optimizer.lr = scheduler
             elif isinstance(scheduler, (schedules.CosineDecay, schedules.ExponentialDecay)):
                 scheduler.initial_learning_rate *= self.base_lr_reduction_factor_during_search
             elif isinstance(scheduler, schedules.PiecewiseConstantDecay):
                 scheduler.values = [lr * self.base_lr_reduction_factor_during_search for lr in scheduler.values]
             else:
-                nncf_logger.warning(f"Learning rate scheduler {scheduler} is not supported yet. "
-                                    f"Won't change the learning rate.")
+                nncf_logger.warning(
+                    f"Learning rate scheduler {scheduler} is not supported yet. " f"Won't change the learning rate."
+                )
 
         self.training_epoch_count = 0
         self.best_val_metric_value = 0
         self.current_val_metric_value = 0
 
     def update_learning_rate(self):
         if self._update_learning_rate_fn is not None:
-            self._update_learning_rate_fn(self.lr_scheduler,
-                                          self.training_epoch_count,
-                                          self.current_val_metric_value,
-                                          self.current_loss)
+            self._update_learning_rate_fn(
+                self.lr_scheduler, self.training_epoch_count, self.current_val_metric_value, self.current_loss
+            )
 
     def _save_checkpoint(self, model, compression_controller, checkpoint_path):
         model.save_weights(checkpoint_path)
 
     def _load_checkpoint(self, model, checkpoint_path):
         if self._load_checkpoint_fn is not None:
             self._load_checkpoint_fn(model, checkpoint_path)
         else:
             model.load_weights(checkpoint_path)
 
     def _make_checkpoint_path(self, is_best, compression_rate=None):
-        extension = '.pt'
+        extension = ".pt"
         return osp.join(self._checkpoint_save_dir, f'acc_aware_checkpoint_{"best" if is_best else "last"}{extension}')
 
     def add_tensorboard_scalar(self, key, data, step):
         if self.verbose and self._tensorboard_writer is not None:
             self._tensorboard_writer.add_scalar(key, data, step)
 
     def add_tensorboard_image(self, key, data, step):
         if self.verbose and self._tensorboard_writer is not None:
             self._tensorboard_writer.add_image(key, data, step)
 
 
-class TFAdaptiveCompressionLevelTrainingRunner(BaseAdaptiveCompressionLevelTrainingRunner,
-                                               TFAccuracyAwareTrainingRunner):
-    def __init__(self, accuracy_aware_training_params, verbose=True, dump_checkpoints=True, lr_updates_needed=True,
-                 minimal_compression_rate=0.0, maximal_compression_rate=0.95):
-        super().__init__(accuracy_aware_training_params, verbose, dump_checkpoints, lr_updates_needed,
-                         minimal_compression_rate=minimal_compression_rate,
-                         maximal_compression_rate=maximal_compression_rate)
+class TFAdaptiveCompressionLevelTrainingRunner(
+    BaseAdaptiveCompressionLevelTrainingRunner, TFAccuracyAwareTrainingRunner
+):
+    def __init__(
+        self,
+        accuracy_aware_training_params,
+        uncompressed_model_accuracy: float,
+        verbose: bool = True,
+        dump_checkpoints: bool = True,
+        lr_updates_needed: bool = True,
+        minimal_compression_rate: float = 0.0,
+        maximal_compression_rate: float = 0.95,
+    ):
+        super().__init__(
+            accuracy_aware_training_params,
+            uncompressed_model_accuracy,
+            verbose,
+            dump_checkpoints,
+            lr_updates_needed,
+            minimal_compression_rate=minimal_compression_rate,
+            maximal_compression_rate=maximal_compression_rate,
+        )
 
     def _make_checkpoint_path(self, is_best, compression_rate=None):
-        extension = '.pt'
-        base_path = osp.join(self._checkpoint_save_dir, 'acc_aware_checkpoint')
+        extension = ".pt"
+        base_path = osp.join(self._checkpoint_save_dir, "acc_aware_checkpoint")
         if is_best:
             if compression_rate is None:
-                raise ValueError('Compression rate cannot be None')
-            return f'{base_path}_best_{compression_rate:.3f}{extension}'
-        return f'{base_path}_last{extension}'
+                raise ValueError("Compression rate cannot be None")
+            return f"{base_path}_best_{compression_rate:.3f}{extension}"
+        return f"{base_path}_last{extension}"
```

### Comparing `nncf-2.4.0/nncf/tensorflow/algorithm_selector.py` & `nncf-2.5.0/nncf/tensorflow/algorithm_selector.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,36 +1,34 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Dict
-from typing import Type
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Dict, Type
 
 import tensorflow as tf
 
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.compression import NO_COMPRESSION_ALGORITHM_NAME
+from nncf.common.compression import BaseCompressionAlgorithmController
 from nncf.common.graph.transformations.layout import TransformationLayout
-from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.logging import nncf_logger
-from nncf.common.utils.registry import Registry
+from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.statistics import NNCFStatistics
-from nncf.common.compression import BaseCompressionAlgorithmController
+from nncf.common.utils.backend import copy_model
+from nncf.common.utils.registry import Registry
 from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
 from nncf.tensorflow.loss import TFZeroCompressionLoss
 
-TF_COMPRESSION_ALGORITHMS = Registry('compression algorithm', add_name_as_attr=True)
+TF_COMPRESSION_ALGORITHMS = Registry("compression algorithm", add_name_as_attr=True)
 
 
 @TF_COMPRESSION_ALGORITHMS.register(NO_COMPRESSION_ALGORITHM_NAME)
 class NoCompressionAlgorithmBuilder(TFCompressionAlgorithmBuilder):
     def get_transformation_layout(self, _) -> TransformationLayout:
         return TransformationLayout()
 
@@ -57,11 +55,17 @@
     @property
     def scheduler(self) -> StubCompressionScheduler:
         return self._scheduler
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         return NNCFStatistics()
 
+    def strip(self, do_copy: bool = True) -> tf.keras.Model:
+        model = self.model
+        if do_copy:
+            model = copy_model(self.model)
+        return model
+
 
 def get_compression_algorithm_builder(algo_name: str) -> Type[TFCompressionAlgorithmBuilder]:
-    nncf_logger.info(f'Creating compression algorithm: {algo_name}')
+    nncf_logger.info(f"Creating compression algorithm: {algo_name}")
     return TF_COMPRESSION_ALGORITHMS.get(algo_name)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/api/composite_compression.py` & `nncf-2.5.0/nncf/tensorflow/api/composite_compression.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,42 +1,41 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import TypeVar
 
 from nncf import NNCFConfig
 from nncf.common.composite_compression import CompositeCompressionAlgorithmBuilder
 from nncf.common.composite_compression import CompositeCompressionAlgorithmController
 from nncf.config.extractors import extract_algorithm_names
 from nncf.tensorflow.algorithm_selector import get_compression_algorithm_builder
 from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
 from nncf.tensorflow.graph.transformations.layout import TFTransformationLayout
 
-TModel = TypeVar('TModel')
+TModel = TypeVar("TModel")
 
 
-class TFCompositeCompressionAlgorithmBuilder(
-    CompositeCompressionAlgorithmBuilder, TFCompressionAlgorithmBuilder):
+class TFCompositeCompressionAlgorithmBuilder(CompositeCompressionAlgorithmBuilder, TFCompressionAlgorithmBuilder):
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
 
         algo_names = extract_algorithm_names(config)
         if len(algo_names) < 2:
-            raise RuntimeError("Composite algorithm builder must be supplied with a config with more than one "
-                               "compression algo specified!")
+            raise RuntimeError(
+                "Composite algorithm builder must be supplied with a config with more than one "
+                "compression algo specified!"
+            )
         for algo_name in algo_names:
             algo_builder_cls = get_compression_algorithm_builder(algo_name)
             self._child_builders.append(algo_builder_cls(config, should_init=should_init))
 
     def _build_controller(self, model: TModel) -> CompositeCompressionAlgorithmController:
         composite_ctrl = CompositeCompressionAlgorithmController(model)
         for builder in self.child_builders:
```

### Comparing `nncf-2.4.0/nncf/tensorflow/api/compression.py` & `nncf-2.5.0/nncf/tensorflow/api/compression.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,44 +1,43 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Any
-from typing import Dict
-from typing import TypeVar
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Any, Dict, TypeVar
 
 from nncf import NNCFConfig
 from nncf.common.compression import BaseCompressionAlgorithmBuilder
 from nncf.tensorflow.graph.model_transformer import TFModelTransformer
 
-TModel = TypeVar('TModel')
+TModel = TypeVar("TModel")
 
 
 class TFCompressionAlgorithmBuilder(BaseCompressionAlgorithmBuilder):
     """
     Determines which modifications should be made to the original model in
     order to enable algorithm-specific compression during fine-tuning.
     """
 
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
-        compression_lr_multiplier = \
-            config.get_redefinable_global_param_value_for_algo('compression_lr_multiplier', self.name)
+        compression_lr_multiplier = config.get_redefinable_global_param_value_for_algo(
+            "compression_lr_multiplier", self.name
+        )
         if compression_lr_multiplier is not None:
-            raise Exception('compression_lr_multiplier is not supported when your work with a TF model in NNCF. '
-                            'Please remove the compression_lr_multiplier attribute from your NNCFConfig.')
+            raise Exception(
+                "compression_lr_multiplier is not supported when your work with a TF model in NNCF. "
+                "Please remove the compression_lr_multiplier attribute from your NNCFConfig."
+            )
 
     def _get_state_without_name(self) -> Dict[str, Any]:
         """
         Implementation of get_state that returns state without builder name.
 
         :return: Returns a dictionary with Python data structures
             (dict, list, tuple, str, int, float, True, False, None) that represents state of the object.
```

### Comparing `nncf-2.4.0/nncf/tensorflow/batchnorm_adaptation.py` & `nncf-2.5.0/nncf/tensorflow/batchnorm_adaptation.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from itertools import islice
 
 import tensorflow as tf
 
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithmImpl
 from nncf.common.logging.progress_bar import ProgressBar
@@ -52,17 +50,19 @@
     def run(self, model: tf.keras.Model) -> None:
         """
         Runs the batch-norm statistics adaptation algorithm.
 
         :param model: A model for which the algorithm will be applied.
         """
         if self._device is not None:
-            raise ValueError('TF implementation of batchnorm adaptation algorithm '
-                             'does not support switch of devices. Model initial device '
-                             'is used by default for batchnorm adaptation.')
+            raise ValueError(
+                "TF implementation of batchnorm adaptation algorithm "
+                "does not support switch of devices. Model initial device "
+                "is used by default for batchnorm adaptation."
+            )
         with BNTrainingStateSwitcher(model):
-            for (x, _) in ProgressBar(
-                    islice(self._data_loader, self._num_bn_adaptation_steps),
-                    total=self._num_bn_adaptation_steps,
-                    desc='BatchNorm statistics adaptation'
+            for x, _ in ProgressBar(
+                islice(self._data_loader, self._num_bn_adaptation_steps),
+                total=self._num_bn_adaptation_steps,
+                desc="BatchNorm statistics adaptation",
             ):
                 model(x, training=True)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/callbacks/checkpoint_callback.py` & `nncf-2.5.0/nncf/tensorflow/callbacks/checkpoint_callback.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,60 +1,56 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Union
 
 import tensorflow as tf
 
 
 class CheckpointManagerCallback(tf.keras.callbacks.Callback):
     """Callback to save the NNCF TF compression model with compression state."""
-    def __init__(self,
-                 checkpoint: tf.train.Checkpoint,
-                 directory: str,
-                 save_freq: Union[str, int] = 'epoch'):
+
+    def __init__(self, checkpoint: tf.train.Checkpoint, directory: str, save_freq: Union[str, int] = "epoch"):
         """
         :param checkpoint: The `tf.train.Checkpoint` instance to save and manage
             checkpoints for.
         :param directory: The path to a directory in which to write checkpoints.
         :param save_freq: `'epoch'` or integer. When using `'epoch'`, the callback saves.
             the model after each epoch and name number of the checkpoint correspond to epoch
             model was saved. When using integer, the callback saves the model at end of this many
             batches and name number of the checkpoint correspond to number of passed batches * `save_freq`.
         """
         super().__init__()
         self._last_batch_seen = 0
         self._batches_seen_since_last_saving = 0
-        if save_freq != 'epoch' and not isinstance(save_freq, int):
-            raise ValueError('Unrecognized save_freq: {}'.format(save_freq))
+        if save_freq != "epoch" and not isinstance(save_freq, int):
+            raise ValueError("Unrecognized save_freq: {}".format(save_freq))
 
         self._checkpoint_manager = tf.train.CheckpointManager(checkpoint, directory, None)
         self._save_freq = save_freq
 
     def on_train_batch_end(self, batch, logs=None):
         if self._should_save_on_batch(batch):
             self._save_model()
 
     def on_epoch_end(self, epoch, logs=None):
-        if self._save_freq == 'epoch':
+        if self._save_freq == "epoch":
             self._save_model()
 
     def _should_save_on_batch(self, batch):
         """Handles batch-level saving logic, supports steps_per_execution."""
-        if self._save_freq == 'epoch':
+        if self._save_freq == "epoch":
             return False
 
         if batch <= self._last_batch_seen:  # New epoch.
             add_batches = batch + 1  # batches are zero-indexed.
         else:
             add_batches = batch - self._last_batch_seen
```

### Comparing `nncf-2.4.0/nncf/tensorflow/callbacks/statistics_callback.py` & `nncf-2.5.0/nncf/tensorflow/callbacks/statistics_callback.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Callable
 
 import tensorflow as tf
 
-from nncf.common.statistics import NNCFStatistics
 from nncf.common.logging import nncf_logger
+from nncf.common.statistics import NNCFStatistics
 
 
 class StatisticsCallback(tf.keras.callbacks.Callback):
     """
     Callback for logging compression statistics to tensorboard and stdout.
     """
 
-    def __init__(self,
-                 statistics_fn: Callable[[], NNCFStatistics],
-                 log_tensorboard: bool = True,
-                 log_text: bool = True,
-                 log_dir: str = None):
+    def __init__(
+        self,
+        statistics_fn: Callable[[], NNCFStatistics],
+        log_tensorboard: bool = True,
+        log_text: bool = True,
+        log_dir: str = None,
+    ):
         """
         Initializes compression statistics callback.
 
         :param statistics_fn: A callable object that provides NNCF statistics.
         :param log_tensorboard: Whether to log statistics to tensorboard or not.
         :param log_text: Whether to log statistics to stdout.
         :param log_dir: The directory for tensorbard logging.
@@ -40,39 +40,42 @@
         super().__init__()
         self._statistics_fn = statistics_fn
         self._log_tensorboard = log_tensorboard
         self._log_text = log_text
         self._file_writer = None
         if log_tensorboard:
             if log_dir is None:
-                raise ValueError('log_dir must be specified if log_tensorboard is true.')
+                raise ValueError("log_dir must be specified if log_tensorboard is true.")
             # pylint: disable=no-member
-            self._file_writer = tf.summary.create_file_writer(log_dir + '/compression')
+            self._file_writer = tf.summary.create_file_writer(log_dir + "/compression")
 
     def _dump_to_tensorboard(self, logs: dict, step: int):
-        with self._file_writer.as_default(): # pylint: disable=E1129
+        with self._file_writer.as_default():  # pylint: disable=E1129
             for name, value in logs.items():
                 tf.summary.scalar(name, value, step=step)
 
     def on_epoch_end(self, epoch: int, logs: dict = None):
         nncf_stats = self._statistics_fn()
         if self._log_tensorboard:
-            self._dump_to_tensorboard(self._prepare_for_tensorboard(nncf_stats),
-                                      self.model.optimizer.iterations.numpy())
+            self._dump_to_tensorboard(
+                self._prepare_for_tensorboard(nncf_stats), self.model.optimizer.iterations.numpy()
+            )
         if self._log_text:
             nncf_logger.info(nncf_stats.to_str())
 
     def on_train_begin(self, logs: dict = None):
         nncf_stats = self._statistics_fn()
         if self._log_tensorboard:
-            self._dump_to_tensorboard(self._prepare_for_tensorboard(nncf_stats),
-                                      self.model.optimizer.iterations.numpy())
+            self._dump_to_tensorboard(
+                self._prepare_for_tensorboard(nncf_stats), self.model.optimizer.iterations.numpy()
+            )
         if self._log_text:
             nncf_logger.info(nncf_stats.to_str())
 
     def on_train_end(self, logs: dict = None):
         if self._file_writer:
             self._file_writer.close()
 
     def _prepare_for_tensorboard(self, stats: NNCFStatistics):
         raise NotImplementedError(
-            'StatisticsCallback class implementation must override the _prepare_for_tensorboard method.')
+            "StatisticsCallback class implementation must override the _prepare_for_tensorboard method."
+        )
```

### Comparing `nncf-2.4.0/nncf/tensorflow/exporter.py` & `nncf-2.5.0/nncf/tensorflow/exporter.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,43 +1,41 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import os
+
 import tensorflow as tf
 from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
 
 from nncf.common.exporter import Exporter
 from nncf.telemetry import tracked_function
 from nncf.telemetry.events import NNCF_TF_CATEGORY
 
 
 class TFExportFormat:
-    SAVED_MODEL = 'tf'
-    KERAS_H5 = 'h5'
-    FROZEN_GRAPH = 'frozen_graph'
+    SAVED_MODEL = "tf"
+    KERAS_H5 = "h5"
+    FROZEN_GRAPH = "frozen_graph"
 
 
 # TODO(andrey-churkin): Add support for `input_names` and `output_names`
 class TFExporter(Exporter):
     """
     This class provides export of the compressed model to the Frozen Graph,
     TensorFlow SavedModel, or Keras H5 formats.
     """
 
-
     @tracked_function(NNCF_TF_CATEGORY, ["save_format"])
     def export_model(self, save_path: str, save_format: str = TFExportFormat.FROZEN_GRAPH) -> None:
         """
         Exports the compressed model to the specified format.
 
         :param save_path: The path where the model will be saved.
         :param save_format: Saving format.
@@ -54,16 +52,15 @@
             TFExportFormat.FROZEN_GRAPH: self._export_to_frozen_graph,
         }
 
         export_fn = format_to_export_fn.get(save_format)
 
         if export_fn is None:
             available_formats = list(format_to_export_fn.keys())
-            raise ValueError(f'Unsupported saving format: \'{save_format}\'. '
-                             f'Available formats: {available_formats}')
+            raise ValueError(f"Unsupported saving format: '{save_format}'. " f"Available formats: {available_formats}")
 
         export_fn(save_path)
 
     def _export_to_saved_model(self, save_path: str) -> None:
         """
         Exports the compressed model to the TensorFlow SavedModel format.
```

### Comparing `nncf-2.4.0/nncf/tensorflow/functions.py` & `nncf-2.5.0/nncf/openvino/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,26 +1,13 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Base subpackage for NNCF OpenVINO functionality.
 """
-
-import tensorflow as tf
-
-
-@tf.function
-def logit(x):
-    return tf.math.log(x / (1 - x))
-
-
-@tf.custom_gradient
-def st_threshold(input_):
-    def grad(upstream):
-        return upstream
-    return tf.round(input_), grad
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/converter.py` & `nncf-2.5.0/nncf/tensorflow/graph/converter.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,66 +1,67 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from abc import ABC
 from abc import abstractmethod
 from collections import defaultdict
-from typing import Dict
-from typing import List
-from typing import Set
-from typing import Tuple
-from typing import Type
+from typing import Dict, List, Set, Tuple, Type
+
 import tensorflow as tf
 from tensorflow.core.framework.node_def_pb2 import NodeDef
 from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
-from nncf.tensorflow.tf_internals import KerasTensor
+
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph import OperatorMetatype
+from nncf.common.graph.definitions import NNCFGraphNodeType
+from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
-from nncf.common.graph.layer_attributes import MultipleInputLayerAttributes
-from nncf.common.graph.layer_attributes import ReshapeLayerAttributes
+from nncf.common.graph.layer_attributes import Dtype
 from nncf.common.graph.layer_attributes import LinearLayerAttributes
+from nncf.common.graph.layer_attributes import MultipleInputLayerAttributes
 from nncf.common.graph.layer_attributes import MultipleOutputLayerAttributes
-from nncf.common.graph.layer_attributes import Dtype
+from nncf.common.graph.layer_attributes import PermuteLayerAttributes
+from nncf.common.graph.layer_attributes import ReshapeLayerAttributes
+from nncf.common.graph.operator_metatypes import OutputNoopMetatype
 from nncf.common.graph.utils import get_concat_axis
 from nncf.common.graph.utils import get_split_axis
 from nncf.common.logging import nncf_logger
+from nncf.tensorflow.graph.metatypes import keras_layers as layer_metatypes
 from nncf.tensorflow.graph.metatypes.common import DECONV_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.common import DEPTHWISE_CONV_LAYER_METATYPES
-from nncf.tensorflow.graph.metatypes.common import \
-    LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_CONCAT_INPUTS
-from nncf.tensorflow.graph.metatypes.common import \
-    LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_OUTPUTS
+from nncf.tensorflow.graph.metatypes.common import DIMENSION_PERMUTATION_METATYPES
 from nncf.tensorflow.graph.metatypes.common import GENERAL_CONV_LAYER_METATYPES
+from nncf.tensorflow.graph.metatypes.common import (
+    LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_CONCAT_INPUTS,
+)
+from nncf.tensorflow.graph.metatypes.common import LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_OUTPUTS
 from nncf.tensorflow.graph.metatypes.common import LINEAR_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.common import RESHAPE_METATYPES
 from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
 from nncf.tensorflow.graph.metatypes.matcher import get_op_metatype
-from nncf.common.graph.operator_metatypes import OutputNoopMetatype
 from nncf.tensorflow.graph.metatypes.tf_ops import WEIGHTABLE_TF_OP_METATYPES
 from nncf.tensorflow.graph.utils import get_shared_node_name
-from nncf.tensorflow.graph.utils import reformat_inbound_nodes_for_oplambda
 from nncf.tensorflow.graph.utils import is_builtin_layer
 from nncf.tensorflow.graph.utils import is_functional_model
 from nncf.tensorflow.graph.utils import is_sequential_model
+from nncf.tensorflow.graph.utils import reformat_inbound_nodes_for_oplambda
 from nncf.tensorflow.graph.utils import unwrap_layer
 from nncf.tensorflow.layers.data_layout import get_input_channel_axis
-from nncf.common.graph.definitions import NNCFGraphNodeType
+from nncf.tensorflow.tf_internals import KerasTensor
 
-PREFIX_AUXILIARY_OUTPUT_NODE = 'output'
+PREFIX_AUXILIARY_OUTPUT_NODE = "output"
 
 
 class TFLayerInfo:
     def __init__(self, layer_name: str, instance_idx: int):
         self.layer_name = layer_name
         self.instance_idx = instance_idx
 
@@ -77,18 +78,24 @@
 
 
 class CustomLayerNodeInfo:
     """
     Describes a future NNCFGraph node corresponding to a TF operation inside a custom layer.
     """
 
-    def __init__(self, graphdef_node_name: str,
-                 custom_layer_name: str, target_node_name: NNCFNodeName,
-                 node_type: str, node_metatype: Type[OperatorMetatype], weight_node_name: str,
-                 dtype: Dtype):
+    def __init__(
+        self,
+        graphdef_node_name: str,
+        custom_layer_name: str,
+        target_node_name: NNCFNodeName,
+        node_type: str,
+        node_metatype: Type[OperatorMetatype],
+        weight_node_name: str,
+        dtype: Dtype,
+    ):
         """
         :param graphdef_node_name: The name of the TF operation node in the GraphDef representation
           of the custom layer.
         :param custom_layer_name: The name of the custom layer in the model that hosts it.
         :param target_node_name: The name of the future NNCFNode that will be present in NNCFGraph
           to represent this TF operation in the corresponding custom layer.
         :param node_type: The TF-specific string describing the type of the TF operation.
@@ -106,18 +113,15 @@
 
 
 class CustomLayerEdgeInfo:
     """
     Describes an edge inside a custom layer graph representation in TF.
     """
 
-    def __init__(self, tensor_shape: List[int],
-                 input_port_id: int,
-                 output_port_id: int,
-                 dtype: Dtype):
+    def __init__(self, tensor_shape: List[int], input_port_id: int, output_port_id: int, dtype: Dtype):
         """
         :param tensor_shape: The shape of the associated tensor.
         :param input_port_id: The ID of the input tensor among the "to_node" inputs.
         :param output_port_id: The ID of the output tensor among the "from_node" outputs.
         :param dtype: The dtype of the tensor.
         """
         self.tensor_shape = tensor_shape
@@ -170,104 +174,116 @@
 
     @staticmethod
     def _get_type_spec(tensor):
         if isinstance(tensor, KerasTensor):
             return tensor.type_spec
         return tf.TensorSpec.from_tensor(tensor)
 
-    def _collect_custom_layer_infos(self, model: tf.keras.Model,
-                                    use_graph_var_names: bool = False) -> Dict[str, CustomLayerInfo]:
+    def _collect_custom_layer_infos(
+        self, model: tf.keras.Model, use_graph_var_names: bool = False
+    ) -> Dict[str, CustomLayerInfo]:
         custom_layers = BaseFunctionalSequentialConverter.get_custom_layers(model)
         retval = {}
         for layer_name, layer in custom_layers.items():
-            layer_input_spec = [self._get_type_spec(tensor)
-                                for tensor in layer.input] if isinstance(layer.input, list) \
+            layer_input_spec = (
+                [self._get_type_spec(tensor) for tensor in layer.input]
+                if isinstance(layer.input, list)
                 else self._get_type_spec(layer.input)
+            )
 
             # TODO (vshampor) : Use the custom layer's inbound_nodes/outbound_nodes to determine what edges
             #  should connect it to the rest of the graph. Currently the custom layer
             #  subgraph will be present in the NNCFGraph after conversion, which is useful
             #  for purposes of weight modification target point creation and usage,
             #  but the subgraph won't be connected to the rest of the graph; in the main graph
             #  component, the custom layer will still be represented by a single node
             concr_fn = tf.function(layer).get_concrete_function(layer_input_spec, training=False)
             wrapped_function = convert_variables_to_constants_v2(concr_fn, lower_control_flow=False)
 
             graphdef_nodes = wrapped_function.graph.as_graph_def().node
-            graphdef_name_to_layer_var_map = {} if use_graph_var_names else \
-                BaseFunctionalSequentialConverter._get_graphdef_name_to_layer_var_map(concr_fn)
+            graphdef_name_to_layer_var_map = (
+                {}
+                if use_graph_var_names
+                else BaseFunctionalSequentialConverter._get_graphdef_name_to_layer_var_map(concr_fn)
+            )
             nodes = {graphdef_name_to_layer_var_map.get(node.name, node.name): node for node in graphdef_nodes}
             graphdef_node_name_vs_node = {node.name: node for node in graphdef_nodes}
 
             custom_layer_info = CustomLayerInfo()
             for pretty_node_name, node in nodes.items():
                 custom_layer_info.graphdef_node_name_to_pretty_node_name[node.name] = pretty_node_name
 
             for pretty_node_name, node in nodes.items():
                 weight_node_name = None
                 metatype = get_op_metatype(node.op)
                 if metatype in WEIGHTABLE_TF_OP_METATYPES:
                     graphdef_weight_node_name = self._get_graphdef_node_name_for_custom_layer_node_weight(
-                        node,
-                        graphdef_node_name_vs_node)
+                        node, graphdef_node_name_vs_node
+                    )
                     if graphdef_weight_node_name in graphdef_name_to_layer_var_map:
                         weight_node_name = graphdef_name_to_layer_var_map[graphdef_weight_node_name]
                     else:
-                        nncf_logger.warning(f'Could not associate a weighted custom layer node {pretty_node_name} '
-                                            f'with a weight attribute of the custom layer - the corresponding weight '
-                                            f'will not be compressed! Make sure that the corresponding custom layer '
-                                            f'weight has a name.')
+                        nncf_logger.warning(
+                            f"Could not associate a weighted custom layer node {pretty_node_name} "
+                            f"with a weight attribute of the custom layer - the corresponding weight "
+                            f"will not be compressed! Make sure that the corresponding custom layer "
+                            f"weight has a name."
+                        )
 
                 custom_layer_info.node_infos[pretty_node_name] = CustomLayerNodeInfo(
                     graphdef_node_name=node.name,
                     custom_layer_name=layer_name,
                     target_node_name=pretty_node_name,
                     node_type=node.op,
                     node_metatype=get_op_metatype(node.op),
                     weight_node_name=weight_node_name,
-                    dtype=Dtype.FLOAT if node.attr['dtype'].type == 1 else Dtype.INTEGER)
+                    dtype=Dtype.FLOAT if node.attr["dtype"].type == 1 else Dtype.INTEGER,
+                )
 
                 custom_layer_info.shared_weight_node_names_vs_weighted_op_node_names[weight_node_name].add(
-                    pretty_node_name)
+                    pretty_node_name
+                )
 
                 for idx, input_graphdef_node_name_and_output_port_str in enumerate(node.input):
-                    if '^' in input_graphdef_node_name_and_output_port_str:
+                    if "^" in input_graphdef_node_name_and_output_port_str:
                         continue  # Skip control_inputs
-                    splits = input_graphdef_node_name_and_output_port_str.split(':')
+                    splits = input_graphdef_node_name_and_output_port_str.split(":")
                     if len(splits) == 1:
                         input_graphdef_node_name = splits[0]
                         output_port_id = 0
                     elif len(splits) == 2:
                         input_graphdef_node_name = splits[0]
                         output_port_id = int(splits[1])
                     else:
                         raise RuntimeError("Could not parse NodeDef's input field!")
 
-                    pretty_input_node_name = \
-                        custom_layer_info.graphdef_node_name_to_pretty_node_name[input_graphdef_node_name]
+                    pretty_input_node_name = custom_layer_info.graphdef_node_name_to_pretty_node_name[
+                        input_graphdef_node_name
+                    ]
 
                     # TODO (vshampor): add proper tensor_shape, will probably involve
                     #                  running as_graph_def(add_shapes=True)
-                    custom_layer_info.edge_infos[(pretty_input_node_name, pretty_node_name)] = \
-                        CustomLayerEdgeInfo(tensor_shape=None,
-                                            input_port_id=idx,
-                                            output_port_id=output_port_id,
-                                            dtype=custom_layer_info.node_infos[pretty_node_name].dtype)
+                    custom_layer_info.edge_infos[(pretty_input_node_name, pretty_node_name)] = CustomLayerEdgeInfo(
+                        tensor_shape=None,
+                        input_port_id=idx,
+                        output_port_id=output_port_id,
+                        dtype=custom_layer_info.node_infos[pretty_node_name].dtype,
+                    )
                 retval[layer_name] = custom_layer_info
         return retval
 
     def _get_layer_output_dtype(self, layer_config: Dict) -> Dtype:
-        if layer_config['class_name'] in ['Functional', 'Sequential']:
-            return self._get_layer_output_dtype(layer_config['config']['layers'][0])
+        if layer_config["class_name"] in ["Functional", "Sequential"]:
+            return self._get_layer_output_dtype(layer_config["config"]["layers"][0])
 
-        layer_name = layer_config['config']['name']
-        if layer_config['class_name'] == 'TensorFlowOpLayer':
-            layer_name = 'tf_op_layer_' + layer_name
+        layer_name = layer_config["config"]["name"]
+        if layer_config["class_name"] == "TensorFlowOpLayer":
+            layer_name = "tf_op_layer_" + layer_name
 
-        if layer_config['class_name'] == 'InputLayer':
+        if layer_config["class_name"] == "InputLayer":
             return Dtype.FLOAT
 
         keras_layer = self._get_layer(layer_name)
         if isinstance(keras_layer.output, (tf.Tensor, KerasTensor)):
             dtype = keras_layer.output.dtype
         else:
             # In case of multiple outputs, assume all outputs have the same type
@@ -275,22 +291,22 @@
 
         if dtype == tf.int32:
             return Dtype.INTEGER
         return Dtype.FLOAT
 
     @staticmethod
     def _get_layer_type(layer_config: Dict) -> str:
-        if layer_config['class_name'] == 'TensorFlowOpLayer':
-            return layer_config['config']['node_def']['op']
-        if layer_config['class_name'] in ['TFOpLambda', 'SlicingOpLambda']:
-            return layer_config['config']['function']
-        if layer_config['class_name'] == 'NNCFWrapper':
+        if layer_config["class_name"] == "TensorFlowOpLayer":
+            return layer_config["config"]["node_def"]["op"]
+        if layer_config["class_name"] in ["TFOpLambda", "SlicingOpLambda"]:
+            return layer_config["config"]["function"]
+        if layer_config["class_name"] == "NNCFWrapper":
             # Return class_name of wrapped layer_config
-            return layer_config['config']['layer']['class_name']
-        return layer_config['class_name']
+            return layer_config["config"]["layer"]["class_name"]
+        return layer_config["class_name"]
 
     @staticmethod
     def get_custom_layers(model: tf.keras.Model) -> Dict[str, tf.Module]:
         """
         Returns the mapping of custom layer names in the model vs associated custom layer
         module objects.
 
@@ -306,42 +322,43 @@
     @staticmethod
     def _get_graphdef_name_to_layer_var_map(concrete_fun) -> Dict[str, str]:
         names_map = {}
         inverse_map = defaultdict(set)
         for layer_var in concrete_fun.variables:
             for value_tensor, graphdef_name in concrete_fun.graph.captures:
                 if layer_var.handle is value_tensor:
-                    graphdef_name = graphdef_name.name.split(':')[0]
-                    layer_var_name = layer_var.name.split(':')[0]
+                    graphdef_name = graphdef_name.name.split(":")[0]
+                    layer_var_name = layer_var.name.split(":")[0]
                     inverse_map[layer_var_name].add(graphdef_name)
                     names_map[graphdef_name] = layer_var_name
 
         for graphdef_names in inverse_map.values():
             if len(graphdef_names) > 1:
                 # Name collision - remove all collided entries
                 for graphdef_name in graphdef_names:
                     del names_map[graphdef_name]
 
         return names_map
 
     @staticmethod
-    def _get_graphdef_node_name_for_custom_layer_node_weight(weighted_node: NodeDef,
-                                                             graphdef_nodes: Dict[str, NodeDef]) -> str:
+    def _get_graphdef_node_name_for_custom_layer_node_weight(
+        weighted_node: NodeDef, graphdef_nodes: Dict[str, NodeDef]
+    ) -> str:
         def get_node_name(graphdef_node_name: str):
-            return graphdef_node_name.split(':')[0]
+            return graphdef_node_name.split(":")[0]
 
         weight_node_name = None
         previous_node_names = [get_node_name(node_input) for node_input in weighted_node.input]
         while previous_node_names:
             weight_node_name = get_node_name(previous_node_names[-1])
             weight_node = graphdef_nodes[weight_node_name]  # TODO (vshampor): how correct is this actually?
             previous_node_names = [get_node_name(node_input) for node_input in weight_node.input]
 
             # Filter control inputs, whatever these are
-            previous_node_names = list(filter(lambda x: '^' not in x, previous_node_names))
+            previous_node_names = list(filter(lambda x: "^" not in x, previous_node_names))
         if weight_node_name is None:
             raise RuntimeError("Could not find a weight node for a weighted node {}".format(weighted_node.name))
         return weight_node_name
 
     @staticmethod
     def _prepare_shape(shape) -> List:
         if not isinstance(shape, list):
@@ -353,26 +370,24 @@
         :param node_name: The node name in the converted NNCFGraph
         :return: A flag indicating whether the node corresponds to a custom layer,
           and the additional TF-specific information about the layer underlying the node.
         """
         if node_name in self._nncf_node_names_vs_custom_layer_name:
             is_custom = True
             custom_layer_name = self._nncf_node_names_vs_custom_layer_name[node_name]
-            insertion_data = TFLayerInfo(custom_layer_name,
-                                         instance_idx=0)
+            insertion_data = TFLayerInfo(custom_layer_name, instance_idx=0)
         else:
             is_custom = False
             node_tf_data = self._node_info[node_name]
-            layer_name = node_tf_data['layer_name']
-            instance_idx = node_tf_data['inbound_node_idx']
+            layer_name = node_tf_data["layer_name"]
+            instance_idx = node_tf_data["inbound_node_idx"]
             if instance_idx is None:
                 instance_idx = 0
 
-            insertion_data = TFLayerInfo(layer_name,
-                                         instance_idx=instance_idx)
+            insertion_data = TFLayerInfo(layer_name, instance_idx=instance_idx)
         return is_custom, insertion_data
 
     def get_node_names_vs_custom_layer_names(self) -> Dict[NNCFNodeName, str]:
         """
         :return: A mapping of NNCFNode names corresponding to custom layers vs the corresponding
           custom layer name.
         """
@@ -384,349 +399,371 @@
         except ValueError:
             for layer in self._model.submodules:
                 if not isinstance(layer, tf.keras.layers.Layer):
                     continue
                 if layer.name == layer_name:
                     return layer
 
-        raise ValueError(f'No such layer: {layer_name}.')
+        raise ValueError(f"No such layer: {layer_name}.")
 
     def _add_custom_layer_subgraph(self, nncf_graph: NNCFGraph, custom_layer_name: str) -> NNCFGraph:
         # TODO (vshampor): filter meaningless ops such as Identity, resource read etc.
         custom_layer_info = self._custom_layer_infos[custom_layer_name]
         node_name_vs_nncf_node_ids = {}  # type: Dict[NNCFNodeName, int]
         for node_info in custom_layer_info.node_infos.values():
             weight_node_name = node_info.weight_node_name
             is_shared = False
             if weight_node_name is not None:
                 shared_node_dict = custom_layer_info.shared_weight_node_names_vs_weighted_op_node_names
                 is_shared = len(shared_node_dict[weight_node_name]) > 1
-            nncf_node = nncf_graph.add_nncf_node(node_name=node_info.target_node_name,
-                                                 node_type=node_info.node_type,
-                                                 node_metatype=node_info.node_metatype,
-                                                 # TODO (vshampor): collect layer attributes for custom nodes
-                                                 layer_attributes=None,
-                                                 layer_name=node_info.weight_node_name,  # sic!
-                                                 is_shared=is_shared,
-                                                 ignored_algorithms=['quantization'])
+            nncf_node = nncf_graph.add_nncf_node(
+                node_name=node_info.target_node_name,
+                node_type=node_info.node_type,
+                node_metatype=node_info.node_metatype,
+                # TODO (vshampor): collect layer attributes for custom nodes
+                layer_attributes=None,
+                layer_name=node_info.weight_node_name,  # sic!
+                is_shared=is_shared,
+                ignored_algorithms=["quantization"],
+            )
             node_name_vs_nncf_node_ids[node_info.target_node_name] = nncf_node.node_id
             self._nncf_node_names_vs_custom_layer_name[node_info.target_node_name] = custom_layer_name
         for edge, edge_data in custom_layer_info.edge_infos.items():
             from_node_name, to_node_name = edge
             from_node_id = node_name_vs_nncf_node_ids[from_node_name]
             to_node_id = node_name_vs_nncf_node_ids[to_node_name]
-            nncf_graph.add_edge_between_nncf_nodes(from_node_id, to_node_id,
-                                                   tensor_shape=edge_data.tensor_shape,
-                                                   input_port_id=edge_data.input_port_id,
-                                                   output_port_id=edge_data.output_port_id,
-                                                   dtype=edge_data.dtype)
+            nncf_graph.add_edge_between_nncf_nodes(
+                from_node_id,
+                to_node_id,
+                tensor_shape=edge_data.tensor_shape,
+                input_port_id=edge_data.input_port_id,
+                output_port_id=edge_data.output_port_id,
+                dtype=edge_data.dtype,
+            )
         return nncf_graph
 
 
 class TFModelConverterFactory:
     @staticmethod
     def create(model) -> TFModelConverter:
         func_model = is_functional_model(model)
         seq_model = is_sequential_model(model)
 
         if not func_model and not seq_model:
-            RuntimeError('Only sequential or functional models are supported')
+            RuntimeError("Only sequential or functional models are supported")
 
         if func_model:
             converter = FunctionalConverter(model)
         else:
             converter = SequentialConverter(model)
         return converter
 
 
 class FunctionalConverter(BaseFunctionalSequentialConverter):
     """
     Converter for TF models that use the Functional API.
     """
+
     def __init__(self, model: tf.keras.Model):
         super().__init__(model)
         self._model_config = self._model.get_config()
         self._layer_info = {}  # type: Dict[str, Dict]
         self._collect_layer_information()
         self._layer_name_to_node_names = defaultdict(set)
         self._collect_node_information()
         self._edge_info = {}  # type: Dict[Tuple[str, str], Dict]
         self._collect_edge_information()
 
     def _collect_layer_information(self):
-        for layer_config in self._model_config['layers']:
-            layer_name = layer_config['name']
+        for layer_config in self._model_config["layers"]:
+            layer_name = layer_config["name"]
             layer_type = self._get_layer_type(layer_config)
             layer_output_dtype = self._get_layer_output_dtype(layer_config)
-            data_format = layer_config['config'].get('data_format')
+            data_format = layer_config["config"].get("data_format")
             model_layer = self._get_layer(layer_name)
             layer_metatype = get_keras_layer_metatype(model_layer)
             self._layer_info[layer_name] = {
-                        'type': layer_type,
-                        'metatype': layer_metatype,
-                        'dtype': layer_output_dtype,
-                        'data_format': data_format,
-                        'inbound_nodes': layer_config.get('inbound_nodes')
-                    }
+                "type": layer_type,
+                "metatype": layer_metatype,
+                "dtype": layer_output_dtype,
+                "data_format": data_format,
+                "inbound_nodes": layer_config.get("inbound_nodes"),
+            }
 
     def _collect_node_information(self):
-        for layer_config in self._model_config['layers']:
-            layer_name = layer_config['name']
+        for layer_config in self._model_config["layers"]:
+            layer_name = layer_config["name"]
             if layer_name not in self._custom_layer_infos:
                 self._add_regular_layer_nodes(layer_config)
             else:
                 # TODO (vshampor): Instead of adding a single node for custom layer and an entire
                 #  unconnected subgraph along with it, stitch the subgraph into the main graph
                 #  properly
                 self._add_regular_layer_nodes(layer_config)
 
     def _add_regular_layer_nodes(self, layer_config: Dict):
-        layer_name = layer_config['name']
+        layer_name = layer_config["name"]
         layer = self._get_layer(layer_name)
-        if layer_config['inbound_nodes']:
-            instances_count = len(layer_config['inbound_nodes'])
+        if layer_config["inbound_nodes"]:
+            instances_count = len(layer_config["inbound_nodes"])
             is_shared = instances_count > 1
             for i in range(instances_count):
                 node_name = get_shared_node_name(layer_name, i) if is_shared else layer_name
                 input_shapes = [tuple(tensor.shape) for tensor in layer.inbound_nodes[i].keras_inputs]
                 output_shapes = self._prepare_shape(layer.inbound_nodes[i].output_shapes)
                 self._node_info[node_name] = {
-                    'layer_name': layer_name,
-                    'target_node_name': layer_name,
-                    'inbound_node_idx': i,
-                    'input_shapes': input_shapes,
-                    'output_shapes': output_shapes,
+                    "layer_name": layer_name,
+                    "target_node_name": layer_name,
+                    "inbound_node_idx": i,
+                    "input_shapes": input_shapes,
+                    "output_shapes": output_shapes,
                 }
                 self._layer_name_to_node_names[layer_name].add(node_name)
         else:
             node_name = layer_name
             input_shapes = self._prepare_shape(layer.input_shape)
             output_shapes = self._prepare_shape(layer.output_shape)
             self._node_info[node_name] = {
-                'layer_name': layer_name,
-                'target_node_name': layer_name,
-                'inbound_node_idx': None,
-                'input_shapes': input_shapes,
-                'output_shapes': output_shapes,
+                "layer_name": layer_name,
+                "target_node_name": layer_name,
+                "inbound_node_idx": None,
+                "input_shapes": input_shapes,
+                "output_shapes": output_shapes,
             }
 
     def _is_layer_shared(self, layer_name: str):
         # Only gives valid results if called after collect_node_information()
         return len(self._layer_name_to_node_names[layer_name]) > 1
 
     def _collect_edge_information(self):
-        for layer_config in self._model_config['layers']:
-            layer_name = layer_config['name']
+        for layer_config in self._model_config["layers"]:
+            layer_name = layer_config["name"]
 
-            inbound_nodes = layer_config['inbound_nodes']
-            if layer_config['class_name'] in ['TFOpLambda', 'SlicingOpLambda']:
+            inbound_nodes = layer_config["inbound_nodes"]
+            if layer_config["class_name"] in ["TFOpLambda", "SlicingOpLambda"]:
                 inbound_nodes = reformat_inbound_nodes_for_oplambda(inbound_nodes)
 
             for layer_instance_idx, inbound_nodes in enumerate(inbound_nodes):
                 if self._is_layer_shared(layer_name):
                     node_name = get_shared_node_name(layer_name, layer_instance_idx)
                 else:
                     node_name = layer_name
-                input_shapes = self._node_info[node_name]['input_shapes']
+                input_shapes = self._node_info[node_name]["input_shapes"]
 
                 layer_instance_input_port_id = 0
                 for inbound_node in inbound_nodes:
-                    producer_layer_name, producer_layer_instance, \
-                    producer_layer_instance_output_port, _ = inbound_node
+                    producer_layer_name, producer_layer_instance, producer_layer_instance_output_port, _ = inbound_node
 
                     if self._is_layer_shared(producer_layer_name):
                         producer_node_name = get_shared_node_name(producer_layer_name, producer_layer_instance)
                     else:
                         producer_node_name = producer_layer_name
 
                     producer_layer_info = self._layer_info[producer_layer_name]
-                    dtype = producer_layer_info['dtype']
+                    dtype = producer_layer_info["dtype"]
                     tensor_shape = input_shapes[layer_instance_input_port_id]
 
                     edge = (producer_node_name, node_name)
                     self._edge_info[edge] = {
-                        'tensor_shape': tensor_shape,
-                        'dtype': dtype,
-                        'to_node_input_port_id': layer_instance_input_port_id,
-                        'from_node_output_port_id': producer_layer_instance_output_port
+                        "tensor_shape": tensor_shape,
+                        "dtype": dtype,
+                        "to_node_input_port_id": layer_instance_input_port_id,
+                        "from_node_output_port_id": producer_layer_instance_output_port,
                     }
                     layer_instance_input_port_id += 1
 
     def convert(self) -> NNCFGraph:
         nncf_graph = NNCFGraph()
         node_name_vs_nncf_node_ids = {}  # type: Dict[str, int]
         output_node_id_vs_model_output_idx = {}  # type: Dict[int, int]
 
         # Regular nodes
         for node_name, node_info in self._node_info.items():
-            layer_name = node_info['layer_name']
+            layer_name = node_info["layer_name"]
             node_name_vs_nncf_node_ids[layer_name] = []
             layer_info = self._layer_info[layer_name]
-            metatype = layer_info['metatype']
+            metatype = layer_info["metatype"]
             layer = self._get_layer(layer_name)
-            if metatype in DEPTHWISE_CONV_LAYER_METATYPES:
-                layer_attributes = _get_conv_layer_attributes(layer, is_depthwise=True)
-            elif metatype in GENERAL_CONV_LAYER_METATYPES:
-                layer_attributes = _get_conv_layer_attributes(layer, is_depthwise=False)
-            elif metatype in LINEAR_LAYER_METATYPES:
-                layer_attributes = _get_linear_layer_attributes(layer)
-            elif metatype in LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_CONCAT_INPUTS:
-                layer_attributes = _get_multiple_input_layer_attributes(layer)
-            elif metatype in RESHAPE_METATYPES:
-                layer_attributes = _get_reshape_layer_attributes(layer)
-            elif metatype in LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_OUTPUTS:
-                layer_attributes = _get_multiple_output_layer_attributes(layer)
-            else:
-                layer_attributes = None
+            layer_attributes = _get_layer_attributes(metatype, layer)
             is_shared = len(self._layer_name_to_node_names[layer_name]) > 1
-            nncf_node = nncf_graph.add_nncf_node(node_name=node_name,
-                                                 node_type=layer_info['type'],
-                                                 node_metatype=metatype,
-                                                 layer_attributes=layer_attributes,
-                                                 layer_name=layer_name,
-                                                 is_shared=is_shared)
+            nncf_node = nncf_graph.add_nncf_node(
+                node_name=node_name,
+                node_type=layer_info["type"],
+                node_metatype=metatype,
+                layer_attributes=layer_attributes,
+                layer_name=layer_name,
+                is_shared=is_shared,
+            )
             node_name_vs_nncf_node_ids[node_name] = nncf_node.node_id
 
-            #pylint:disable=protected-access
+            # pylint:disable=protected-access
             if layer in self._model._output_layers:
                 output_idx = self._model._output_layers.index(layer)
                 output_node_id_vs_model_output_idx[nncf_node.node_id] = output_idx
 
         # Regular edges
         for edge, edge_info in self._edge_info.items():
             from_node_name, to_node_name = edge
             from_node_id = node_name_vs_nncf_node_ids[from_node_name]
             to_node_id = node_name_vs_nncf_node_ids[to_node_name]
-            nncf_graph.add_edge_between_nncf_nodes(from_node_id,
-                                                   to_node_id,
-                                                   tensor_shape=edge_info['tensor_shape'],
-                                                   input_port_id=edge_info['to_node_input_port_id'],
-                                                   output_port_id=edge_info['from_node_output_port_id'],
-                                                   dtype=edge_info['dtype'])
+            nncf_graph.add_edge_between_nncf_nodes(
+                from_node_id,
+                to_node_id,
+                tensor_shape=edge_info["tensor_shape"],
+                input_port_id=edge_info["to_node_input_port_id"],
+                output_port_id=edge_info["from_node_output_port_id"],
+                dtype=edge_info["dtype"],
+            )
 
         # Custom nodes and edges
         for custom_layer_name in self._custom_layer_infos:
             nncf_graph = self._add_custom_layer_subgraph(nncf_graph, custom_layer_name)
             # TODO (vshampor): connect the subgraphs with the rest of the graph
 
         for output_node_id, model_output_idx in output_node_id_vs_model_output_idx.items():
             # Ticket: 56853
             # We won't add an NNCF output auxiliary node for all of the NNCF nodes corresponding to real
             # model output, but only for the nodes that do not serve as a tensor source for any other node.
             # The reason is that current TF capabilities do not allow to insert post-hooks after TF functional model
             # output nodes without changing the name of the corresponding output, which won't be obvious to the user.
             nncf_node = nncf_graph.get_node_by_id(output_node_id)
             if not nncf_graph.get_next_nodes(nncf_node):
-                output_aux_node_name = f'{nncf_node.node_name}_{PREFIX_AUXILIARY_OUTPUT_NODE}_{model_output_idx}'
+                output_aux_node_name = f"{nncf_node.node_name}_{PREFIX_AUXILIARY_OUTPUT_NODE}_{model_output_idx}"
                 output_node = nncf_graph.add_nncf_node(
                     node_name=output_aux_node_name,
                     node_type=NNCFGraphNodeType.OUTPUT_NODE,
-                    node_metatype=OutputNoopMetatype)
+                    node_metatype=OutputNoopMetatype,
+                )
                 node_info = self._node_info[nncf_node.node_name]  # works if _node_info keys are identical to node_names
-                nncf_graph.add_edge_between_nncf_nodes(nncf_node.node_id,
-                                                       output_node.node_id,
-                                                       tensor_shape=node_info['output_shapes'][0],
-                                                       input_port_id=0,
-                                                       output_port_id=0,
-                                                       dtype=Dtype.FLOAT)
+                nncf_graph.add_edge_between_nncf_nodes(
+                    nncf_node.node_id,
+                    output_node.node_id,
+                    tensor_shape=node_info["output_shapes"][0],
+                    input_port_id=0,
+                    output_port_id=0,
+                    dtype=Dtype.FLOAT,
+                )
 
         return nncf_graph
 
 
 class SequentialConverter(BaseFunctionalSequentialConverter):
     """
     Converter for the TF models using the Sequential API.
     """
+
     def convert(self) -> NNCFGraph:
         nncf_graph = NNCFGraph()
         producer_layer_id = None
         model_config = self._model.get_config()
 
         layer_name = None
-        for layer_config in model_config['layers']:
-            layer_name = layer_config['config']['name']
+        for layer_config in model_config["layers"]:
+            layer_name = layer_config["config"]["name"]
             if layer_name in self._custom_layer_infos:
                 nncf_graph = self._add_custom_layer_subgraph(nncf_graph, layer_name)
                 continue
             layer_type = self._get_layer_type(layer_config)
             layer_output_dtype = self._get_layer_output_dtype(layer_config)
-            data_format = layer_config['config'].get('data_format')
+            data_format = layer_config["config"].get("data_format")
             model_layer = self._get_layer(layer_name)
             layer_metatype = get_keras_layer_metatype(model_layer)
 
-            attrs = dict(type=layer_type,
-                         dtype=layer_output_dtype,
-                         metatype=layer_metatype,
-                         data_format=data_format,
-                         in_ports=[0],
-                         out_ports=[0],
-                         is_shared=False)
-
-            layer_attributes = None
-            if layer_metatype in DEPTHWISE_CONV_LAYER_METATYPES:
-                layer_attributes = _get_conv_layer_attributes(model_layer, is_depthwise=True)
-            elif layer_metatype in GENERAL_CONV_LAYER_METATYPES:
-                layer_attributes = _get_conv_layer_attributes(model_layer, is_depthwise=False)
-            elif layer_metatype in LINEAR_LAYER_METATYPES:
-                layer_attributes = _get_linear_layer_attributes(model_layer)
-            elif layer_metatype in LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_CONCAT_INPUTS:
-                layer_attributes = _get_multiple_input_layer_attributes(model_layer)
-            elif layer_metatype in RESHAPE_METATYPES:
-                layer_attributes = _get_reshape_layer_attributes(model_layer)
-            elif layer_metatype in LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_OUTPUTS:
-                layer_attributes = _get_multiple_output_layer_attributes(model_layer)
+            attrs = dict(
+                type=layer_type,
+                dtype=layer_output_dtype,
+                metatype=layer_metatype,
+                data_format=data_format,
+                in_ports=[0],
+                out_ports=[0],
+                is_shared=False,
+            )
 
+            layer_attributes = _get_layer_attributes(layer_metatype, model_layer)
             if layer_attributes is not None:
                 attrs.update({NNCFGraph.LAYER_ATTRIBUTES: layer_attributes})
 
             node_name = layer_name
-            nncf_node = nncf_graph.add_nncf_node(node_name,
-                                                 node_type=layer_type,
-                                                 node_metatype=layer_metatype,
-                                                 layer_attributes=layer_attributes,
-                                                 layer_name=layer_name,
-                                                 is_shared=False)
+            nncf_node = nncf_graph.add_nncf_node(
+                node_name,
+                node_type=layer_type,
+                node_metatype=layer_metatype,
+                layer_attributes=layer_attributes,
+                layer_name=layer_name,
+                is_shared=False,
+            )
 
             input_shapes = self._prepare_shape(model_layer.input_shape)
             output_shapes = self._prepare_shape(model_layer.output_shape)
             self._node_info[node_name] = {
-                'layer_name': layer_name,
-                'target_node_name': layer_name,
-                'inbound_node_idx': None,
-                'input_shapes': input_shapes,
-                'output_shapes': output_shapes,
+                "layer_name": layer_name,
+                "target_node_name": layer_name,
+                "inbound_node_idx": None,
+                "input_shapes": input_shapes,
+                "output_shapes": output_shapes,
             }
 
             if producer_layer_id is not None:
                 input_shapes = self._prepare_shape(self._model.get_layer(layer_name).input_shape)
-                nncf_graph.add_edge_between_nncf_nodes(producer_layer_id,
-                                                       nncf_node.node_id,
-                                                       tensor_shape=input_shapes[0],
-                                                       input_port_id=0,
-                                                       output_port_id=0,
-                                                       dtype=Dtype.FLOAT)  # TODO(vshampor): determine from keras layers
+                nncf_graph.add_edge_between_nncf_nodes(
+                    producer_layer_id,
+                    nncf_node.node_id,
+                    tensor_shape=input_shapes[0],
+                    input_port_id=0,
+                    output_port_id=0,
+                    dtype=Dtype.FLOAT,
+                )  # TODO(vshampor): determine from keras layers
             producer_layer_id = nncf_node.node_id
 
         if layer_name is not None:
             last_producer_layer_name = layer_name
             last_producer_layer_id = producer_layer_id
             output_model_layer = self._model.get_layer(last_producer_layer_name)
-            output_aux_node_name = PREFIX_AUXILIARY_OUTPUT_NODE + '_0'
-            output_node = nncf_graph.add_nncf_node(node_name=output_aux_node_name,
-                                                   node_type=NNCFGraphNodeType.OUTPUT_NODE,
-                                                   node_metatype=OutputNoopMetatype)
-            nncf_graph.add_edge_between_nncf_nodes(last_producer_layer_id, output_node.node_id,
-                                                   tensor_shape=self._prepare_shape(output_model_layer.output_shape),
-                                                   input_port_id=0, output_port_id=0, dtype=Dtype.FLOAT)
+            output_aux_node_name = PREFIX_AUXILIARY_OUTPUT_NODE + "_0"
+            output_node = nncf_graph.add_nncf_node(
+                node_name=output_aux_node_name,
+                node_type=NNCFGraphNodeType.OUTPUT_NODE,
+                node_metatype=OutputNoopMetatype,
+            )
+            nncf_graph.add_edge_between_nncf_nodes(
+                last_producer_layer_id,
+                output_node.node_id,
+                tensor_shape=self._prepare_shape(output_model_layer.output_shape),
+                input_port_id=0,
+                output_port_id=0,
+                dtype=Dtype.FLOAT,
+            )
 
         return nncf_graph
 
 
+def _get_layer_attributes(
+    layer_metatype: Type[OperatorMetatype], model_layer: tf.keras.layers.Layer
+) -> BaseLayerAttributes:
+    layer_attributes = None
+    if layer_metatype in DEPTHWISE_CONV_LAYER_METATYPES:
+        layer_attributes = _get_conv_layer_attributes(model_layer, is_depthwise=True)
+    elif layer_metatype in GENERAL_CONV_LAYER_METATYPES:
+        layer_attributes = _get_conv_layer_attributes(model_layer, is_depthwise=False)
+    elif layer_metatype in LINEAR_LAYER_METATYPES:
+        layer_attributes = _get_linear_layer_attributes(model_layer)
+    elif layer_metatype in LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_CONCAT_INPUTS:
+        layer_attributes = _get_multiple_input_layer_attributes(model_layer)
+    elif layer_metatype in RESHAPE_METATYPES:
+        layer_attributes = _get_reshape_layer_attributes(model_layer)
+    elif layer_metatype in DIMENSION_PERMUTATION_METATYPES:
+        layer_attributes = _get_permutation_layer_attributes(model_layer, layer_metatype)
+    elif layer_metatype in LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_OUTPUTS:
+        layer_attributes = _get_multiple_output_layer_attributes(model_layer)
+
+    return layer_attributes
+
+
 def _get_multiple_input_layer_attributes(layer: tf.keras.layers.Layer) -> MultipleInputLayerAttributes:
-    if hasattr(layer, 'axis'):
+    if hasattr(layer, "axis"):
         axis = layer.axis
     else:
         input_shape = layer.input_shape
         output_shape = layer.output_shape
         if not isinstance(output_shape, list):
             output_shape = [output_shape]
         axis = get_concat_axis(input_shape, output_shape)
@@ -745,42 +782,62 @@
     # with common code the groups attribute of the returned ConvolutionLayerAttribute must be set equal
     # to the in_channels attribute in order for the layer to be detected as depthwise
     groups = layer_.groups if not is_depthwise else in_channels
     kernel_size = layer_.kernel_size
 
     transpose = layer_metatype in DECONV_LAYER_METATYPES
 
-    return ConvolutionLayerAttributes(layer.trainable,
-                                      in_channels,
-                                      out_channels,
-                                      kernel_size,
-                                      strides,
-                                      groups, transpose=transpose,
-                                      padding_values=([0, 0, 0, 0]))
+    return ConvolutionLayerAttributes(
+        layer.trainable,
+        in_channels,
+        out_channels,
+        kernel_size,
+        strides,
+        groups,
+        transpose=transpose,
+        padding_values=([0, 0, 0, 0]),
+    )
 
 
 def _get_linear_layer_attributes(layer: tf.keras.layers.Layer) -> LinearLayerAttributes:
     channel_axis = get_input_channel_axis(layer)
     in_features = layer.get_input_shape_at(0)[channel_axis]
     out_features = layer.get_output_shape_at(0)[channel_axis]
     bias = layer.use_bias
-    return LinearLayerAttributes(layer.trainable,
-                                 in_features,
-                                 out_features,
-                                 bias)
+    return LinearLayerAttributes(layer.trainable, in_features, out_features, bias)
 
 
 def _get_reshape_layer_attributes(layer: tf.keras.layers.Layer) -> ReshapeLayerAttributes:
     input_shape = layer.input_shape
     output_shape = layer.output_shape
     if isinstance(output_shape, list):
         output_shape = output_shape[0]
     return ReshapeLayerAttributes(input_shape, output_shape)
 
 
+def _get_permutation_layer_attributes(
+    layer: tf.keras.layers.Layer, layer_metatype: OperatorMetatype
+) -> PermuteLayerAttributes:
+    dims = None
+    if hasattr(layer, "dims"):
+        dims = list(layer.dims)
+    else:
+        inbound_nodes = layer.inbound_nodes
+        if len(inbound_nodes) > 0 and hasattr(inbound_nodes[0], "call_kwargs"):
+            call_kwargs = inbound_nodes[0].call_kwargs
+            if "perm" in call_kwargs:
+                dims = call_kwargs["perm"]
+    assert dims, "failed to parse permute attributes"
+    if layer_metatype is layer_metatypes.TFPermuteLayerMetatype:
+        # Indexing starts at 1 for Permute.
+        # Need a zero dimension for consistent representation of permutation.
+        dims.insert(0, 0)
+    return PermuteLayerAttributes(dims)
+
+
 def _get_multiple_output_layer_attributes(layer: tf.keras.layers.Layer) -> MultipleOutputLayerAttributes:
     input_shape = layer.input_shape
     output_shape = layer.output_shape
     chunks = len(output_shape)
     if not isinstance(input_shape, list):
         input_shape = [input_shape]
     axis = get_split_axis(input_shape, output_shape)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/metatypes/common.py` & `nncf-2.5.0/nncf/tensorflow/graph/metatypes/common.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,19 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List
-from typing import Type
+from typing import List, Type
 
 from nncf.common.graph import INPUT_NOOP_METATYPES
 from nncf.common.graph import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.tensorflow.graph.metatypes import keras_layers as layer_metatypes
 from nncf.tensorflow.graph.metatypes import tf_ops as op_metatypes
@@ -71,15 +68,15 @@
     # DEPTHWISE_CONV_TF_OP_METATYPES
     op_metatypes.TFDepthwiseConv2dNativeOpMetatype,
 ]
 
 DECONV_LAYER_METATYPES = [
     layer_metatypes.TFConv1DTransposeLayerMetatype,
     layer_metatypes.TFConv2DTransposeLayerMetatype,
-    layer_metatypes.TFConv3DTransposeLayerMetatype
+    layer_metatypes.TFConv3DTransposeLayerMetatype,
 ]
 
 LINEAR_LAYER_METATYPES = [
     layer_metatypes.TFDenseLayerMetatype,
     # LINEAR_TF_OP_METATYPES
     op_metatypes.TFMatMulOpMetatype,
 ]
@@ -129,22 +126,21 @@
 ]
 
 LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_INPUTS = [
     op_metatypes.TFMaximumOpMetatype,
     op_metatypes.TFMinimumOpMetatype,
 ]
 
-LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_OUTPUTS = [
-    op_metatypes.TFSplitOpMetatype
-]
+LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_OUTPUTS = [op_metatypes.TFSplitOpMetatype]
 
-LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION = \
-    LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_ONE_INPUT + \
-    LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_CONCAT_INPUTS + \
-    LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_INPUTS
+LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION = (
+    LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_ONE_INPUT
+    + LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_CONCAT_INPUTS
+    + LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_MULTIPLE_INPUTS
+)
 
 ELEMENTWISE_LAYER_METATYPES = [
     layer_metatypes.TFAddLayerMetatype,
     layer_metatypes.TFMultiplyLayerMetatype,
     layer_metatypes.TFRescalingLayerMetatype,
     # ELEMENTWISE_TF_OP_METATYPES
     op_metatypes.TFAddOpMetatype,
@@ -157,17 +153,28 @@
     op_metatypes.TFMaximumOpMetatype,
     op_metatypes.TFMinimumOpMetatype,
 ]
 
 RESHAPE_METATYPES = [
     layer_metatypes.TFReshapeLayerMetatype,
     layer_metatypes.TFFlattenLayerMetatype,
-    op_metatypes.TFReshapeOpMetatype
+    op_metatypes.TFReshapeOpMetatype,
 ]
 
+DIMENSION_PERMUTATION_METATYPES = [
+    op_metatypes.TFTransposeOpMetatype,
+    layer_metatypes.TFPermuteLayerMetatype,
+]
+
+
 def get_operator_metatypes() -> List[Type[OperatorMetatype]]:
     keras_metatypes_list = list(layer_metatypes.KERAS_LAYER_METATYPES.registry_dict.values())
     tf_metatypes_list = list(op_metatypes.TF_OPERATION_METATYPES.registry_dict.values())
-    return list(set(keras_metatypes_list + tf_metatypes_list + \
-                    list(INPUT_NOOP_METATYPES.registry_dict.values()) +
-                    list(OUTPUT_NOOP_METATYPES.registry_dict.values()) +
-                    list(NOOP_METATYPES.registry_dict.values())))
+    return list(
+        set(
+            keras_metatypes_list
+            + tf_metatypes_list
+            + list(INPUT_NOOP_METATYPES.registry_dict.values())
+            + list(OUTPUT_NOOP_METATYPES.registry_dict.values())
+            + list(NOOP_METATYPES.registry_dict.values())
+        )
+    )
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/metatypes/keras_layers.py` & `nncf-2.5.0/nncf/tensorflow/graph/metatypes/keras_layers.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,35 +1,33 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from collections import namedtuple
 from typing import List, Optional, Type
 
 import tensorflow as tf
 
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.operator_metatypes import OperatorMetatypeRegistry
 from nncf.common.hardware.opset import HWConfigOpName
 from nncf.tensorflow.graph.metatypes.tf_ops import TF_OPERATION_METATYPES
 
-WeightDef = namedtuple('WeightDef', ['weight_attr_name', 'channel_axes'])
+WeightDef = namedtuple("WeightDef", ["weight_attr_name", "channel_axes"])
 
-KERAS_LAYER_METATYPES = OperatorMetatypeRegistry('keras_layer_metatypes')
+KERAS_LAYER_METATYPES = OperatorMetatypeRegistry("keras_layer_metatypes")
 
 
 class TFLayerMetatype(OperatorMetatype):
     keras_layer_names = []  # type: List[str]
     subtypes = []  # type: List[Type[OperatorMetatype]]
 
     @classmethod
@@ -38,616 +36,606 @@
 
     @classmethod
     def determine_subtype(cls, layer: tf.keras.layers.Layer) -> Optional[Type[OperatorMetatype]]:
         return cls._determine_subtype(layer)
 
     @classmethod
     def determine_subtype_wrapped_layer(
-            cls,
-            layer: tf.keras.layers.Layer,
-            wrapper: Optional[tf.keras.layers.Wrapper] = None) -> Optional[Type[OperatorMetatype]]:
+        cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None
+    ) -> Optional[Type[OperatorMetatype]]:
         return cls._determine_subtype(layer, wrapper)
 
     @classmethod
     def get_all_aliases(cls) -> List[str]:
         return cls.keras_layer_names
 
     @classmethod
-    def matches(cls,
-                layer: tf.keras.layers.Layer,
-                _: Optional[tf.keras.layers.Wrapper] = None) -> bool:
+    def matches(cls, layer: tf.keras.layers.Layer, _: Optional[tf.keras.layers.Wrapper] = None) -> bool:
         return layer.__class__.__name__ in cls.keras_layer_names
 
     @classmethod
     def _determine_subtype(
-            cls,
-            layer: tf.keras.layers.Layer,
-            wrapper: Optional[tf.keras.layers.Wrapper] = None) -> Optional[Type[OperatorMetatype]]:
+        cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None
+    ) -> Optional[Type[OperatorMetatype]]:
         matches = []
         for subtype in cls.get_subtypes():
             if subtype.matches(layer, wrapper):
                 subtype_matches = subtype.determine_subtype_wrapped_layer(layer, wrapper)
                 if subtype_matches is not None:
                     matches.extend(subtype_matches)
                 else:
                     matches.append(subtype)
         if len(matches) > 1:
-            raise RuntimeError('Multiple subtypes match operator call - '
-                               'cannot determine single subtype.')
+            raise RuntimeError("Multiple subtypes match operator call - cannot determine single subtype.")
         if not matches:
             return None
         return matches[0]
 
 
 class TFLayerWithWeightsMetatype(TFLayerMetatype):
     weight_definitions = []  # type: List[WeightDef]
     bias_attr_name = None  # type: Optional[str]
 
 
 @KERAS_LAYER_METATYPES.register()
 @NOOP_METATYPES.register()
 class TFLayerNoopMetatype(TFLayerMetatype):
-    name = 'noop'
+    name = "noop"
 
     @classmethod
     def get_all_aliases(cls) -> List[str]:
         return [cls.name]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFDepthwiseConv1DSubLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'DepthwiseConv1D(Conv1DKerasLayer)'
-    keras_layer_names = ['Conv1D', 'Convolution1D']
+    name = "DepthwiseConv1D(Conv1DKerasLayer)"
+    keras_layer_names = ["Conv1D", "Convolution1D"]
     hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-1)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-1)]
+    bias_attr_name = "bias"
 
     @classmethod
-    def matches(cls,
-                layer: tf.keras.layers.Layer,
-                wrapper: Optional[tf.keras.layers.Wrapper] = None) -> bool:
-        return layer.__class__.__name__ in cls.keras_layer_names and \
-               _is_depthwise_conv(layer, wrapper)
+    def matches(cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None) -> bool:
+        return layer.__class__.__name__ in cls.keras_layer_names and _is_depthwise_conv(layer, wrapper)
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFConv1DLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'Conv1DKerasLayer'
-    keras_layer_names = ['Conv1D', 'Convolution1D']
+    name = "Conv1DKerasLayer"
+    keras_layer_names = ["Conv1D", "Convolution1D"]
     hw_config_names = [HWConfigOpName.CONVOLUTION]
     subtypes = [TFDepthwiseConv1DSubLayerMetatype]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-1)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-1)]
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFDepthwiseConv2DSubLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'DepthwiseConv2D(Conv2DKerasLayer)'
-    keras_layer_names = ['Conv2D', 'Convolution2D']
+    name = "DepthwiseConv2D(Conv2DKerasLayer)"
+    keras_layer_names = ["Conv2D", "Convolution2D"]
     hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-1)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-1)]
+    bias_attr_name = "bias"
 
     @classmethod
-    def matches(cls,
-                layer: tf.keras.layers.Layer,
-                wrapper: Optional[tf.keras.layers.Wrapper] = None) -> bool:
-        return layer.__class__.__name__ in cls.keras_layer_names and \
-               _is_depthwise_conv(layer, wrapper)
+    def matches(cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None) -> bool:
+        return layer.__class__.__name__ in cls.keras_layer_names and _is_depthwise_conv(layer, wrapper)
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFConv2DLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'Conv2DKerasLayer'
-    keras_layer_names = ['Conv2D', 'Convolution2D']
+    name = "Conv2DKerasLayer"
+    keras_layer_names = ["Conv2D", "Convolution2D"]
     hw_config_names = [HWConfigOpName.CONVOLUTION]
     subtypes = [TFDepthwiseConv2DSubLayerMetatype]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-1)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-1)]
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFDepthwiseConv3DSubLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'DepthwiseConv3D(Conv3DKerasLayer)'
-    keras_layer_names = ['Conv3D', 'Convolution3D']
+    name = "DepthwiseConv3D(Conv3DKerasLayer)"
+    keras_layer_names = ["Conv3D", "Convolution3D"]
     hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-1)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-1)]
+    bias_attr_name = "bias"
 
     @classmethod
-    def matches(cls,
-                layer: tf.keras.layers.Layer,
-                wrapper: Optional[tf.keras.layers.Wrapper] = None) -> bool:
-        return layer.__class__.__name__ in cls.keras_layer_names and \
-               _is_depthwise_conv(layer, wrapper)
+    def matches(cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None) -> bool:
+        return layer.__class__.__name__ in cls.keras_layer_names and _is_depthwise_conv(layer, wrapper)
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFConv3DLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'Conv3DKerasLayer'
-    keras_layer_names = ['Conv3D', 'Convolution3D']
+    name = "Conv3DKerasLayer"
+    keras_layer_names = ["Conv3D", "Convolution3D"]
     hw_config_names = [HWConfigOpName.CONVOLUTION]
     subtypes = [TFDepthwiseConv3DSubLayerMetatype]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-1)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-1)]
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFDepthwiseConv2DLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'DepthwiseConv2DKerasLayer'
-    keras_layer_names = ['DepthwiseConv2D']
+    name = "DepthwiseConv2DKerasLayer"
+    keras_layer_names = ["DepthwiseConv2D"]
     hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]
 
-    weight_definitions = [WeightDef(weight_attr_name='depthwise_kernel', channel_axes=(2, 3))]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="depthwise_kernel", channel_axes=(2, 3))]
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFConv1DTransposeLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'Conv1DTransposeKerasLayer'
-    keras_layer_names = ['Conv1DTranspose', 'Convolution1DTranspose']
+    name = "Conv1DTransposeKerasLayer"
+    keras_layer_names = ["Conv1DTranspose", "Convolution1DTranspose"]
     hw_config_names = [HWConfigOpName.CONVOLUTION]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-2)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-2)]
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFConv2DTransposeLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'Conv2DTransposeKerasLayer'
-    keras_layer_names = ['Conv2DTranspose', 'Convolution2DTranspose']
+    name = "Conv2DTransposeKerasLayer"
+    keras_layer_names = ["Conv2DTranspose", "Convolution2DTranspose"]
     hw_config_names = [HWConfigOpName.CONVOLUTION]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-2)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-2)]
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFConv3DTransposeLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'Conv3DTransposeKerasLayer'
-    keras_layer_names = ['Conv3DTranspose', 'Convolution3DTranspose']
+    name = "Conv3DTransposeKerasLayer"
+    keras_layer_names = ["Conv3DTranspose", "Convolution3DTranspose"]
     hw_config_names = [HWConfigOpName.CONVOLUTION]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-2)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-2)]
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFDenseLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'DenseKerasLayer'
-    keras_layer_names = ['Dense']
+    name = "DenseKerasLayer"
+    keras_layer_names = ["Dense"]
     hw_config_names = [HWConfigOpName.MATMUL]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=-1)]
-    bias_attr_name = 'bias'
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=-1)]
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFBatchNormalizationLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'BatchNormalizationKerasLayer'
-    keras_layer_names = ['BatchNormalization', 'SyncBatchNormalization']
+    name = "BatchNormalizationKerasLayer"
+    keras_layer_names = ["BatchNormalization", "SyncBatchNormalization"]
 
-    weight_definitions = [WeightDef(weight_attr_name='gamma', channel_axes=0)]
-    bias_attr_name = 'beta'
+    weight_definitions = [WeightDef(weight_attr_name="gamma", channel_axes=0)]
+    bias_attr_name = "beta"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFSeparableConv1DLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'SeparableConv1DKerasLayer'
-    keras_layer_names = ['SeparableConv1D', 'SeparableConvolution1D']
+    name = "SeparableConv1DKerasLayer"
+    keras_layer_names = ["SeparableConv1D", "SeparableConvolution1D"]
 
     weight_definitions = [
-        WeightDef(weight_attr_name='depthwise_kernel', channel_axes=(1, 2)),
-        WeightDef(weight_attr_name='pointwise_kernel', channel_axes=-1),
+        WeightDef(weight_attr_name="depthwise_kernel", channel_axes=(1, 2)),
+        WeightDef(weight_attr_name="pointwise_kernel", channel_axes=-1),
     ]
-    bias_attr_name = 'bias'
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFSeparableConv2DLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'SeparableConv2DKerasLayer'
-    keras_layer_names = ['SeparableConv2D', 'SeparableConvolution2D']
+    name = "SeparableConv2DKerasLayer"
+    keras_layer_names = ["SeparableConv2D", "SeparableConvolution2D"]
 
     weight_definitions = [
-        WeightDef(weight_attr_name='depthwise_kernel', channel_axes=(2, 3)),
-        WeightDef(weight_attr_name='pointwise_kernel', channel_axes=-1),
+        WeightDef(weight_attr_name="depthwise_kernel", channel_axes=(2, 3)),
+        WeightDef(weight_attr_name="pointwise_kernel", channel_axes=-1),
     ]
-    bias_attr_name = 'bias'
+    bias_attr_name = "bias"
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFEmbeddingLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'EmbeddingKerasLayer'
-    keras_layer_names = ['Embedding']
+    name = "EmbeddingKerasLayer"
+    keras_layer_names = ["Embedding"]
 
-    weight_definitions = [WeightDef(weight_attr_name='embeddings', channel_axes=None)]
+    weight_definitions = [WeightDef(weight_attr_name="embeddings", channel_axes=None)]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFLocallyConnected1DLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'LocallyConnected1DKerasLayer'
-    keras_layer_names = ['LocallyConnected1D']
+    name = "LocallyConnected1DKerasLayer"
+    keras_layer_names = ["LocallyConnected1D"]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=None)]
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=None)]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFLocallyConnected2DLayerMetatype(TFLayerWithWeightsMetatype):
-    name = 'LocallyConnected2DKerasLayer'
-    keras_layer_names = ['LocallyConnected2D']
+    name = "LocallyConnected2DKerasLayer"
+    keras_layer_names = ["LocallyConnected2D"]
 
-    weight_definitions = [WeightDef(weight_attr_name='kernel', channel_axes=None)]
+    weight_definitions = [WeightDef(weight_attr_name="kernel", channel_axes=None)]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFCropping1DLayerMetatype(TFLayerMetatype):
-    name = 'Cropping1DKerasLayer'
-    keras_layer_names = ['Cropping1D']
+    name = "Cropping1DKerasLayer"
+    keras_layer_names = ["Cropping1D"]
     hw_config_names = [HWConfigOpName.CROP]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFCropping2DLayerMetatype(TFLayerMetatype):
-    name = 'Cropping2DKerasLayer'
-    keras_layer_names = ['Cropping2D']
+    name = "Cropping2DKerasLayer"
+    keras_layer_names = ["Cropping2D"]
     hw_config_names = [HWConfigOpName.CROP]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFCropping3DLayerMetatype(TFLayerMetatype):
-    name = 'Cropping3DKerasLayer'
-    keras_layer_names = ['Cropping3D']
+    name = "Cropping3DKerasLayer"
+    keras_layer_names = ["Cropping3D"]
     hw_config_names = [HWConfigOpName.CROP]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFFlattenLayerMetatype(TFLayerMetatype):
-    name = 'FlattenKerasLayer'
-    keras_layer_names = ['Flatten']
+    name = "FlattenKerasLayer"
+    keras_layer_names = ["Flatten"]
     hw_config_names = [HWConfigOpName.FLATTEN]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFGlobalMaxPooling1DLayerMetatype(TFLayerMetatype):
-    name = 'GlobalMaxPooling1DKerasLayer'
-    keras_layer_names = ['GlobalMaxPool1D', 'GlobalMaxPooling1D']
+    name = "GlobalMaxPooling1DKerasLayer"
+    keras_layer_names = ["GlobalMaxPool1D", "GlobalMaxPooling1D"]
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFGlobalMaxPooling2DLayerMetatype(TFLayerMetatype):
-    name = 'GlobalMaxPooling2DKerasLayer'
-    keras_layer_names = ['GlobalMaxPool2D', 'GlobalMaxPooling2D']
+    name = "GlobalMaxPooling2DKerasLayer"
+    keras_layer_names = ["GlobalMaxPool2D", "GlobalMaxPooling2D"]
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFGlobalMaxPooling3DLayerMetatype(TFLayerMetatype):
-    name = 'GlobalMaxPooling3DKerasLayer'
-    keras_layer_names = ['GlobalMaxPool3D', 'GlobalMaxPooling3D']
+    name = "GlobalMaxPooling3DKerasLayer"
+    keras_layer_names = ["GlobalMaxPool3D", "GlobalMaxPooling3D"]
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFMaxPooling1DLayerMetatype(TFLayerMetatype):
-    name = 'MaxPooling1DKerasLayer'
-    keras_layer_names = ['MaxPool1D', 'MaxPooling1D']
+    name = "MaxPooling1DKerasLayer"
+    keras_layer_names = ["MaxPool1D", "MaxPooling1D"]
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFMaxPooling2DLayerMetatype(TFLayerMetatype):
-    name = 'MaxPooling2DKerasLayer'
-    keras_layer_names = ['MaxPool2D', 'MaxPooling2D']
+    name = "MaxPooling2DKerasLayer"
+    keras_layer_names = ["MaxPool2D", "MaxPooling2D"]
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFMaxPooling3DLayerMetatype(TFLayerMetatype):
-    name = 'MaxPooling3DKerasLayer'
-    keras_layer_names = ['MaxPool3D', 'MaxPooling3D']
+    name = "MaxPooling3DKerasLayer"
+    keras_layer_names = ["MaxPool3D", "MaxPooling3D"]
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFRepeatVectorLayerMetatype(TFLayerMetatype):
-    name = 'RepeatVectorKerasLayer'
-    keras_layer_names = ['RepeatVector']
+    name = "RepeatVectorKerasLayer"
+    keras_layer_names = ["RepeatVector"]
     hw_config_names = [HWConfigOpName.TILE]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFReshapeLayerMetatype(TFLayerMetatype):
-    name = 'ReshapeKerasLayer'
-    keras_layer_names = ['Reshape']
+    name = "ReshapeKerasLayer"
+    keras_layer_names = ["Reshape"]
     hw_config_names = [HWConfigOpName.RESHAPE]
 
 
 @KERAS_LAYER_METATYPES.register()
+class TFPermuteLayerMetatype(TFLayerMetatype):
+    name = "PermuteKerasLayer"
+    keras_layer_names = ["Permute"]
+    hw_config_names = [HWConfigOpName.TRANSPOSE]
+
+
+@KERAS_LAYER_METATYPES.register()
 class TFZeroPadding1DLayerMetatype(TFLayerMetatype):
-    name = 'ZeroPadding1DKerasLayer'
-    keras_layer_names = ['ZeroPadding1D']
+    name = "ZeroPadding1DKerasLayer"
+    keras_layer_names = ["ZeroPadding1D"]
     hw_config_names = [HWConfigOpName.PAD]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFZeroPadding2DLayerMetatype(TFLayerMetatype):
-    name = 'ZeroPadding2DKerasLayer'
-    keras_layer_names = ['ZeroPadding2D']
+    name = "ZeroPadding2DKerasLayer"
+    keras_layer_names = ["ZeroPadding2D"]
     hw_config_names = [HWConfigOpName.PAD]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFZeroPadding3DLayerMetatype(TFLayerMetatype):
-    name = 'ZeroPadding3DKerasLayer'
-    keras_layer_names = ['ZeroPadding3D']
+    name = "ZeroPadding3DKerasLayer"
+    keras_layer_names = ["ZeroPadding3D"]
     hw_config_names = [HWConfigOpName.PAD]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFUpSampling1DLayerMetatype(TFLayerMetatype):
     # Split->Concat pattern
-    name = 'UpSampling1DKerasLayer'
-    keras_layer_names = ['UpSampling1D']
+    name = "UpSampling1DKerasLayer"
+    keras_layer_names = ["UpSampling1D"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFUpSampling2DLayerMetatype(TFLayerMetatype):
-    name = 'UpSampling2DKerasLayer'
-    keras_layer_names = ['UpSampling2D']
+    name = "UpSampling2DKerasLayer"
+    keras_layer_names = ["UpSampling2D"]
     hw_config_names = [HWConfigOpName.INTERPOLATE]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFUpSampling3DLayerMetatype(TFLayerMetatype):
-    name = 'UpSampling3DKerasLayer'
-    keras_layer_names = ['UpSampling3D']
+    name = "UpSampling3DKerasLayer"
+    keras_layer_names = ["UpSampling3D"]
     hw_config_names = [HWConfigOpName.INTERPOLATE]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFConcatenateLayerMetatype(TFLayerMetatype):
-    name = 'ConcatenateKerasLayer'
-    keras_layer_names = ['Concatenate']
+    name = "ConcatenateKerasLayer"
+    keras_layer_names = ["Concatenate"]
     hw_config_names = [HWConfigOpName.CONCAT]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFAddLayerMetatype(TFLayerMetatype):
-    name = 'AddKerasLayer'
-    keras_layer_names = ['Add']
+    name = "AddKerasLayer"
+    keras_layer_names = ["Add"]
     hw_config_names = [HWConfigOpName.ADD]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFSubtractLayerMetatype(TFLayerMetatype):
-    name = 'SubtractKerasLayer'
-    keras_layer_names = ['Subtract']
+    name = "SubtractKerasLayer"
+    keras_layer_names = ["Subtract"]
     hw_config_names = [HWConfigOpName.SUBTRACT]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFMultiplyLayerMetatype(TFLayerMetatype):
-    name = 'MultiplyKerasLayer'
-    keras_layer_names = ['Multiply']
+    name = "MultiplyKerasLayer"
+    keras_layer_names = ["Multiply"]
     hw_config_names = [HWConfigOpName.MULTIPLY]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFRescalingLayerMetatype(TFLayerMetatype):
-    name = 'RescalingKerasLayer'
-    keras_layer_names = ['Rescaling']
+    name = "RescalingKerasLayer"
+    keras_layer_names = ["Rescaling"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFAveragePooling1DLayerMetatype(TFLayerMetatype):
-    name = 'AveragePooling1DKerasLayer'
-    keras_layer_names = ['AveragePooling1D', 'AvgPool1D']
+    name = "AveragePooling1DKerasLayer"
+    keras_layer_names = ["AveragePooling1D", "AvgPool1D"]
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFAveragePooling2DLayerMetatype(TFLayerMetatype):
-    name = 'AveragePooling2DKerasLayer'
-    keras_layer_names = ['AveragePooling2D', 'AvgPool2D']
+    name = "AveragePooling2DKerasLayer"
+    keras_layer_names = ["AveragePooling2D", "AvgPool2D"]
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFAveragePooling3DLayerMetatype(TFLayerMetatype):
-    name = 'AveragePooling3DKerasLayer'
-    keras_layer_names = ['AveragePooling3D', 'AvgPool3D']
+    name = "AveragePooling3DKerasLayer"
+    keras_layer_names = ["AveragePooling3D", "AvgPool3D"]
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFGlobalAveragePooling1DLayerMetatype(TFLayerMetatype):
-    name = 'GlobalAveragePooling1DKerasLayer'
-    keras_layer_names = ['GlobalAveragePooling1D', 'GlobalAvgPool1D']
+    name = "GlobalAveragePooling1DKerasLayer"
+    keras_layer_names = ["GlobalAveragePooling1D", "GlobalAvgPool1D"]
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFGlobalAveragePooling2DLayerMetatype(TFLayerMetatype):
-    name = 'GlobalAveragePooling2DKerasLayer'
-    keras_layer_names = ['GlobalAveragePooling2D', 'GlobalAvgPool2D']
+    name = "GlobalAveragePooling2DKerasLayer"
+    keras_layer_names = ["GlobalAveragePooling2D", "GlobalAvgPool2D"]
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFGlobalAveragePooling3DLayerMetatype(TFLayerMetatype):
-    name = 'GlobalAveragePooling3DKerasLayer'
-    keras_layer_names = ['GlobalAveragePooling3D', 'GlobalAvgPool3D']
+    name = "GlobalAveragePooling3DKerasLayer"
+    keras_layer_names = ["GlobalAveragePooling3D", "GlobalAvgPool3D"]
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFReLULayerMetatype(TFLayerMetatype):
-    name = 'ReLUKerasLayer'
-    keras_layer_names = ['ReLU']
+    name = "ReLUKerasLayer"
+    keras_layer_names = ["ReLU"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFThresholdedReLULayerMetatype(TFLayerMetatype):
-    name = 'ThresholdedReLUKerasLayer'
-    keras_layer_names = ['ThresholdedReLU']
+    name = "ThresholdedReLUKerasLayer"
+    keras_layer_names = ["ThresholdedReLU"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFELULayerMetatype(TFLayerMetatype):
-    name = 'ELUKerasLayer'
-    keras_layer_names = ['ELU']
+    name = "ELUKerasLayer"
+    keras_layer_names = ["ELU"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFPReLULayerMetatype(TFLayerMetatype):
-    name = 'PReLUKerasLayer'
-    keras_layer_names = ['PReLU']
+    name = "PReLUKerasLayer"
+    keras_layer_names = ["PReLU"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFLeakyReLULayerMetatype(TFLayerMetatype):
-    name = 'LeakyReLUKerasLayer'
-    keras_layer_names = ['LeakyReLU']
+    name = "LeakyReLUKerasLayer"
+    keras_layer_names = ["LeakyReLU"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFSoftmaxLayerMetatype(TFLayerMetatype):
-    name = 'SoftmaxKerasLayer'
-    keras_layer_names = ['Softmax']
+    name = "SoftmaxKerasLayer"
+    keras_layer_names = ["Softmax"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFActivationLayerMetatype(TFLayerMetatype):
-    name = 'ActivationKerasLayer'
-    keras_layer_names = ['Activation']
+    name = "ActivationKerasLayer"
+    keras_layer_names = ["Activation"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFAverageLayerMetatype(TFLayerMetatype):
-    name = 'AverageKerasLayer'
-    keras_layer_names = ['Average']
+    name = "AverageKerasLayer"
+    keras_layer_names = ["Average"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFLayerNormalizationLayerMetatype(TFLayerMetatype):
-    name = 'LayerNormalizationKerasLayer'
-    keras_layer_names = ['LayerNormalization']
+    name = "LayerNormalizationKerasLayer"
+    keras_layer_names = ["LayerNormalization"]
 
 
 @KERAS_LAYER_METATYPES.register()
 @INPUT_NOOP_METATYPES.register()
 class TFInputLayerMetatype(TFLayerMetatype):
-    name = 'InputLayer'
-    keras_layer_names = ['InputLayer']
+    name = "InputLayer"
+    keras_layer_names = ["InputLayer"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFDropoutLayerMetatype(TFLayerMetatype):
-    name = 'DropoutKerasLayer'
-    keras_layer_names = ['Dropout']
+    name = "DropoutKerasLayer"
+    keras_layer_names = ["Dropout"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFLambdaLayerMetatype(TFLayerMetatype):
-    name = 'LambdaKerasLayer'
-    keras_layer_names = ['Lambda']
+    name = "LambdaKerasLayer"
+    keras_layer_names = ["Lambda"]
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFTensorFlowOpLayerMetatype(TFLayerMetatype):
-    name = 'TensorFlowOpKerasLayer'
-    keras_layer_names = ['TensorFlowOpLayer']
+    name = "TensorFlowOpKerasLayer"
+    keras_layer_names = ["TensorFlowOpLayer"]
 
     @classmethod
     def get_subtypes(cls) -> List[Type[OperatorMetatype]]:
         return list(TF_OPERATION_METATYPES.registry_dict.values())
 
     @classmethod
-    def determine_subtype(cls,
-                          layer: tf.keras.layers.Layer,
-                          wrapper: Optional[tf.keras.layers.Wrapper] = None) -> Optional[Type[OperatorMetatype]]:
+    def determine_subtype(
+        cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None
+    ) -> Optional[Type[OperatorMetatype]]:
         return TF_OPERATION_METATYPES.get_operator_metatype_by_op_name(layer.node_def.op)
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFOpLambdaMetatype(TFLayerMetatype):
-    name = 'TFOpLambdaKerasLayer'
-    keras_layer_names = ['TFOpLambda']
+    name = "TFOpLambdaKerasLayer"
+    keras_layer_names = ["TFOpLambda"]
 
     @classmethod
     def get_subtypes(cls) -> List[Type[OperatorMetatype]]:
         return list(TF_OPERATION_METATYPES.registry_dict.values())
 
     @classmethod
-    def determine_subtype(cls,
-                          layer: tf.keras.layers.Layer,
-                          wrapper: Optional[tf.keras.layers.Wrapper] = None) -> Optional[Type[OperatorMetatype]]:
+    def determine_subtype(
+        cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None
+    ) -> Optional[Type[OperatorMetatype]]:
         return TF_OPERATION_METATYPES.get_operator_metatype_by_op_name(layer.symbol)
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFSlicingOpLambdaMetatype(TFLayerMetatype):
-    name = 'SlicingOpLambdaKerasLayer'
-    keras_layer_names = ['SlicingOpLambda']
+    name = "SlicingOpLambdaKerasLayer"
+    keras_layer_names = ["SlicingOpLambda"]
 
     @classmethod
     def get_subtypes(cls) -> List[Type[OperatorMetatype]]:
         return list(TF_OPERATION_METATYPES.registry_dict.values())
 
     @classmethod
-    def determine_subtype(cls,
-                          layer: tf.keras.layers.Layer,
-                          wrapper: Optional[tf.keras.layers.Wrapper] = None) -> Optional[Type[OperatorMetatype]]:
+    def determine_subtype(
+        cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None
+    ) -> Optional[Type[OperatorMetatype]]:
         return TF_OPERATION_METATYPES.get_operator_metatype_by_op_name(layer.symbol)
 
 
 @KERAS_LAYER_METATYPES.register()
 class TFNNCFWrapperLayerMetatype(TFLayerMetatype):
-    name = 'NNCFWrapperLayer'
-    keras_layer_names = ['NNCFWrapper']
+    name = "NNCFWrapperLayer"
+    keras_layer_names = ["NNCFWrapper"]
 
     @classmethod
     def get_subtypes(cls) -> List[Type[OperatorMetatype]]:
         return list(KERAS_LAYER_METATYPES.registry_dict.values())
 
     @classmethod
     def determine_subtype(cls, layer: tf.keras.layers.Layer) -> Optional[Type[OperatorMetatype]]:
         unwrapped_layer = layer.layer
         unwrapped_layer_metatype = KERAS_LAYER_METATYPES.get_operator_metatype_by_op_name(
-            unwrapped_layer.__class__.__name__)
+            unwrapped_layer.__class__.__name__
+        )
         subtype = unwrapped_layer_metatype.determine_subtype_wrapped_layer(unwrapped_layer, layer)
         if subtype is not None:
             return subtype
         return unwrapped_layer_metatype
 
     @classmethod
     def _determine_subtype(
-            cls,
-            layer: tf.keras.layers.Layer,
-            wrapper: Optional[tf.keras.layers.Wrapper] = None) -> Optional[Type[OperatorMetatype]]:
+        cls, layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None
+    ) -> Optional[Type[OperatorMetatype]]:
         unwrapped_layer = layer.layer
         return super()._determine_subtype(unwrapped_layer, wrapper)
 
 
-def _is_depthwise_conv(
-        layer: tf.keras.layers.Layer,
-        wrapper: Optional[tf.keras.layers.Wrapper] = None) -> bool:
-    channel_axis = -1 - layer.rank \
-        if layer.data_format == 'channels_first' else -1
+def _is_depthwise_conv(layer: tf.keras.layers.Layer, wrapper: Optional[tf.keras.layers.Wrapper] = None) -> bool:
+    channel_axis = -1 - layer.rank if layer.data_format == "channels_first" else -1
 
-    channels = layer.get_input_shape_at(0)[channel_axis] if wrapper is None \
-        else wrapper.get_input_shape_at(0)[channel_axis]
+    channels = (
+        layer.get_input_shape_at(0)[channel_axis] if wrapper is None else wrapper.get_input_shape_at(0)[channel_axis]
+    )
 
     if channels is None:
-        raise ValueError('The channel dimension of the inputs '
-                         'should be defined. Found `None`.')
+        raise ValueError("The channel dimension of the inputs should be defined. Found `None`.")
 
     input_channels = int(channels)
 
     return input_channels == layer.groups and input_channels > 1
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/metatypes/matcher.py` & `nncf-2.5.0/nncf/tensorflow/graph/metatypes/matcher.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,33 +1,29 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Type
 
 import tensorflow as tf
 
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.operator_metatypes import UnknownMetatype
 from nncf.tensorflow.graph.metatypes.keras_layers import KERAS_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.tf_ops import TF_OPERATION_METATYPES
 
 
-def get_keras_layer_metatype(
-        layer: tf.keras.layers.Layer,
-        determine_subtype: bool = True) -> Type[OperatorMetatype]:
+def get_keras_layer_metatype(layer: tf.keras.layers.Layer, determine_subtype: bool = True) -> Type[OperatorMetatype]:
     """
     Returns a metatype of the Keras layer.
 
     The flag 'determine_subtype' specifies which metatype for the layer the subtype
     or main type will be returned.
 
     For example, you created instance of the depthwise convolution using
@@ -47,16 +43,15 @@
     ```
 
     :param layer: The Keras layer.
     :param determine_subtype: Determines the subtype of the metatype if True and
         returns the primary metatype otherwise.
     :return: A metatype.
     """
-    layer_metatype = KERAS_LAYER_METATYPES.get_operator_metatype_by_op_name(
-        layer.__class__.__name__)
+    layer_metatype = KERAS_LAYER_METATYPES.get_operator_metatype_by_op_name(layer.__class__.__name__)
 
     if not determine_subtype:
         return layer_metatype
     subtype = None
     if layer_metatype is not UnknownMetatype:
         subtype = layer_metatype.determine_subtype(layer)
     if subtype is not None:
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/model_transformer.py` & `nncf-2.5.0/nncf/tensorflow/graph/model_transformer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,27 +1,21 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import copy
 from collections import OrderedDict
 from collections import namedtuple
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Set
-from typing import Union
+from typing import Callable, Dict, List, Set, Union
 
 import tensorflow as tf
 
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
@@ -30,48 +24,45 @@
 from nncf.tensorflow.graph.transformations.commands import TFBeforeLayer
 from nncf.tensorflow.graph.transformations.commands import TFLayer
 from nncf.tensorflow.graph.transformations.commands import TFLayerWeight
 from nncf.tensorflow.graph.transformations.commands import TFMultiLayerPoint
 from nncf.tensorflow.graph.transformations.commands import TFOperationWithWeights
 from nncf.tensorflow.graph.transformations.layout import TFTransformationLayout
 from nncf.tensorflow.graph.utils import get_custom_objects
-from nncf.tensorflow.graph.utils import reformat_inbound_nodes_for_oplambda
 from nncf.tensorflow.graph.utils import get_weight_name
 from nncf.tensorflow.graph.utils import is_functional_model
 from nncf.tensorflow.graph.utils import is_sequential_or_functional_model
+from nncf.tensorflow.graph.utils import reformat_inbound_nodes_for_oplambda
 from nncf.tensorflow.layers.custom_objects import get_nncf_custom_objects
 from nncf.tensorflow.layers.wrapper import NNCFWrapper
 
-WeightOperations = namedtuple('WeightOperations',
-                              ('weights_attr_name', 'operations'))
+WeightOperations = namedtuple("WeightOperations", ("weights_attr_name", "operations"))
 
 
 class TFModelTransformer(ModelTransformer):
     """
     Applies transformations to a Keras model graph.
     """
+
     def __init__(self, model):
         """
         Initializes Model Transformer
 
         :param model: Keras model to be transformed
         """
         if not is_sequential_or_functional_model(model):
-            raise ValueError(
-                'Only tf.keras sequential or functional models can be transformed.')
+            raise ValueError("Only tf.keras sequential or functional models can be transformed.")
 
         super().__init__(model)
         self._model_config = model.get_config()
-        self._custom_objects = dict(
-            list(get_custom_objects(model).items()) + list(get_nncf_custom_objects().items())
-        )
+        self._custom_objects = dict(list(get_custom_objects(model).items()) + list(get_nncf_custom_objects().items()))
         self._name_mapping = {}
 
     def transform(self, transformation_layout: TFTransformationLayout):
-        """ Applies transformations to the Keras model.
+        """Applies transformations to the Keras model.
 
         :param transformation_layout: List of transformations
         :return: The transformed Keras model
         """
         layer_weights_map = {layer.name: self._get_layer_weights(layer) for layer in self._model.layers}
 
         for transform in transformation_layout.transformations:
@@ -87,28 +78,26 @@
             if weights:
                 self._set_layer_weights(layer, weights)
 
         return transformed_model
 
     def _get_layer_weights(self, layer):
         weights_map = OrderedDict()
-        for weight_tensor, weight_numpy in \
-                zip(layer.weights, layer.get_weights()):
+        for weight_tensor, weight_numpy in zip(layer.weights, layer.get_weights()):
             weights_map[get_weight_name(weight_tensor.name, layer.name)] = weight_numpy
 
         return weights_map
 
     @staticmethod
     def _set_layer_weights(layer, weights_map: Dict):
         weight_value_tuples = []
         for weight_tensor in layer.weights:
             weight_name = get_weight_name(weight_tensor.name, layer.name)
             if weight_name in weights_map:
-                weight_value_tuples.append(
-                    (weight_tensor, weights_map[weight_name]))
+                weight_value_tuples.append((weight_tensor, weights_map[weight_name]))
 
         tf.keras.backend.batch_set_value(weight_value_tuples)
 
     def _get_layer(self, layer_name: str):
         # For purposes of performance, will try to get the original unmodified layer
         # first. This function could have been made to return the modified layer state by
         # first trying to deserialize the corresponding name from the config (which is potentially
@@ -117,23 +106,21 @@
         # we would have to deserialize the layer object.
         for layer in self._model.layers:
             if layer.name == layer_name:
                 return layer
 
         _, layer_config = self._find_layer_config(layer_name)
         if layer_config:
-            return tf.keras.utils.deserialize_keras_object(
-                layer_config, custom_objects=self._custom_objects)
+            return tf.keras.utils.deserialize_keras_object(layer_config, custom_objects=self._custom_objects)
 
         return None
 
     def _find_layer_config(self, layer_name: str):
-        for idx, layer in enumerate(self._model_config['layers']):
-            layer_name_ = layer['name'] if is_functional_model(self._model) \
-                else layer['config']['name']
+        for idx, layer in enumerate(self._model_config["layers"]):
+            layer_name_ = layer["name"] if is_functional_model(self._model) else layer["config"]["name"]
             if layer_name_ == layer_name:
                 return idx, layer
         return None, None
 
     def _update_layer_mapping(self, src_layer_name: str, dst_layer_name: str):
         if src_layer_name in self._name_mapping.values():
             for orig_layer_name, orig_layer in self._name_mapping.items():
@@ -146,138 +133,137 @@
         if transformation.type == TransformationType.INSERT:
             self._insert(transformation.target_point, transformation.insertion_objects)
         elif transformation.type == TransformationType.MULTI_INSERT:
             self._multi_insertion(transformation.target_point, transformation.commands)
         elif transformation.type == TransformationType.REMOVE:
             self._remove(transformation.target_point)
         else:
-            raise TypeError('Transformation type {} does not support'
-                            .format(transformation.type))
+            raise TypeError("Transformation type {} does not support".format(transformation.type))
 
     def _insert(self, target_point: Union[TargetPoint, TFMultiLayerPoint], insertion_objects: List[Callable]):
         if isinstance(target_point, TFMultiLayerPoint):
             self._shared_insert_layers(target_point.target_points, insertion_objects)
         elif isinstance(target_point, TFLayerWeight):
-            weight_operations = [
-                WeightOperations(target_point.weights_attr_name, insertion_objects)]
+            weight_operations = [WeightOperations(target_point.weights_attr_name, insertion_objects)]
             self._insert_weight_operations(target_point.layer_name, weight_operations)
         elif isinstance(target_point, TFBeforeLayer):
-            self._insert_layers_before(target_point.layer_name,
-                                       target_point.instance_idx,
-                                       target_point.input_port_id,
-                                       insertion_objects)
+            self._insert_layers_before(
+                target_point.layer_name, target_point.instance_idx, target_point.input_port_id, insertion_objects
+            )
         elif isinstance(target_point, TFAfterLayer):
-            self._insert_layers_after(target_point.layer_name,
-                                      target_point.instance_idx,
-                                      target_point.output_port_id,
-                                      insertion_objects)
+            self._insert_layers_after(
+                target_point.layer_name, target_point.instance_idx, target_point.output_port_id, insertion_objects
+            )
         else:
-            raise TypeError('Insertion transform does not support {} '
-                            'target point type'.format(target_point.type))
+            raise TypeError("Insertion transform does not support {} target point type".format(target_point.type))
 
     # pylint:disable=too-many-branches
     def _shared_insert_layers(self, target_points: List[TargetPoint], layers_to_insert: List[Callable]):
         functional_model = is_functional_model(self._model)
         if functional_model:
-            for layer in self._model_config['input_layers']:
+            for layer in self._model_config["input_layers"]:
                 for tp in target_points:
                     if isinstance(tp, TFBeforeLayer) and tp.layer_name == layer[0]:
-                        raise RuntimeError(f'Insertion before input layer: {tp.layer_name} is not supported')
+                        raise RuntimeError(f"Insertion before input layer: {tp.layer_name} is not supported")
 
         layer_configs = []
         for layer in layers_to_insert:
             config = tf.keras.utils.serialize_keras_object(layer)
             if functional_model:
-                config['name'] = config['config']['name']
-                config['inbound_nodes'] = []
+                config["name"] = config["config"]["name"]
+                config["inbound_nodes"] = []
                 for i, tp in enumerate(target_points):
                     if isinstance(tp, TFAfterLayer):
-                        config['inbound_nodes'].append([[tp.layer_name, tp.instance_idx, tp.output_port_id, {}]])
+                        config["inbound_nodes"].append([[tp.layer_name, tp.instance_idx, tp.output_port_id, {}]])
                     elif isinstance(tp, TFBeforeLayer):
                         idx, input_layer_cfg = self._find_layer_config(tp.layer_name)
-                        inbound = input_layer_cfg['inbound_nodes'][tp.instance_idx][tp.input_port_id]
-                        config['inbound_nodes'].append([[inbound[0], inbound[1], inbound[2], {}]])
-                        self._model_config['layers'][idx]['inbound_nodes'][tp.instance_idx][tp.input_port_id] = \
-                                [config['name'], i, 0, inbound[3]]
+                        inbound = input_layer_cfg["inbound_nodes"][tp.instance_idx][tp.input_port_id]
+                        config["inbound_nodes"].append([[inbound[0], inbound[1], inbound[2], {}]])
+                        self._model_config["layers"][idx]["inbound_nodes"][tp.instance_idx][tp.input_port_id] = [
+                            config["name"],
+                            i,
+                            0,
+                            inbound[3],
+                        ]
                     else:
                         raise TypeError(
-                            f'Insertion transform does not support {target_points[0].type} target point type')
+                            f"Insertion transform does not support {target_points[0].type} target point type"
+                        )
 
             layer_configs.append(config)
 
         for config in layer_configs:
             for i, tp in enumerate(target_points):
                 if functional_model and isinstance(tp, TFAfterLayer):
                     layer_out_ports = set()
-                    replace_layer_name = config['name']
-                    for layer in self._model_config['layers']:
-                        inbound_nodes = layer['inbound_nodes']
-                        if layer['class_name'] in ['TFOpLambda', 'SlicingOpLambda']:
+                    replace_layer_name = config["name"]
+                    for layer in self._model_config["layers"]:
+                        inbound_nodes = layer["inbound_nodes"]
+                        if layer["class_name"] in ["TFOpLambda", "SlicingOpLambda"]:
                             inbound_nodes = reformat_inbound_nodes_for_oplambda(inbound_nodes)
 
                         for inbound_node in inbound_nodes:
-                            self._process_insertion_after(inbound_node, tp.layer_name, tp.instance_idx,
-                                                          layer_out_ports, replace_layer_name, i)
-
-                    self._insert_after_model_outputs(tp.layer_name, tp.instance_idx, layer_out_ports,
-                                                     replace_layer_name, i)
+                            self._process_insertion_after(
+                                inbound_node, tp.layer_name, tp.instance_idx, layer_out_ports, replace_layer_name, i
+                            )
+
+                    self._insert_after_model_outputs(
+                        tp.layer_name, tp.instance_idx, layer_out_ports, replace_layer_name, i
+                    )
                     if len(layer_out_ports) > 1:
-                        raise RuntimeError('Insertion after layer ({}) with multiple ports '
-                                           'is not supported'.format(tp.layer_name))
+                        raise RuntimeError(
+                            "Insertion after layer ({}) with multiple ports is not supported".format(tp.layer_name)
+                        )
 
             layer_name = target_points[0].layer_name
             self._insert_layer_after_sequential(layer_name, config)
 
     def _multi_insertion(self, target_point: TargetPoint, commands: List[TransformationCommand]):
         if not isinstance(target_point, TFLayer):
-            raise TypeError('Multiple insertion transform does not support '
-                            '{} target point type'.format(target_point.type))
+            raise TypeError(
+                "Multiple insertion transform does not support {} target point type".format(target_point.type)
+            )
 
         weight_operations = []
         for cmd in commands:
-            if cmd.type != TransformationType.INSERT or \
-                    cmd.target_point.type != TargetType.OPERATION_WITH_WEIGHTS:
-                raise TypeError('Multiple insertion transform does not support command: '
-                                'command type - {}; target point type - {}'
-                                .format(cmd.type, cmd.target_point.type))
-            weight_operations.append(
-                WeightOperations(
-                    cmd.target_point.weights_attr_name,
-                    cmd.insertion_objects
-                ))
+            if cmd.type != TransformationType.INSERT or cmd.target_point.type != TargetType.OPERATION_WITH_WEIGHTS:
+                raise TypeError(
+                    "Multiple insertion transform does not support command: "
+                    "command type - {}; target point type - {}".format(cmd.type, cmd.target_point.type)
+                )
+            weight_operations.append(WeightOperations(cmd.target_point.weights_attr_name, cmd.insertion_objects))
 
         self._insert_weight_operations(target_point.layer_name, weight_operations)
 
     def _remove(self, target_point: TargetPoint):
         if isinstance(target_point, TFOperationWithWeights):
             target_layer_name = target_point.layer_name
             self._remove_weight_operation(
-                target_layer_name,
-                target_point.weights_attr_name,
-                target_point.operation_name)
+                target_layer_name, target_point.weights_attr_name, target_point.operation_name
+            )
         else:
-            raise TypeError('{} removal does not support'.format(target_point.type))
+            raise TypeError("{} removal does not support".format(target_point.type))
 
     def _remove_weight_operation(self, layer_name: str, weights_attr_name: str, operation_name: str):
         _, layer_config = self._find_layer_config(layer_name)
-        weights_operations = layer_config['config']['weights_attr_operations'].pop(weights_attr_name)
+        weights_operations = layer_config["config"]["weights_attr_operations"].pop(weights_attr_name)
 
         def find_weights_operation(operations, name):
             for op in operations:
-                if op['config']['name'] == name:
+                if op["config"]["name"] == name:
                     return op
             return None
 
         found = find_weights_operation(weights_operations, operation_name)
         weights_operations.remove(found)
 
         if weights_operations:
-            layer_config['config']['weights_attr_operations'][weights_attr_name] = weights_operations
-        elif not layer_config['config']['weights_attr_operations']:
-            self._replace_config(layer_name, layer_config['config']['layer'])
+            layer_config["config"]["weights_attr_operations"][weights_attr_name] = weights_operations
+        elif not layer_config["config"]["weights_attr_operations"]:
+            self._replace_config(layer_name, layer_config["config"]["layer"])
 
     def _insert_weight_operations(self, layer_name: str, weight_operations: List[WeightOperations]):
         """
         Assigns the operations to be executed with a layer weight while accessing it.
         Any operation that have already been associated with the weight will be replaced with the
         new operation(s).
         :param layer_name: The name of the weighted layer.
@@ -295,103 +281,105 @@
     def _replace(self, layer_name: str, replace_layer):
         # NOTE: the change to the layer will only get propagated to self._model.layers
         # once the model is deserialized from config, i.e. not immediately.
         replace_layer_config = tf.keras.utils.serialize_keras_object(replace_layer)
         self._replace_config(layer_name, replace_layer_config)
 
     def _replace_config(self, layer_name: str, replace_layer_config: Dict):
-        replace_layer_name = replace_layer_config['config']['name']
+        replace_layer_name = replace_layer_config["config"]["name"]
         if is_functional_model(self._model):
-            if 'name' not in replace_layer_config:
-                replace_layer_config['name'] = replace_layer_name
+            if "name" not in replace_layer_config:
+                replace_layer_config["name"] = replace_layer_name
             self._replace_functional(layer_name, replace_layer_config)
         else:
             self._replace_sequential(layer_name, replace_layer_config)
 
         self._update_layer_mapping(layer_name, replace_layer_name)
 
     def _replace_functional(self, layer_name: str, replace_layer_config: Dict):
-        replace_layer_name = replace_layer_config['name']
-        for layer in self._model_config['layers']:
-            for inbound_node in layer['inbound_nodes']:
+        replace_layer_name = replace_layer_config["name"]
+        for layer in self._model_config["layers"]:
+            for inbound_node in layer["inbound_nodes"]:
                 self._process_replacement(inbound_node, layer_name, replace_layer_name)
 
         self._replace_in_model_outputs(layer_name, replace_layer_name)
 
         idx, layer_config = self._find_layer_config(layer_name)
-        replace_layer_config['inbound_nodes'] = layer_config['inbound_nodes']
-        self._model_config['layers'][idx] = replace_layer_config
+        replace_layer_config["inbound_nodes"] = layer_config["inbound_nodes"]
+        self._model_config["layers"][idx] = replace_layer_config
 
     def _replace_sequential(self, layer_name: str, replace_layer_config: Dict):
         idx, _ = self._find_layer_config(layer_name)
-        self._model_config['layers'][idx] = replace_layer_config
+        self._model_config["layers"][idx] = replace_layer_config
 
-    def _insert_layers_before(self, layer_name: str, instance_idx: int, input_port_id: int,
-                              layers_to_insert: List):
+    def _insert_layers_before(self, layer_name: str, instance_idx: int, input_port_id: int, layers_to_insert: List):
         """
         Performs insertion before the (downstream) layer.
 
         :param layer_name: Name of the layer, before which to insert (downstream).
         :param instance_idx: Instance ID of the layer, before which to insert (downstream).
         :param input_port_id: Input port ID of the layer, before which to insert (downstream).
         :param layers_to_insert: List of the layers, which will be inserted into the graph.
         """
         functional_model = is_functional_model(self._model)
 
         if functional_model:
-            for layer in self._model_config['input_layers']:
+            for layer in self._model_config["input_layers"]:
                 if layer_name == layer[0]:
-                    raise RuntimeError('Insertion before input layer: {} is not supported'.format(layer_name))
+                    raise RuntimeError("Insertion before input layer: {} is not supported".format(layer_name))
 
         layer_configs = []
         idx, downstream_layer_cfg = self._find_layer_config(layer_name)
 
         for layer in layers_to_insert:
             config = tf.keras.utils.serialize_keras_object(layer)
             if functional_model:
-                config['name'] = config['config']['name']
+                config["name"] = config["config"]["name"]
 
-                downstream_layer_inbound_nodes = downstream_layer_cfg['inbound_nodes']
-                if downstream_layer_cfg['class_name'] in ['TFOpLambda', 'SlicingOpLambda']:
+                downstream_layer_inbound_nodes = downstream_layer_cfg["inbound_nodes"]
+                if downstream_layer_cfg["class_name"] in ["TFOpLambda", "SlicingOpLambda"]:
                     downstream_layer_inbound_nodes = reformat_inbound_nodes_for_oplambda(downstream_layer_inbound_nodes)
 
                 # Update config of the layer to insert
                 inbound_node_info = copy.deepcopy(downstream_layer_inbound_nodes)[instance_idx][input_port_id]
-                config['inbound_nodes'] = [[inbound_node_info[0], inbound_node_info[1], inbound_node_info[2], {}]]
+                config["inbound_nodes"] = [[inbound_node_info[0], inbound_node_info[1], inbound_node_info[2], {}]]
 
                 # Downstream layer config update
-                if downstream_layer_cfg['class_name'] in ['TFOpLambda', 'SlicingOpLambda']:
-                    downstream_layer_inbound_nodes[instance_idx][input_port_id][0] = config['name']
+                if downstream_layer_cfg["class_name"] in ["TFOpLambda", "SlicingOpLambda"]:
+                    downstream_layer_inbound_nodes[instance_idx][input_port_id][0] = config["name"]
                 else:
-                    self._model_config['layers'][idx]['inbound_nodes'][instance_idx][input_port_id] = \
-                        [config['name'], 0, 0, {}]
+                    self._model_config["layers"][idx]["inbound_nodes"][instance_idx][input_port_id] = [
+                        config["name"],
+                        0,
+                        0,
+                        {},
+                    ]
 
             layer_configs.append(config)
 
         for config in layer_configs:
-            self._model_config['layers'].insert(idx, config)
+            self._model_config["layers"].insert(idx, config)
 
-    def _insert_layers_after(self, layer_name: str, instance_idx: int, output_port_id: int,
-                             layers_to_insert: List):
+    def _insert_layers_after(self, layer_name: str, instance_idx: int, output_port_id: int, layers_to_insert: List):
         """
         Performs insertion after the (upstream) layer.
 
         :param layer_name: Name of the layer, after which to insert (upstream).
         :param instance_idx: Instance ID of the layer, after which to insert (upstream).
         :param input_port_id: Input port ID of the layer, after which to insert (upstream).
         :param layers_to_insert: List of the layers, which will be inserted into the graph.
         """
         functional_model = is_functional_model(self._model)
 
         layer_configs = []
         for layer in layers_to_insert:
             config = tf.keras.utils.serialize_keras_object(layer)
             if functional_model:
-                config['name'] = config['config']['name']
-                config['inbound_nodes'] = [[[layer_name, instance_idx, output_port_id, {}]]]
+                config["name"] = config["config"]["name"]
+                config["inbound_nodes"] = [[[layer_name, instance_idx, output_port_id, {}]]]
             layer_configs.append(config)
 
         for config in layer_configs:
             if functional_model:
                 self._insert_layer_after_functional(layer_name, instance_idx, config)
             else:
                 self._insert_layer_after_sequential(layer_name, config)
@@ -401,85 +389,100 @@
         Performs insertion after the (upstream) layer (functional model).
 
         :param layer_name: Name of the layer, after which to insert (upstream).
         :param instance_idx: Instance ID of the Layer, after which to insert (upstream).
         :param layer_to_insert_config: Config of the layer, which is supposed to be inserted into the graph.
         """
         layer_out_ports = set()
-        replace_layer_name = layer_to_insert_config['name']  # new inbound_node name in the downstream layer
+        replace_layer_name = layer_to_insert_config["name"]  # new inbound_node name in the downstream layer
 
-        for layer in self._model_config['layers']:
-            inbound_nodes = layer['inbound_nodes']
+        for layer in self._model_config["layers"]:
+            inbound_nodes = layer["inbound_nodes"]
 
-            if layer['class_name'] in ['TFOpLambda', 'SlicingOpLambda']:
+            if layer["class_name"] in ["TFOpLambda", "SlicingOpLambda"]:
                 inbound_nodes = reformat_inbound_nodes_for_oplambda(inbound_nodes)
 
             for inbound_node in inbound_nodes:
-                self._process_insertion_after(inbound_node, layer_name, instance_idx,
-                                              layer_out_ports, replace_layer_name)
+                self._process_insertion_after(
+                    inbound_node, layer_name, instance_idx, layer_out_ports, replace_layer_name
+                )
 
-        self._insert_after_model_outputs(layer_name, instance_idx, layer_out_ports,
-                                         replace_layer_name)
+        self._insert_after_model_outputs(layer_name, instance_idx, layer_out_ports, replace_layer_name)
         if len(layer_out_ports) > 1:
-            raise RuntimeError('Insertion after layer ({}) with multiple ports '
-                               'is not supported'.format(layer_name))
+            raise RuntimeError("Insertion after layer ({}) with multiple ports is not supported".format(layer_name))
         self._insert_layer_after_sequential(layer_name, layer_to_insert_config)
 
     def _insert_layer_after_sequential(self, layer_name: str, layer_configs):
         idx, _ = self._find_layer_config(layer_name)
         if idx is None:
-            raise RuntimeError('Layer is not found: {}'.format(layer_name))
-        self._model_config['layers'].insert(idx + 1, layer_configs)
+            raise RuntimeError("Layer is not found: {}".format(layer_name))
+        self._model_config["layers"].insert(idx + 1, layer_configs)
 
     @staticmethod
-    def _process_insertion_after(connection_infos,
-                                 layer_name: str,
-                                 instance_idx: int,
-                                 layer_out_ports: Set,
-                                 replace_layer_name: str,
-                                 insert_with_instance_idx: int = 0):
+    def _process_insertion_after(
+        connection_infos,
+        layer_name: str,
+        instance_idx: int,
+        layer_out_ports: Set,
+        replace_layer_name: str,
+        insert_with_instance_idx: int = 0,
+    ):
         if not isinstance(connection_infos[0], list):
             connection_infos = [connection_infos[0]]
 
         for connection_info in connection_infos:
             if connection_info[0] == layer_name:
                 layer_out_ports.add(connection_info[2])
                 if connection_info[1] == instance_idx:
                     connection_info[0] = replace_layer_name
                     connection_info[1] = insert_with_instance_idx
 
-    def _insert_after_model_outputs(self, layer_name: str,
-                                    instance_idx: int,
-                                    layer_out_ports: Set,
-                                    replace_layer_name: str,
-                                    insert_with_instance_idx: int = 0):
-        output_layers = self._model_config['output_layers']
+    def _insert_after_model_outputs(
+        self,
+        layer_name: str,
+        instance_idx: int,
+        layer_out_ports: Set,
+        replace_layer_name: str,
+        insert_with_instance_idx: int = 0,
+    ):
+        output_layers = self._model_config["output_layers"]
         if isinstance(output_layers, list):
-            self._process_insertion_after(output_layers, layer_name, instance_idx,
-                                          layer_out_ports, replace_layer_name)
+            self._process_insertion_after(output_layers, layer_name, instance_idx, layer_out_ports, replace_layer_name)
         elif isinstance(output_layers, dict):
             for out_layers in output_layers.values():
                 if isinstance(out_layers, list):
-                    self._process_insertion_after([out_layers], layer_name, instance_idx,
-                                                  layer_out_ports, replace_layer_name, insert_with_instance_idx)
+                    self._process_insertion_after(
+                        [out_layers],
+                        layer_name,
+                        instance_idx,
+                        layer_out_ports,
+                        replace_layer_name,
+                        insert_with_instance_idx,
+                    )
                 elif isinstance(out_layers, dict):
-                    self._process_insertion_after(list(out_layers.values()), layer_name, instance_idx,
-                                                  layer_out_ports, replace_layer_name, insert_with_instance_idx)
+                    self._process_insertion_after(
+                        list(out_layers.values()),
+                        layer_name,
+                        instance_idx,
+                        layer_out_ports,
+                        replace_layer_name,
+                        insert_with_instance_idx,
+                    )
 
     @staticmethod
     def _process_replacement(connection_infos, layer_name: str, replace_layer_name: str):
         if not isinstance(connection_infos[0], list):
             connection_infos = [connection_infos[0]]
 
         for connection_info in connection_infos:
             if connection_info[0] == layer_name:
                 connection_info[0] = replace_layer_name
 
     def _replace_in_model_outputs(self, layer_name: str, replace_layer_name: str):
-        output_layers = self._model_config['output_layers']
+        output_layers = self._model_config["output_layers"]
         if isinstance(output_layers, list):
             self._process_replacement(output_layers, layer_name, replace_layer_name)
         elif isinstance(output_layers, dict):
             for out_layers in output_layers.values():
                 if isinstance(out_layers, list):
                     self._process_replacement([out_layers], layer_name, replace_layer_name)
                 elif isinstance(out_layers, dict):
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/pattern_operations.py` & `nncf-2.5.0/nncf/tensorflow/graph/pattern_operations.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,99 +1,89 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.common.graph.patterns import merge_two_types_of_operations
 from nncf.tensorflow.graph.metatypes.common import ELEMENTWISE_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.common import GENERAL_CONV_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.common import LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_ONE_INPUT
 from nncf.tensorflow.graph.metatypes.common import LINEAR_LAYER_METATYPES
 
-LINEAR_OPERATIONS = {'type': list(
-    {
-        *{layer_name for m in GENERAL_CONV_LAYER_METATYPES for layer_name in m.get_all_aliases()},
-        *{layer_name for m in LINEAR_LAYER_METATYPES for layer_name in m.get_all_aliases()},
-
-    }
-),
-    'label': 'LINEAR'
+LINEAR_OPERATIONS = {
+    "type": list(
+        {
+            *{layer_name for m in GENERAL_CONV_LAYER_METATYPES for layer_name in m.get_all_aliases()},
+            *{layer_name for m in LINEAR_LAYER_METATYPES for layer_name in m.get_all_aliases()},
+        }
+    ),
+    "label": "LINEAR",
 }
 
-ELEMENTWISE_OPERATIONS = {'type': list(set(
-    layer_name for m in ELEMENTWISE_LAYER_METATYPES for layer_name in m.get_all_aliases()
-)),
-    'label': 'ELEMENTWISE'
+ELEMENTWISE_OPERATIONS = {
+    "type": list(set(layer_name for m in ELEMENTWISE_LAYER_METATYPES for layer_name in m.get_all_aliases())),
+    "label": "ELEMENTWISE",
 }
 
 QUANTIZATION_AGNOSTIC_OPERATIONS = {
-'type': list(set(
-    layer_name for m in LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_ONE_INPUT for layer_name in m.get_all_aliases()
-)),
-    'label': 'ELEMENTWISE'
+    "type": list(
+        set(
+            layer_name
+            for m in LAYER_METATYPES_AGNOSTIC_TO_DATA_PRECISION_WITH_ONE_INPUT
+            for layer_name in m.get_all_aliases()
+        )
+    ),
+    "label": "ELEMENTWISE",
 }
 
-BATCH_NORMALIZATION_OPERATIONS = {'type': ['BatchNormalization',
-                                           'SyncBatchNormalization',
-                                           'FusedBatchNormV3'],
-                                  'label': 'BATCH_NORMALIZATION'
-                                  }
+BATCH_NORMALIZATION_OPERATIONS = {
+    "type": ["BatchNormalization", "SyncBatchNormalization", "FusedBatchNormV3"],
+    "label": "BATCH_NORMALIZATION",
+}
 
 KERAS_ACTIVATIONS_OPERATIONS = {
-    'type': ['ReLU',
-             'ThresholdedReLU',
-             'ELU',
-             'PReLU',
-             'LeakyReLU',
-             'Activation'],
-    'label': 'KERAS_ACTIVATIONS'
+    "type": ["ReLU", "ThresholdedReLU", "ELU", "PReLU", "LeakyReLU", "Activation"],
+    "label": "KERAS_ACTIVATIONS",
 }
 
 
 TF_ACTIVATIONS_OPERATIONS = {
-    'type': [
-        'Relu',
-        'nn.relu',
-        'Elu',
-        'LeakyRelu',
-        'Relu6',
-        'Selu',
-        'Sigmoid',
-        'Tanh',
+    "type": [
+        "Relu",
+        "nn.relu",
+        "Elu",
+        "LeakyRelu",
+        "Relu6",
+        "Selu",
+        "Sigmoid",
+        "Tanh",
     ],
-    'label': 'TF_ACTIVATIONS'
+    "label": "TF_ACTIVATIONS",
 }
 
-ATOMIC_ACTIVATIONS_OPERATIONS = merge_two_types_of_operations(KERAS_ACTIVATIONS_OPERATIONS,
-                                                              TF_ACTIVATIONS_OPERATIONS,
-                                                              'ATOMIC_ACTIVATIONS')
-
-POOLING_OPERATIONS = {'type': ['AveragePooling2D',
-                               'AveragePooling3D',
-                               'GlobalAveragePooling2D',
-                               'GlobalAveragePooling3D',
-                               'AvgPool',
-                               'AvgPool3D',
-                               'Mean',],
-                      'label': 'POOLING'}
-
-SINGLE_OPS = merge_two_types_of_operations(POOLING_OPERATIONS,
-                                           {
-                                               'type': [
-                                                   'Average',
-                                                   'LayerNormalization',
-                                                   'UpSampling2D'
-                                               ]
-                                           }, label='SINGLE_OPS')
-
-ARITHMETIC_OPERATIONS = {'type': ['__iadd__',
-                                  '__add__',
-                                  '__mul__',
-                                  '__rmul__'],
-                         'label': 'ARITHMETIC'}
+ATOMIC_ACTIVATIONS_OPERATIONS = merge_two_types_of_operations(
+    KERAS_ACTIVATIONS_OPERATIONS, TF_ACTIVATIONS_OPERATIONS, "ATOMIC_ACTIVATIONS"
+)
+
+POOLING_OPERATIONS = {
+    "type": [
+        "AveragePooling2D",
+        "AveragePooling3D",
+        "GlobalAveragePooling2D",
+        "GlobalAveragePooling3D",
+        "AvgPool",
+        "AvgPool3D",
+        "Mean",
+    ],
+    "label": "POOLING",
+}
+
+SINGLE_OPS = merge_two_types_of_operations(
+    POOLING_OPERATIONS, {"type": ["Average", "LayerNormalization", "UpSampling2D"]}, label="SINGLE_OPS"
+)
+
+ARITHMETIC_OPERATIONS = {"type": ["__iadd__", "__add__", "__mul__", "__rmul__"], "label": "ARITHMETIC"}
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/patterns.py` & `nncf-2.5.0/nncf/tensorflow/graph/patterns.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,55 +1,53 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.common.graph.patterns import GraphPattern
 from nncf.tensorflow.graph.metatypes.tf_ops import TFAddOpMetatype
-from nncf.tensorflow.graph.metatypes.tf_ops import TFRelu6OpMetatype
-from nncf.tensorflow.graph.metatypes.tf_ops import TFReluOpMetatype
-from nncf.tensorflow.graph.metatypes.tf_ops import TFMulOpMetatype
 from nncf.tensorflow.graph.metatypes.tf_ops import TFBiasAddOpMetatype
-from nncf.tensorflow.graph.metatypes.tf_ops import TFMatMulOpMetatype
 from nncf.tensorflow.graph.metatypes.tf_ops import TFConv2DOpMetatype
+from nncf.tensorflow.graph.metatypes.tf_ops import TFMatMulOpMetatype
+from nncf.tensorflow.graph.metatypes.tf_ops import TFMulOpMetatype
+from nncf.tensorflow.graph.metatypes.tf_ops import TFRelu6OpMetatype
+from nncf.tensorflow.graph.metatypes.tf_ops import TFReluOpMetatype
 
 
 def create_h_sigmoid_act() -> GraphPattern:
     main_pattern = GraphPattern()
 
     # ReLU version
     pattern = GraphPattern()
 
-    input_pattern_node = pattern.add_node(label='*INPUT_NODE*', type=GraphPattern.NON_PATTERN_NODE_TYPE)
-    add_node = pattern.add_node(label='ADD', type=TFAddOpMetatype.get_all_aliases())
-    relu_node = pattern.add_node(label='RELU', type=TFReluOpMetatype.get_all_aliases())
-    mul_node = pattern.add_node(label='TF_OP_MUL', type=TFMulOpMetatype.get_all_aliases())
+    input_pattern_node = pattern.add_node(label="*INPUT_NODE*", type=GraphPattern.NON_PATTERN_NODE_TYPE)
+    add_node = pattern.add_node(label="ADD", type=TFAddOpMetatype.get_all_aliases())
+    relu_node = pattern.add_node(label="RELU", type=TFReluOpMetatype.get_all_aliases())
+    mul_node = pattern.add_node(label="TF_OP_MUL", type=TFMulOpMetatype.get_all_aliases())
 
     pattern.add_edge(input_pattern_node, add_node)
     pattern.add_edge(add_node, relu_node)
     pattern.add_edge(relu_node, mul_node)
 
     main_pattern.add_pattern_alternative(pattern)
 
     # ReLU6 version
 
     pattern = GraphPattern()
 
-    input_pattern_node = pattern.add_node(label='*INPUT_NODE*', type=GraphPattern.NON_PATTERN_NODE_TYPE)
-    add_node = pattern.add_node(label='ADD', type=TFAddOpMetatype.get_all_aliases())
-    relu6_node = pattern.add_node(label='RELU6', type=TFRelu6OpMetatype.get_all_aliases())
-    mul_node = pattern.add_node(label='TF_OP_MUL', type=TFMulOpMetatype.get_all_aliases())
+    input_pattern_node = pattern.add_node(label="*INPUT_NODE*", type=GraphPattern.NON_PATTERN_NODE_TYPE)
+    add_node = pattern.add_node(label="ADD", type=TFAddOpMetatype.get_all_aliases())
+    relu6_node = pattern.add_node(label="RELU6", type=TFRelu6OpMetatype.get_all_aliases())
+    mul_node = pattern.add_node(label="TF_OP_MUL", type=TFMulOpMetatype.get_all_aliases())
 
     pattern.add_edge(input_pattern_node, add_node)
     pattern.add_edge(add_node, relu6_node)
     pattern.add_edge(relu6_node, mul_node)
 
     main_pattern.add_pattern_alternative(pattern)
 
@@ -64,60 +62,60 @@
     #  after joining must return a list of output nodes so that these can be joined to later.
     #  Currently cannot specify h_swish in terms of h_sigmoid due to this.
 
     main_pattern = GraphPattern()
 
     # ReLU version
     pattern = GraphPattern()
-    input_pattern_node = pattern.add_node(label='*INPUT_NODE*', type=GraphPattern.NON_PATTERN_NODE_TYPE)
+    input_pattern_node = pattern.add_node(label="*INPUT_NODE*", type=GraphPattern.NON_PATTERN_NODE_TYPE)
 
-    add_node = pattern.add_node(label='ADD', type=TFAddOpMetatype.get_all_aliases())
-    relu_node = pattern.add_node(label='RELU', type=TFReluOpMetatype.get_all_aliases())
-    mul_node = pattern.add_node(label='TF_OP_MUL', type=TFMulOpMetatype.get_all_aliases())
+    add_node = pattern.add_node(label="ADD", type=TFAddOpMetatype.get_all_aliases())
+    relu_node = pattern.add_node(label="RELU", type=TFReluOpMetatype.get_all_aliases())
+    mul_node = pattern.add_node(label="TF_OP_MUL", type=TFMulOpMetatype.get_all_aliases())
 
     pattern.add_edge(input_pattern_node, add_node)
     pattern.add_edge(add_node, relu_node)
     pattern.add_edge(relu_node, mul_node)
 
-    mul_2_node = pattern.add_node(label='MULTIPLY', type=['Multiply', 'Mul'])
+    mul_2_node = pattern.add_node(label="MULTIPLY", type=["Multiply", "Mul"])
     pattern.add_edge(input_pattern_node, mul_2_node)
     pattern.add_edge(mul_node, mul_2_node)
     main_pattern.add_pattern_alternative(pattern)
 
     # ReLU6 version
     pattern = GraphPattern()
-    input_pattern_node = pattern.add_node(label='*INPUT_NODE*', type=GraphPattern.NON_PATTERN_NODE_TYPE)
+    input_pattern_node = pattern.add_node(label="*INPUT_NODE*", type=GraphPattern.NON_PATTERN_NODE_TYPE)
 
-    add_node = pattern.add_node(label='ADD', type=TFAddOpMetatype.get_all_aliases())
-    relu6_node = pattern.add_node(label='RELU6', type=TFRelu6OpMetatype.get_all_aliases())
-    mul_node = pattern.add_node(label='TF_OP_MUL', type=TFMulOpMetatype.get_all_aliases())
+    add_node = pattern.add_node(label="ADD", type=TFAddOpMetatype.get_all_aliases())
+    relu6_node = pattern.add_node(label="RELU6", type=TFRelu6OpMetatype.get_all_aliases())
+    mul_node = pattern.add_node(label="TF_OP_MUL", type=TFMulOpMetatype.get_all_aliases())
 
     pattern.add_edge(input_pattern_node, add_node)
     pattern.add_edge(add_node, relu6_node)
     pattern.add_edge(relu6_node, mul_node)
 
-    mul_2_node = pattern.add_node(label='MULTIPLY', type='Multiply')
+    mul_2_node = pattern.add_node(label="MULTIPLY", type="Multiply")
     pattern.add_edge(input_pattern_node, mul_2_node)
     pattern.add_edge(mul_node, mul_2_node)
     main_pattern.add_pattern_alternative(pattern)
 
     return main_pattern
 
 
 def create_matmul_biasadd_pattern() -> GraphPattern:
     pattern = GraphPattern()
 
-    matmul_node = pattern.add_node(label='MATMUL', type=TFMatMulOpMetatype.get_all_aliases())
-    biasadd_node = pattern.add_node(label='BIASADD', type=TFBiasAddOpMetatype.get_all_aliases())
+    matmul_node = pattern.add_node(label="MATMUL", type=TFMatMulOpMetatype.get_all_aliases())
+    biasadd_node = pattern.add_node(label="BIASADD", type=TFBiasAddOpMetatype.get_all_aliases())
     pattern.add_edge(matmul_node, biasadd_node)
 
     return pattern
 
 
 def create_conv2d_biasadd_pattern() -> GraphPattern:
     pattern = GraphPattern()
 
-    conv2d_node = pattern.add_node(label='CONV2D', type=TFConv2DOpMetatype.get_all_aliases())
-    biasadd_node = pattern.add_node(label='BIASADD', type=TFBiasAddOpMetatype.get_all_aliases())
+    conv2d_node = pattern.add_node(label="CONV2D", type=TFConv2DOpMetatype.get_all_aliases())
+    biasadd_node = pattern.add_node(label="BIASADD", type=TFBiasAddOpMetatype.get_all_aliases())
     pattern.add_edge(conv2d_node, biasadd_node)
 
     return pattern
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/transformations/commands.py` & `nncf-2.5.0/nncf/tensorflow/graph/transformations/commands.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,38 +1,31 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Union
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Any, Callable, Dict, List, Optional, Union
 
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.graph.transformations.commands import TransformationType
 from nncf.common.stateful_classes_registry import TF_STATEFUL_CLASSES
 
 
 class TFLayerPointStateNames:
-    LAYER_NAME = 'layer_name'
-    TARGET_TYPE = 'target_type'
+    LAYER_NAME = "layer_name"
+    TARGET_TYPE = "target_type"
 
 
 @TF_STATEFUL_CLASSES.register()
 class TFLayerPoint(TargetPoint):
     """
     `TFLayerPoint` defines an object or spot relative to the layer in the
     TensorFlow model graph. It can be the layer itself, layer weights, specific
@@ -46,36 +39,36 @@
         super().__init__(target_type)
         self._layer_name = layer_name
 
     @property
     def layer_name(self) -> str:
         return self._layer_name
 
-    def __eq__(self, other: 'TFLayerPoint') -> bool:
+    def __eq__(self, other: "TFLayerPoint") -> bool:
         if isinstance(other, TFLayerPoint):
             return self.type == other.type and self.layer_name == other.layer_name
         return False
 
     def __str__(self) -> str:
-        return super().__str__() + ' ' + self.layer_name
+        return super().__str__() + " " + self.layer_name
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
         return {
             self._state_names.TARGET_TYPE: self._target_type.get_state(),
             self._state_names.LAYER_NAME: self.layer_name,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFLayerPoint':
+    def from_state(cls, state: Dict[str, Any]) -> "TFLayerPoint":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         kwargs = {
             cls._state_names.TARGET_TYPE: TargetType.from_state(state[cls._state_names.TARGET_TYPE]),
@@ -94,16 +87,17 @@
     def __init__(self, target_points: List[TargetPoint]):
         self._target_points = target_points
 
     @property
     def target_points(self) -> List[TargetPoint]:
         return self._target_points
 
+
 class TFLayerStateNames:
-    LAYER_NAME = 'layer_name'
+    LAYER_NAME = "layer_name"
 
 
 @TF_STATEFUL_CLASSES.register()
 class TFLayer(TFLayerPoint):
     """
     `TFLayer` defines a layer in the TensorFlow model graph.
 
@@ -124,27 +118,27 @@
         :return: state of the object
         """
         return {
             self._state_names.LAYER_NAME: self.layer_name,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFLayer':
+    def from_state(cls, state: Dict[str, Any]) -> "TFLayer":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
 
 class TFBeforeLayerStateNames:
-    LAYER_NAME = 'layer_name'
-    INSTANCE_IDX = 'instance_idx'
-    INPUT_PORT_ID = 'input_port_id'
+    LAYER_NAME = "layer_name"
+    INSTANCE_IDX = "instance_idx"
+    INPUT_PORT_ID = "input_port_id"
 
 
 @TF_STATEFUL_CLASSES.register()
 class TFBeforeLayer(TFLayerPoint):
     """
     `TFBeforeLayer` defines a spot before the layer in the TensorFlow model graph.
 
@@ -164,64 +158,65 @@
         return self._instance_idx
 
     @property
     def input_port_id(self) -> int:
         return self._input_port_id
 
     def __eq__(self, other: Any) -> bool:
-        return isinstance(other, TFBeforeLayer) \
-               and self.layer_name == other.layer_name \
-               and self.instance_idx == other.instance_idx \
-               and self.input_port_id == other.input_port_id
+        return (
+            isinstance(other, TFBeforeLayer)
+            and self.layer_name == other.layer_name
+            and self.instance_idx == other.instance_idx
+            and self.input_port_id == other.input_port_id
+        )
 
     def __str__(self) -> str:
-        return ' '.join([super().__str__(),
-                         self.instance_idx,
-                         str(self.input_port_id)])
+        return " ".join([super().__str__(), self.instance_idx, str(self.input_port_id)])
 
     def __hash__(self) -> int:
         return hash(str(self))
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
         return {
             self._state_names.LAYER_NAME: self.layer_name,
             self._state_names.INSTANCE_IDX: self.instance_idx,
-            self._state_names.INPUT_PORT_ID: self.input_port_id
+            self._state_names.INPUT_PORT_ID: self.input_port_id,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFBeforeLayer':
+    def from_state(cls, state: Dict[str, Any]) -> "TFBeforeLayer":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
 
 class TFAfterLayerStateNames:
-    LAYER_NAME = 'layer_name'
-    INSTANCE_IDX = 'instance_idx'
-    OUTPUT_PORT_ID = 'output_port_id'
+    LAYER_NAME = "layer_name"
+    INSTANCE_IDX = "instance_idx"
+    OUTPUT_PORT_ID = "output_port_id"
 
 
 @TF_STATEFUL_CLASSES.register()
 class TFAfterLayer(TFLayerPoint):
     """
     `TFAfterLayer` defines a spot after the layer in the TensorFlow model graph.
 
     For example, `TFAfterLayer` is used in the insertion commands to specify
     where the new object should be inserted.
     """
+
     _state_names = TFAfterLayerStateNames
 
     def __init__(self, layer_name: str, instance_idx: int = 0, output_port_id: int = 0):
         super().__init__(TargetType.AFTER_LAYER, layer_name)
         self._instance_idx = instance_idx
         self._output_port_id = output_port_id
 
@@ -230,24 +225,24 @@
         return self._instance_idx
 
     @property
     def output_port_id(self) -> int:
         return self._output_port_id
 
     def __eq__(self, other: Any) -> bool:
-        return isinstance(other, TFAfterLayer) \
-               and self.type == other.type \
-               and self.layer_name == other.layer_name \
-               and self.instance_idx == other.instance_idx \
-               and self._output_port_id == other.output_port_id
+        return (
+            isinstance(other, TFAfterLayer)
+            and self.type == other.type
+            and self.layer_name == other.layer_name
+            and self.instance_idx == other.instance_idx
+            and self._output_port_id == other.output_port_id
+        )
 
     def __str__(self) -> str:
-        return ' '.join([super().__str__(),
-                         self.instance_idx,
-                         str(self.output_port_id)])
+        return " ".join([super().__str__(), self.instance_idx, str(self.output_port_id)])
 
     def __hash__(self) -> int:
         return hash(str(self))
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
@@ -258,26 +253,26 @@
         return {
             self._state_names.LAYER_NAME: self.layer_name,
             self._state_names.INSTANCE_IDX: self.instance_idx,
             self._state_names.OUTPUT_PORT_ID: self.output_port_id,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFAfterLayer':
+    def from_state(cls, state: Dict[str, Any]) -> "TFAfterLayer":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
 
 class TFLayerWeightsStateNames:
-    LAYER_NAME = 'layer_name'
-    WEIGHTS_ATTR_NAME = 'weights_attr_name'
+    LAYER_NAME = "layer_name"
+    WEIGHTS_ATTR_NAME = "weights_attr_name"
 
 
 @TF_STATEFUL_CLASSES.register()
 class TFLayerWeight(TFLayerPoint):
     """
     `TFLayerWeight` defines the layer weights.
 
@@ -292,21 +287,23 @@
         self._weights_attr_name = weights_attr_name
 
     @property
     def weights_attr_name(self) -> str:
         return self._weights_attr_name
 
     def __eq__(self, other: Any) -> bool:
-        return isinstance(other, TFLayerWeight) and \
-               self.type == other.type and \
-               self.layer_name == other.layer_name and \
-               self.weights_attr_name == other.weights_attr_name
+        return (
+            isinstance(other, TFLayerWeight)
+            and self.type == other.type
+            and self.layer_name == other.layer_name
+            and self.weights_attr_name == other.weights_attr_name
+        )
 
     def __str__(self) -> str:
-        return super().__str__() + ' ' + self.weights_attr_name
+        return super().__str__() + " " + self.weights_attr_name
 
     def __hash__(self) -> int:
         return hash(str(self))
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
@@ -316,27 +313,27 @@
         """
         return {
             self._state_names.LAYER_NAME: self.layer_name,
             self._state_names.WEIGHTS_ATTR_NAME: self.weights_attr_name,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFLayerWeight':
+    def from_state(cls, state: Dict[str, Any]) -> "TFLayerWeight":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
 
 class TFOperationWithWeightsStateNames:
-    LAYER_NAME = 'layer_name'
-    WEIGHTS_ATTR_NAME = 'weights_attr_name'
-    OPERATION_NAME = 'operation_name'
+    LAYER_NAME = "layer_name"
+    WEIGHTS_ATTR_NAME = "weights_attr_name"
+    OPERATION_NAME = "operation_name"
 
 
 @TF_STATEFUL_CLASSES.register()
 class TFOperationWithWeights(TFLayerWeight):
     """
     `TFOperationWithWeights` defines an operation with weights.
 
@@ -351,77 +348,81 @@
         self._operation_name = operation_name
 
     @property
     def operation_name(self) -> str:
         return self._operation_name
 
     def __eq__(self, other: Any) -> bool:
-        return isinstance(other, TFOperationWithWeights) and \
-               self.type == other.type and \
-               self.layer_name == other.layer_name and \
-               self.weights_attr_name == other.weights_attr_name and \
-               self.operation_name == other.operation_name
+        return (
+            isinstance(other, TFOperationWithWeights)
+            and self.type == other.type
+            and self.layer_name == other.layer_name
+            and self.weights_attr_name == other.weights_attr_name
+            and self.operation_name == other.operation_name
+        )
 
     def __str__(self) -> str:
-        return super().__str__() + ' ' + self.operation_name
+        return super().__str__() + " " + self.operation_name
 
     def __hash__(self) -> int:
         return hash(str(self))
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
         return {
             self._state_names.LAYER_NAME: self._layer_name,
             self._state_names.WEIGHTS_ATTR_NAME: self._weights_attr_name,
-            self._state_names.OPERATION_NAME: self._operation_name
+            self._state_names.OPERATION_NAME: self._operation_name,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFOperationWithWeights':
+    def from_state(cls, state: Dict[str, Any]) -> "TFOperationWithWeights":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
 
 class TFInsertionCommand(TransformationCommand):
     """
     Inserts objects at the target point in the TensorFlow model graph.
     """
 
-    def __init__(self,
-                 target_point: Union[TargetPoint, TFMultiLayerPoint],
-                 callable_object: Optional[Callable] = None,
-                 priority: Optional[TransformationPriority] = None):
+    def __init__(
+        self,
+        target_point: Union[TargetPoint, TFMultiLayerPoint],
+        callable_object: Optional[Callable] = None,
+        priority: Optional[TransformationPriority] = None,
+    ):
         super().__init__(TransformationType.INSERT, target_point)
         self.callable_objects = []
         if callable_object is not None:
-            _priority = TransformationPriority.DEFAULT_PRIORITY \
-                if priority is None else priority
+            _priority = TransformationPriority.DEFAULT_PRIORITY if priority is None else priority
             self.callable_objects.append((callable_object, _priority))
 
     @property
     def insertion_objects(self) -> List[Callable]:
         return [x for x, _ in self.callable_objects]
 
-    def union(self, other: TransformationCommand) -> 'TFInsertionCommand':
+    def union(self, other: TransformationCommand) -> "TFInsertionCommand":
         if isinstance(self.target_point, TFMultiLayerPoint):
-            raise NotImplementedError('A command of TFInsertionCommand type with TFMultiLayerPoint '
-                                      'could not be united with another command')
+            raise NotImplementedError(
+                "A command of TFInsertionCommand type with TFMultiLayerPoint "
+                "could not be united with another command"
+            )
 
         if not self.check_command_compatibility(other):
-            raise ValueError('{} and {} commands could not be united'.format(
-                type(self).__name__, type(other).__name__))
+            raise ValueError("{} and {} commands could not be united".format(type(self).__name__, type(other).__name__))
 
         com = TFInsertionCommand(self.target_point)
         com.callable_objects = self.callable_objects + other.callable_objects
         com.callable_objects = sorted(com.callable_objects, key=lambda x: x[1])
         return com
 
 
@@ -429,81 +430,80 @@
     """
     Removes the target object.
     """
 
     def __init__(self, target_point: TargetPoint):
         super().__init__(TransformationType.REMOVE, target_point)
 
-    def union(self, other: TransformationCommand) -> 'TFRemovalCommand':
-        raise NotImplementedError('A command of TFRemovalCommand type '
-                                  'could not be united with another command')
+    def union(self, other: TransformationCommand) -> "TFRemovalCommand":
+        raise NotImplementedError("A command of TFRemovalCommand type could not be united with another command")
 
 
 class TFMultipleInsertionCommands(TransformationCommand):
     """
     A list of insertion commands combined by a common global target point but
     with different target points in between.
 
     For example, If a layer has multiple weight variables you can use this
     transformation command to insert operations with weights for each layer
     weights variable at one multiple insertion command.
     """
 
-    def __init__(self,
-                 target_point: TargetPoint,
-                 check_target_points_fn: Optional[Callable] = None,
-                 commands: Optional[List[TransformationCommand]] = None):
+    def __init__(
+        self,
+        target_point: TargetPoint,
+        check_target_points_fn: Optional[Callable] = None,
+        commands: Optional[List[TransformationCommand]] = None,
+    ):
         super().__init__(TransformationType.MULTI_INSERT, target_point)
         self.check_target_points_fn = check_target_points_fn
         if check_target_points_fn is None:
             self.check_target_points_fn = lambda tp0, tp1: tp0 == tp1
         self._commands = []
         if commands is not None:
             for cmd in commands:
                 self.add_insertion_command(cmd)
 
     @property
     def commands(self) -> List[TransformationCommand]:
         return self._commands
 
     def check_insertion_command(self, command: TransformationCommand) -> bool:
-        if isinstance(command, TransformationCommand) and \
-            command.type == TransformationType.INSERT and \
-            self.check_target_points_fn(self.target_point, command.target_point):
+        if (
+            isinstance(command, TransformationCommand)
+            and command.type == TransformationType.INSERT
+            and self.check_target_points_fn(self.target_point, command.target_point)
+        ):
             return True
         return False
 
     def add_insertion_command(self, command: TransformationCommand) -> None:
         if not self.check_insertion_command(command):
-            raise ValueError('{} command could not be added'.format(
-                type(command).__name__))
+            raise ValueError("{} command could not be added".format(type(command).__name__))
 
         for idx, cmd in enumerate(self.commands):
             if cmd.target_point == command.target_point:
                 self.commands[idx] = cmd + command
                 break
         else:
             self.commands.append(command)
 
-    def union(self, other: TransformationCommand) -> 'TFMultipleInsertionCommands':
+    def union(self, other: TransformationCommand) -> "TFMultipleInsertionCommands":
         if not self.check_command_compatibility(other):
-            raise ValueError('{} and {} commands could not be united'.format(
-                type(self).__name__, type(other).__name__))
+            raise ValueError("{} and {} commands could not be united".format(type(self).__name__, type(other).__name__))
 
         def make_check_target_points_fn(fn1, fn2):
             def check_target_points(tp0, tp1):
                 return fn1(tp0, tp1) or fn2(tp0, tp1)
 
             return check_target_points
 
-        check_target_points_fn = self.check_target_points_fn \
-            if self.check_target_points_fn == other.check_target_points_fn else \
-            make_check_target_points_fn(self.check_target_points_fn, other.check_target_points_fn)
-
-        multi_cmd = TFMultipleInsertionCommands(
-            self.target_point,
-            check_target_points_fn,
-            self.commands
+        check_target_points_fn = (
+            self.check_target_points_fn
+            if self.check_target_points_fn == other.check_target_points_fn
+            else make_check_target_points_fn(self.check_target_points_fn, other.check_target_points_fn)
         )
+
+        multi_cmd = TFMultipleInsertionCommands(self.target_point, check_target_points_fn, self.commands)
         for cmd in other.commands:
             multi_cmd.add_insertion_command(cmd)
         return multi_cmd
```

### Comparing `nncf-2.4.0/nncf/tensorflow/graph/utils.py` & `nncf-2.5.0/nncf/tensorflow/graph/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,95 +1,98 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import List, Tuple
-import sys
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import inspect
+import sys
+from typing import List, Tuple
 
 import tensorflow as tf
 
-from nncf.tensorflow.graph.metatypes.keras_layers import TFNNCFWrapperLayerMetatype
-from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
-from nncf.tensorflow.layers.wrapper import NNCFWrapper
-from nncf.tensorflow.layers.operation import NNCFOperation
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
+from nncf.tensorflow.graph.metatypes.keras_layers import TFNNCFWrapperLayerMetatype
+from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
+from nncf.tensorflow.layers.operation import NNCFOperation
+from nncf.tensorflow.layers.wrapper import NNCFWrapper
 
-SHARED_OPERATION_MARK = '^'
+SHARED_OPERATION_MARK = "^"
 
 
 def is_sequential_or_functional_model(model):
     return is_sequential_model(model) or is_functional_model(model)
 
 
 def is_sequential_model(model):
     return isinstance(model, tf.keras.Sequential)
 
 
 def is_functional_model(model):
-    return isinstance(model, tf.keras.Model) \
-           and not isinstance(model, tf.keras.Sequential) \
-           and getattr(model, '_is_graph_network', False)
+    return (
+        isinstance(model, tf.keras.Model)
+        and not isinstance(model, tf.keras.Sequential)
+        and getattr(model, "_is_graph_network", False)
+    )
 
 
 def is_keras_layer_model(model: tf.keras.Model) -> bool:
     """
     Checks if there is `tensorflow_hub.KerasLayer` layer in the model or not.
 
     :param model: Keras model to check.
     :return: `True` if there is `hub.KerasLayer` in the model and
         `False` otherwise.
     """
     for layer in model.submodules:
-        if layer.__class__.__name__ == 'KerasLayer':
+        if layer.__class__.__name__ == "KerasLayer":
             return True
     return False
 
 
 def get_keras_layers_class_names():
-    keras_layers = [class_name for class_name, _ in
-                    inspect.getmembers(sys.modules[tf.keras.layers.__name__], inspect.isclass)]
-    keras_layers += ['TensorFlowOpLayer', 'TFOpLambda', 'SlicingOpLambda']
+    keras_layers = [
+        class_name for class_name, _ in inspect.getmembers(sys.modules[tf.keras.layers.__name__], inspect.isclass)
+    ]
+    keras_layers += ["TensorFlowOpLayer", "TFOpLambda", "SlicingOpLambda"]
     return keras_layers
 
 
 def get_keras_activation_names():
-    keras_activations = [activation_name for activation_name, _ in
-                    inspect.getmembers(sys.modules[tf.keras.activations.__name__], inspect.isfunction)]
+    keras_activations = [
+        activation_name
+        for activation_name, _ in inspect.getmembers(sys.modules[tf.keras.activations.__name__], inspect.isfunction)
+    ]
     return keras_activations
 
 
 def get_custom_objects(model):
     keras_layers = get_keras_layers_class_names()
     keras_activations = get_keras_activation_names()
     custom_objects = {}
     for layer in model.submodules:
-        if layer.__class__.__name__ == 'NNCFWrapper':
+        if layer.__class__.__name__ == "NNCFWrapper":
             layer = layer.layer
         if layer.__class__.__name__ not in keras_layers:
             custom_objects[layer.__class__.__name__] = layer.__class__
-        if layer.__class__.__name__ == 'Activation':
+        if layer.__class__.__name__ == "Activation":
             if layer.activation.__name__ not in keras_activations:
                 custom_objects[layer.activation.__name__] = layer.activation
     return custom_objects
 
 
 def get_weight_name(name, layer_name=None):
     if layer_name and layer_name in name:
-        return name.split(layer_name + '/')[-1]
+        return name.split(layer_name + "/")[-1]
     return name
 
 
 def get_weight_by_name(layer, weight_name):
     for w in layer.weights:
         if w.name.split(":")[0] == weight_name:
             return w
@@ -103,37 +106,35 @@
             wrapped_layers += collect_wrapped_layers(layer)
         if isinstance(layer, NNCFWrapper):
             wrapped_layers.append(layer)
     return wrapped_layers
 
 
 def get_shared_node_name(layer_name: str, instance_idx: int):
-    return '{}{}{}'.format(layer_name, SHARED_OPERATION_MARK, instance_idx)
+    return "{}{}{}".format(layer_name, SHARED_OPERATION_MARK, instance_idx)
 
 
 def get_original_name_and_instance_idx(node_name: NNCFNodeName):
     result = node_name.split(SHARED_OPERATION_MARK)
     original_name = result[0]
     instance_idx = 0 if len(result) == 1 else int(result[1])
     return original_name, instance_idx
 
 
 def get_original_name(node_name):
     return get_original_name_and_instance_idx(node_name)[0]
 
 
 def get_layer_to_graph_nodes_map(model, node_names):
-    layer_to_nodes_map = {layer.name: {'type': layer.__class__.__name__,
-                                       'nodes': []}
-                          for layer in model.layers}
+    layer_to_nodes_map = {layer.name: {"type": layer.__class__.__name__, "nodes": []} for layer in model.layers}
     for node in node_names:
-        parent_layer_name = node.split('/')[1]  # model_name/layer_name/layer_op_name/...
+        parent_layer_name = node.split("/")[1]  # model_name/layer_name/layer_op_name/...
         if parent_layer_name not in layer_to_nodes_map:
-            raise RuntimeError('Could not find {} layer in Model'.format(parent_layer_name))
-        layer_to_nodes_map[parent_layer_name]['nodes'].append(node)
+            raise RuntimeError("Could not find {} layer in Model".format(parent_layer_name))
+        layer_to_nodes_map[parent_layer_name]["nodes"].append(node)
     return layer_to_nodes_map
 
 
 def get_weight_node_name(graph: NNCFGraph, node_name: NNCFNodeName) -> NNCFNodeName:
     node = graph.get_node_by_name(node_name)
     while list(graph.get_previous_nodes(node)):
         node = list(graph.get_previous_nodes(node))[-1]
@@ -168,24 +169,25 @@
         for weight_attr, ops in wrapped_layer.weights_attr_ops.items():
             for op in ops.values():
                 if op.name in operation_names:
                     yield wrapped_layer, weight_attr, op
 
 
 def _was_specially_wrapped_with_keras_export(layer, attr_name) -> bool:
-    return hasattr(layer, attr_name) and \
-           getattr(layer, attr_name) != ('keras.layers.Layer', )
+    return hasattr(layer, attr_name) and getattr(layer, attr_name) != ("keras.layers.Layer",)
 
 
 def is_builtin_layer(layer) -> bool:
     # A similar logic is actually what gets used in TF as
     # tensorflow.python.keras.utils.layer_utils.is_builtin_layer.
-    return layer.__class__.__name__ in ['SlicingOpLambda', 'TFOpLambda', 'TensorFlowOpLayer'] or \
-           _was_specially_wrapped_with_keras_export(layer, '_keras_api_names') or \
-           _was_specially_wrapped_with_keras_export(layer, '_keras_api_names_v1')
+    return (
+        layer.__class__.__name__ in ["SlicingOpLambda", "TFOpLambda", "TensorFlowOpLayer"]
+        or _was_specially_wrapped_with_keras_export(layer, "_keras_api_names")
+        or _was_specially_wrapped_with_keras_export(layer, "_keras_api_names_v1")
+    )
 
 
 def get_list_level(lst: List) -> int:
     return isinstance(lst, list) and list(map(get_list_level, lst or [0]))[0] + 1
 
 
 def check_oplambda_input_data(x: List) -> bool:
@@ -215,15 +217,15 @@
     if get_list_level(inbound_nodes) == 2:  # [[ ]] -> [[[ ]]]
         inbound_nodes = [inbound_nodes]
     if get_list_level(inbound_nodes) == 4:  # [[[[ ]]]] -> [[[ ]]]
         inbound_nodes = inbound_nodes[0]
 
     inbound_nodes_oplambda = []
     for inbound_node in inbound_nodes[0]:
-        if inbound_node[0] != '_CONSTANT_VALUE':
+        if inbound_node[0] != "_CONSTANT_VALUE":
             # If an element in the first call argument did not originate as a keras tensor
             # and is a constant value, it is saved using '_CONSTANT_VALUE'.
             inbound_nodes_oplambda.append(inbound_node)
 
         # Check for nested inbound nodes in kwargs
         kwargs = inbound_node[3]
         for item in kwargs.values():
```

### Comparing `nncf-2.4.0/nncf/tensorflow/hardware/config.py` & `nncf-2.5.0/nncf/torch/hardware/config.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,24 +1,21 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import List
-from typing import Type
+from typing import List, Type
 
 from nncf.common.graph import OperatorMetatype
 from nncf.common.hardware.config import HWConfig
-from nncf.tensorflow.graph.metatypes.common import get_operator_metatypes
+from nncf.torch.graph.operator_metatypes import get_operator_metatypes
 
 
-class TFHWConfig(HWConfig):
+class PTHWConfig(HWConfig):
     def _get_available_operator_metatypes_for_matching(self) -> List[Type[OperatorMetatype]]:
         return get_operator_metatypes()
```

### Comparing `nncf-2.4.0/nncf/tensorflow/hardware/fused_patterns.py` & `nncf-2.5.0/nncf/tensorflow/hardware/fused_patterns.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,73 +1,72 @@
-"""
- Copyright (c) 2021-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 
 from nncf.common.graph.patterns import GraphPattern
-from nncf.common.graph.patterns import HWFusedPatterns
+from nncf.common.graph.patterns import Patterns
 from nncf.tensorflow.graph.metatypes.tf_ops import TFIdentityOpMetatype
 from nncf.tensorflow.graph.pattern_operations import ATOMIC_ACTIVATIONS_OPERATIONS
 from nncf.tensorflow.graph.pattern_operations import BATCH_NORMALIZATION_OPERATIONS
 from nncf.tensorflow.graph.pattern_operations import ELEMENTWISE_OPERATIONS
 from nncf.tensorflow.graph.pattern_operations import LINEAR_OPERATIONS
 from nncf.tensorflow.graph.pattern_operations import QUANTIZATION_AGNOSTIC_OPERATIONS
+from nncf.tensorflow.graph.patterns import create_conv2d_biasadd_pattern
 from nncf.tensorflow.graph.patterns import create_h_sigmoid_act
 from nncf.tensorflow.graph.patterns import create_h_swish_act
 from nncf.tensorflow.graph.patterns import create_matmul_biasadd_pattern
-from nncf.tensorflow.graph.patterns import create_conv2d_biasadd_pattern
 
 
-def _get_tf_hw_fused_patterns() -> HWFusedPatterns:
-    retval = HWFusedPatterns()
+def _get_tf_hw_fused_patterns() -> Patterns:
+    retval = Patterns()
     linear_ops = GraphPattern()
     linear_ops.add_node(**LINEAR_OPERATIONS)
 
     eltwise_ops = GraphPattern()
     eltwise_ops.add_node(**ELEMENTWISE_OPERATIONS)
 
     batch_norm = GraphPattern()
     batch_norm.add_node(**BATCH_NORMALIZATION_OPERATIONS)
 
     h_sigmoid = create_h_sigmoid_act()
     h_swish = create_h_swish_act()
-    retval.register(h_sigmoid, 'H_SIGMOID', match=True)
-    retval.register(h_swish, 'H_SWISH', match=True)
+    retval.register(h_sigmoid, "H_SIGMOID", match=True)
+    retval.register(h_swish, "H_SWISH", match=True)
 
     matmul_biasadd = create_matmul_biasadd_pattern()
-    retval.register(matmul_biasadd, 'MATMUL_BIASADD', match=True)
+    retval.register(matmul_biasadd, "MATMUL_BIASADD", match=True)
 
     conv2d_biasadd = create_conv2d_biasadd_pattern()
-    retval.register(conv2d_biasadd, 'CONV2D_BIASADD', match=True)
+    retval.register(conv2d_biasadd, "CONV2D_BIASADD", match=True)
 
     atomic_activations = GraphPattern()
     atomic_activations.add_node(**ATOMIC_ACTIVATIONS_OPERATIONS)
     activations = atomic_activations | h_swish | h_sigmoid
     batch_norm_activations_permutation = batch_norm + activations | activations + batch_norm
     any_bn_act_combo = batch_norm | activations | batch_norm_activations_permutation
 
     identity = GraphPattern()
-    identity.add_node(type=TFIdentityOpMetatype.get_all_aliases(), label='IDENTITY')
+    identity.add_node(type=TFIdentityOpMetatype.get_all_aliases(), label="IDENTITY")
     linear_ops_maybe_followed_by_identity = linear_ops | (linear_ops + identity) | matmul_biasadd | conv2d_biasadd
 
     agnostic_ops = GraphPattern()
     agnostic_ops.add_node(**QUANTIZATION_AGNOSTIC_OPERATIONS)
     any_ag_bn_act_combo = agnostic_ops + activations | any_bn_act_combo
 
-    retval.register(linear_ops_maybe_followed_by_identity, name='LINEAR', match=True)
-    retval.register(batch_norm_activations_permutation, name='BN_ACT_OR_ACT_BN', match=True)
-    retval.register(linear_ops_maybe_followed_by_identity + any_ag_bn_act_combo, 'LINEAR + ANY_AG_BN_ACT_COMBO',
-                    match=True)
-    retval.register(eltwise_ops + any_ag_bn_act_combo, 'ELTWISE + ANY_AG_BN_ACT_COMBO',
-                    match=True)
+    retval.register(linear_ops_maybe_followed_by_identity, name="LINEAR", match=True)
+    retval.register(batch_norm_activations_permutation, name="BN_ACT_OR_ACT_BN", match=True)
+    retval.register(
+        linear_ops_maybe_followed_by_identity + any_ag_bn_act_combo, "LINEAR + ANY_AG_BN_ACT_COMBO", match=True
+    )
+    retval.register(eltwise_ops + any_ag_bn_act_combo, "ELTWISE + ANY_AG_BN_ACT_COMBO", match=True)
     return retval
 
 
 TF_HW_FUSED_PATTERNS = _get_tf_hw_fused_patterns()
```

### Comparing `nncf-2.4.0/nncf/tensorflow/helpers/__init__.py` & `nncf-2.5.0/nncf/torch/sparsity/const/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,14 +1,13 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Backend-specific implementation of the auxiliary "constant" sparsity algorithm.
 """
-
-from nncf.tensorflow.helpers.model_creation import create_compressed_model
```

### Comparing `nncf-2.4.0/nncf/tensorflow/helpers/model_creation.py` & `nncf-2.5.0/nncf/tensorflow/helpers/model_creation.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,46 +1,42 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import types
 from typing import Any, Dict, Optional, Tuple
 
 import tensorflow as tf
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.compression import BaseCompressionAlgorithmController as BaseController
-from nncf.config.structures import ModelEvaluationArgs
+from nncf.common.utils.api_marker import api
+from nncf.config.extractors import extract_algorithm_names
 from nncf.config.telemetry_extractors import CompressionStartedFromConfig
-from nncf.config.utils import is_accuracy_aware_training
 from nncf.config.utils import is_experimental_quantization
 from nncf.telemetry import tracked_function
 from nncf.telemetry.events import NNCF_TF_CATEGORY
 from nncf.tensorflow.accuracy_aware_training.keras_model_utils import accuracy_aware_fit
-from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
-from nncf.config.extractors import extract_algorithm_names
 from nncf.tensorflow.algorithm_selector import NoCompressionAlgorithmBuilder
 from nncf.tensorflow.algorithm_selector import get_compression_algorithm_builder
 from nncf.tensorflow.api.composite_compression import TFCompositeCompressionAlgorithmBuilder
-from nncf.tensorflow.helpers.utils import get_built_model
+from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
 from nncf.tensorflow.graph.utils import is_keras_layer_model
+from nncf.tensorflow.helpers.utils import get_built_model
 
 
-def create_compression_algorithm_builder(config: NNCFConfig,
-                                         should_init: bool) -> TFCompressionAlgorithmBuilder:
+def create_compression_algorithm_builder(config: NNCFConfig, should_init: bool) -> TFCompressionAlgorithmBuilder:
     """
     Factory to create an instance of the compression algorithm builder
     by NNCFConfig.
 
     :param config: An instance of NNCFConfig that defines compression methods.
     :param should_init: The flag indicates that the generated compression builder
         will initialize (True) or not (False) the training parameters of the model
@@ -54,78 +50,78 @@
     if number_compression_algorithms == 1:
         algo_name = next(iter(algo_names))
         return get_compression_algorithm_builder(algo_name)(config, should_init)
 
     return TFCompositeCompressionAlgorithmBuilder(config, should_init)
 
 
-@tracked_function(NNCF_TF_CATEGORY, [CompressionStartedFromConfig(argname="config"), ])
-def create_compressed_model(model: tf.keras.Model,
-                            config: NNCFConfig,
-                            compression_state: Optional[Dict[str, Any]] = None) \
-        -> Tuple[CompressionAlgorithmController, tf.keras.Model]:
+@api(canonical_alias="nncf.tensorflow.create_compressed_model")
+@tracked_function(
+    NNCF_TF_CATEGORY,
+    [
+        CompressionStartedFromConfig(argname="config"),
+    ],
+)
+def create_compressed_model(
+    model: tf.keras.Model, config: NNCFConfig, compression_state: Optional[Dict[str, Any]] = None
+) -> Tuple[CompressionAlgorithmController, tf.keras.Model]:
     """
     The main function used to produce a model ready for compression fine-tuning
     from an original TensorFlow Keras model and a configuration object.
 
     :param model: The original model. Should have its parameters already loaded
         from a checkpoint or another source.
     :param config: A configuration object used to determine the exact compression
         modifications to be applied to the model.
+    :type config: nncf.NNCFConfig
     :param compression_state: compression state to unambiguously restore the compressed model.
         Includes builder and controller states. If it is specified, trainable parameter initialization will be skipped
         during building.
-    :return: A tuple (compression_ctrl, compressed_model) where
-        - compression_ctrl: The controller of the compression algorithm.
-        - compressed_model: The model with additional modifications
-            necessary to enable algorithm-specific compression during fine-tuning.
+    :return: A tuple of the compression controller for the requested algorithm(s) and the model object with additional
+     modifications necessary to enable algorithm-specific compression during fine-tuning.
     """
     if is_experimental_quantization(config):
         if is_keras_layer_model(model):
-            raise ValueError('Experimental quantization algorithm has not supported models with '
-                             '`tensorflow_hub.KerasLayer` layer yet.')
+            raise ValueError(
+                "Experimental quantization algorithm has not supported models with "
+                "`tensorflow_hub.KerasLayer` layer yet."
+            )
+
+        from nncf.experimental.tensorflow.nncf_network import NNCFNetwork  # pylint: disable=cyclic-import
 
-        from nncf.experimental.tensorflow.nncf_network import NNCFNetwork #pylint: disable=cyclic-import
         input_signature = get_input_signature(config)
         model = NNCFNetwork(model, input_signature)
         model.compute_output_signature(model.input_signature)
 
     model = get_built_model(model, config)
-    original_model_accuracy = None
-
-    if is_accuracy_aware_training(config):
-        if config.has_extra_struct(ModelEvaluationArgs):
-            evaluation_args = config.get_extra_struct(ModelEvaluationArgs)
-            original_model_accuracy = evaluation_args.eval_fn(model)
 
     builder = create_compression_algorithm_builder(config, should_init=not compression_state)
 
     if compression_state:
         builder.load_state(compression_state[BaseController.BUILDER_STATE])
     compressed_model = builder.apply_to(model)
     compression_ctrl = builder.build_controller(compressed_model)
-    compressed_model.original_model_accuracy = original_model_accuracy
     if isinstance(compressed_model, tf.keras.Model):
         compressed_model.accuracy_aware_fit = types.MethodType(accuracy_aware_fit, compressed_model)
     return compression_ctrl, compressed_model
 
 
 def get_input_signature(config: NNCFConfig):
-    input_info = config.get('input_info', {})
+    input_info = config.get("input_info", {})
     samples_sizes = []
 
     if isinstance(input_info, dict):
-        sample_size = input_info['sample_size']
+        sample_size = input_info["sample_size"]
         samples_sizes.append(sample_size)
     elif isinstance(input_info, list):
         for info in input_info:
-            sample_size = info['sample_size']
+            sample_size = info["sample_size"]
             samples_sizes.append(sample_size)
     else:
-        raise RuntimeError('sample_size must be provided in configuration file')
+        raise RuntimeError("sample_size must be provided in configuration file")
 
     input_signature = []
     for sample_size in samples_sizes:
         shape = [None] + list(sample_size[1:])
         input_signature.append(tf.TensorSpec(shape=shape, dtype=tf.float32))
 
     return input_signature if len(input_signature) > 1 else input_signature[0]
```

### Comparing `nncf-2.4.0/nncf/tensorflow/helpers/model_manager.py` & `nncf-2.5.0/nncf/tensorflow/helpers/model_manager.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import gc
 import weakref
 
 import tensorflow as tf
 
 from nncf.config.utils import is_experimental_quantization
```

### Comparing `nncf-2.4.0/nncf/tensorflow/helpers/utils.py` & `nncf-2.5.0/nncf/tensorflow/helpers/utils.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 
 def get_built_model(model, config):
     if not model.built:
-        input_info = config.get('input_info', {})
+        input_info = config.get("input_info", {})
         if isinstance(input_info, dict):
-            sample_size = input_info.get('sample_size', None)
+            sample_size = input_info.get("sample_size", None)
         else:
-            sample_size = input_info[0].get('sample_size', None) if input_info else None
+            sample_size = input_info[0].get("sample_size", None) if input_info else None
         if not sample_size:
-            raise RuntimeError('sample_size must be provided in configuration file')
+            raise RuntimeError("sample_size must be provided in configuration file")
         model.build([None] + list(sample_size[1:]))
 
     return model
```

### Comparing `nncf-2.4.0/nncf/tensorflow/initialization.py` & `nncf-2.5.0/nncf/tensorflow/initialization.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,23 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 from nncf.common.initialization.dataloader import NNCFDataLoader
+from nncf.common.utils.api_marker import api
 from nncf.config import NNCFConfig
 from nncf.config.structures import BNAdaptationInitArgs
 from nncf.config.structures import QuantizationRangeInitArgs
 
 
 class TFInitializingDataLoader(NNCFDataLoader):
     """
@@ -34,29 +33,31 @@
     def batch_size(self) -> int:
         return self._batch_size
 
     def __iter__(self):
         return iter(self._data_loader)
 
 
-def register_default_init_args(nncf_config: NNCFConfig,
-                               data_loader: tf.data.Dataset,
-                               batch_size: int,
-                               device: str = None) -> NNCFConfig:
+@api(canonical_alias="nncf.tensorflow.register_default_init_args")
+def register_default_init_args(
+    nncf_config: NNCFConfig, data_loader: tf.data.Dataset, batch_size: int, device: str = None
+) -> NNCFConfig:
     """
     Register extra structures in the NNCFConfig. Initialization of some
     compression algorithms requires certain extra structures.
 
     :param nncf_config: An instance of the NNCFConfig class without extra structures.
+    :type nncf_config: nncf.NNCFConfig
     :param data_loader: Dataset used for initialization.
     :param batch_size: Batch size used for initialization.
     :param device: Device to perform initialization. If `device` is `None` then the device
         of the model parameters will be used.
     :return: An instance of the NNCFConfig class with extra structures.
+    :rtype: nncf.NNCFConfig
     """
-    nncf_config.register_extra_structs([
-        QuantizationRangeInitArgs(data_loader=TFInitializingDataLoader(data_loader, batch_size),
-                                  device=device),
-        BNAdaptationInitArgs(data_loader=TFInitializingDataLoader(data_loader, batch_size),
-                             device=device)
-    ])
+    nncf_config.register_extra_structs(
+        [
+            QuantizationRangeInitArgs(data_loader=TFInitializingDataLoader(data_loader, batch_size), device=device),
+            BNAdaptationInitArgs(data_loader=TFInitializingDataLoader(data_loader, batch_size), device=device),
+        ]
+    )
     return nncf_config
```

### Comparing `nncf-2.4.0/nncf/tensorflow/layers/custom_objects.py` & `nncf-2.5.0/nncf/torch/quantization/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,16 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Backend-specific implementations of quantization algorithms.
 """
 
-from nncf.common.utils.registry import Registry
-
-NNCF_CUSTOM_OBJECTS = Registry('NNCF Custom Objects')
-NNCF_QUANTIZATION_OPERATIONS = Registry('NNCF Quantization Operations')
-
-
-def get_nncf_custom_objects():
-    return NNCF_CUSTOM_OBJECTS.registry_dict
+# Required for correct QUANTIZATION_MODULES registry functioning
+from . import layers
```

### Comparing `nncf-2.4.0/nncf/tensorflow/layers/data_layout.py` & `nncf-2.5.0/nncf/tensorflow/layers/data_layout.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 from nncf.tensorflow.graph.metatypes.common import ALL_LAYER_METATYPES_WITH_WEIGHTS
 from nncf.tensorflow.graph.metatypes.common import GENERAL_CONV_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.common import NORMALIZATION_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
@@ -33,36 +31,37 @@
 def get_channel_axis(input_type, input_name, layer):
     if input_type == InputType.INPUTS:
         return get_input_channel_axis(layer)
     return get_weight_channel_axis(layer, input_name)
 
 
 def get_data_format(layer):
-    return getattr(layer, 'data_format', 'channels_last')
+    return getattr(layer, "data_format", "channels_last")
 
 
 def get_input_channel_axis(layer):
     original_layer = unwrap_layer(layer)
     layer_metatype = get_keras_layer_metatype(original_layer, determine_subtype=False)
     data_format = get_data_format(original_layer)
     if layer_metatype in GENERAL_CONV_LAYER_METATYPES:
-        return -1 if data_format == 'channels_last' else -1 - original_layer.rank
+        return -1 if data_format == "channels_last" else -1 - original_layer.rank
     if layer_metatype in NORMALIZATION_LAYER_METATYPES:
         return original_layer.axis
 
-    return -1 if data_format == 'channels_last' else 1
+    return -1 if data_format == "channels_last" else 1
 
 
 def get_weight_channel_axis(layer, weight_attr):
     original_layer = unwrap_layer(layer)
     layer_metatype = get_keras_layer_metatype(original_layer, determine_subtype=False)
     if layer_metatype in ALL_LAYER_METATYPES_WITH_WEIGHTS:
         for weight_def in layer_metatype.weight_definitions:
             if weight_def.weight_attr_name == weight_attr:
                 return weight_def.channel_axes
 
     return -1
 
+
 def get_weight_shape(layer: tf.keras.layers.Layer, weight_attr: str) -> tf.TensorShape:
     original_layer = unwrap_layer(layer)
     weight = getattr(original_layer, weight_attr)
     return weight.shape
```

### Comparing `nncf-2.4.0/nncf/tensorflow/layers/operation.py` & `nncf-2.5.0/nncf/tensorflow/layers/operation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,35 +1,34 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from collections import OrderedDict
 
 from nncf.tensorflow.utils.hook_handle import HookHandle
 
 
 class InputType:
-    INPUTS = 'inputs'
-    WEIGHTS = 'weights'
+    INPUTS = "inputs"
+    WEIGHTS = "weights"
 
 
 class NNCFOperation:
     """
     The abstract class represents main building block for adding compression
     extensions to a model.
     """
+
     def __init__(self, name, trainable=True):
         """
         Initializes internal NNCF operation state
 
         :param name: unique operation name in algorithm scope.
         """
         self._call_pre_hooks = OrderedDict()
@@ -93,12 +92,12 @@
         for hook in self._call_pre_hooks.values():
             result = hook(inputs)
             if result is not None:
                 inputs = result
         return self.call(inputs, *args[1:], **kwargs)
 
     def get_config(self):
-        return {'name': self._name}
+        return {"name": self._name}
 
     @classmethod
     def from_config(cls, config):
         return cls(**config)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/layers/wrapper.py` & `nncf-2.5.0/nncf/tensorflow/layers/wrapper.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,60 +1,58 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from collections import OrderedDict
 from inspect import getfullargspec
 from typing import Dict
 
 import tensorflow as tf
 
-from nncf.tensorflow.layers.custom_objects import get_nncf_custom_objects
 from nncf.tensorflow.layers.custom_objects import NNCF_CUSTOM_OBJECTS
+from nncf.tensorflow.layers.custom_objects import get_nncf_custom_objects
 from nncf.tensorflow.layers.operation import InputType
 from nncf.tensorflow.layers.operation import NNCFOperation
 
 
 @NNCF_CUSTOM_OBJECTS.register()
 class NNCFWrapper(tf.keras.layers.Wrapper):
     """
     This wrapper augments a keras layer so the NNCF Operations may be applied to weights,
     callable attributes (like activations), input and output of the wrapped layer.
     """
+
     def __init__(self, layer, **kwargs):
         """
         Create a pruning wrapper for a keras layer.
 
         :param layer: the keras layer to be wrapped
         :param kwargs: additional keyword arguments to be passed to the keras layer.
         """
         if layer is None:
-            raise ValueError('`layer` cannot be None.')
+            raise ValueError("`layer` cannot be None.")
 
-        if not isinstance(layer, tf.keras.layers.Layer) or \
-                isinstance(layer, tf.keras.Model):
+        if not isinstance(layer, tf.keras.layers.Layer) or isinstance(layer, tf.keras.Model):
             raise ValueError(
-                '`layer` can only be a `tf.keras.layers.Layer` instance. '
-                'You passed an instance of type: {input}.'.format(
-                    input=layer.__class__.__name__))
+                "`layer` can only be a `tf.keras.layers.Layer` instance. "
+                "You passed an instance of type: {input}.".format(input=layer.__class__.__name__)
+            )
 
-        if 'name' not in kwargs:
-            kwargs['name'] = layer.name
+        if "name" not in kwargs:
+            kwargs["name"] = layer.name
 
         super().__init__(layer, **kwargs)
-        self._track_trackable(layer, name='layer')
+        self._track_trackable(layer, name="layer")
 
         self.weights_attr_ops = {}  # type: Dict[str, Dict[str, NNCFOperation]]
 
         self._init_layer_call_fn_args()
         self._trainable_weights = []
         self._non_trainable_weights = []
         self._ops_weights = {}
@@ -155,31 +153,30 @@
 
     @property
     def losses(self):
         return self.layer.losses + self._losses
 
     @property
     def data_format(self):
-        return getattr(self.layer, 'data_format', 'channels_last')
+        return getattr(self.layer, "data_format", "channels_last")
 
     @property
     def ops_weights(self):
         return self._ops_weights
 
     @property
     def layer_weights(self):
         return self._layer_weights
 
     def build(self, input_shape=None):
         super().build(input_shape)
         for weight_attr, ops in self.weights_attr_ops.items():
             weight = self.get_layer_weight(weight_attr)
             for op_name, op in ops.items():
-                self._ops_weights[op_name] = op.build(
-                    weight.shape, InputType.WEIGHTS, weight_attr, self)
+                self._ops_weights[op_name] = op.build(weight.shape, InputType.WEIGHTS, weight_attr, self)
             self._layer_weights[weight_attr] = weight
             self._trainable_weights.append(weight)
         self._op_build = True
 
     def call(self, inputs, training=None):
         training = self._get_training_value(training)
 
@@ -192,25 +189,23 @@
 
         return outputs
 
     def _apply_ops(self, training):
         for weight_attr, ops in self.weights_attr_ops.items():
             layer_weight = self._layer_weights[weight_attr]
             for op_name, op in ops.items():
-                layer_weight = op(layer_weight,
-                                  self._ops_weights[op_name],
-                                  training)
+                layer_weight = op(layer_weight, self._ops_weights[op_name], training)
             self.set_layer_weight(weight_attr, layer_weight)
 
     def registry_weight_operation(self, weights_attr: str, op: NNCFOperation):
         if weights_attr not in self.weights_attr_ops:
             self.weights_attr_ops[weights_attr] = OrderedDict()
 
         if op.name in self.weights_attr_ops[weights_attr]:
-            raise RuntimeError(f'Attempt to apply an operation with the same name {op.name} on layer weight twice')
+            raise RuntimeError(f"Attempt to apply an operation with the same name {op.name} on layer weight twice")
 
         self.weights_attr_ops[weights_attr][op.name] = op
 
     def get_operation_weights(self, operation_name):
         return self._ops_weights[operation_name]
 
     def get_layer_weight(self, weight_attr):
@@ -232,15 +227,15 @@
         call_full_argspec = getfullargspec(self.layer.call)
         call_fn_args = self._get_call_fn_args(call_full_argspec)
         self._layer_expects_training_arg = "training" in call_fn_args
 
     @staticmethod
     def _get_call_fn_args(call_full_argspec):
         all_args = call_full_argspec.args + call_full_argspec.kwonlyargs
-        if all_args and all_args[0] == 'self':
+        if all_args and all_args[0] == "self":
             return all_args[1:]
         return all_args
 
     @staticmethod
     def _get_training_value(training):
         if training is None:
             training = tf.keras.backend.learning_phase()
@@ -255,29 +250,26 @@
 
         weights_attr_ops = {}
         for weights_attr, ops in self.weights_attr_ops.items():
             weights_attr_ops[weights_attr] = []
             for op_name in ops:
                 op_config = tf.keras.utils.serialize_keras_object(ops[op_name])
                 weights_attr_ops[weights_attr].append(op_config)
-        config['weights_attr_operations'] = weights_attr_ops
+        config["weights_attr_operations"] = weights_attr_ops
         return config
 
     @classmethod
     def from_config(cls, config, custom_objects=None):
         config = config.copy()
 
-        weights_attr_ops_config = config.pop('weights_attr_operations')
+        weights_attr_ops_config = config.pop("weights_attr_operations")
 
-        layer = tf.keras.layers.deserialize(config.pop('layer'), custom_objects=custom_objects)
+        layer = tf.keras.layers.deserialize(config.pop("layer"), custom_objects=custom_objects)
         wrapper = cls(layer=layer, **config)
 
         for weights_attr, operations in weights_attr_ops_config.items():
             for op_config in operations:
                 wrapper.registry_weight_operation(
-                    weights_attr,
-                    tf.keras.layers.deserialize(
-                        op_config,
-                        custom_objects=get_nncf_custom_objects())
+                    weights_attr, tf.keras.layers.deserialize(op_config, custom_objects=get_nncf_custom_objects())
                 )
 
         return wrapper
```

### Comparing `nncf-2.4.0/nncf/tensorflow/loss.py` & `nncf-2.5.0/nncf/tensorflow/loss.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,29 +1,27 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Any, Dict
 
 import tensorflow as tf
 
 from nncf.api.compression import CompressionLoss
 
 
 class TFZeroCompressionLoss(CompressionLoss):
     def calculate(self, *args, **kwargs) -> Any:
-        return tf.constant(0.)
+        return tf.constant(0.0)
 
     def load_state(self, state: Dict[str, Any]) -> None:
         pass
 
     def get_state(self) -> Dict[str, Any]:
         return {}
```

### Comparing `nncf-2.4.0/nncf/tensorflow/pruning/base_algorithm.py` & `nncf-2.5.0/nncf/tensorflow/pruning/base_algorithm.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,23 +1,19 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Dict
-from typing import List
-from typing import Tuple
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from abc import ABC
+from typing import Dict, List, Tuple
 
 import tensorflow as tf
 
 from nncf import NNCFConfig
 from nncf.common.compression import BaseCompressionAlgorithmController
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
@@ -29,14 +25,15 @@
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
 from nncf.common.pruning.node_selector import PruningNodeSelector
 from nncf.common.pruning.statistics import PrunedLayerSummary
 from nncf.common.pruning.structs import PrunedLayerInfoBase
 from nncf.common.pruning.utils import get_output_channels
 from nncf.common.pruning.utils import is_prunable_depthwise_conv
 from nncf.common.scopes import check_scopes_in_graph
+from nncf.common.utils.api_marker import api
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.schemata.defaults import PRUNE_BATCH_NORMS
 from nncf.config.schemata.defaults import PRUNE_DOWNSAMPLE_CONVS
 from nncf.config.schemata.defaults import PRUNE_FIRST_CONV
 from nncf.config.schemata.defaults import PRUNING_INIT
 from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
 from nncf.tensorflow.graph.converter import TFModelConverterFactory
@@ -68,31 +65,33 @@
     """
     Determines which modifications should be made to the original model in
     order to enable pruning during fine-tuning.
     """
 
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
-        params = self._algo_config.get('params', {})
+        params = self._algo_config.get("params", {})
         self._params = params
 
         self._ignore_frozen_layers = True
-        self._prune_first = params.get('prune_first_conv', PRUNE_FIRST_CONV)
-        self._prune_batch_norms = params.get('prune_batch_norms', PRUNE_BATCH_NORMS)
-        self._prune_downsample_convs = params.get('prune_downsample_convs', PRUNE_DOWNSAMPLE_CONVS)
+        self._prune_first = params.get("prune_first_conv", PRUNE_FIRST_CONV)
+        self._prune_batch_norms = params.get("prune_batch_norms", PRUNE_BATCH_NORMS)
+        self._prune_downsample_convs = params.get("prune_downsample_convs", PRUNE_DOWNSAMPLE_CONVS)
 
         self._prunable_types = self._get_op_types_of_pruned_layers()
         types_of_grouping_ops = self._get_types_of_grouping_ops()
-        self._pruning_node_selector = PruningNodeSelector(TF_PRUNING_OPERATOR_METATYPES,
-                                                          self._prunable_types,
-                                                          types_of_grouping_ops,
-                                                          self.ignored_scopes,
-                                                          self.target_scopes,
-                                                          self._prune_first,
-                                                          self._prune_downsample_convs)
+        self._pruning_node_selector = PruningNodeSelector(
+            TF_PRUNING_OPERATOR_METATYPES,
+            self._prunable_types,
+            types_of_grouping_ops,
+            self.ignored_scopes,
+            self.target_scopes,
+            self._prune_first,
+            self._prune_downsample_convs,
+        )
 
         self._pruned_layer_groups_info = None
         self._graph = None
         self._op_names = []
 
     def apply_to(self, model: tf.keras.Model) -> tf.keras.Model:
         """
@@ -126,119 +125,124 @@
         self._pruned_layer_groups_info = Clusterization[PrunedLayerInfo](lambda x: x.layer_name)
 
         for i, group in enumerate(groups_of_nodes_to_prune.get_all_clusters()):
             group_minfos = []
             for node in group.elements:
                 layer_name = get_layer_identifier(node)
                 layer = model.get_layer(layer_name)
-                group_minfos.append(PrunedLayerInfo(node.node_name, layer_name, node.node_id,
-                                                    is_prunable_depthwise_conv(node)))
+                group_minfos.append(
+                    PrunedLayerInfo(node.node_name, layer_name, node.node_id, is_prunable_depthwise_conv(node))
+                )
 
                 # Add output_mask to elements to run mask_propagation
                 # and detect spec_nodes that will be pruned.
                 # It should be done for all elements of shared layer.
-                node.data['output_mask'] = TFNNCFTensor(tf.ones(get_output_channels(node)))
+                node.data["output_mask"] = TFNNCFTensor(tf.ones(get_output_channels(node)))
                 if layer_name in shared_layers:
                     continue
                 if node.is_shared():
                     shared_layers.add(layer_name)
                 # Check that we need to prune weights in this op
                 assert self._is_pruned_layer(layer)
-                nncf_logger.debug(f'Will prune the weights for the layer: {layer_name}')
+                nncf_logger.debug(f"Will prune the weights for the layer: {layer_name}")
 
                 _, layer_info = converter.get_layer_info_for_node(node.node_name)
                 for weight_def in node.metatype.weight_definitions:
                     transformations.register(
-                        self._get_insertion_command_binary_mask(
-                            layer_info.layer_name, weight_def.weight_attr_name)
+                        self._get_insertion_command_binary_mask(layer_info.layer_name, weight_def.weight_attr_name)
                     )
-                if node.metatype.bias_attr_name is not None and \
-                        getattr(layer, node.metatype.bias_attr_name) is not None:
+                if (
+                    node.metatype.bias_attr_name is not None
+                    and getattr(layer, node.metatype.bias_attr_name) is not None
+                ):
                     transformations.register(
-                        self._get_insertion_command_binary_mask(
-                            layer_info.layer_name, node.metatype.bias_attr_name)
+                        self._get_insertion_command_binary_mask(layer_info.layer_name, node.metatype.bias_attr_name)
                     )
 
             cluster = Cluster[PrunedLayerInfo](i, group_minfos, [n.node_id for n in group.elements])
             self._pruned_layer_groups_info.add_cluster(cluster)
 
         # Propagating masks across the graph to detect spec_nodes that will be pruned
-        mask_propagator = MaskPropagationAlgorithm(self._graph, TF_PRUNING_OPERATOR_METATYPES,
-                                                   TFNNCFPruningTensorProcessor)
+        mask_propagator = MaskPropagationAlgorithm(
+            self._graph, TF_PRUNING_OPERATOR_METATYPES, TFNNCFPruningTensorProcessor
+        )
         mask_propagator.mask_propagation()
 
         # Add masks for all spec modules, because prunable batchnorm layers can be determined
         # at the moment of mask propagation
-        types_spec_layers = [TFBatchNormalizationLayerMetatype] \
-            if self._prune_batch_norms else []
+        types_spec_layers = [TFBatchNormalizationLayerMetatype] if self._prune_batch_norms else []
 
         spec_nodes = self._graph.get_nodes_by_metatypes(types_spec_layers)
         for spec_node in spec_nodes:
             layer_name = get_layer_identifier(spec_node)
             layer = model.get_layer(layer_name)
-            if spec_node.data['output_mask'] is None:
+            if spec_node.data["output_mask"] is None:
                 # Skip elements that will not be pruned
                 continue
             if layer_name in shared_layers:
                 continue
             if spec_node.is_shared():
                 shared_layers.add(layer_name)
-            nncf_logger.debug(f'Will prune the weights for the layer: {layer_name}')
+            nncf_logger.debug(f"Will prune the weights for the layer: {layer_name}")
 
             _, layer_info = converter.get_layer_info_for_node(spec_node.node_name)
             for weight_def in spec_node.metatype.weight_definitions:
-                if spec_node.metatype is TFBatchNormalizationLayerMetatype \
-                        and not layer.scale and weight_def.weight_attr_name == 'gamma':
-                    nncf_logger.debug('Fused gamma parameter encountered in BatchNormalization layer. '
-                                      'Won\'t add a pruning mask to it.')
+                if (
+                    spec_node.metatype is TFBatchNormalizationLayerMetatype
+                    and not layer.scale
+                    and weight_def.weight_attr_name == "gamma"
+                ):
+                    nncf_logger.debug(
+                        "Fused gamma parameter encountered in BatchNormalization layer. "
+                        "Won't add a pruning mask to it."
+                    )
                     continue
 
                 transformations.register(
-                    self._get_insertion_command_binary_mask(
-                        layer_info.layer_name, weight_def.weight_attr_name)
+                    self._get_insertion_command_binary_mask(layer_info.layer_name, weight_def.weight_attr_name)
                 )
             transformations.register(
-                self._get_insertion_command_binary_mask(
-                    layer_info.layer_name, spec_node.metatype.bias_attr_name)
+                self._get_insertion_command_binary_mask(layer_info.layer_name, spec_node.metatype.bias_attr_name)
             )
         return transformations
 
     def initialize(self, model: tf.keras.Model) -> None:
         pass
 
-    def _get_insertion_command_binary_mask(self, layer_name: str,
-                                           attr_name: str) -> TFInsertionCommand:
+    def _get_insertion_command_binary_mask(self, layer_name: str, attr_name: str) -> TFInsertionCommand:
         op_name = self._get_pruning_operation_name(layer_name, attr_name)
         self._op_names.append(op_name)
 
         return TFInsertionCommand(
             target_point=TFLayerWeight(layer_name, attr_name),
             callable_object=BinaryMask(op_name),
-            priority=TransformationPriority.PRUNING_PRIORITY
+            priority=TransformationPriority.PRUNING_PRIORITY,
         )
 
     @staticmethod
     def _get_bn_for_node(node: NNCFNode, bn_nodes: List[NNCFNode]) -> Tuple[bool, List[NNCFNode]]:
         is_finished = False
-        propagating_ops = [op_name for meta_op in [TFIdentityMaskForwardPruningOp, TFElementwisePruningOp]
-                           for op_name in meta_op.get_all_op_aliases()]
-        if node.node_type == 'BatchNormalization':
+        propagating_ops = [
+            op_name
+            for meta_op in [TFIdentityMaskForwardPruningOp, TFElementwisePruningOp]
+            for op_name in meta_op.get_all_op_aliases()
+        ]
+        if node.node_type == "BatchNormalization":
             is_finished = True
             bn_nodes.append(node)
         elif node.node_type not in propagating_ops:
             is_finished = True
         return is_finished, bn_nodes
 
     def _get_related_batchnorms(self, layer_name: str, group: Cluster, graph: NNCFGraph) -> List[NNCFNode]:
         """
         Returns List of batchnorm elements related to the layer.
         Note: Single node per layer for shared bactchnorm layers
         """
-        layer_nodes = [node_ for node_ in group.elements
-                       if node_.layer_name == layer_name]
+        layer_nodes = [node_ for node_ in group.elements if node_.layer_name == layer_name]
         bn_nodes = []
         bn_layer_names = []
         for layer_node in layer_nodes:
             for next_node in graph.get_next_nodes(layer_node):
                 for bn_node in graph.traverse_graph(next_node, self._get_bn_for_node):
                     bn_layer_name = get_layer_identifier(bn_node)
                     if bn_layer_name not in bn_layer_names:
@@ -258,37 +262,38 @@
         """
         raise NotImplementedError
 
     def _get_types_of_grouping_ops(self) -> List[str]:
         raise NotImplementedError
 
     def _get_pruning_operation_name(self, layer_name: str, weight_attr_name: str) -> str:
-        return f'{layer_name}_{weight_attr_name}_pruning_binary_mask'
+        return f"{layer_name}_{weight_attr_name}_pruning_binary_mask"
 
 
-class BasePruningAlgoController(BaseCompressionAlgorithmController):
+@api()
+class BasePruningAlgoController(BaseCompressionAlgorithmController, ABC):
     """
-    Serves as a handle to the additional modules, parameters and hooks inserted
-    into the original uncompressed model to enable pruning.
+    Base class for TF pruning algorithm controllers.
     """
 
-    def __init__(self,
-                 target_model: tf.keras.Model,
-                 op_names: List[str],
-                 prunable_types: List[str],
-                 pruned_layer_groups_info: Clusterization[PrunedLayerInfo],
-                 config):
+    def __init__(
+        self,
+        target_model: tf.keras.Model,
+        op_names: List[str],
+        prunable_types: List[str],
+        pruned_layer_groups_info: Clusterization[PrunedLayerInfo],
+        config,
+    ):
         super().__init__(target_model)
         self._op_names = op_names
         self._prunable_types = prunable_types
         self.config = config
-        self.pruning_config = extract_algo_specific_config(config,
-                                                           "filter_pruning")
-        params = self.pruning_config.get('params', {})
-        self.pruning_init = self.pruning_config.get('pruning_init', PRUNING_INIT)
+        self.pruning_config = extract_algo_specific_config(config, "filter_pruning")
+        params = self.pruning_config.get("params", {})
+        self.pruning_init = self.pruning_config.get("pruning_init", PRUNING_INIT)
         self.pruning_level = self.pruning_init
         self._pruned_layer_groups_info = pruned_layer_groups_info
         self.prune_flops = False
         self._check_pruning_level(params)
         self._num_of_sparse_elements_by_node = None
 
     def freeze(self):
@@ -300,18 +305,18 @@
     def step(self, next_step):
         pass
 
     def _check_pruning_level(self, params):
         """
         Check that set only one of pruning target params
         """
-        pruning_target = params.get('pruning_target', None)
-        pruning_flops_target = params.get('pruning_flops_target', None)
+        pruning_target = params.get("pruning_target", None)
+        pruning_flops_target = params.get("pruning_flops_target", None)
         if pruning_target and pruning_flops_target:
-            raise ValueError('Only one parameter from \'pruning_target\' and \'pruning_flops_target\' can be set.')
+            raise ValueError("Only one parameter from 'pruning_target' and 'pruning_flops_target' can be set.")
         if pruning_flops_target:
             self.prune_flops = True
 
     def _calculate_num_of_sparse_elements_by_node(self) -> Dict[NNCFNodeName, int]:
         """Returns the number of sparse elements per node. Take into account names ('^') for the shared ops."""
         if self._num_of_sparse_elements_by_node is None:
             self._calculate_pruned_layers_summary()
@@ -325,15 +330,15 @@
     def _calculate_pruned_layers_summary(self) -> List[PrunedLayerSummary]:
         pruning_levels = []
         mask_names = []
         weights_shapes = []
         mask_shapes = []
         self._num_of_sparse_elements_by_node = {}
         for wrapped_layer, weight_attr, op_name in get_nncf_operations(self._model, self._op_names):
-            mask = wrapped_layer.ops_weights[op_name.name]['mask']
+            mask = wrapped_layer.ops_weights[op_name.name]["mask"]
             mask_names.append(mask.name)
             weights_shapes.append(list(mask.shape))
             reduce_axes = list(range(len(mask.shape)))
             filter_axis = get_filter_axis(wrapped_layer, weight_attr)
             if filter_axis == -1:
                 filter_axis = reduce_axes[filter_axis]
             reduce_axes.remove(filter_axis)
@@ -350,9 +355,10 @@
 
         pruned_layers_summary = []
         for mask_name, weights_shape, mask_shape, pruning_level in mask_pruning:
             pruned_layers_summary.append(PrunedLayerSummary(mask_name, weights_shape, mask_shape, pruning_level))
 
         return pruned_layers_summary
 
-    def strip_model(self, model: tf.keras.Model) -> tf.keras.Model:
+    def strip_model(self, model: tf.keras.Model, do_copy: bool = False) -> tf.keras.Model:
+        # Transform model for pruning creates copy of the model.
         return strip_model_from_masks(model, self._op_names)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/pruning/callbacks.py` & `nncf-2.5.0/nncf/tensorflow/pruning/callbacks.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,39 +1,37 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.common.statistics import NNCFStatistics
 from nncf.tensorflow.callbacks.statistics_callback import StatisticsCallback
 
 
 class PruningStatisticsCallback(StatisticsCallback):
     """
     Callback for logging pruning compression statistics to tensorboard and stdout.
     """
 
     def _prepare_for_tensorboard(self, stats: NNCFStatistics):
-        base_prefix = '2.compression/statistics'
-        detailed_prefix = '3.compression_details/statistics'
+        base_prefix = "2.compression/statistics"
+        detailed_prefix = "3.compression_details/statistics"
 
         ms = stats.filter_pruning.model_statistics
         tensorboard_stats = {
-            f'{base_prefix}/algo_current_pruning_level': stats.filter_pruning.current_pruning_level,
-            f'{base_prefix}/model_FLOPS_pruning_level': ms.flops_pruning_level,
-            f'{base_prefix}/model_params_pruning_level': ms.params_pruning_level,
-            f'{base_prefix}/model_filters_pruning_level': ms.filter_pruning_level,
+            f"{base_prefix}/algo_current_pruning_level": stats.filter_pruning.current_pruning_level,
+            f"{base_prefix}/model_FLOPS_pruning_level": ms.flops_pruning_level,
+            f"{base_prefix}/model_params_pruning_level": ms.params_pruning_level,
+            f"{base_prefix}/model_filters_pruning_level": ms.filter_pruning_level,
         }
 
         for ls in ms.pruned_layers_summary:
             layer_name, pruning_level = ls.name, ls.filter_pruning_level
-            tensorboard_stats[f'{detailed_prefix}/{layer_name}/pruning_level'] = pruning_level
+            tensorboard_stats[f"{detailed_prefix}/{layer_name}/pruning_level"] = pruning_level
 
         return tensorboard_stats
```

### Comparing `nncf-2.4.0/nncf/tensorflow/pruning/filter_pruning/algorithm.py` & `nncf-2.5.0/nncf/tensorflow/pruning/filter_pruning/algorithm.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,157 +1,161 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from math import floor
-from typing import List, Set
 from math import isclose
+from typing import List, Set
 
 import tensorflow as tf
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionStage
+from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
 from nncf.common.graph import NNCFGraph
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
+from nncf.common.logging import nncf_logger
 from nncf.common.pruning.clusterization import Cluster
 from nncf.common.pruning.clusterization import Clusterization
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
 from nncf.common.pruning.schedulers import PRUNING_SCHEDULERS
 from nncf.common.pruning.schedulers import PruningScheduler
+from nncf.common.pruning.shape_pruning_processor import ShapePruningProcessor
 from nncf.common.pruning.statistics import FilterPruningStatistics
 from nncf.common.pruning.statistics import PrunedModelStatistics
-from nncf.common.pruning.weights_flops_calculator import WeightsFlopsCalculator
-from nncf.common.pruning.shape_pruning_processor import ShapePruningProcessor
+from nncf.common.pruning.statistics import PrunedModelTheoreticalBorderline
 from nncf.common.pruning.utils import get_prunable_layers_in_out_channels
 from nncf.common.pruning.utils import get_rounded_pruned_element_number
+from nncf.common.pruning.weights_flops_calculator import WeightsFlopsCalculator
+from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.statistics import NNCFStatistics
-from nncf.common.pruning.statistics import PrunedModelTheoreticalBorderline
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.debug import is_debug
-from nncf.common.logging import nncf_logger
-from nncf.common.schedulers import StubCompressionScheduler
-from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
 from nncf.config.extractors import extract_bn_adaptation_init_params
 from nncf.config.schemata.defaults import PRUNING_ALL_WEIGHTS
 from nncf.config.schemata.defaults import PRUNING_FILTER_IMPORTANCE
 from nncf.config.schemata.defaults import PRUNING_SCHEDULE
 from nncf.tensorflow.algorithm_selector import TF_COMPRESSION_ALGORITHMS
 from nncf.tensorflow.graph.metatypes.common import GENERAL_CONV_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.common import LINEAR_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
 from nncf.tensorflow.graph.utils import collect_wrapped_layers
-from nncf.tensorflow.tensor import TFNNCFTensor
-from nncf.tensorflow.pruning.tensor_processor import TFNNCFPruningTensorProcessor
 from nncf.tensorflow.layers.wrapper import NNCFWrapper
 from nncf.tensorflow.loss import TFZeroCompressionLoss
 from nncf.tensorflow.pruning.base_algorithm import BasePruningAlgoBuilder
 from nncf.tensorflow.pruning.base_algorithm import BasePruningAlgoController
 from nncf.tensorflow.pruning.base_algorithm import PrunedLayerInfo
+from nncf.tensorflow.pruning.filter_pruning.functions import FILTER_IMPORTANCE_FUNCTIONS
+from nncf.tensorflow.pruning.filter_pruning.functions import calculate_binary_mask
+from nncf.tensorflow.pruning.filter_pruning.functions import tensor_l2_normalizer
 from nncf.tensorflow.pruning.operations import TF_PRUNING_OPERATOR_METATYPES
 from nncf.tensorflow.pruning.operations import TFConvolutionPruningOp
 from nncf.tensorflow.pruning.operations import TFElementwisePruningOp
-from nncf.tensorflow.pruning.operations import TFTransposeConvolutionPruningOp
 from nncf.tensorflow.pruning.operations import TFLinearPruningOp
-from nncf.tensorflow.pruning.filter_pruning.functions import calculate_binary_mask
-from nncf.tensorflow.pruning.filter_pruning.functions import FILTER_IMPORTANCE_FUNCTIONS
-from nncf.tensorflow.pruning.filter_pruning.functions import tensor_l2_normalizer
+from nncf.tensorflow.pruning.operations import TFTransposeConvolutionPruningOp
+from nncf.tensorflow.pruning.tensor_processor import TFNNCFPruningTensorProcessor
 from nncf.tensorflow.pruning.utils import broadcast_filter_mask
 from nncf.tensorflow.pruning.utils import collect_output_shapes
 from nncf.tensorflow.pruning.utils import get_filter_axis
 from nncf.tensorflow.pruning.utils import get_filters_num
 from nncf.tensorflow.sparsity.magnitude.operation import BinaryMask
+from nncf.tensorflow.tensor import TFNNCFTensor
 
 
-@TF_COMPRESSION_ALGORITHMS.register('filter_pruning')
+@TF_COMPRESSION_ALGORITHMS.register("filter_pruning")
 class FilterPruningBuilder(BasePruningAlgoBuilder):
     """
     Determines which modifications should be made to the original model in
     order to enable filter pruning during fine-tuning.
     """
 
     def _build_controller(self, model: tf.keras.Model):
-        return FilterPruningController(model,
-                                       self._graph,
-                                       self._op_names,
-                                       self._prunable_types,
-                                       self._pruned_layer_groups_info,
-                                       self.config)
+        return FilterPruningController(
+            model, self._graph, self._op_names, self._prunable_types, self._pruned_layer_groups_info, self.config
+        )
 
     def _is_pruned_layer(self, layer: tf.keras.layers.Layer) -> bool:
         return layer.__class__.__name__ in self._prunable_types
 
     def _get_op_types_of_pruned_layers(self) -> List[str]:
-        return [op_name for meta_op in [TFConvolutionPruningOp, TFTransposeConvolutionPruningOp,
-                                        TFLinearPruningOp]
-                for op_name in meta_op.get_all_op_aliases()]
+        return [
+            op_name
+            for meta_op in [TFConvolutionPruningOp, TFTransposeConvolutionPruningOp, TFLinearPruningOp]
+            for op_name in meta_op.get_all_op_aliases()
+        ]
 
     def _get_types_of_grouping_ops(self) -> List[str]:
         return TFElementwisePruningOp.get_all_op_aliases()
 
 
-@ADAPTIVE_COMPRESSION_CONTROLLERS.register('tf_filter_pruning')
+@api()
+@ADAPTIVE_COMPRESSION_CONTROLLERS.register("tf_filter_pruning")
 class FilterPruningController(BasePruningAlgoController):
     """
-    Serves as a handle to the additional modules, parameters and hooks inserted
-    into the original uncompressed model to enable filter pruning.
+    Controller class for the filter pruning algorithm.
     """
 
-    def __init__(self,
-                 target_model: tf.keras.Model,
-                 graph: NNCFGraph,
-                 op_names: List[str],
-                 prunable_types: List[str],
-                 pruned_layer_groups: Clusterization[PrunedLayerInfo],
-                 config: NNCFConfig):
+    def __init__(
+        self,
+        target_model: tf.keras.Model,
+        graph: NNCFGraph,
+        op_names: List[str],
+        prunable_types: List[str],
+        pruned_layer_groups: Clusterization[PrunedLayerInfo],
+        config: NNCFConfig,
+    ):
         super().__init__(target_model, op_names, prunable_types, pruned_layer_groups, config)
         self._original_graph = graph
-        params = self.pruning_config.get('params', {})
+        params = self.pruning_config.get("params", {})
         self.frozen = False
         self.pruning_quota = 0.9
 
         self._weights_flops_calc = WeightsFlopsCalculator(
-            conv_op_metatypes=GENERAL_CONV_LAYER_METATYPES,
-            linear_op_metatypes=LINEAR_LAYER_METATYPES)
+            conv_op_metatypes=GENERAL_CONV_LAYER_METATYPES, linear_op_metatypes=LINEAR_LAYER_METATYPES
+        )
 
         self._shape_pruning_proc = ShapePruningProcessor(
-            pruning_operations_metatype=TF_PRUNING_OPERATOR_METATYPES,
-            prunable_types=prunable_types)
+            pruning_operations_metatype=TF_PRUNING_OPERATOR_METATYPES, prunable_types=prunable_types
+        )
 
         self._pruning_quotas = {}
         self._next_nodes = {}
         self._output_shapes = {}
         _, output_channels = get_prunable_layers_in_out_channels(self._original_graph)
         self._init_pruned_layers_params(output_channels)
 
         self.full_flops, self.full_params_num = self._weights_flops_calc.count_flops_and_weights(
-            graph, self._output_shapes)
+            graph, self._output_shapes
+        )
         self.full_filters_num = self._weights_flops_calc.count_filters_num(graph, output_channels)
 
         self.current_flops, self.current_params_num = self.full_flops, self.full_params_num
         self.current_filters_num = self.full_filters_num
 
         self._pruned_layers_num = len(self._pruned_layer_groups_info.get_all_nodes())
         self._prunable_layers_num = len(self._original_graph.get_nodes_by_types(self._prunable_types))
-        self._min_possible_flops, self._min_possible_params = \
-            self._calculate_flops_and_weights_in_uniformly_pruned_model(1.)
+        (
+            self._min_possible_flops,
+            self._min_possible_params,
+        ) = self._calculate_flops_and_weights_in_uniformly_pruned_model(1.0)
 
         self._weights_normalizer = tensor_l2_normalizer  # for all weights in common case
-        self._filter_importance = FILTER_IMPORTANCE_FUNCTIONS.get(params.get('filter_importance',
-                                                                             PRUNING_FILTER_IMPORTANCE))
-        self.all_weights = params.get('all_weights', PRUNING_ALL_WEIGHTS)
-        scheduler_cls = PRUNING_SCHEDULERS.get(params.get('schedule', PRUNING_SCHEDULE))
+        self._filter_importance = FILTER_IMPORTANCE_FUNCTIONS.get(
+            params.get("filter_importance", PRUNING_FILTER_IMPORTANCE)
+        )
+        self.all_weights = params.get("all_weights", PRUNING_ALL_WEIGHTS)
+        scheduler_cls = PRUNING_SCHEDULERS.get(params.get("schedule", PRUNING_SCHEDULE))
         self._scheduler = scheduler_cls(self, params)
         self._bn_adaptation = None
         self.set_pruning_level(self.pruning_init)
         self._loss = TFZeroCompressionLoss()
 
     @property
     def scheduler(self) -> PruningScheduler:
@@ -162,16 +166,18 @@
         return self._loss
 
     def compression_stage(self) -> CompressionStage:
         target_pruning_level = self.scheduler.target_level
         actual_pruning_level = self.pruning_level
         if actual_pruning_level == 0:
             return CompressionStage.UNCOMPRESSED
-        if isclose(actual_pruning_level, target_pruning_level, abs_tol=1e-5) or \
-                actual_pruning_level > target_pruning_level:
+        if (
+            isclose(actual_pruning_level, target_pruning_level, abs_tol=1e-5)
+            or actual_pruning_level > target_pruning_level
+        ):
             return CompressionStage.FULLY_COMPRESSED
         return CompressionStage.PARTIALLY_COMPRESSED
 
     @property
     def compression_rate(self) -> float:
         if self.prune_flops:
             return 1 - self.current_flops / self.full_flops
@@ -187,49 +193,58 @@
     def disable_scheduler(self):
         self._scheduler = StubCompressionScheduler()
         self._scheduler.current_pruning_level = 0.0
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         if not quickly_collected_only and is_debug():
             stats = PrunedModelTheoreticalBorderline(
-                self._pruned_layers_num, self._prunable_layers_num, self._min_possible_flops,
-                self._min_possible_params, self.full_flops, self.full_params_num)
+                self._pruned_layers_num,
+                self._prunable_layers_num,
+                self._min_possible_flops,
+                self._min_possible_params,
+                self.full_flops,
+                self.full_params_num,
+            )
 
             nncf_logger.debug(stats.to_str())
 
         pruned_layers_summary = self._calculate_pruned_layers_summary()
         self._update_benchmark_statistics()
-        model_statistics = PrunedModelStatistics(self.full_flops, self.current_flops,
-                                                 self.full_params_num, self.current_params_num,
-                                                 self.full_filters_num, self.current_filters_num,
-                                                 pruned_layers_summary)
-
-        stats = FilterPruningStatistics(model_statistics,
-                                        self.scheduler.current_pruning_level,
-                                        self.scheduler.target_level,
-                                        self.prune_flops)
+        model_statistics = PrunedModelStatistics(
+            self.full_flops,
+            self.current_flops,
+            self.full_params_num,
+            self.current_params_num,
+            self.full_filters_num,
+            self.current_filters_num,
+            pruned_layers_summary,
+        )
+
+        stats = FilterPruningStatistics(
+            model_statistics, self.scheduler.current_pruning_level, self.scheduler.target_level, self.prune_flops
+        )
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('filter_pruning', stats)
+        nncf_stats.register("filter_pruning", stats)
         return nncf_stats
 
     def freeze(self, freeze: bool = True):
         self.frozen = freeze
 
-    def set_pruning_level(self, pruning_level: float,
-                          run_batchnorm_adaptation: bool = False):
+    def set_pruning_level(self, pruning_level: float, run_batchnorm_adaptation: bool = False):
         """
-        Setup pruning masks in accordance to provided pruning level
-        :param pruning_level: pruning ratio
-        :return:
+        Setup pruning masks in accordance to provided pruning level.
+
+        :param pruning_level: Pruning level to be set.
+        :param run_batchnorm_adaptation: Whether to run batchnorm adaptation after setting the pruning level.
         """
         # Pruning level from scheduler can be percentage of params that should be pruned
         self.pruning_level = pruning_level
         if not self.frozen:
-            nncf_logger.info('Computing filter importance scores and binary masks...')
+            nncf_logger.info("Computing filter importance scores and binary masks...")
             if self.all_weights:
                 if self.prune_flops:
                     self._set_binary_masks_for_pruned_modules_globally_by_flops_target(pruning_level)
                 else:
                     self._set_binary_masks_for_pruned_layers_globally(pruning_level)
             else:
                 if self.prune_flops:
@@ -251,72 +266,74 @@
         self._output_shapes = collect_output_shapes(self._model, self._original_graph)
 
         # 2. Initialize next_nodes for each pruning cluster
         self._next_nodes = self._shape_pruning_proc.get_next_nodes(self._original_graph, self._pruned_layer_groups_info)
 
         # 3. Initialize pruning quotas
         for cluster in self._pruned_layer_groups_info.get_all_clusters():
-            self._pruning_quotas[cluster.id] = floor(output_channels[cluster.elements[0].node_name]
-                                                     * self.pruning_quota)
+            self._pruning_quotas[cluster.id] = floor(
+                output_channels[cluster.elements[0].node_name] * self.pruning_quota
+            )
 
     def _set_binary_masks_for_pruned_layers_groupwise(self, pruning_level: float):
-        nncf_logger.debug('Setting new binary masks for pruned layers.')
+        nncf_logger.debug("Setting new binary masks for pruned layers.")
         wrapped_layers = collect_wrapped_layers(self._model)
 
         # 0. Removing masks at the elements of the NNCFGraph
         for node in self._original_graph.topological_sort():
-            node.data.pop('output_mask', None)
+            node.data.pop("output_mask", None)
 
         # 1. Calculate masks
         for group in self._pruned_layer_groups_info.get_all_clusters():
             # a. Calculate the cumulative importance for all filters in the group
             cumulative_filters_importance = self._calculate_filters_importance_in_group(group)
             filters_num = len(cumulative_filters_importance)
 
             # b. Calculate threshold
-            num_of_sparse_elems = get_rounded_pruned_element_number(cumulative_filters_importance.shape[0],
-                                                                    pruning_level)
+            num_of_sparse_elems = get_rounded_pruned_element_number(
+                cumulative_filters_importance.shape[0], pruning_level
+            )
             threshold = sorted(cumulative_filters_importance)[min(num_of_sparse_elems, filters_num - 1)]
 
             # c. Initialize masks
             filter_mask = calculate_binary_mask(cumulative_filters_importance, threshold)
             for node in group.elements:
                 nncf_node = self._original_graph.get_node_by_id(node.nncf_node_id)
-                nncf_node.data['output_mask'] = TFNNCFTensor(filter_mask)
+                nncf_node.data["output_mask"] = TFNNCFTensor(filter_mask)
 
         # 2. Propagating masks across the graph
-        mask_propagator = MaskPropagationAlgorithm(self._original_graph, TF_PRUNING_OPERATOR_METATYPES,
-                                                   TFNNCFPruningTensorProcessor)
+        mask_propagator = MaskPropagationAlgorithm(
+            self._original_graph, TF_PRUNING_OPERATOR_METATYPES, TFNNCFPruningTensorProcessor
+        )
         mask_propagator.mask_propagation()
 
         # 3. Apply masks to the model
         nncf_sorted_nodes = self._original_graph.topological_sort()
         for layer in wrapped_layers:
-            nncf_node = [n for n in nncf_sorted_nodes
-                         if layer.name == n.layer_name][0]
-            if nncf_node.data['output_mask'] is not None:
-                self._set_operation_masks([layer], nncf_node.data['output_mask'].tensor)
+            nncf_node = [n for n in nncf_sorted_nodes if layer.name == n.layer_name][0]
+            if nncf_node.data["output_mask"] is not None:
+                self._set_operation_masks([layer], nncf_node.data["output_mask"].tensor)
 
         # Calculate actual flops and weights number with new masks
         self._update_benchmark_statistics()
 
     def _set_binary_masks_for_pruned_layers_globally(self, pruning_level: float):
         """
         Sets the binary mask values for layer groups according to the global pruning level.
         Filter importance scores in each group are merged into a single global list and a
         threshold value separating the pruning_level proportion of the least important filters
         in the model is calculated. Filters are pruned globally according to the threshold value.
         """
-        nncf_logger.debug('Setting new binary masks for all pruned modules together.')
+        nncf_logger.debug("Setting new binary masks for all pruned modules together.")
         filter_importances = {}
         wrapped_layers = collect_wrapped_layers(self._model)
 
         # 0. Remove masks at the elements of the NNCFGraph
         for node in self._original_graph.topological_sort():
-            node.data.pop('output_mask', None)
+            node.data.pop("output_mask", None)
 
         # 1. Calculate masks
         # a. Calculate importances for all groups of filters
         for group in self._pruned_layer_groups_info.get_all_clusters():
             cumulative_filters_importance = self._calculate_filters_importance_in_group(group)
             filter_importances[group.id] = cumulative_filters_importance
 
@@ -325,48 +342,46 @@
         threshold = sorted(importances)[int(pruning_level * importances.shape[0])]
 
         # c. Initialize masks
         for group in self._pruned_layer_groups_info.get_all_clusters():
             filter_mask = calculate_binary_mask(filter_importances[group.id], threshold)
             for node in group.elements:
                 nncf_node = self._original_graph.get_node_by_id(node.nncf_node_id)
-                nncf_node.data['output_mask'] = TFNNCFTensor(filter_mask)
+                nncf_node.data["output_mask"] = TFNNCFTensor(filter_mask)
 
         # 2. Propagate masks across the graph
-        mask_propagator = MaskPropagationAlgorithm(self._original_graph, TF_PRUNING_OPERATOR_METATYPES,
-                                                   TFNNCFPruningTensorProcessor)
+        mask_propagator = MaskPropagationAlgorithm(
+            self._original_graph, TF_PRUNING_OPERATOR_METATYPES, TFNNCFPruningTensorProcessor
+        )
         mask_propagator.mask_propagation()
 
         # 3. Apply masks to the model
         nncf_sorted_nodes = self._original_graph.topological_sort()
         for layer in wrapped_layers:
-            nncf_node = [n for n in nncf_sorted_nodes
-                         if layer.name == n.layer_name][0]
-            if nncf_node.data['output_mask'] is not None:
-                self._set_operation_masks([layer], nncf_node.data['output_mask'].tensor)
+            nncf_node = [n for n in nncf_sorted_nodes if layer.name == n.layer_name][0]
+            if nncf_node.data["output_mask"] is not None:
+                self._set_operation_masks([layer], nncf_node.data["output_mask"].tensor)
 
         # Calculate actual flops with new masks
         self._update_benchmark_statistics()
 
-    def _set_binary_masks_for_pruned_modules_globally_by_flops_target(self,
-                                                                      target_flops_pruning_level: float):
+    def _set_binary_masks_for_pruned_modules_globally_by_flops_target(self, target_flops_pruning_level: float):
         """
         Prunes least important filters one-by-one until target FLOPs pruning level is achieved.
         Filters are sorted by filter importance score.
         """
-        nncf_logger.debug('Setting new binary masks for pruned layers.')
+        nncf_logger.debug("Setting new binary masks for pruned layers.")
         target_flops = self.full_flops * (1 - target_flops_pruning_level)
         wrapped_layers = collect_wrapped_layers(self._model)
         masks = {}
 
         nncf_sorted_nodes = self._original_graph.topological_sort()
         for layer in wrapped_layers:
-            nncf_node = [n for n in nncf_sorted_nodes
-                         if layer.name == n.layer_name][0]
-            nncf_node.data['output_mask'] = TFNNCFTensor(tf.ones(get_filters_num(layer)))
+            nncf_node = [n for n in nncf_sorted_nodes if layer.name == n.layer_name][0]
+            nncf_node.data["output_mask"] = TFNNCFTensor(tf.ones(get_filters_num(layer)))
 
         # 1. Calculate importances for all groups of filters. Initialize masks.
         filter_importances = []
         group_indexes = []
         filter_indexes = []
         for group in self._pruned_layer_groups_info.get_all_clusters():
             cumulative_filters_importance = self._calculate_filters_importance_in_group(group)
@@ -374,67 +389,69 @@
             filters_num = len(cumulative_filters_importance)
             group_indexes.extend([group.id] * filters_num)
             filter_indexes.extend(range(filters_num))
             masks[group.id] = tf.ones(filters_num)
 
         # 2.
         tmp_in_channels, tmp_out_channels = get_prunable_layers_in_out_channels(self._original_graph)
-        sorted_importances = sorted(zip(filter_importances, group_indexes, filter_indexes),
-                                    key=lambda x: x[0])
+        sorted_importances = sorted(zip(filter_importances, group_indexes, filter_indexes), key=lambda x: x[0])
         for _, group_id, filter_index in sorted_importances:
             if self._pruning_quotas[group_id] == 0:
                 continue
             masks[group_id] = tf.tensor_scatter_nd_update(masks[group_id], [[filter_index]], [0])
             self._pruning_quotas[group_id] -= 1
 
             cluster = self._pruned_layer_groups_info.get_cluster_by_id(group_id)
             # Update input/output shapes of pruned elements
             self._shape_pruning_proc.prune_cluster_shapes(
-                cluster=cluster, pruned_elems=1,
+                cluster=cluster,
+                pruned_elems=1,
                 pruning_groups_next_nodes=self._next_nodes,
-                input_channels=tmp_in_channels, output_channels=tmp_out_channels)
+                input_channels=tmp_in_channels,
+                output_channels=tmp_out_channels,
+            )
 
             flops, params_num = self._weights_flops_calc.count_flops_and_weights(
-                                                         graph=self._original_graph,
-                                                         output_shapes=self._output_shapes,
-                                                         input_channels=tmp_in_channels,
-                                                         output_channels=tmp_out_channels)
+                graph=self._original_graph,
+                output_shapes=self._output_shapes,
+                input_channels=tmp_in_channels,
+                output_channels=tmp_out_channels,
+            )
             if flops <= target_flops:
                 # 3. Add masks to the graph and propagate them
                 for group in self._pruned_layer_groups_info.get_all_clusters():
                     for node in group.elements:
                         nncf_node = self._original_graph.get_node_by_id(node.nncf_node_id)
-                        nncf_node.data['output_mask'] = TFNNCFTensor(masks[group.id])
+                        nncf_node.data["output_mask"] = TFNNCFTensor(masks[group.id])
 
-                mask_propagator = MaskPropagationAlgorithm(self._original_graph, TF_PRUNING_OPERATOR_METATYPES,
-                                                           TFNNCFPruningTensorProcessor)
+                mask_propagator = MaskPropagationAlgorithm(
+                    self._original_graph, TF_PRUNING_OPERATOR_METATYPES, TFNNCFPruningTensorProcessor
+                )
                 mask_propagator.mask_propagation()
 
                 # 4. Set binary masks to the model
                 self.current_flops = flops
                 self.current_params_num = params_num
                 nncf_sorted_nodes = self._original_graph.topological_sort()
                 for layer in wrapped_layers:
-                    nncf_node = [n for n in nncf_sorted_nodes
-                                 if layer.name == n.layer_name][0]
-                    if nncf_node.data['output_mask'] is not None:
-                        self._set_operation_masks([layer], nncf_node.data['output_mask'].tensor)
+                    nncf_node = [n for n in nncf_sorted_nodes if layer.name == n.layer_name][0]
+                    if nncf_node.data["output_mask"] is not None:
+                        self._set_operation_masks([layer], nncf_node.data["output_mask"].tensor)
                 return
-        raise RuntimeError(f'Unable to prune model to required flops pruning level:'
-                           f' {target_flops_pruning_level}')
+        raise RuntimeError(f"Unable to prune model to required flops pruning level:" f" {target_flops_pruning_level}")
 
     def _set_operation_masks(self, layers: List[NNCFWrapper], filter_mask):
         for layer in layers:
             for weight_attr, ops in layer.weights_attr_ops.items():
                 weight_shape = layer.layer_weights[weight_attr].shape
                 for op_name, op in ops.items():
                     if isinstance(op, BinaryMask):
                         filter_axis = get_filter_axis(layer, weight_attr)
                         broadcasted_mask = broadcast_filter_mask(filter_mask, weight_shape, filter_axis)
-                        layer.ops_weights[op_name]['mask'].assign(broadcasted_mask)
+                        layer.ops_weights[op_name]["mask"].assign(broadcasted_mask)
 
     def _find_uniform_pruning_level_for_target_flops(self, target_flops_pruning_level):
         error = 0.01
         target_flops = self.full_flops * (1 - target_flops_pruning_level)
         left, right = 0.0, 1.0
         while abs(right - left) > error:
             middle = (left + right) / 2
@@ -444,30 +461,35 @@
             else:
                 left = middle
         flops, params_num = self._calculate_flops_and_weights_in_uniformly_pruned_model(right)
         if flops <= target_flops:
             self.current_flops = flops
             self.current_params_num = params_num
             return right
-        raise RuntimeError(f'Unable to prune the model to get the required '
-                           f'pruning level in flops = {target_flops_pruning_level}')
+        raise RuntimeError(
+            f"Unable to prune the model to get the required " f"pruning level in flops = {target_flops_pruning_level}"
+        )
 
     def _calculate_flops_and_weights_in_uniformly_pruned_model(self, pruning_level):
-        tmp_in_channels, tmp_out_channels = \
-            self._shape_pruning_proc.calculate_in_out_channels_in_uniformly_pruned_model(
-                self._original_graph,
-                pruning_groups=self._pruned_layer_groups_info,
-                pruning_groups_next_nodes=self._next_nodes,
-                pruning_level=pruning_level)
+        (
+            tmp_in_channels,
+            tmp_out_channels,
+        ) = self._shape_pruning_proc.calculate_in_out_channels_in_uniformly_pruned_model(
+            self._original_graph,
+            pruning_groups=self._pruned_layer_groups_info,
+            pruning_groups_next_nodes=self._next_nodes,
+            pruning_level=pruning_level,
+        )
 
         return self._weights_flops_calc.count_flops_and_weights(
             graph=self._original_graph,
             output_shapes=self._output_shapes,
             input_channels=tmp_in_channels,
-            output_channels=tmp_out_channels)
+            output_channels=tmp_out_channels,
+        )
 
     def _calculate_filters_importance_in_group(self, group: Cluster[PrunedLayerInfo]):
         """
         Calculates cumulative filters importance in the group.
         :param group: Nodes cluster
         :return a list of filter importance scores
         """
@@ -488,43 +510,46 @@
                 shared_nodes.add(layer_name)
             filters_importance = self._layer_filter_importance(self._model.get_layer(layer_name))
             cumulative_filters_importance += filters_importance
 
         return cumulative_filters_importance
 
     def _update_benchmark_statistics(self):
-        tmp_in_channels, tmp_out_channels = \
-            self._shape_pruning_proc.calculate_in_out_channels_by_masks(
-                graph=self._original_graph,
-                pruning_groups=self._pruned_layer_groups_info,
-                pruning_groups_next_nodes=self._next_nodes,
-                num_of_sparse_elements_by_node=self._calculate_num_of_sparse_elements_by_node())
+        tmp_in_channels, tmp_out_channels = self._shape_pruning_proc.calculate_in_out_channels_by_masks(
+            graph=self._original_graph,
+            pruning_groups=self._pruned_layer_groups_info,
+            pruning_groups_next_nodes=self._next_nodes,
+            num_of_sparse_elements_by_node=self._calculate_num_of_sparse_elements_by_node(),
+        )
 
         self.current_filters_num = self._weights_flops_calc.count_filters_num(
-            graph=self._original_graph,
-            output_channels=tmp_out_channels)
+            graph=self._original_graph, output_channels=tmp_out_channels
+        )
 
-        self.current_flops, self.current_params_num = \
-            self._weights_flops_calc.count_flops_and_weights(
-                graph=self._original_graph,
-                output_shapes=self._output_shapes,
-                input_channels=tmp_in_channels,
-                output_channels=tmp_out_channels)
+        self.current_flops, self.current_params_num = self._weights_flops_calc.count_flops_and_weights(
+            graph=self._original_graph,
+            output_shapes=self._output_shapes,
+            input_channels=tmp_in_channels,
+            output_channels=tmp_out_channels,
+        )
 
     def _layer_filter_importance(self, layer: NNCFWrapper):
         layer_metatype = get_keras_layer_metatype(layer)
         if len(layer_metatype.weight_definitions) != 1:
-            raise RuntimeError(f'The layer {layer.layer.name} does not support by the pruning '
-                               f'algorithm because it contains several weight attributes.')
+            raise RuntimeError(
+                f"The layer {layer.layer.name} does not support by the pruning "
+                f"algorithm because it contains several weight attributes."
+            )
         weight_attr = layer_metatype.weight_definitions[0].weight_attr_name
         weight = layer.layer_weights[weight_attr]
         if self.all_weights:
             weight = self._weights_normalizer(weight)
         target_weight_dim_for_compression = get_filter_axis(layer, weight_attr)
         filters_importance = self._filter_importance(weight, target_weight_dim_for_compression)
         return filters_importance
 
     def _run_batchnorm_adaptation(self):
         if self._bn_adaptation is None:
-            self._bn_adaptation = BatchnormAdaptationAlgorithm(**extract_bn_adaptation_init_params(self.config,
-                                                                                                   'filter_pruning'))
+            self._bn_adaptation = BatchnormAdaptationAlgorithm(
+                **extract_bn_adaptation_init_params(self.config, "filter_pruning")
+            )
         self._bn_adaptation.run(self.model)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/pruning/filter_pruning/functions.py` & `nncf-2.5.0/nncf/tensorflow/pruning/filter_pruning/functions.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 
 def l1_filter_norm(weight_tensor, dim=0):
     """
     Calculates L1 for weight_tensor for the selected dimension.
@@ -46,27 +44,29 @@
     Compute geometric median norm for filters.
     :param weight_tensor: tensor with weights
     :param dim: dimension of output channel
     :return: metric value for every weight from weights_tensor
     """
     weight_vec = _pull_tensor(weight_tensor, dim)
     square_norms = tf.reduce_sum(tf.square(weight_vec), axis=1, keepdims=True)
-    similar_matrix = tf.sqrt(tf.maximum(square_norms - 2 * tf.matmul(weight_vec, weight_vec, transpose_b=True)
-                                        + tf.transpose(square_norms),
-                                        0))
+    similar_matrix = tf.sqrt(
+        tf.maximum(
+            square_norms - 2 * tf.matmul(weight_vec, weight_vec, transpose_b=True) + tf.transpose(square_norms), 0
+        )
+    )
     similar_sum = tf.reduce_sum(similar_matrix, axis=0)
     return similar_sum
 
 
 def _l2_distance(x, y):
     return tf.sqrt(tf.reduce_sum(tf.square(x - y)))
 
 
 FILTER_IMPORTANCE_FUNCTIONS = {
-    'L2': l2_filter_norm,
-    'L1': l1_filter_norm,
-    'geometric_median': geometric_median_filter_norm
+    "L2": l2_filter_norm,
+    "L1": l1_filter_norm,
+    "geometric_median": geometric_median_filter_norm,
 }
 
 
 def calculate_binary_mask(weight_importance, threshold):
     return tf.cast(weight_importance >= threshold, tf.float32)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/pruning/operations.py` & `nncf-2.5.0/nncf/tensorflow/pruning/operations.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,120 +1,126 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict
-from typing import List
+from typing import Dict, List
 
-from nncf.tensorflow.graph.pattern_operations import KERAS_ACTIVATIONS_OPERATIONS
-from nncf.tensorflow.graph.pattern_operations import ELEMENTWISE_OPERATIONS
-from nncf.tensorflow.graph.pattern_operations import TF_ACTIVATIONS_OPERATIONS
-from nncf.tensorflow.graph.metatypes import keras_layers as layer_metatypes
-from nncf.tensorflow.graph.metatypes import tf_ops as op_metatypes
 from nncf.common.graph.definitions import NNCFGraphNodeType
+from nncf.common.pruning.operations import BatchNormPruningOp
+from nncf.common.pruning.operations import ConcatPruningOp
+from nncf.common.pruning.operations import ConvolutionPruningOp
+from nncf.common.pruning.operations import ElementwisePruningOp
+from nncf.common.pruning.operations import FlattenPruningOp
+from nncf.common.pruning.operations import IdentityMaskForwardPruningOp
+from nncf.common.pruning.operations import InputPruningOp
+from nncf.common.pruning.operations import LinearPruningOp
+from nncf.common.pruning.operations import OutputPruningOp
+from nncf.common.pruning.operations import ReshapePruningOp
+from nncf.common.pruning.operations import SplitPruningOp
+from nncf.common.pruning.operations import StopMaskForwardPruningOp
+from nncf.common.pruning.operations import TransposeConvolutionPruningOp
 from nncf.common.pruning.utils import PruningOperationsMetatypeRegistry
-from nncf.common.pruning.operations import (
-    InputPruningOp,
-    OutputPruningOp,
-    IdentityMaskForwardPruningOp,
-    ConvolutionPruningOp,
-    TransposeConvolutionPruningOp,
-    LinearPruningOp,
-    BatchNormPruningOp,
-    ConcatPruningOp,
-    ElementwisePruningOp,
-    ReshapePruningOp,
-    FlattenPruningOp,
-    StopMaskForwardPruningOp,
-    SplitPruningOp
-)
+from nncf.tensorflow.graph.metatypes import keras_layers as layer_metatypes
+from nncf.tensorflow.graph.metatypes import tf_ops as op_metatypes
+from nncf.tensorflow.graph.pattern_operations import ELEMENTWISE_OPERATIONS
+from nncf.tensorflow.graph.pattern_operations import KERAS_ACTIVATIONS_OPERATIONS
+from nncf.tensorflow.graph.pattern_operations import TF_ACTIVATIONS_OPERATIONS
 
 TF_PRUNING_OPERATOR_METATYPES = PruningOperationsMetatypeRegistry("operator_metatypes")
 
 
 def _get_types(operations_dict: Dict) -> List[str]:
-    return operations_dict['type']
+    return operations_dict["type"]
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('model_input')
+@TF_PRUNING_OPERATOR_METATYPES.register("model_input")
 class TFInputPruningOp(InputPruningOp):
-    additional_types = ['InputLayer', NNCFGraphNodeType.INPUT_NODE]
+    additional_types = ["InputLayer", NNCFGraphNodeType.INPUT_NODE]
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('model_output')
+@TF_PRUNING_OPERATOR_METATYPES.register("model_output")
 class TFOutputPruningOp(OutputPruningOp):
     additional_types = [NNCFGraphNodeType.OUTPUT_NODE]
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('identity_mask_propagation')
+@TF_PRUNING_OPERATOR_METATYPES.register("identity_mask_propagation")
 class TFIdentityMaskForwardPruningOp(IdentityMaskForwardPruningOp):
-    additional_types = _get_types(KERAS_ACTIVATIONS_OPERATIONS) + _get_types(TF_ACTIVATIONS_OPERATIONS) \
-                       + layer_metatypes.TFAveragePooling2DLayerMetatype.get_all_aliases() \
-                       + layer_metatypes.TFGlobalAveragePooling2DLayerMetatype.get_all_aliases() \
-                       + layer_metatypes.TFMaxPooling2DLayerMetatype.get_all_aliases() \
-                       + layer_metatypes.TFGlobalMaxPooling2DLayerMetatype.get_all_aliases() \
-                       + layer_metatypes.TFDropoutLayerMetatype.get_all_aliases() \
-                       + layer_metatypes.TFZeroPadding2DLayerMetatype.get_all_aliases() \
-                       + layer_metatypes.TFUpSampling2DLayerMetatype.get_all_aliases() \
-                       + op_metatypes.TFIdentityOpMetatype.get_all_aliases() \
-                       + op_metatypes.TFPadOpMetatype.get_all_aliases()
+    additional_types = (
+        _get_types(KERAS_ACTIVATIONS_OPERATIONS)
+        + _get_types(TF_ACTIVATIONS_OPERATIONS)
+        + layer_metatypes.TFAveragePooling2DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFGlobalAveragePooling2DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFMaxPooling2DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFGlobalMaxPooling2DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFZeroPadding2DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFUpSampling2DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFAveragePooling3DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFGlobalAveragePooling3DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFMaxPooling3DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFGlobalMaxPooling3DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFZeroPadding3DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFUpSampling3DLayerMetatype.get_all_aliases()
+        + layer_metatypes.TFDropoutLayerMetatype.get_all_aliases()
+        + op_metatypes.TFIdentityOpMetatype.get_all_aliases()
+        + op_metatypes.TFPadOpMetatype.get_all_aliases()
+    )
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('convolution')
+@TF_PRUNING_OPERATOR_METATYPES.register("convolution")
 class TFConvolutionPruningOp(ConvolutionPruningOp):
-    additional_types = ['Conv1D', 'Conv2D', 'Conv3D', 'DepthwiseConv2D']
+    additional_types = ["Conv1D", "Conv2D", "Conv3D", "DepthwiseConv2D"]
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('transpose_convolution')
+@TF_PRUNING_OPERATOR_METATYPES.register("transpose_convolution")
 class TFTransposeConvolutionPruningOp(TransposeConvolutionPruningOp):
-    additional_types = ['Conv1DTranspose', 'Conv2DTranspose', 'Conv3DTranspose']
+    additional_types = ["Conv1DTranspose", "Conv2DTranspose", "Conv3DTranspose"]
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('linear')
+@TF_PRUNING_OPERATOR_METATYPES.register("linear")
 class TFLinearPruningOp(LinearPruningOp):
-    additional_types = layer_metatypes.TFDenseLayerMetatype.get_all_aliases() \
-                       + op_metatypes.TFMatMulOpMetatype.get_all_aliases()
+    additional_types = (
+        layer_metatypes.TFDenseLayerMetatype.get_all_aliases() + op_metatypes.TFMatMulOpMetatype.get_all_aliases()
+    )
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('batch_norm')
+@TF_PRUNING_OPERATOR_METATYPES.register("batch_norm")
 class TFBatchNormPruningOp(BatchNormPruningOp):
-    additional_types = ['BatchNormalization', 'SyncBatchNormalization']
+    additional_types = ["BatchNormalization", "SyncBatchNormalization"]
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('elementwise')
+@TF_PRUNING_OPERATOR_METATYPES.register("elementwise")
 class TFElementwisePruningOp(ElementwisePruningOp):
     additional_types = _get_types(ELEMENTWISE_OPERATIONS)
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('reshape')
+@TF_PRUNING_OPERATOR_METATYPES.register("reshape")
 class TFReshapeOps(ReshapePruningOp):
     additional_types = op_metatypes.TFReshapeOpMetatype.get_all_aliases()
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('flatten')
+@TF_PRUNING_OPERATOR_METATYPES.register("flatten")
 class TFFlattenOps(FlattenPruningOp):
     additional_types = layer_metatypes.TFFlattenLayerMetatype.get_all_aliases()
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('stop_propagation_ops')
+@TF_PRUNING_OPERATOR_METATYPES.register("stop_propagation_ops")
 class TFStopMaskForwardPruningOp(StopMaskForwardPruningOp):
     additional_types = []
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('concat')
+@TF_PRUNING_OPERATOR_METATYPES.register("concat")
 class TFConcatPruningOp(ConcatPruningOp):
-    additional_types = layer_metatypes.TFConcatenateLayerMetatype.get_all_aliases() \
-                       + op_metatypes.TFConcatOpMetatype.get_all_aliases()
+    additional_types = (
+        layer_metatypes.TFConcatenateLayerMetatype.get_all_aliases() + op_metatypes.TFConcatOpMetatype.get_all_aliases()
+    )
 
 
-@TF_PRUNING_OPERATOR_METATYPES.register('split')
+@TF_PRUNING_OPERATOR_METATYPES.register("split")
 class TFSplitPruningOp(SplitPruningOp):
     additional_types = op_metatypes.TFSplitOpMetatype.get_all_aliases()
```

### Comparing `nncf-2.4.0/nncf/tensorflow/pruning/tensor_processor.py` & `nncf-2.5.0/nncf/tensorflow/pruning/tensor_processor.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List, Union
 
 import tensorflow as tf
 
 from nncf.common.pruning.tensor_processor import NNCFPruningBaseTensorProcessor
 from nncf.common.tensor import NNCFTensor
```

### Comparing `nncf-2.4.0/nncf/tensorflow/pruning/utils.py` & `nncf-2.5.0/nncf/tensorflow/pruning/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,63 +1,62 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Dict, List
 
 import numpy as np
 import tensorflow as tf
 
-from typing import List, Dict
-
-from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFGraph
+from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
-from nncf.tensorflow.graph.utils import unwrap_layer
-from nncf.tensorflow.graph.utils import get_original_name_and_instance_idx
 from nncf.tensorflow.graph.metatypes.common import GENERAL_CONV_LAYER_METATYPES
 from nncf.tensorflow.graph.metatypes.common import LINEAR_LAYER_METATYPES
-from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
 from nncf.tensorflow.graph.metatypes.keras_layers import TFBatchNormalizationLayerMetatype
+from nncf.tensorflow.graph.metatypes.matcher import get_keras_layer_metatype
+from nncf.tensorflow.graph.utils import get_original_name_and_instance_idx
+from nncf.tensorflow.graph.utils import unwrap_layer
 from nncf.tensorflow.layers.data_layout import get_input_channel_axis
 from nncf.tensorflow.layers.data_layout import get_weight_channel_axis
 from nncf.tensorflow.layers.wrapper import NNCFWrapper
 
 
 def is_shared(node: NNCFNode) -> bool:
-    return node.data['is_shared']
+    return node.data["is_shared"]
 
 
 def get_filter_axis(layer: NNCFWrapper, weight_attr: str) -> int:
     channel_axes = get_weight_channel_axis(layer, weight_attr)
     filter_axis = channel_axes[0] if isinstance(channel_axes, tuple) else channel_axes
     return filter_axis
 
 
 def get_filters_num(layer: NNCFWrapper):
     layer_metatype = get_keras_layer_metatype(layer)
     if len(layer_metatype.weight_definitions) != 1:
-        raise ValueError(f'Could not calculate the number of filters '
-                         f'for the layer {layer.layer.name}.')
+        raise ValueError(f"Could not calculate the number of filters " f"for the layer {layer.layer.name}.")
 
     weight_def = layer_metatype.weight_definitions[0]
     weight_attr = weight_def.weight_attr_name
 
     if layer_metatype is TFBatchNormalizationLayerMetatype and not layer.layer.scale:
-        nncf_logger.debug('Fused gamma parameter encountered in BatchNormalization layer. '
-                          'Using beta parameter instead to calculate the number of filters.')
-        weight_attr = 'beta'
+        nncf_logger.debug(
+            "Fused gamma parameter encountered in BatchNormalization layer. "
+            "Using beta parameter instead to calculate the number of filters."
+        )
+        weight_attr = "beta"
 
     filter_axis = get_filter_axis(layer, weight_attr)
     filters_num = layer.layer_weights[weight_attr].shape[filter_axis]
     return filters_num
 
 
 def is_valid_shape(shape):
@@ -72,44 +71,47 @@
     broadcasted_filter_mask = tf.zeros(shape)
     meta_shape = np.ones(len(shape), dtype=np.int64)
     meta_shape[dim] = filter_mask.shape[0]
     broadcasted_filter_mask += tf.reshape(filter_mask, tuple(meta_shape))
     return broadcasted_filter_mask
 
 
-def collect_output_shapes(model: 'NNCFNetwork', graph: NNCFGraph) -> Dict[NNCFNodeName, List[int]]:
+def collect_output_shapes(model: "NNCFNetwork", graph: NNCFGraph) -> Dict[NNCFNodeName, List[int]]:
     """
     Collects output dimension shapes for convolutions and fully connected layers
     from the connected edges in the NNCFGraph.
 
     :param graph: NNCFGraph.
     :return: Dictionary of output dimension shapes. E.g {node_name: (height, width)}
     """
     layers_out_shapes = {}
     for node in graph.get_nodes_by_metatypes(GENERAL_CONV_LAYER_METATYPES):
         node_name, node_index = get_original_name_and_instance_idx(node.node_name)
         layer = model.get_layer(node_name)
         layer_ = unwrap_layer(layer)
 
         channel_axis = get_input_channel_axis(layer)
-        dims_slice = slice(channel_axis - layer_.rank, channel_axis) \
-            if layer.data_format == 'channels_last' else slice(channel_axis + 1, None)
+        dims_slice = (
+            slice(channel_axis - layer_.rank, channel_axis)
+            if layer.data_format == "channels_last"
+            else slice(channel_axis + 1, None)
+        )
         in_shape = layer.get_input_shape_at(node_index)[dims_slice]
         out_shape = layer.get_output_shape_at(node_index)[dims_slice]
 
         if not is_valid_shape(in_shape) or not is_valid_shape(out_shape):
-            raise RuntimeError(f'Input/output shape is not defined for layer `{layer.name}` ')
+            raise RuntimeError(f"Input/output shape is not defined for layer `{layer.name}` ")
 
         layers_out_shapes[node.node_name] = out_shape
 
     for node in graph.get_nodes_by_metatypes(LINEAR_LAYER_METATYPES):
         node_name, node_index = get_original_name_and_instance_idx(node.node_name)
         layer = model.get_layer(node_name)
 
         in_shape = layer.get_input_shape_at(node_index)[1:]
         out_shape = layer.get_output_shape_at(node_index)[1:]
 
         if not is_valid_shape(in_shape) or not is_valid_shape(out_shape):
-            raise RuntimeError(f'Input/output shape is not defined for layer `{layer.name}` ')
+            raise RuntimeError(f"Input/output shape is not defined for layer `{layer.name}` ")
 
         layers_out_shapes[node.node_name] = out_shape
     return layers_out_shapes
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/__init__.py` & `nncf-2.5.0/nncf/common/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,16 +1,13 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Backend-agnostic or backend-common NNCF functionality.
 """
-# Required for correct registry functioning
-from nncf.tensorflow.quantization.layers import FakeQuantize
-from nncf.tensorflow.quantization.quantizers import AsymmetricQuantizer
-from nncf.tensorflow.quantization.quantizers import SymmetricQuantizer
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/algorithm.py` & `nncf-2.5.0/nncf/tensorflow/quantization/algorithm.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,19 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from copy import deepcopy
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Tuple
+from typing import Any, Dict, List, Tuple
 
 import tensorflow as tf
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
@@ -44,17 +39,18 @@
 from nncf.common.quantization.structs import QuantizationConstraints
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.common.quantization.structs import QuantizationPreset
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import QuantizerGroup
 from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.scopes import check_scopes_in_graph
-from nncf.common.scopes import should_consider_scope
 from nncf.common.stateful_classes_registry import TF_STATEFUL_CLASSES
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
+from nncf.common.utils.backend import copy_model
 from nncf.config.extractors import extract_range_init_params
 from nncf.config.schemata.defaults import QUANTIZATION_OVERFLOW_FIX
 from nncf.config.schemata.defaults import QUANTIZE_INPUTS
 from nncf.config.schemata.defaults import QUANTIZE_OUTPUTS
 from nncf.config.schemata.defaults import TARGET_DEVICE
 from nncf.tensorflow.algorithm_selector import TF_COMPRESSION_ALGORITHMS
 from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
@@ -84,30 +80,29 @@
 from nncf.tensorflow.quantization.layers import FakeQuantize
 from nncf.tensorflow.quantization.quantizers import Quantizer
 from nncf.tensorflow.quantization.quantizers import TFQuantizerSpec
 from nncf.tensorflow.quantization.utils import apply_overflow_fix
 
 QUANTIZATION_LAYER_METATYPES = GENERAL_CONV_LAYER_METATYPES + LINEAR_LAYER_METATYPES
 
-UNSUPPORTED_LAYER_METATYPES = [
-    TFLambdaLayerMetatype
-]
+UNSUPPORTED_LAYER_METATYPES = [TFLambdaLayerMetatype]
 
 
 class TFQuantizationPointStateNames:
-    QUANTIZER_SPEC = 'quantizer_spec'
-    TARGET_POINT = 'target_point'
-    TARGET_POINT_CLASS_NAME = 'target_point_class_name'
-    OP_NAME = 'op_name'
+    QUANTIZER_SPEC = "quantizer_spec"
+    TARGET_POINT = "target_point"
+    TARGET_POINT_CLASS_NAME = "target_point_class_name"
+    OP_NAME = "op_name"
 
 
 class TFQuantizationPoint:
     """
     Characterizes where and how to insert a single quantization node to the model's graph. Stores TF-specific data.
     """
+
     _state_names = TFQuantizationPointStateNames
 
     def __init__(self, op_name: str, quantizer_spec: TFQuantizerSpec, target_point: TargetPoint):
         self.target_point = target_point
         self.op_name = op_name
         self.quantizer_spec = quantizer_spec
 
@@ -125,42 +120,43 @@
 
         :return: state of the object
         """
         return {
             self._state_names.TARGET_POINT: self.target_point.get_state(),
             self._state_names.TARGET_POINT_CLASS_NAME: self.target_point.__class__.__name__,
             self._state_names.QUANTIZER_SPEC: self.quantizer_spec.get_state(),
-            self._state_names.OP_NAME: self.op_name
+            self._state_names.OP_NAME: self.op_name,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFQuantizationPoint':
+    def from_state(cls, state: Dict[str, Any]) -> "TFQuantizationPoint":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         target_point_cls = TF_STATEFUL_CLASSES.get_registered_class(state[cls._state_names.TARGET_POINT_CLASS_NAME])
         kwargs = {
             cls._state_names.TARGET_POINT: target_point_cls.from_state(state[cls._state_names.TARGET_POINT]),
             cls._state_names.QUANTIZER_SPEC: TFQuantizerSpec.from_state(state[cls._state_names.QUANTIZER_SPEC]),
-            cls._state_names.OP_NAME: state[cls._state_names.OP_NAME]
+            cls._state_names.OP_NAME: state[cls._state_names.OP_NAME],
         }
         return cls(**kwargs)
 
 
 class TFQuantizationSetupStateNames:
-    QUANTIZATION_POINTS = 'quantization_points'
-    UNIFIED_SCALE_GROUPS = 'unified_scale_groups'
+    QUANTIZATION_POINTS = "quantization_points"
+    UNIFIED_SCALE_GROUPS = "unified_scale_groups"
 
 
 class TFQuantizationSetup:
     """
     Characterizes where and how to insert all quantization nodes to the model's graph
     """
+
     _state_names = TFQuantizationSetupStateNames
 
     def __init__(self):
         super().__init__()
         self._quantization_points = []  # type: List[TFQuantizationPoint]
         self._unified_scale_groups = []  # type: List[List[QuantizationPointId]]
 
@@ -206,19 +202,19 @@
 
         :return: state of the object
         """
 
         quantization_points_state = [qp.get_state() for qp in self._quantization_points]
         return {
             self._state_names.QUANTIZATION_POINTS: quantization_points_state,
-            self._state_names.UNIFIED_SCALE_GROUPS: self._unified_scale_groups
+            self._state_names.UNIFIED_SCALE_GROUPS: self._unified_scale_groups,
         }
 
     @classmethod
-    def from_state(cls, state: Dict) -> 'TFQuantizationSetup':
+    def from_state(cls, state: Dict) -> "TFQuantizationSetup":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         setup = TFQuantizationSetup()
         for quantization_point_state in state[cls._state_names.QUANTIZATION_POINTS]:
@@ -228,35 +224,34 @@
         if cls._state_names.UNIFIED_SCALE_GROUPS in state:
             for quantization_group in state[cls._state_names.UNIFIED_SCALE_GROUPS]:
                 setup.register_unified_scale_group(quantization_group)
         return setup
 
 
 class QBuilderStateNames:
-    QUANTIZER_SETUP = 'quantizer_setup'
+    QUANTIZER_SETUP = "quantizer_setup"
 
 
-@TF_COMPRESSION_ALGORITHMS.register('quantization')
+@TF_COMPRESSION_ALGORITHMS.register("quantization")
 class QuantizationBuilder(TFCompressionAlgorithmBuilder):
     _state_names = QBuilderStateNames
 
-    DEFAULT_QCONFIG = QuantizerConfig(num_bits=8,
-                                      mode=QuantizationMode.SYMMETRIC,
-                                      signedness_to_force=None,
-                                      per_channel=False)
+    DEFAULT_QCONFIG = QuantizerConfig(
+        num_bits=8, mode=QuantizationMode.SYMMETRIC, signedness_to_force=None, per_channel=False
+    )
 
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
 
-        self.quantize_inputs = self._algo_config.get('quantize_inputs', QUANTIZE_INPUTS)
-        self.quantize_outputs = self._algo_config.get('quantize_outputs', QUANTIZE_OUTPUTS)
-        self._overflow_fix = self._algo_config.get('overflow_fix', QUANTIZATION_OVERFLOW_FIX)
-        self._target_device = config.get('target_device', TARGET_DEVICE)
+        self.quantize_inputs = self._algo_config.get("quantize_inputs", QUANTIZE_INPUTS)
+        self.quantize_outputs = self._algo_config.get("quantize_outputs", QUANTIZE_OUTPUTS)
+        self._overflow_fix = self._algo_config.get("overflow_fix", QUANTIZATION_OVERFLOW_FIX)
+        self._target_device = config.get("target_device", TARGET_DEVICE)
         algo_config = self._get_algo_specific_config_section()
-        if self._target_device == 'VPU' and 'preset' in algo_config:
+        if self._target_device == "VPU" and "preset" in algo_config:
             raise RuntimeError("The VPU target device does not support presets.")
 
         self.global_quantizer_constraints = {}
         self.ignored_scopes_per_group = {}
         self.target_scopes_per_group = {}
         self._op_names = []
 
@@ -303,93 +298,98 @@
         range_init_params = extract_range_init_params(self.config)
         return TFRangeInitParams(**range_init_params) if range_init_params is not None else None
 
     def _parse_group_params(self, quant_config: Dict, quantizer_group: QuantizerGroup) -> None:
         group_name = quantizer_group.value
         params_dict = {}
         params_dict_from_config = quant_config.get(group_name, {})
-        preset = quant_config.get('preset')
-        if self._target_device in ['ANY', 'CPU', 'GPU'] or self._target_device == 'TRIAL' and preset is not None:
-            preset = QuantizationPreset(quant_config.get('preset', 'performance'))
+        preset = quant_config.get("preset")
+        if self._target_device in ["ANY", "CPU", "GPU"] or self._target_device == "TRIAL" and preset is not None:
+            preset = QuantizationPreset(quant_config.get("preset", "performance"))
             params_dict = preset.get_params_configured_by_preset(quantizer_group)
             overriden_params = params_dict.keys() & params_dict_from_config.keys()
             if overriden_params:
-                nncf_logger.info(f'Preset quantizer parameters {overriden_params} explicitly overridden by config.')
+                nncf_logger.info(f"Preset quantizer parameters {overriden_params} explicitly overridden by config.")
         params_dict.update(params_dict_from_config)
         self.global_quantizer_constraints[quantizer_group] = QuantizationConstraints.from_config_dict(params_dict)
-        self.ignored_scopes_per_group[quantizer_group] = params_dict_from_config.get('ignored_scopes', [])
+        self.ignored_scopes_per_group[quantizer_group] = params_dict_from_config.get("ignored_scopes", [])
         if self.ignored_scopes is not None:
             self.ignored_scopes_per_group[quantizer_group] += self.ignored_scopes
-        target_scopes = params_dict_from_config.get('target_scopes')
+        target_scopes = params_dict_from_config.get("target_scopes")
         if target_scopes is None and self.target_scopes is not None:
             self.target_scopes_per_group[quantizer_group] = self.target_scopes
         else:
             self.target_scopes_per_group[quantizer_group] = target_scopes
 
     def _get_default_qconfig(self, constraints: QuantizationConstraints = None) -> QuantizerConfig:
         qconfig = deepcopy(self.DEFAULT_QCONFIG)
         if constraints is not None:
             qconfig = constraints.apply_constraints_to(qconfig)
         return qconfig
 
-    def _get_half_range(self, qconfig: QuantizerConfig, target_node: NNCFNode,
-                        first_conv_nodes: List[NNCFNode]) -> bool:
-        if self._target_device in ['CPU', 'ANY'] and qconfig.num_bits == 8:
-            if self._overflow_fix == 'enable':
+    def _get_half_range(
+        self, qconfig: QuantizerConfig, target_node: NNCFNode, first_conv_nodes: List[NNCFNode]
+    ) -> bool:
+        if self._target_device in ["CPU", "ANY"] and qconfig.num_bits == 8:
+            if self._overflow_fix == "enable":
                 return True
-            if self._overflow_fix == 'first_layer_only':
+            if self._overflow_fix == "first_layer_only":
                 if target_node in first_conv_nodes:
                     return True
         return False
 
     def _create_quantizer(self, name: str, qspec: TFQuantizerSpec) -> Quantizer:
         quantizer_cls = NNCF_QUANTIZATION_OPERATIONS.get(qspec.mode)
         return quantizer_cls(name, qspec)
 
-    def _build_insertion_commands_for_quantizer_setup(self,
-                                                      quantizer_setup: TFQuantizationSetup) \
-            -> List[TFInsertionCommand]:
+    def _build_insertion_commands_for_quantizer_setup(
+        self, quantizer_setup: TFQuantizationSetup
+    ) -> List[TFInsertionCommand]:
         insertion_commands = []
         quantization_points = quantizer_setup.get_quantization_points()
         non_unified_scales_quantization_point_ids = set(range(len(quantization_points)))
 
         for unified_scales_group in quantizer_setup.get_unified_scale_groups():
             us_qp_id = unified_scales_group[0]
             qp = quantization_points[us_qp_id]
             quantizer_spec = qp.quantizer_spec
-            op_name = qp.op_name + '/unified_scale_group'
+            op_name = qp.op_name + "/unified_scale_group"
             quantizer = FakeQuantize(quantizer_spec, name=op_name)
             self._op_names.append(quantizer.op_name)
             target_points = []
             for us_qp_id in unified_scales_group:
                 non_unified_scales_quantization_point_ids.discard(us_qp_id)
                 qp = quantization_points[us_qp_id]
                 assert quantizer_spec.get_state() == qp.quantizer_spec.get_state()
                 target_points.append(qp.target_point)
 
-            command = TFInsertionCommand(target_point=TFMultiLayerPoint(target_points),
-                                         callable_object=quantizer,
-                                         priority=TransformationPriority.QUANTIZATION_PRIORITY)
+            command = TFInsertionCommand(
+                target_point=TFMultiLayerPoint(target_points),
+                callable_object=quantizer,
+                priority=TransformationPriority.QUANTIZATION_PRIORITY,
+            )
 
             insertion_commands.append(command)
 
         for qp_id in non_unified_scales_quantization_point_ids:
             quantization_point = quantization_points[qp_id]
             op_name = quantization_point.op_name
             quantizer_spec = quantization_point.quantizer_spec
             target_point = quantization_point.target_point
             if quantization_point.is_weight_quantization():
                 quantizer = self._create_quantizer(op_name, quantizer_spec)
                 self._op_names.append(op_name)
             else:
                 quantizer = FakeQuantize(quantizer_spec, name=op_name)
                 self._op_names.append(quantizer.op_name)
-            command = TFInsertionCommand(target_point=target_point,
-                                         callable_object=quantizer,
-                                         priority=TransformationPriority.QUANTIZATION_PRIORITY)
+            command = TFInsertionCommand(
+                target_point=target_point,
+                callable_object=quantizer,
+                priority=TransformationPriority.QUANTIZATION_PRIORITY,
+            )
             insertion_commands.append(command)
         return insertion_commands
 
     def get_transformation_layout(self, model: tf.keras.Model) -> TFTransformationLayout:
         transformations = TFTransformationLayout()
         if self._quantizer_setup is None:
             self._quantizer_setup = self._get_quantizer_setup(model)
@@ -405,15 +405,15 @@
             if metatype in OUTPUT_NOOP_METATYPES:
                 continue
             is_custom, _ = converter.get_layer_info_for_node(node.node_name)
             if is_custom:
                 retval.append(node.node_name)
         return retval
 
-    def _build_controller(self, model: tf.keras.Model) -> 'QuantizationController':
+    def _build_controller(self, model: tf.keras.Model) -> "QuantizationController":
         return QuantizationController(model, self.config, self._op_names)
 
     def initialize(self, model: tf.keras.Model) -> None:
         if self._range_init_params is not None:
             self._run_range_initialization(model)
         if self._bn_adapt_params is not None:
             self._run_batchnorm_adaptation(model)
@@ -421,97 +421,96 @@
     def _run_range_initialization(self, model: tf.keras.Model) -> None:
         if self._range_initializer is None:
             self._range_initializer = RangeInitializer(self._range_init_params)
         self._range_initializer.run(model)
 
     def _run_batchnorm_adaptation(self, model: tf.keras.Model) -> None:
         if self._bn_adaptation is None:
-            self._bn_adaptation = BatchnormAdaptationAlgorithm(self._bn_adapt_params['data_loader'],
-                                                               self._bn_adapt_params['num_bn_adaptation_samples'],
-                                                               self._bn_adapt_params['device'])
+            self._bn_adaptation = BatchnormAdaptationAlgorithm(
+                self._bn_adapt_params["data_loader"],
+                self._bn_adapt_params["num_bn_adaptation_samples"],
+                self._bn_adapt_params["device"],
+            )
         self._bn_adaptation.run(model)
 
     def _get_quantizer_setup(self, model: tf.keras.Model) -> TFQuantizationSetup:
         converter = TFModelConverterFactory.create(model)
         nncf_graph = converter.convert()
 
         check_scopes_in_graph(nncf_graph, self.ignored_scopes, self.target_scopes)
 
         self._raise_not_supported_warning(nncf_graph)
 
         quantizable_weighted_layer_nodes = self._get_quantizable_weighted_layer_nodes(nncf_graph)
         custom_layer_nodes = self._get_custom_layer_node_names(nncf_graph, converter)
 
-        quantizer_setup = self._get_quantizer_propagation_solution(nncf_graph,
-                                                                   quantizable_weighted_layer_nodes,
-                                                                   custom_layer_nodes,
-                                                                   model)
+        quantizer_setup = self._get_quantizer_propagation_solution(
+            nncf_graph, quantizable_weighted_layer_nodes, custom_layer_nodes, model
+        )
         setup = TFQuantizationSetup()
 
         quantized_layer_names_vs_qconfigs = {}  # type: Dict[str, QuantizerConfig]
         qp_id_to_index = {}  # type: Dict[QuantizationPointId, int]
         tf_setup_qp_index = 0
         applied_overflow_fix = False
-        first_conv_nodes = get_first_nodes_of_type(nncf_graph, ['Conv2D'])
+        first_conv_nodes = get_first_nodes_of_type(nncf_graph, ["Conv2D", "Conv3D"])
         for qp_id, qp in quantizer_setup.quantization_points.items():
             if qp.is_weight_quantization_point():
                 target_node = nncf_graph.get_node_by_name(qp.insertion_point.target_node_name)
                 is_custom, layer_info = converter.get_layer_info_for_node(target_node.node_name)
                 if is_custom:
                     raise RuntimeError("Quantizing custom layer weights is currently unsupported!")
                 layer_name = layer_info.layer_name
                 qconfig = qp.qconfig
                 if layer_name in quantized_layer_names_vs_qconfigs:
                     assigned_qconfig = quantized_layer_names_vs_qconfigs[layer_name]
                     if qconfig != assigned_qconfig:
-                        raise RuntimeError(f"Inconsistent quantizer configurations selected by solver for one and the "
-                                           f"same quantizable layer! Tried to assign {qconfig} to {layer_name} as "
-                                           f"specified by QP {qp_id}, but the layer already has quantizer "
-                                           f"config {assigned_qconfig} assigned to it!")
+                        raise RuntimeError(
+                            f"Inconsistent quantizer configurations selected by solver for one and the "
+                            f"same quantizable layer! Tried to assign {qconfig} to {layer_name} as "
+                            f"specified by QP {qp_id}, but the layer already has quantizer "
+                            f"config {assigned_qconfig} assigned to it!"
+                        )
                     continue  # The layer has already been quantized
                 quantized_layer_names_vs_qconfigs[layer_name] = qconfig
                 metatype = target_node.metatype
                 assert issubclass(metatype, TFLayerWithWeightsMetatype)
                 for weight_def in metatype.weight_definitions:
-                    op_name = self._get_quantizer_operation_name(
-                        target_node.node_name,
-                        weight_def.weight_attr_name)
+                    op_name = self._get_quantizer_operation_name(target_node.node_name, weight_def.weight_attr_name)
                     self._op_names.append(op_name)
 
                     half_range = self._get_half_range(qconfig, target_node, first_conv_nodes)
                     applied_overflow_fix = applied_overflow_fix or half_range
-                    quantizer_spec = TFQuantizerSpec.from_config(qconfig,
-                                                                 narrow_range=not half_range,
-                                                                 half_range=half_range)
+                    quantizer_spec = TFQuantizerSpec.from_config(
+                        qconfig, narrow_range=not half_range, half_range=half_range
+                    )
                     target_point = TFLayerWeight(layer_info.layer_name, weight_def.weight_attr_name)
                     qpoint = TFQuantizationPoint(op_name, quantizer_spec, target_point)
             else:
                 assert qp.is_activation_quantization_point()
                 ip = qp.insertion_point
                 assert isinstance(ip, ActivationQuantizationInsertionPoint)
                 target_node_name = ip.target_node_name
                 input_port_id = ip.input_port_id
                 fake_quantize_name = self._get_fake_quantize_name(target_node_name, input_port_id)
                 quantizer_spec = TFQuantizerSpec.from_config(qp.qconfig, narrow_range=False, half_range=False)
-                fake_quantize_layer = FakeQuantize(
-                    quantizer_spec,
-                    name=fake_quantize_name)
+                fake_quantize_layer = FakeQuantize(quantizer_spec, name=fake_quantize_name)
                 self._op_names.append(fake_quantize_layer.op_name)
 
                 is_custom, layer_info = converter.get_layer_info_for_node(target_node_name)
                 if is_custom:
                     raise RuntimeError("Quantizing custom layer activations is currently unsupported!")
                 if input_port_id is not None:
-                    target_point = TFBeforeLayer(layer_info.layer_name,
-                                                 instance_idx=layer_info.instance_idx,
-                                                 input_port_id=input_port_id)
+                    target_point = TFBeforeLayer(
+                        layer_info.layer_name, instance_idx=layer_info.instance_idx, input_port_id=input_port_id
+                    )
                 else:
-                    target_point = TFAfterLayer(layer_info.layer_name,
-                                                instance_idx=layer_info.instance_idx,
-                                                output_port_id=0)
+                    target_point = TFAfterLayer(
+                        layer_info.layer_name, instance_idx=layer_info.instance_idx, output_port_id=0
+                    )
                 qpoint = TFQuantizationPoint(fake_quantize_name, quantizer_spec, target_point)
 
             setup.add_quantization_point(qpoint)
             qp_id_to_index[qp_id] = tf_setup_qp_index
             tf_setup_qp_index += 1
 
         setup = self._generate_unified_scale_groups(model, quantizer_setup, qp_id_to_index, setup)
@@ -519,34 +518,37 @@
         self._log_if_overflow_fix_was_applied(applied_overflow_fix)
 
         return setup
 
     def _raise_not_supported_warning(self, graph: NNCFGraph) -> None:
         for node in graph.get_all_nodes():
             if node.metatype in UNSUPPORTED_LAYER_METATYPES:
-                nncf_logger.warning(f'Layer '
-                                    f'{get_original_name_and_instance_idx(node.node_name)[0]} '
-                                    f'is not supported by the quantization algorithm')
-
+                nncf_logger.warning(
+                    f"Layer "
+                    f"{get_original_name_and_instance_idx(node.node_name)[0]} "
+                    f"is not supported by the quantization algorithm"
+                )
 
     def _log_if_overflow_fix_was_applied(self, applied_overflow_fix: bool):
         if applied_overflow_fix:
-            if self._overflow_fix == 'enable':
-                quantizers_with_overflow_fix_str = 'all weight quantizers'
-            elif self._overflow_fix == 'first_layer_only':
-                quantizers_with_overflow_fix_str = 'first convolution weight quantizers'
-            elif self._overflow_fix != 'disable':
+            if self._overflow_fix == "enable":
+                quantizers_with_overflow_fix_str = "all weight quantizers"
+            elif self._overflow_fix == "first_layer_only":
+                quantizers_with_overflow_fix_str = "first convolution weight quantizers"
+            elif self._overflow_fix != "disable":
                 raise RuntimeError(f"Unknown overflow fix type: {self._overflow_fix}")
-            nncf_logger.info(f'Overflow issue fix was applied to {quantizers_with_overflow_fix_str}.')
+            nncf_logger.info(f"Overflow issue fix was applied to {quantizers_with_overflow_fix_str}.")
 
-    def _generate_unified_scale_groups(self,
-                                       model: tf.keras.Model,
-                                       quantizer_setup: SingleConfigQuantizerSetup,
-                                       qp_id_to_index: Dict[QuantizationPointId, int],
-                                       setup: TFQuantizationSetup) -> TFQuantizationSetup:
+    def _generate_unified_scale_groups(
+        self,
+        model: tf.keras.Model,
+        quantizer_setup: SingleConfigQuantizerSetup,
+        qp_id_to_index: Dict[QuantizationPointId, int],
+        setup: TFQuantizationSetup,
+    ) -> TFQuantizationSetup:
         # To properly set the instance indices for FQ need to save layers order like in the model config
         layer_names = [layer.name for layer in model.layers]
         for unified_group in quantizer_setup.unified_scale_groups.values():
             sorted_unified_group = []
             for qp_id in unified_group:
                 qp = quantizer_setup.quantization_points[qp_id]
                 qp_layer_name = qp.insertion_point.target_node_name
@@ -562,76 +564,83 @@
     def _get_quantizable_weighted_layer_nodes(self, nncf_graph: NNCFGraph) -> List[QuantizableWeightedLayerNode]:
         nodes_with_weights = []
         for node in nncf_graph.get_all_nodes():
             metatype = node.metatype
             if metatype in OUTPUT_NOOP_METATYPES:
                 continue
 
-            if not (metatype in QUANTIZATION_LAYER_METATYPES
-                    and should_consider_scope(node.node_name,
-                                              ignored_scopes=self.ignored_scopes_per_group[QuantizerGroup.WEIGHTS],
-                                              target_scopes=None)):
+            if not metatype in QUANTIZATION_LAYER_METATYPES:
                 continue
 
-            assert issubclass(metatype, TFLayerWithWeightsMetatype) or \
-                issubclass(metatype, TFOpWithWeightsMetatype)
+            assert issubclass(metatype, TFLayerWithWeightsMetatype) or issubclass(metatype, TFOpWithWeightsMetatype)
             nodes_with_weights.append(node)
-        scope_overrides_dict = self._get_algo_specific_config_section().get('scope_overrides', {})
-        weighted_node_and_qconf_lists = assign_qconfig_lists_to_modules(nodes_with_weights,
-                                                                        self.DEFAULT_QCONFIG,
-                                                                        self.global_quantizer_constraints[
-                                                                            QuantizerGroup.WEIGHTS],
-                                                                        scope_overrides_dict,
-                                                                        hw_config=self.hw_config)
-        return [QuantizableWeightedLayerNode(node, qconf_list) for node, qconf_list
-                in weighted_node_and_qconf_lists.items()]
-
-    def _get_quantizer_propagation_solution(self, nncf_graph: NNCFGraph,
-                                            quantizable_weighted_layer_nodes: List[QuantizableWeightedLayerNode],
-                                            custom_layer_node_names: List[NNCFNodeName],
-                                            model: tf.keras.Model) \
-            -> SingleConfigQuantizerSetup:
-        ip_graph = InsertionPointGraph(nncf_graph,
-                                       [qn.node.node_name for qn in quantizable_weighted_layer_nodes])
+        scope_overrides_dict = self._get_algo_specific_config_section().get("scope_overrides", {})
+        weighted_node_and_qconf_lists = assign_qconfig_lists_to_modules(
+            nodes_with_weights,
+            self.DEFAULT_QCONFIG,
+            self.global_quantizer_constraints[QuantizerGroup.WEIGHTS],
+            scope_overrides_dict,
+            hw_config=self.hw_config,
+        )
+        return [
+            QuantizableWeightedLayerNode(node, qconf_list) for node, qconf_list in weighted_node_and_qconf_lists.items()
+        ]
+
+    def _get_quantizer_propagation_solution(
+        self,
+        nncf_graph: NNCFGraph,
+        quantizable_weighted_layer_nodes: List[QuantizableWeightedLayerNode],
+        custom_layer_node_names: List[NNCFNodeName],
+        model: tf.keras.Model,
+    ) -> SingleConfigQuantizerSetup:
+        ip_graph = InsertionPointGraph(nncf_graph, [qn.node.node_name for qn in quantizable_weighted_layer_nodes])
 
         pattern = TF_HW_FUSED_PATTERNS.get_full_pattern_graph()
         ip_graph = ip_graph.get_ip_graph_with_merged_hw_optimized_operations(pattern)
 
         input_preprocessing_nodes = self._get_input_preprocessing_nodes(nncf_graph, model)
         input_preprocessing_node_names = [n.node_name for n in input_preprocessing_nodes]
         if custom_layer_node_names:
             custom_layer_node_names_str = ", ".join([str(l) for l in custom_layer_node_names])
             nncf_logger.warning(
-                f'Following custom layers will be ignored during quantization (custom layer quantization not supported '
-                f'by NNCF yet):\n[{custom_layer_node_names_str}]')
-        ignored_scopes_for_solver = self.ignored_scopes_per_group[QuantizerGroup.ACTIVATIONS] + \
-                                    input_preprocessing_node_names + custom_layer_node_names
-
+                f"Following custom layers will be ignored during quantization (custom layer quantization not supported "
+                f"by NNCF yet):\n[{custom_layer_node_names_str}]"
+            )
+        ignored_scopes_for_solver = (
+            self.ignored_scopes_per_group[QuantizerGroup.ACTIVATIONS]
+            + input_preprocessing_node_names
+            + custom_layer_node_names
+        )
         solver = QuantizerPropagationSolver(
-            ignored_scopes=ignored_scopes_for_solver,
-            target_scopes=self.target_scopes_per_group[QuantizerGroup.ACTIVATIONS],
+            activation_ignored_scopes=ignored_scopes_for_solver,
+            weight_ignored_scopes=self.ignored_scopes_per_group[QuantizerGroup.WEIGHTS],
+            activation_target_scopes=self.target_scopes_per_group[QuantizerGroup.ACTIVATIONS],
+            weight_target_scopes=self.target_scopes_per_group[QuantizerGroup.WEIGHTS],
             hw_config=self.hw_config,
             default_trait_to_metatype_map=DEFAULT_TF_QUANT_TRAIT_TO_OP_DICT,
-            default_qconfig_list=[self._get_default_qconfig(
-                self.global_quantizer_constraints[QuantizerGroup.ACTIVATIONS])],
+            default_qconfig_list=[
+                self._get_default_qconfig(self.global_quantizer_constraints[QuantizerGroup.ACTIVATIONS])
+            ],
             quantizable_layer_nodes=quantizable_weighted_layer_nodes,
             global_constraints=self.global_quantizer_constraints,
-            quantize_outputs=self.quantize_outputs)
+            quantize_outputs=self.quantize_outputs,
+        )
 
         quantization_proposal = solver.run_on_ip_graph(ip_graph)
         multi_config_setup = quantization_proposal.quantizer_setup
         single_config_setup = multi_config_setup.select_first_qconfig_for_each_point()
         finalized_proposal = quantization_proposal.finalize(single_config_setup)
         final_setup = solver.get_final_quantizer_setup(finalized_proposal)
         final_setup = self._handle_quantize_inputs_option(final_setup, nncf_graph)
 
         return final_setup
 
-    def _handle_quantize_inputs_option(self, quantizer_setup: SingleConfigQuantizerSetup,
-                                       nncf_graph: NNCFGraph) -> SingleConfigQuantizerSetup:
+    def _handle_quantize_inputs_option(
+        self, quantizer_setup: SingleConfigQuantizerSetup, nncf_graph: NNCFGraph
+    ) -> SingleConfigQuantizerSetup:
         qp_ids_to_discard = []
         for qp_id, qp in quantizer_setup.quantization_points.items():
             if qp.is_activation_quantization_point():
                 insertion_point = qp.insertion_point
                 target_node = nncf_graph.get_node_by_name(insertion_point.target_node_name)
                 if not self.quantize_inputs and target_node.metatype in INPUT_NOOP_METATYPES:
                     qp_ids_to_discard.append(qp_id)
@@ -649,15 +658,15 @@
                 successor = next(iter(successors))
                 # It is necessary to determine the number of input nodes from the model
                 # in order to correctly count the duplicated edges
                 original_name, _ = get_original_name_and_instance_idx(successor.node_name)
                 layer = model.get_layer(name=original_name)
 
                 num_previous_nodes = len(layer.input) if isinstance(layer.input, list) else 1
-                if layer.__class__.__name__ in ['TFOpLambda', 'SlicingOpLambda']:
+                if layer.__class__.__name__ in ["TFOpLambda", "SlicingOpLambda"]:
                     num_previous_nodes = 0
                     for inbound_node in layer.inbound_nodes:
                         num_previous_nodes += len(inbound_node.keras_inputs)
 
                 if successor.metatype in ELEMENTWISE_LAYER_METATYPES and num_previous_nodes == 1:
                     preprocessing_nodes.append(successor)
                     is_finished = False
@@ -667,48 +676,59 @@
             preprocessing_nodes_for_this_input = nncf_graph.traverse_graph(nncf_node, traverse_fn)
             retval += preprocessing_nodes_for_this_input
 
         return retval
 
     def _get_fake_quantize_name(self, node_name: NNCFNodeName, input_port_id: int = None) -> str:
         original_node_name, instance_idx = get_original_name_and_instance_idx(node_name)
-        fq_name = '{}/fake_quantize'.format(original_node_name)
+        fq_name = "{}/fake_quantize".format(original_node_name)
         if instance_idx != 0:
             fq_name += f"_{instance_idx}"
         if input_port_id is not None:
             fq_name += f"_I{input_port_id}"
         return fq_name
 
     def _get_quantizer_operation_name(self, layer_name, weight_attr_name):
-        return f'{layer_name}_{weight_attr_name}_quantizer'
+        return f"{layer_name}_{weight_attr_name}_quantizer"
 
 
+@api()
 class QuantizationController(BaseCompressionAlgorithmController):
-    def __init__(self, target_model, config, op_names: List[str]):
+    """
+    Controller for the quantization algorithm in TensorFlow.
+    """
+
+    def __init__(self, target_model, config: NNCFConfig, op_names: List[str]):
         super().__init__(target_model)
         self._scheduler = BaseCompressionScheduler()
         self._loss = TFZeroCompressionLoss()
         self._op_names = op_names
         self._config = config
 
     @property
     def scheduler(self) -> CompressionScheduler:
         return self._scheduler
 
     @property
     def loss(self) -> CompressionLoss:
+        """
+        Returns the loss that is always zero since the quantization algorithm is driven by the original loss and does
+        not require additional losses.
+        """
         return self._loss
 
-    def strip_model(self, model: tf.keras.Model) -> tf.keras.Model:
+    def strip_model(self, model: tf.keras.Model, do_copy: bool = False) -> tf.keras.Model:
+        if do_copy:
+            model = copy_model(model)
         apply_overflow_fix(model, self._op_names)
         return model
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         collector = TFQuantizationStatisticsCollector(self.model, self._op_names)
         stats = collector.collect()
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('quantization', stats)
+        nncf_stats.register("quantization", stats)
         return nncf_stats
 
     def compression_stage(self) -> CompressionStage:
         return CompressionStage.FULLY_COMPRESSED
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/collectors.py` & `nncf-2.5.0/nncf/tensorflow/quantization/collectors.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,29 +1,27 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List, Tuple
 
 import tensorflow as tf
 
-from nncf.common.quantization.structs import QuantizationMode
-from nncf.common.quantization.collectors import QuantizerDescription
 from nncf.common.quantization.collectors import QuantizationStatisticsCollector
-from nncf.tensorflow.quantization.utils import collect_fake_quantize_layers
+from nncf.common.quantization.collectors import QuantizerDescription
+from nncf.common.quantization.structs import QuantizationMode
 from nncf.tensorflow.graph.utils import get_nncf_operations
+from nncf.tensorflow.quantization.utils import collect_fake_quantize_layers
 
 
 class TFQuantizationStatisticsCollector(QuantizationStatisticsCollector):
     """
     Implementation of the quantization statistics collector for the TensorFlow backend.
     """
 
@@ -43,43 +41,31 @@
         Collects descriptions of the quantizers.
 
         :return: Descriptions of the quantizers.
         """
         quantizers_descriptions = []
 
         for wrapped_layer, _, op in get_nncf_operations(self._model, self._operation_names):
-            is_symmetric = (op.mode == QuantizationMode.SYMMETRIC)
+            is_symmetric = op.mode == QuantizationMode.SYMMETRIC
 
             is_signed = True
             if is_symmetric:
                 operation_weights = wrapped_layer.get_operation_weights(op.name)
                 is_signed = op.signed(operation_weights)
 
             quantizers_descriptions.append(
-                QuantizerDescription(
-                    op.num_bits,
-                    op.per_channel,
-                    is_signed,
-                    is_symmetric,
-                    True,
-                    op.enabled
-                )
+                QuantizerDescription(op.num_bits, op.per_channel, is_signed, is_symmetric, True, op.enabled)
             )
 
         for fq_layer in collect_fake_quantize_layers(self._model):
-            is_symmetric = (fq_layer.mode == QuantizationMode.SYMMETRIC)
+            is_symmetric = fq_layer.mode == QuantizationMode.SYMMETRIC
 
             quantizers_descriptions.append(
                 QuantizerDescription(
-                    fq_layer.num_bits,
-                    fq_layer.per_channel,
-                    fq_layer.signed,
-                    is_symmetric,
-                    False,
-                    fq_layer.enabled
+                    fq_layer.num_bits, fq_layer.per_channel, fq_layer.signed, is_symmetric, False, fq_layer.enabled
                 )
             )
 
         return quantizers_descriptions
 
     def _get_potential_quantizers_num(self) -> Tuple[int, int]:
         """
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/default_quantization.py` & `nncf-2.5.0/nncf/tensorflow/quantization/default_quantization.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from nncf.common.graph.operator_metatypes import UnknownMetatype
 from nncf.common.quantization.quantizer_propagation.structs import QuantizationTrait
+from nncf.tensorflow.graph.metatypes import common
 from nncf.tensorflow.graph.metatypes import keras_layers as layer_metatypes
 from nncf.tensorflow.graph.metatypes import tf_ops as op_metatypes
-from nncf.tensorflow.graph.metatypes import common
-from nncf.common.graph.operator_metatypes import UnknownMetatype
 
 # If there are no some metatypes it means that they are considered as QuantizationTrait.QuantizationAgnostic
 
 DEFAULT_TF_QUANT_TRAIT_TO_OP_DICT = {
     QuantizationTrait.INPUTS_QUANTIZABLE: [
         *common.GENERAL_CONV_LAYER_METATYPES,
         *common.DEPTHWISE_CONV_LAYER_METATYPES,
@@ -48,19 +46,25 @@
         op_metatypes.TFMeanOpMetatype,
         op_metatypes.TFResizeNearestNeighborOpMetatype,
         op_metatypes.TFEluOpMetatype,
         op_metatypes.TFLeakyReluOpMetatype,
         op_metatypes.TFRelu6OpMetatype,
         op_metatypes.TFBatchMatMulV2OpMetatype,
     ],
-    QuantizationTrait.NON_QUANTIZABLE: [layer_metatypes.TFSoftmaxLayerMetatype,
-                                        op_metatypes.TFSigmoidOpMetatype,
-                                        op_metatypes.TFExpOpMetatype,
-                                        op_metatypes.TFLogOpMetatype,
-                                        op_metatypes.TFSoftmaxOpMetatype,
-                                        UnknownMetatype],
+    QuantizationTrait.NON_QUANTIZABLE: [
+        layer_metatypes.TFSoftmaxLayerMetatype,
+        op_metatypes.TFSigmoidOpMetatype,
+        op_metatypes.TFSoftmaxOpMetatype,
+        UnknownMetatype,
+        # Ticket: 108478
+        op_metatypes.TFReluOpMetatype,
+        op_metatypes.TFAbsOpMetatype,
+        op_metatypes.TFExpOpMetatype,
+        op_metatypes.TFLogOpMetatype,
+        op_metatypes.TFSqrtOpMetatype,
+    ],
     QuantizationTrait.CONCAT: [
         layer_metatypes.TFConcatenateLayerMetatype,
         op_metatypes.TFConcatOpMetatype,
     ],
-    QuantizationTrait.OUTPUT_QUANTIZATION_AS_WEIGHTS: [layer_metatypes.TFEmbeddingLayerMetatype]
+    QuantizationTrait.OUTPUT_QUANTIZATION_AS_WEIGHTS: [layer_metatypes.TFEmbeddingLayerMetatype],
 }
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/functions.py` & `nncf-2.5.0/nncf/tensorflow/quantization/functions.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,55 +1,42 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 
-def symmetric_quantize(inputs,
-                       scale_var,
-                       signed_var,
-                       num_bits,
-                       per_channel,
-                       narrow_range,
-                       eps,
-                       name_prefix='SymmQuant'):
+def symmetric_quantize(
+    inputs, scale_var, signed_var, num_bits, per_channel, narrow_range, eps, name_prefix="SymmQuant"
+):
     with tf.name_scope(name_prefix):
         scale_safe = tf.abs(scale_var) + eps
         min_var = scale_safe * signed_var
         max_var = scale_safe
-        return _fake_quant_with_min_max_vars(inputs, min_var, max_var, num_bits,
-                                             narrow_range, per_channel)
+        return _fake_quant_with_min_max_vars(inputs, min_var, max_var, num_bits, narrow_range, per_channel)
 
 
-def asymmetric_quantize(inputs,
-                        input_low,
-                        input_range,
-                        num_bits,
-                        per_channel,
-                        narrow_range,
-                        eps,
-                        name_prefix='AsymmQuant'):
+def asymmetric_quantize(
+    inputs, input_low, input_range, num_bits, per_channel, narrow_range, eps, name_prefix="AsymmQuant"
+):
     with tf.name_scope(name_prefix):
         input_range_safe = tf.abs(input_range) + eps
         min_var = input_low
         max_var = input_low + input_range_safe
-        return _fake_quant_with_min_max_vars(inputs, min_var, max_var, num_bits,
-                                             narrow_range, per_channel)
+        return _fake_quant_with_min_max_vars(inputs, min_var, max_var, num_bits, narrow_range, per_channel)
 
 
-def _fake_quant_with_min_max_vars(inputs, min_var, max_var, num_bits, narrow_range,
-                                  per_channel):
+def _fake_quant_with_min_max_vars(inputs, min_var, max_var, num_bits, narrow_range, per_channel):
     if per_channel:
         return tf.quantization.fake_quant_with_min_max_vars_per_channel(
-            inputs, min_var, max_var, num_bits=num_bits, narrow_range=narrow_range)
+            inputs, min_var, max_var, num_bits=num_bits, narrow_range=narrow_range
+        )
     return tf.quantization.fake_quant_with_min_max_vars(
-        inputs, min_var, max_var, num_bits=num_bits, narrow_range=narrow_range)
+        inputs, min_var, max_var, num_bits=num_bits, narrow_range=narrow_range
+    )
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/init_range.py` & `nncf-2.5.0/nncf/tensorflow/quantization/init_range.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import math
 from copy import deepcopy
 from itertools import islice
 from typing import List
 
 import numpy as np
@@ -51,153 +49,157 @@
         if self.global_init_config is not None:
             steps.append(self.global_init_config.num_init_samples)
         for pl_config in self.per_layer_range_init_configs:
             steps.append(pl_config.num_init_samples)
         batch_size = self.init_range_data_loader.batch_size
         return math.ceil(max(steps) / batch_size)
 
-    def get_init_config_for_quantization_point(self, layer: tf.keras.layers.Layer,
-                                               input_type: str) -> RangeInitConfig:
+    def get_init_config_for_quantization_point(self, layer: tf.keras.layers.Layer, input_type: str) -> RangeInitConfig:
         if input_type == InputType.WEIGHTS:
             node_name = layer.name
             group = QuantizerGroup.WEIGHTS
         else:
-            node_name = layer.name.replace('/fake_quantize', '')
+            node_name = layer.name.replace("/fake_quantize", "")
             group = QuantizerGroup.ACTIVATIONS
         return self.get_init_config_for_scope_and_group(node_name, group)
 
     def get_init_config_for_scope_and_group(self, node_name: str, group: QuantizerGroup) -> RangeInitConfig:
         matches = []  # type: List[RangeInitConfig]
         for pl_config in self.per_layer_range_init_configs:
-            if should_consider_scope(node_name,
-                                     ignored_scopes=pl_config.ignored_scopes,
-                                     target_scopes=pl_config.target_scopes):
+            if should_consider_scope(
+                node_name, ignored_scopes=pl_config.ignored_scopes, target_scopes=pl_config.target_scopes
+            ):
                 if group == pl_config.target_group or pl_config.target_group is None:
-                    matches.append(RangeInitConfig(pl_config.init_type, pl_config.num_init_samples,
-                                                   pl_config.init_type_specific_params))
+                    matches.append(
+                        RangeInitConfig(
+                            pl_config.init_type, pl_config.num_init_samples, pl_config.init_type_specific_params
+                        )
+                    )
         if len(matches) > 1:
-            raise ValueError('Location {} matches more than one per-layer initialization parameter '
-                             'definition!'.format(str(node_name)))
+            raise ValueError(
+                "Location {} matches more than one per-layer initialization parameter "
+                "definition!".format(str(node_name))
+            )
         if len(matches) == 1:
             return matches[0]
         if not matches and self.global_init_config is not None:
             return deepcopy(self.global_init_config)
 
-        raise ValueError('Location {} does not match any per-layer initialization parameter '
-                         'definition!'.format(str(node_name)))
+        raise ValueError(
+            "Location {} does not match any per-layer initialization parameter definition!".format(str(node_name))
+        )
 
 
 class RangeInitializer:
     def __init__(self, range_init_params: TFRangeInitParams):
         self.range_init_params = range_init_params
         self.dataset = range_init_params.init_range_data_loader
         self.num_steps = range_init_params.get_max_num_init_steps()
 
         self.nncf_quantization_operation_classes = NNCF_QUANTIZATION_OPERATIONS.registry_dict.values()
 
     @staticmethod
-    def generate_stat_collector(reduction_shape: ReductionShape,
-                                collector_params: RangeInitCollectorParams,
-                                init_config: RangeInitConfig,
-                                num_samples_to_collect_override: int = None) -> TensorStatisticCollectorBase:
+    def generate_stat_collector(
+        reduction_shape: ReductionShape,
+        collector_params: RangeInitCollectorParams,
+        init_config: RangeInitConfig,
+        num_samples_to_collect_override: int = None,
+    ) -> TensorStatisticCollectorBase:
         range_type = init_config.init_type
         num_samples = init_config.num_init_samples
         if num_samples_to_collect_override is not None:
             num_samples = num_samples_to_collect_override
 
-        if range_type == 'min_max':
+        if range_type == "min_max":
             return TFMinMaxStatisticCollector(collector_params.use_abs_max, reduction_shape, num_samples)
-        if range_type == 'mixed_min_max':
-            return TFMixedMinMaxStatisticCollector(collector_params.use_per_sample_stats(per_sample_stats=True),
-                                                   collector_params.use_abs_max,
-                                                   collector_params.use_means_of_mins,
-                                                   collector_params.use_means_of_maxs,
-                                                   reduction_shape,
-                                                   num_samples)
-        if range_type == 'mean_min_max':
-            return TFMeanMinMaxStatisticCollector(collector_params.use_per_sample_stats(per_sample_stats=True),
-                                                  collector_params.use_abs_max,
-                                                  reduction_shape, num_samples)
-        if range_type == 'threesigma':
-            return TFMedianMADStatisticCollector(reduction_shape,
-                                                 num_samples)
-        if range_type == 'percentile':
-            min_percentile = init_config.init_type_specific_params.get('min_percentile', MIN_PERCENTILE)
-            max_percentile = init_config.init_type_specific_params.get('max_percentile', MAX_PERCENTILE)
-            return TFPercentileStatisticCollector([min_percentile, max_percentile],
-                                                  reduction_shape,
-                                                  num_samples)
-        if range_type == 'mean_percentile':
-            min_percentile = init_config.init_type_specific_params.get('min_percentile', MIN_PERCENTILE)
-            max_percentile = init_config.init_type_specific_params.get('max_percentile', MAX_PERCENTILE)
-            return TFMeanPercentileStatisticCollector([min_percentile, max_percentile],
-                                                      reduction_shape,
-                                                      num_samples)
-        raise ValueError(f'Range type {range_type} is not supported.')
+        if range_type == "mixed_min_max":
+            return TFMixedMinMaxStatisticCollector(
+                collector_params.use_per_sample_stats(per_sample_stats=True),
+                collector_params.use_abs_max,
+                collector_params.use_means_of_mins,
+                collector_params.use_means_of_maxs,
+                reduction_shape,
+                num_samples,
+            )
+        if range_type == "mean_min_max":
+            return TFMeanMinMaxStatisticCollector(
+                collector_params.use_per_sample_stats(per_sample_stats=True),
+                collector_params.use_abs_max,
+                reduction_shape,
+                num_samples,
+            )
+        if range_type == "threesigma":
+            return TFMedianMADStatisticCollector(reduction_shape, num_samples)
+        if range_type == "percentile":
+            min_percentile = init_config.init_type_specific_params.get("min_percentile", MIN_PERCENTILE)
+            max_percentile = init_config.init_type_specific_params.get("max_percentile", MAX_PERCENTILE)
+            return TFPercentileStatisticCollector([min_percentile, max_percentile], reduction_shape, num_samples)
+        if range_type == "mean_percentile":
+            min_percentile = init_config.init_type_specific_params.get("min_percentile", MIN_PERCENTILE)
+            max_percentile = init_config.init_type_specific_params.get("max_percentile", MAX_PERCENTILE)
+            return TFMeanPercentileStatisticCollector([min_percentile, max_percentile], reduction_shape, num_samples)
+        raise ValueError(f"Range type {range_type} is not supported.")
 
     def _register_layer_statistics(self, layer: tf.keras.layers.Layer, layer_statistics: list, handles: list):
-        channel_axes = get_channel_axis(InputType.INPUTS, '', layer)
+        channel_axes = get_channel_axis(InputType.INPUTS, "", layer)
         init_config = self.range_init_params.get_init_config_for_quantization_point(layer, InputType.INPUTS)
 
         is_weights = False
         collector_params = RangeInitCollectorParams(is_weights, layer.mode, layer.per_channel)
-        per_sample_stats = init_config.init_type in ['mixed_min_max', 'mean_min_max']
+        per_sample_stats = init_config.init_type in ["mixed_min_max", "mean_min_max"]
 
-        reduction_shape = get_reduction_shape_activations(layer, channel_axes,
-                                                          collector_params.use_per_sample_stats(per_sample_stats))
+        reduction_shape = get_reduction_shape_activations(
+            layer, channel_axes, collector_params.use_per_sample_stats(per_sample_stats)
+        )
 
-        num_batches = int(np.ceil(
-            init_config.num_init_samples / self.dataset.batch_size))
+        num_batches = int(np.ceil(init_config.num_init_samples / self.dataset.batch_size))
 
-        collector = RangeInitializer.generate_stat_collector(reduction_shape,
-                                                             collector_params,
-                                                             init_config,
-                                                             num_batches)
+        collector = RangeInitializer.generate_stat_collector(
+            reduction_shape, collector_params, init_config, num_batches
+        )
         handles.append(layer.register_hook_pre_quantizer(collector.register_input))
         layer.enabled = False
         layer_statistics.append((layer, collector))
 
     def _register_op_statistics(self, layer: tf.keras.layers.Layer, op_statistics: list, handles: list):
         for weight_attr, ops in layer.weights_attr_ops.items():
             for op_name, op in ops.items():
                 if op.__class__ in self.nncf_quantization_operation_classes:
                     channel_axes = get_channel_axis(InputType.WEIGHTS, weight_attr, layer)
-                    init_config = self.range_init_params. \
-                        get_init_config_for_quantization_point(layer, InputType.WEIGHTS)
+                    init_config = self.range_init_params.get_init_config_for_quantization_point(
+                        layer, InputType.WEIGHTS
+                    )
 
                     is_weights = True
                     collector_params = RangeInitCollectorParams(is_weights, op.mode, op.per_channel)
 
                     reduction_shape = get_reduction_shape_weights(layer, weight_attr, channel_axes, op.per_channel)
 
                     # No need to store extra statistics in memory since weights won't change during range init
                     num_batches = 1
 
-                    collector = RangeInitializer.generate_stat_collector(reduction_shape,
-                                                                         collector_params,
-                                                                         init_config,
-                                                                         num_batches)
+                    collector = RangeInitializer.generate_stat_collector(
+                        reduction_shape, collector_params, init_config, num_batches
+                    )
                     handles.append(op.register_hook_pre_call(collector.register_input))
                     op.enabled = False
                     op_statistics.append((layer, op_name, op, collector))
 
     def run(self, model: tf.keras.Model) -> None:
         layer_statistics = []
         op_statistics = []
         handles = []
         for layer in model.layers:
             if isinstance(layer, FakeQuantize):
                 self._register_layer_statistics(layer, layer_statistics, handles)
             elif isinstance(layer, NNCFWrapper):
                 self._register_op_statistics(layer, op_statistics, handles)
 
-        for (x, _) in ProgressBar(
-                islice(self.dataset, self.num_steps),
-                total=self.num_steps,
-                desc='Collecting tensor statistics/data'
+        for x, _ in ProgressBar(
+            islice(self.dataset, self.num_steps), total=self.num_steps, desc="Collecting tensor statistics/data"
         ):
             model(x, training=False)
 
         for layer, collector in layer_statistics:
             target_stat = collector.get_statistics()
             minmax_stats = tf_convert_stat_to_min_max_tensor_stat(target_stat)
             layer.apply_range_initialization(tf.squeeze(minmax_stats.min_values), tf.squeeze(minmax_stats.max_values))
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/layers.py` & `nncf-2.5.0/nncf/tensorflow/quantization/layers.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,55 +1,53 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.tensorflow.layers.custom_objects import NNCF_CUSTOM_OBJECTS
 from nncf.tensorflow.layers.custom_objects import NNCF_QUANTIZATION_OPERATIONS
 from nncf.tensorflow.layers.operation import InputType
 from nncf.tensorflow.quantization.quantizers import Quantizer
 from nncf.tensorflow.quantization.quantizers import TFQuantizerSpec
 
 
 @NNCF_CUSTOM_OBJECTS.register()
 class FakeQuantize(tf.keras.layers.Layer):
-    def __init__(self, config: TFQuantizerSpec, data_format: str ='channels_last', **kwargs):
+    def __init__(self, config: TFQuantizerSpec, data_format: str = "channels_last", **kwargs):
         """
         Create a FakeQuantize layer.
         """
         super().__init__(**kwargs)
         self._mode = config.mode
         self.data_format = data_format
 
-        self._op_name = f'{self.name}_quantizer'
+        self._op_name = f"{self.name}_quantizer"
         self._quantizer = self._create_quantizer(config, self._op_name)
         self._quantizer_weights = {}
 
     @property
     def num_bits(self):
-        return getattr(self._quantizer, 'num_bits', None)
+        return getattr(self._quantizer, "num_bits", None)
 
     @property
     def per_channel(self):
-        return getattr(self._quantizer, 'per_channel', None)
+        return getattr(self._quantizer, "per_channel", None)
 
     @property
     def narrow_range(self):
-        return getattr(self._quantizer, 'narrow_range', None)
+        return getattr(self._quantizer, "narrow_range", None)
 
     @property
     def signed(self) -> bool:
         """
         Returns `True` for signed quantization, `False` for unsigned.
 
         :return: `True` for signed quantization, `False` for unsigned.
@@ -76,16 +74,15 @@
         return self._quantizer.enabled
 
     @enabled.setter
     def enabled(self, v):
         self._quantizer.enabled = v
 
     def build(self, input_shape):
-        self._quantizer_weights = self._quantizer.build(
-            input_shape, InputType.INPUTS, self.name, self)
+        self._quantizer_weights = self._quantizer.build(input_shape, InputType.INPUTS, self.name, self)
 
     def call(self, inputs, training=None):
         training = self._get_training_value(training)
         return self._quantizer(inputs, self._quantizer_weights, training)
 
     def register_hook_pre_quantizer(self, hook):
         return self._quantizer.register_hook_pre_call(hook)
@@ -105,20 +102,17 @@
                 training = tf.cast(training, tf.bool)
             else:
                 training = bool(training)
         return training
 
     def get_config(self):
         config = super().get_config()
-        config.update({
-            'quantizer_config': {
-                **self._quantizer.get_config()['quantizer_spec']
-            },
-            'data_format': self.data_format
-        })
+        config.update(
+            {"quantizer_config": {**self._quantizer.get_config()["quantizer_spec"]}, "data_format": self.data_format}
+        )
         return config
 
     @classmethod
     def from_config(cls, config):
         config = config.copy()
-        quantizer_config = config.pop('quantizer_config')
+        quantizer_config = config.pop("quantizer_config")
         return cls(TFQuantizerSpec(**quantizer_config), **config)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/quantizers.py` & `nncf-2.5.0/nncf/tensorflow/quantization/quantizers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,20 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from functools import partial
-from typing import Any
-from typing import Dict
-from typing import Optional
+from typing import Any, Dict, Optional
 
 import tensorflow as tf
 
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import QuantizerSpec
 from nncf.tensorflow.layers.custom_objects import NNCF_CUSTOM_OBJECTS
@@ -27,50 +23,50 @@
 from nncf.tensorflow.layers.data_layout import get_channel_size
 from nncf.tensorflow.layers.operation import NNCFOperation
 from nncf.tensorflow.quantization.functions import asymmetric_quantize
 from nncf.tensorflow.quantization.functions import symmetric_quantize
 
 
 class TFQuantizerSpec(QuantizerSpec):
-    def __init__(self, num_bits: int,
-                 mode: QuantizationMode,
-                 signedness_to_force: Optional[bool],
-                 narrow_range: bool,
-                 half_range: bool,
-                 per_channel: bool):
+    def __init__(
+        self,
+        num_bits: int,
+        mode: QuantizationMode,
+        signedness_to_force: Optional[bool],
+        narrow_range: bool,
+        half_range: bool,
+        per_channel: bool,
+    ):
         super().__init__(num_bits, mode, signedness_to_force, narrow_range, half_range)
         self.per_channel = per_channel
 
     @classmethod
-    def from_config(cls, qconfig: QuantizerConfig, narrow_range: bool, half_range: bool) -> 'TFQuantizerSpec':
-        return cls(qconfig.num_bits,
-                   qconfig.mode,
-                   qconfig.signedness_to_force,
-                   narrow_range,
-                   half_range,
-                   qconfig.per_channel)
+    def from_config(cls, qconfig: QuantizerConfig, narrow_range: bool, half_range: bool) -> "TFQuantizerSpec":
+        return cls(
+            qconfig.num_bits, qconfig.mode, qconfig.signedness_to_force, narrow_range, half_range, qconfig.per_channel
+        )
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
         return {
-            'num_bits': self.num_bits,
-            'mode': self.mode,
-            'signedness_to_force': self.signedness_to_force,
-            'narrow_range': self.narrow_range,
-            'half_range': self.half_range,
-            'per_channel': self.per_channel
+            "num_bits": self.num_bits,
+            "mode": self.mode,
+            "signedness_to_force": self.signedness_to_force,
+            "narrow_range": self.narrow_range,
+            "half_range": self.half_range,
+            "per_channel": self.per_channel,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'TFQuantizerSpec':
+    def from_state(cls, state: Dict[str, Any]) -> "TFQuantizerSpec":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
@@ -149,18 +145,19 @@
         to one of the supported shapes, then quantizes and then transforms quantized tensor to
         the original inputs shape.
 
         :param input_shape: Shape of the input.
         :param channel_axes: Channel axes.
         """
         try:
-            self._pre_processing_fn, self._post_processing_fn = \
-                Quantizer._make_transformation_fns(input_shape, channel_axes)
+            self._pre_processing_fn, self._post_processing_fn = Quantizer._make_transformation_fns(
+                input_shape, channel_axes
+            )
         except NotImplementedError as e:
-            raise NotImplementedError(f'Additional information: quantizer name {self.name}') from e
+            raise NotImplementedError(f"Additional information: quantizer name {self.name}") from e
 
     @staticmethod
     def _make_transformation_fns(input_shape, channel_axes):
         fns_registry = []
         if isinstance(channel_axes, (tuple, list)):
             switch_counter = 0
             accumulate = False
@@ -176,38 +173,39 @@
                         new_shape.append(val)
                         switch_counter += 1
                 else:
                     accumulate = False
                     new_shape.append(val)
             if switch_counter > 1:
                 raise NotImplementedError(
-                    'Quntizer could not transform input to apply per-channel quantization: '
-                     f'input_shape {input_shape}, channel_axes {channel_axes}')
-            forward_params = {'shape': new_shape}
-            backward_params = {'shape': input_shape}
+                    "Quntizer could not transform input to apply per-channel quantization: "
+                    f"input_shape {input_shape}, channel_axes {channel_axes}"
+                )
+            forward_params = {"shape": new_shape}
+            backward_params = {"shape": input_shape}
             fns_registry.append((tf.reshape, forward_params, backward_params))
             input_shape = new_shape
             channel_axes = new_channel_axes
 
         ndims = len(input_shape)
         if channel_axes % ndims != ndims - 1:
             perm = [i for i, _ in enumerate(input_shape)]
             perm[channel_axes], perm[-1] = perm[-1], perm[channel_axes]
-            params = {'perm': perm}
+            params = {"perm": perm}
             fns_registry.append((tf.transpose, params, params))
             new_shape = list(input_shape)
             new_shape[channel_axes], new_shape[-1] = new_shape[-1], new_shape[channel_axes]
             input_shape = new_shape
 
         if ndims not in [1, 2, 4]:
             size = 1
             for val in input_shape[:-1]:
                 size *= val
-            forward_params = {'shape': [size, input_shape[-1]]}
-            backward_params = {'shape': input_shape}
+            forward_params = {"shape": [size, input_shape[-1]]}
+            backward_params = {"shape": input_shape}
             fns_registry.append((tf.reshape, forward_params, backward_params))
 
         def fuse_functions(fns_registry):
             if not fns_registry:
                 return fns_registry
 
             fused_fns_registry = []
@@ -257,15 +255,15 @@
                 result = func(result)
             return result
 
         return post_processing_fn
 
     @staticmethod
     def _min_adj(bits, low, range_len, narrow_range):
-        quants_count = 2 ** bits - (2 if narrow_range else 1)
+        quants_count = 2**bits - (2 if narrow_range else 1)
         return range_len / quants_count * tf.round(quants_count * low / range_len)
 
     def get_quantizer_config(self) -> QuantizerConfig:
         """
         Used to get a current quantizer state in terms of QuantizerConfig objects.
 
         :return: A QuantizerConfig struct that corresponds to current state of the quantizer.
@@ -297,134 +295,125 @@
 
     def signed(self, op_weights) -> bool:
         """
         Returns `True` for signed quantization, `False` for unsigned.
 
         :return: `True` for signed quantization, `False` for unsigned.
         """
-        signed_var = op_weights['signed_var']
+        signed_var = op_weights["signed_var"]
         return signed_var.numpy() < 0.0
 
     def build(self, input_shape, input_type, name, layer):
         channel_axes = None
         if self.per_channel:
             channel_axes = get_channel_axis(input_type, name, layer)
         return self._create_variables(layer, input_shape, channel_axes, name)
 
-    def _create_variables(self,
-                          layer,
-                          input_shape,
-                          channel_axes,
-                          name: str = ''):
+    def _create_variables(self, layer, input_shape, channel_axes, name: str = ""):
         shape = None
         if self.per_channel:
             self.setup_input_transformation(input_shape, channel_axes)
             shape = (get_channel_size(input_shape, channel_axes),)
 
         scale = layer.add_weight(
-            name + '_scale',
-            shape=shape,
-            initializer=tf.keras.initializers.Constant(1.0),
-            trainable=True)
+            name + "_scale", shape=shape, initializer=tf.keras.initializers.Constant(1.0), trainable=True
+        )
         signed = layer.add_weight(
-            name + '_signed',
-            initializer=tf.keras.initializers.Constant(
-                -1.0 if self.signedness_to_force in (True, None) else 0.0),
-            trainable=False)
-        return {
-            'scale_var': scale,
-            'signed_var': signed
-        }
+            name + "_signed",
+            initializer=tf.keras.initializers.Constant(-1.0 if self.signedness_to_force in (True, None) else 0.0),
+            trainable=False,
+        )
+        return {"scale_var": scale, "signed_var": signed}
 
     def apply_overflow_fix(self, weights):
         if self.num_bits != 8 or not self._half_range:
-            raise RuntimeError('Attempt to apply overflow issue fix '
-                               'to quantizer which is not configured for that.')
+            raise RuntimeError("Attempt to apply overflow issue fix to quantizer which is not configured for that.")
 
         # Multiplier to expand scale from 7 bit to 8 bit
         multiplier = 127 / 63 if self.narrow_range else 255 / 127
-        weights['scale_var'].assign(multiplier * weights['scale_var'])
+        weights["scale_var"].assign(multiplier * weights["scale_var"])
         self._eps *= multiplier
         self._half_range = False
 
     def quantize(self, inputs, weights, _):
         def _half_range_quantize():
             return symmetric_quantize(
                 inputs,
-                weights['scale_var'],
-                weights['signed_var'],
+                weights["scale_var"],
+                weights["signed_var"],
                 num_bits=self.num_bits - 1,
                 per_channel=self.per_channel,
                 narrow_range=self.narrow_range,
-                eps=self._eps
+                eps=self._eps,
             )
 
         def _default_quantize():
             return symmetric_quantize(
                 inputs,
-                weights['scale_var'],
-                weights['signed_var'],
+                weights["scale_var"],
+                weights["signed_var"],
                 num_bits=self.num_bits,
                 per_channel=self.per_channel,
                 narrow_range=self.narrow_range,
-                eps=self._eps
+                eps=self._eps,
             )
 
         if self._half_range:
             return _half_range_quantize()
 
         return _default_quantize()
 
     def apply_range_initialization(self, weights, min_values, max_values, min_range=0.1, eps=0.01):
         if self.signedness_to_force is None:
             sign = tf.reduce_any(tf.less(min_values, 0))
-            weights['signed_var'].assign(-1.0 if sign else 0.0)
+            weights["signed_var"].assign(-1.0 if sign else 0.0)
         ranges = tf.maximum(tf.abs(max_values), tf.abs(min_values))
         max_range = tf.reduce_max(ranges)
         lower_threshold = tf.maximum(eps * max_range, min_range)
         scale = tf.maximum(ranges, lower_threshold)
-        weights['scale_var'].assign(scale)
+        weights["scale_var"].assign(scale)
 
     def get_quantizer_config(self) -> QuantizerConfig:
         return QuantizerConfig(
             num_bits=self.num_bits,
             mode=QuantizationMode.SYMMETRIC,
             signedness_to_force=self.signedness_to_force,
-            per_channel=self.per_channel
+            per_channel=self.per_channel,
         )
 
     def get_config(self):
         qspec_dict = {
-            'num_bits':  self.num_bits,
-            'mode': QuantizationMode.SYMMETRIC,
-            'signedness_to_force': self.signedness_to_force,
-            'narrow_range': self.narrow_range,
-            'half_range': self._half_range,
-            'per_channel': self.per_channel,
+            "num_bits": self.num_bits,
+            "mode": QuantizationMode.SYMMETRIC,
+            "signedness_to_force": self.signedness_to_force,
+            "narrow_range": self.narrow_range,
+            "half_range": self._half_range,
+            "per_channel": self.per_channel,
         }
         config = {
-            'quantizer_spec': qspec_dict,
-            'name': self.name,
+            "quantizer_spec": qspec_dict,
+            "name": self.name,
         }
         return config
 
     @classmethod
     def from_config(cls, config):
-        qspec_dict = config['quantizer_spec']
-        qspec = TFQuantizerSpec(num_bits=qspec_dict['num_bits'],
-                                mode=QuantizationMode.SYMMETRIC,
-                                signedness_to_force=qspec_dict['signedness_to_force'],
-                                narrow_range=qspec_dict['narrow_range'],
-                                half_range=qspec_dict['half_range'],
-                                per_channel=qspec_dict['per_channel'])
-        name = config['name']
+        qspec_dict = config["quantizer_spec"]
+        qspec = TFQuantizerSpec(
+            num_bits=qspec_dict["num_bits"],
+            mode=QuantizationMode.SYMMETRIC,
+            signedness_to_force=qspec_dict["signedness_to_force"],
+            narrow_range=qspec_dict["narrow_range"],
+            half_range=qspec_dict["half_range"],
+            per_channel=qspec_dict["per_channel"],
+        )
+        name = config["name"]
         return cls(name, qspec)
 
 
-
 @NNCF_CUSTOM_OBJECTS.register()
 @NNCF_QUANTIZATION_OPERATIONS.register(QuantizationMode.ASYMMETRIC)
 class AsymmetricQuantizer(Quantizer):
     def __init__(self, name: str, qspec: TFQuantizerSpec):
         super().__init__(name)
         self.num_bits = qspec.num_bits
         self.narrow_range = qspec.narrow_range
@@ -441,120 +430,112 @@
 
     def build(self, input_shape, input_type, name, layer):
         channel_axes = None
         if self.per_channel:
             channel_axes = get_channel_axis(input_type, name, layer)
         return self._create_variables(layer, input_shape, channel_axes, name)
 
-    def _create_variables(self,
-                          layer,
-                          input_shape,
-                          channel_axes,
-                          name: str = ''):
+    def _create_variables(self, layer, input_shape, channel_axes, name: str = ""):
         shape = None
         if self.per_channel:
             self.setup_input_transformation(input_shape, channel_axes)
             shape = (get_channel_size(input_shape, channel_axes),)
 
         input_low = layer.add_weight(
-            name + '_input_low',
-            shape=shape,
-            initializer=tf.keras.initializers.Constant(0.0),
-            trainable=True)
+            name + "_input_low", shape=shape, initializer=tf.keras.initializers.Constant(0.0), trainable=True
+        )
         input_range = layer.add_weight(
-            name + '_input_range',
-            shape=shape,
-            initializer=tf.keras.initializers.Constant(1.0),
-            trainable=True)
-        return {
-            'input_low_var': input_low,
-            'input_range_var': input_range
-        }
+            name + "_input_range", shape=shape, initializer=tf.keras.initializers.Constant(1.0), trainable=True
+        )
+        return {"input_low_var": input_low, "input_range_var": input_range}
 
     def apply_overflow_fix(self, weights):
         if self.num_bits != 8 or not self._half_range:
-            raise RuntimeError('Attempt to apply overflow issue fix '
-                               'to quantizer which is not configured for that.')
+            raise RuntimeError("Attempt to apply overflow issue fix to quantizer which is not configured for that.")
 
         # Low value shift to expand quantize range from 7 bit to 8 bit properly
-        weights['input_low_var'].assign(weights['input_low_var'] + self._min_adj(
-                                        7, weights['input_low_var'],
-                                        weights['input_range_var'] + self._eps,
-                                        self.narrow_range))
+        weights["input_low_var"].assign(
+            weights["input_low_var"]
+            + self._min_adj(7, weights["input_low_var"], weights["input_range_var"] + self._eps, self.narrow_range)
+        )
         # Multiplier to expand scale from 7 bit to 8 bit
         multiplier = 127 / 63 if self.narrow_range else 255 / 127
-        weights['input_range_var'].assign(multiplier * weights['input_range_var'])
+        weights["input_range_var"].assign(multiplier * weights["input_range_var"])
         self._eps *= multiplier
         self._half_range = False
 
     def quantize(self, inputs, weights, _):
         def _half_range_quantize():
             return asymmetric_quantize(
                 inputs,
-                weights['input_low_var'],
-                weights['input_range_var'],
+                weights["input_low_var"],
+                weights["input_range_var"],
                 num_bits=self.num_bits - 1,
                 per_channel=self.per_channel,
                 narrow_range=self.narrow_range,
-                eps=self._eps
+                eps=self._eps,
             )
 
         def _default_quantize():
             return asymmetric_quantize(
                 inputs,
-                weights['input_low_var'],
-                weights['input_range_var'],
+                weights["input_low_var"],
+                weights["input_range_var"],
                 num_bits=self.num_bits,
                 per_channel=self.per_channel,
                 narrow_range=self.narrow_range,
-                eps=self._eps
+                eps=self._eps,
             )
 
         if self._half_range:
             return _half_range_quantize()
 
         return _default_quantize()
 
     def apply_range_initialization(self, weights, min_values, max_values, min_range=0.1, eps=0.01):
+        min_values = tf.minimum(min_values, 0)
+        max_values = tf.maximum(max_values, 0)
         ranges = max_values - min_values
         max_range = tf.reduce_max(ranges)
         lower_threshold = tf.maximum(eps * max_range, min_range)
         correction = (tf.maximum(ranges, lower_threshold) - ranges) * 0.5
         input_low = min_values - correction
         input_range = ranges + 2 * correction
-        weights['input_low_var'].assign(input_low)
-        weights['input_range_var'].assign(input_range)
+        weights["input_low_var"].assign(input_low)
+        weights["input_range_var"].assign(input_range)
 
     def get_quantizer_config(self) -> QuantizerConfig:
         return QuantizerConfig(
             num_bits=self.num_bits,
             mode=QuantizationMode.ASYMMETRIC,
             signedness_to_force=None,
-            per_channel=self.per_channel
+            per_channel=self.per_channel,
         )
 
     def get_config(self):
         qspec_dict = {
-            'num_bits': self.num_bits,
-            'mode': QuantizationMode.ASYMMETRIC,
-            'signedness_to_force': None,
-            'narrow_range': self.narrow_range,
-            'half_range': self._half_range,
-            'per_channel': self.per_channel,
+            "num_bits": self.num_bits,
+            "mode": QuantizationMode.ASYMMETRIC,
+            "signedness_to_force": None,
+            "narrow_range": self.narrow_range,
+            "half_range": self._half_range,
+            "per_channel": self.per_channel,
         }
         config = {
-            'quantizer_spec': qspec_dict,
-            'name': self.name,
+            "quantizer_spec": qspec_dict,
+            "name": self.name,
         }
         return config
 
     @classmethod
     def from_config(cls, config):
-        qspec_dict = config['quantizer_spec']
-        qspec = TFQuantizerSpec(num_bits=qspec_dict['num_bits'],
-                                mode=QuantizationMode.ASYMMETRIC,
-                                signedness_to_force=None,
-                                narrow_range=qspec_dict['narrow_range'],
-                                half_range=qspec_dict['half_range'],
-                                per_channel=qspec_dict['per_channel'])
-        name = config['name']
+        qspec_dict = config["quantizer_spec"]
+        qspec = TFQuantizerSpec(
+            num_bits=qspec_dict["num_bits"],
+            mode=QuantizationMode.ASYMMETRIC,
+            signedness_to_force=None,
+            narrow_range=qspec_dict["narrow_range"],
+            half_range=qspec_dict["half_range"],
+            per_channel=qspec_dict["per_channel"],
+        )
+        name = config["name"]
         return cls(name, qspec)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/quantization/utils.py` & `nncf-2.5.0/nncf/tensorflow/quantization/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,50 +1,48 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 import tensorflow as tf
 
 from nncf.tensorflow.graph.utils import get_nncf_operations
-from nncf.tensorflow.layers.wrapper import NNCFWrapper
 from nncf.tensorflow.layers.operation import NNCFOperation
+from nncf.tensorflow.layers.wrapper import NNCFWrapper
 from nncf.tensorflow.quantization.layers import FakeQuantize
 
 
 def apply_overflow_fix(model: tf.keras.Model, op_names: List[str]) -> None:
     if not isinstance(model, tf.keras.Model):
-        raise ValueError(f'Expected model to be a `tf.keras.Model` instance but got: {type(model)}')
+        raise ValueError(f"Expected model to be a `tf.keras.Model` instance but got: {type(model)}")
 
     for wrapped_layer, weight_attr, op in get_nncf_operations(model, op_names):
         if op.half_range:
             apply_overflow_fix_to_layer(wrapped_layer, weight_attr, op)
 
 
 def apply_overflow_fix_to_layer(wrapped_layer: NNCFWrapper, weight_attr: str, op: NNCFOperation) -> None:
     layer_weight = wrapped_layer.layer_weights[weight_attr]
     ops_weights = wrapped_layer.get_operation_weights(op.name)
     # Keep zero weights to prevent
     # zero quant calculation arithmetic errors
-    mask = layer_weight == 0.
+    mask = layer_weight == 0.0
     layer_weight_updated = op.call(layer_weight, ops_weights, False)
 
     # Assign exact zero to weights which
     # was exact zero before overflow fix
-    layer_weight_updated = tf.where(mask, [0.], layer_weight_updated)
+    layer_weight_updated = tf.where(mask, [0.0], layer_weight_updated)
     layer_weight.assign(layer_weight_updated)
     op.apply_overflow_fix(ops_weights)
 
 
 def collect_fake_quantize_layers(model: tf.keras.Model) -> List[FakeQuantize]:
     """
     Collects all fake quantize layers from the provided model.
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/base_algorithm.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/base_algorithm.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,22 +1,21 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import tensorflow as tf
 
-from nncf.common.sparsity.controller import SparsityController
 from nncf.common.compression import BaseCompressionAlgorithmController
+from nncf.common.sparsity.controller import SparsityController
 from nncf.tensorflow.graph.metatypes import keras_layers as layer_metatypes
 from nncf.tensorflow.sparsity.utils import strip_model_from_masks
 
 SPARSITY_LAYER_METATYPES = [
     layer_metatypes.TFConv1DLayerMetatype,
     layer_metatypes.TFConv2DLayerMetatype,
     layer_metatypes.TFConv3DLayerMetatype,
@@ -28,15 +27,15 @@
     layer_metatypes.TFConv2DTransposeLayerMetatype,
     layer_metatypes.TFConv3DTransposeLayerMetatype,
     layer_metatypes.TFSeparableConv1DLayerMetatype,
     layer_metatypes.TFSeparableConv2DLayerMetatype,
     layer_metatypes.TFEmbeddingLayerMetatype,
     layer_metatypes.TFLocallyConnected1DLayerMetatype,
     layer_metatypes.TFLocallyConnected2DLayerMetatype,
-    layer_metatypes.TFDenseLayerMetatype
+    layer_metatypes.TFDenseLayerMetatype,
 ]
 
 
 class BaseSparsityController(BaseCompressionAlgorithmController, SparsityController):
     """
     Serves as a handle to the additional modules, parameters and hooks inserted
     into the original uncompressed model to enable sparsity-specific compression.
@@ -44,9 +43,10 @@
     compression scheduler and compression loss.
     """
 
     def __init__(self, target_model, op_names):
         super().__init__(target_model)
         self._op_names = op_names
 
-    def strip_model(self, model):
+    def strip_model(self, model: tf.keras.Model, do_copy: bool = False) -> tf.keras.Model:
+        # Transform model for sparsity creates copy of the model.
         return strip_model_from_masks(model, self._op_names)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/callbacks.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/callbacks.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 from nncf.common.statistics import NNCFStatistics
 from nncf.tensorflow.callbacks.statistics_callback import StatisticsCallback
 
 
@@ -31,27 +29,27 @@
 
 class SparsityStatisticsCallback(StatisticsCallback):
     """
     Callback for logging sparsity compression statistics to tensorboard and stdout.
     """
 
     def _prepare_for_tensorboard(self, stats: NNCFStatistics):
-        base_prefix = '2.compression/statistics'
-        detailed_prefix = '3.compression_details/statistics'
+        base_prefix = "2.compression/statistics"
+        detailed_prefix = "3.compression_details/statistics"
 
         if stats.magnitude_sparsity:
             stats = stats.magnitude_sparsity
         else:
             stats = stats.rb_sparsity
 
         ms = stats.model_statistics
         tensorboard_stats = {
-            f'{base_prefix}/sparsity_level_for_model': ms.sparsity_level,
-            f'{base_prefix}/sparsity_level_for_sparsified_layers': ms.sparsity_level_for_layers,
-            f'{base_prefix}/target_sparsity_level': stats.target_sparsity_level,
+            f"{base_prefix}/sparsity_level_for_model": ms.sparsity_level,
+            f"{base_prefix}/sparsity_level_for_sparsified_layers": ms.sparsity_level_for_layers,
+            f"{base_prefix}/target_sparsity_level": stats.target_sparsity_level,
         }
 
         for ls in ms.sparsified_layers_summary:
             layer_name, sparsity_level = ls.name, ls.sparsity_level
-            tensorboard_stats[f'{detailed_prefix}/{layer_name}/sparsity_level'] = sparsity_level
+            tensorboard_stats[f"{detailed_prefix}/{layer_name}/sparsity_level"] = sparsity_level
 
         return tensorboard_stats
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/collector.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/collector.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,29 +1,27 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 import tensorflow as tf
 
-from nncf.common.sparsity.collector import WeightDescription
 from nncf.common.sparsity.collector import BaseSparseModelStatisticsCollector
+from nncf.common.sparsity.collector import WeightDescription
+from nncf.tensorflow.graph.utils import get_nncf_operations
 from nncf.tensorflow.sparsity.magnitude.functions import apply_mask
 from nncf.tensorflow.sparsity.magnitude.operation import BinaryMaskWithWeightsBackup
-from nncf.tensorflow.graph.utils import get_nncf_operations
 
 
 def _get_standardized_weight_shape(shape):
     return [0 if x is None else x for x in shape]
 
 
 class TFSparseModelStatisticsCollector(BaseSparseModelStatisticsCollector):
@@ -56,15 +54,15 @@
             sparse_weight = apply_mask(weight, binary_mask)
 
             weights_descriptions.append(
                 WeightDescription(
                     weight.name,
                     _get_standardized_weight_shape(weight.shape.as_list()),
                     tf.math.count_nonzero(sparse_weight).numpy().item(),
-                    is_sparse=True
+                    is_sparse=True,
                 )
             )
 
             # Exclude this name because it has been processed.
             excluded_names.append(weight.name)
 
             # Exclude these names because they were added to the model
@@ -77,12 +75,12 @@
         unique_weights = {id(w): w for w in self._model.weights if w.name not in excluded_names}.values()
         for weight in unique_weights:
             weights_descriptions.append(
                 WeightDescription(
                     weight.name,
                     _get_standardized_weight_shape(weight.shape.as_list()),
                     tf.math.count_nonzero(weight).numpy().item(),
-                    is_sparse=False
+                    is_sparse=False,
                 )
             )
 
             return weights_descriptions
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/magnitude/algorithm.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/magnitude/algorithm.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from copy import deepcopy
 from typing import Set
 
 import tensorflow as tf
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionLoss
@@ -26,14 +24,15 @@
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.scopes import check_scopes_in_graph
 from nncf.common.scopes import should_consider_scope
 from nncf.common.sparsity.schedulers import SPARSITY_SCHEDULERS
 from nncf.common.sparsity.statistics import LayerThreshold
 from nncf.common.sparsity.statistics import MagnitudeSparsityStatistics
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.extractors import extract_bn_adaptation_init_params
 from nncf.config.schemata.defaults import MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE
 from nncf.config.schemata.defaults import SPARSITY_INIT
 from nncf.tensorflow.algorithm_selector import TF_COMPRESSION_ALGORITHMS
 from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
 from nncf.tensorflow.graph.converter import TFModelConverterFactory
@@ -49,19 +48,19 @@
 from nncf.tensorflow.sparsity.collector import TFSparseModelStatisticsCollector
 from nncf.tensorflow.sparsity.magnitude.functions import WEIGHT_IMPORTANCE_FUNCTIONS
 from nncf.tensorflow.sparsity.magnitude.functions import calc_magnitude_binary_mask
 from nncf.tensorflow.sparsity.magnitude.operation import BinaryMask
 from nncf.tensorflow.sparsity.magnitude.operation import BinaryMaskWithWeightsBackup
 
 
-@TF_COMPRESSION_ALGORITHMS.register('magnitude_sparsity')
+@TF_COMPRESSION_ALGORITHMS.register("magnitude_sparsity")
 class MagnitudeSparsityBuilder(TFCompressionAlgorithmBuilder):
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
-        self.ignored_scopes = self._algo_config.get('ignored_scopes', [])
+        self.ignored_scopes = self._algo_config.get("ignored_scopes", [])
         self._op_names = []
 
     def get_transformation_layout(self, model: tf.keras.Model) -> TFTransformationLayout:
         converter = TFModelConverterFactory.create(model)
         nncf_graph = converter.convert()
 
         check_scopes_in_graph(nncf_graph, self.ignored_scopes, self.target_scopes)
@@ -83,84 +82,83 @@
             if node.metatype in OUTPUT_NOOP_METATYPES:
                 continue
 
             is_custom, layer_info = converter.get_layer_info_for_node(node.node_name)
             if node.metatype in SPARSITY_LAYER_METATYPES:
                 # Processing a regular weighted node
                 for weight_def in node.metatype.weight_definitions:
-                    op_name = self._get_sparsity_operation_name(node.node_name,
-                                                                weight_def.weight_attr_name)
+                    op_name = self._get_sparsity_operation_name(node.node_name, weight_def.weight_attr_name)
                     self._op_names.append(op_name)
 
                     transformations.register(
                         TFInsertionCommand(
                             target_point=TFLayerWeight(layer_info.layer_name, weight_def.weight_attr_name),
                             callable_object=BinaryMask(op_name),
-                            priority=TransformationPriority.SPARSIFICATION_PRIORITY
-                        ))
+                            priority=TransformationPriority.SPARSIFICATION_PRIORITY,
+                        )
+                    )
             elif node.metatype in WEIGHTABLE_TF_OP_METATYPES:
                 assert is_custom
                 # Processing a custom layer weighted node
                 # Caution: here layer_name will refer to the weight itself, not to the op
                 weight_attr_name = node.layer_name
                 op_name = self._get_sparsity_operation_name(node.node_name, weight_attr_name)
                 self._op_names.append(op_name)
 
                 transformations.register(
                     TFInsertionCommand(
                         target_point=TFLayerWeight(layer_info.layer_name, weight_attr_name),
                         callable_object=BinaryMaskWithWeightsBackup(op_name, weight_attr_name),
-                        priority=TransformationPriority.SPARSIFICATION_PRIORITY
-                    ))
+                        priority=TransformationPriority.SPARSIFICATION_PRIORITY,
+                    )
+                )
 
         return transformations
 
     def _get_sparsity_operation_name(self, layer_name: str, weight_attr_name: str) -> str:
-        return f'{layer_name}_{weight_attr_name}_sparsity_binary_mask'
+        return f"{layer_name}_{weight_attr_name}_sparsity_binary_mask"
 
-    def _build_controller(self, model: tf.keras.Model) -> 'MagnitudeSparsityController':
+    def _build_controller(self, model: tf.keras.Model) -> "MagnitudeSparsityController":
         """
         Simple implementation of building controller without setting builder state and loading controller's one.
         Should be called once the compressed model target_model is fully constructed.
 
         :param model: The model with additional modifications necessary to enable
             algorithm-specific compression during fine-tuning.
         :return: The instance of the `MagnitudeSparsityController`.
         """
         return MagnitudeSparsityController(model, self.config, self._op_names)
 
     def initialize(self, model: tf.keras.Model) -> None:
         pass
 
 
-@ADAPTIVE_COMPRESSION_CONTROLLERS.register('tf_magnitude_sparsity')
+@api()
+@ADAPTIVE_COMPRESSION_CONTROLLERS.register("tf_magnitude_sparsity")
 class MagnitudeSparsityController(BaseSparsityController):
     """
-    Serves as a handle to the additional modules, parameters and hooks inserted
-    into the original uncompressed model in order to enable algorithm-specific compression.
-    Hosts entities that are to be used during the training process, such as compression scheduler and
-    compression loss.
+    Controller class for magnitude sparsity in TF.
     """
 
     def __init__(self, target_model, config: NNCFConfig, op_names):
         super().__init__(target_model, op_names)
-        algo_config = extract_algo_specific_config(config,
-                                                   'magnitude_sparsity')
-        params = deepcopy(algo_config.get('params', {}))
+        algo_config = extract_algo_specific_config(config, "magnitude_sparsity")
+        params = deepcopy(algo_config.get("params", {}))
         self._threshold = 0
         self._frozen = False
-        self._weight_importance_fn = WEIGHT_IMPORTANCE_FUNCTIONS[params.get('weight_importance',
-                                                                            MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE)]
-
-        sparsity_init = algo_config.get('sparsity_init', SPARSITY_INIT)
-        params['sparsity_init'] = sparsity_init
-        scheduler_type = params.get('schedule', 'polynomial')
+        self._weight_importance_fn = WEIGHT_IMPORTANCE_FUNCTIONS[
+            params.get("weight_importance", MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE)
+        ]
+
+        sparsity_init = algo_config.get("sparsity_init", SPARSITY_INIT)
+        params["sparsity_init"] = sparsity_init
+        scheduler_type = params.get("schedule", "polynomial")
 
-        if scheduler_type == 'adaptive':
-            raise ValueError('Magnitude sparsity algorithm do not support adaptive scheduler')
+        if scheduler_type == "adaptive":
+            raise ValueError("Magnitude sparsity algorithm do not support adaptive scheduler")
 
         scheduler_cls = SPARSITY_SCHEDULERS.get(scheduler_type)
         self._scheduler = scheduler_cls(self, params)
         self._loss = TFZeroCompressionLoss()
         self._bn_adaptation = None
         self._config = config
         self.set_sparsity_level(sparsity_init)
@@ -172,20 +170,26 @@
     @property
     def loss(self) -> CompressionLoss:
         return self._loss
 
     def freeze(self, freeze: bool = True):
         self._frozen = freeze
 
-    def set_sparsity_level(self, sparsity_level,
-                           run_batchnorm_adaptation: bool = False):
+    def set_sparsity_level(self, sparsity_level, run_batchnorm_adaptation: bool = False):
+        """
+        Sets the sparsity level that should be applied to the model's weights.
+
+        :param sparsity_level: Sparsity level that should be applied to the model's weights.
+        :param run_batchnorm_adaptation: Whether to run batchnorm adaptation after setting the sparsity level.
+        """
         if not self._frozen:
             if sparsity_level >= 1 or sparsity_level < 0:
                 raise AttributeError(
-                    'Sparsity level should be within interval [0,1), actual value to set is: {}'.format(sparsity_level))
+                    "Sparsity level should be within interval [0,1), actual value to set is: {}".format(sparsity_level)
+                )
 
             self._threshold = self._select_threshold(sparsity_level)
             self._set_masks_for_threshold(self._threshold)
 
         if run_batchnorm_adaptation:
             self._run_batchnorm_adaptation()
 
@@ -201,29 +205,27 @@
     def _set_masks_for_threshold(self, threshold_val):
         for wrapped_layer in collect_wrapped_layers(self._model):
             for weight_attr, ops in wrapped_layer.weights_attr_ops.items():
                 weight = wrapped_layer.layer_weights[weight_attr]
 
                 for op_name in ops:
                     if op_name in self._op_names:
-                        wrapped_layer.ops_weights[op_name]['mask'].assign(
-                            calc_magnitude_binary_mask(weight,
-                                                       self._weight_importance_fn,
-                                                       threshold_val)
+                        wrapped_layer.ops_weights[op_name]["mask"].assign(
+                            calc_magnitude_binary_mask(weight, self._weight_importance_fn, threshold_val)
                         )
 
     def _collect_all_weights(self):
         all_weights = []
         for wrapped_layer in collect_wrapped_layers(self._model):
             for weight_attr, ops in wrapped_layer.weights_attr_ops.items():
                 for op_name in ops:
                     if op_name in self._op_names:
-                        all_weights.append(tf.reshape(
-                            self._weight_importance_fn(wrapped_layer.layer_weights[weight_attr]),
-                            [-1]))
+                        all_weights.append(
+                            tf.reshape(self._weight_importance_fn(wrapped_layer.layer_weights[weight_attr]), [-1])
+                        )
         return all_weights
 
     @property
     def compression_rate(self) -> float:
         return self.statistics().magnitude_sparsity.model_statistics.sparsity_level
 
     @compression_rate.setter
@@ -247,23 +249,23 @@
             threshold_stats.append(LayerThreshold(s.name, threshold))
 
         target_sparsity_level = self.scheduler.current_sparsity_level
 
         stats = MagnitudeSparsityStatistics(model_stats, threshold_stats, target_sparsity_level)
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('magnitude_sparsity', stats)
+        nncf_stats.register("magnitude_sparsity", stats)
         return nncf_stats
 
     def compression_stage(self) -> CompressionStage:
         if self.scheduler.current_sparsity_level >= self.scheduler.target_level:
             return CompressionStage.FULLY_COMPRESSED
         if self.scheduler.current_sparsity_level == 0:
             return CompressionStage.UNCOMPRESSED
         return CompressionStage.PARTIALLY_COMPRESSED
 
     def _run_batchnorm_adaptation(self):
         if self._bn_adaptation is None:
             self._bn_adaptation = BatchnormAdaptationAlgorithm(
-                **extract_bn_adaptation_init_params(self._config,
-                                                    'magnitude_sparsity'))
+                **extract_bn_adaptation_init_params(self._config, self.name)
+            )
         self._bn_adaptation.run(self.model)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/magnitude/functions.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/magnitude/functions.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,35 +1,30 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 
 def abs_magnitude(weight):
     return tf.abs(weight)
 
 
 def normed_magnitude(weight):
     return tf.abs(tf.math.l2_normalize(weight))
 
 
-WEIGHT_IMPORTANCE_FUNCTIONS = {
-    'abs': abs_magnitude,
-    'normed_abs': normed_magnitude
-}
+WEIGHT_IMPORTANCE_FUNCTIONS = {"abs": abs_magnitude, "normed_abs": normed_magnitude}
 
 
 def calc_magnitude_binary_mask(weight, weight_importance, threshold):
     return tf.cast(weight_importance(weight) > threshold, tf.float32)
 
 
 def apply_mask(weights, mask):
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/magnitude/operation.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/magnitude/operation.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,86 +1,79 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 from nncf.tensorflow.graph.utils import get_weight_by_name
 from nncf.tensorflow.layers.custom_objects import NNCF_CUSTOM_OBJECTS
 from nncf.tensorflow.layers.operation import InputType
 from nncf.tensorflow.layers.operation import NNCFOperation
 from nncf.tensorflow.sparsity.magnitude.functions import apply_mask
 
 
 @NNCF_CUSTOM_OBJECTS.register()
 class BinaryMask(NNCFOperation):
     def build(self, input_shape, input_type, name, layer):
         if input_type is not InputType.WEIGHTS:
-            raise ValueError(
-                'Binary Mask operation could not be applied to input of the layer: {}'.
-                    format(layer.name))
+            raise ValueError("Binary Mask operation could not be applied to input of the layer: {}".format(layer.name))
 
         mask = layer.add_weight(
-            name + '_mask',
+            name + "_mask",
             shape=input_shape,
             initializer=tf.keras.initializers.Constant(1.0),
             trainable=False,
-            aggregation=tf.VariableAggregation.MEAN)
+            aggregation=tf.VariableAggregation.MEAN,
+        )
 
-        return {
-            'mask': mask
-        }
+        return {"mask": mask}
 
     def call(self, inputs, weights, _):
-        return apply_mask(inputs, weights['mask'])
+        return apply_mask(inputs, weights["mask"])
 
     @staticmethod
     def get_binary_mask(op_weights):
         """
         Returns binary mask from weights of the operation.
 
         :param op_weights: Weights of the operaton.
         :return: Binary mask.
         """
-        return op_weights['mask']
+        return op_weights["mask"]
 
 
 @NNCF_CUSTOM_OBJECTS.register()
 class BinaryMaskWithWeightsBackup(BinaryMask):
     def __init__(self, name: str, w_name_to_bkup: str = None):
         super().__init__(name)
         self.w_name_to_bkup = w_name_to_bkup
         self.bkup_var = None
 
     def build(self, input_shape, input_type, name, layer):
         self.bkup_var = self._create_bkup_weights(layer, self.w_name_to_bkup)
         return super().build(input_shape, input_type, name, layer)
 
     def call(self, inputs, weights, _):
-        self.bkup_var.assign(tf.where(weights['mask'] > 0.5, inputs, self.bkup_var))
-        return apply_mask(self.bkup_var, weights['mask'])
+        self.bkup_var.assign(tf.where(weights["mask"] > 0.5, inputs, self.bkup_var))
+        return apply_mask(self.bkup_var, weights["mask"])
 
     @staticmethod
     def _create_bkup_weights(layer, w_name):
         var = get_weight_by_name(layer, w_name)
         bkup_var = layer.add_weight(
-            w_name + '_bkup',
-            shape=var.shape,
-            trainable=False,
-            aggregation=tf.VariableAggregation.MEAN)
+            w_name + "_bkup", shape=var.shape, trainable=False, aggregation=tf.VariableAggregation.MEAN
+        )
 
         bkup_var.assign(var.read_value())
         return bkup_var
 
     def get_config(self):
         config = super().get_config()
-        config['w_name_to_bkup'] = self.w_name_to_bkup
+        config["w_name_to_bkup"] = self.w_name_to_bkup
         return config
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/rb/algorithm.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/rb/algorithm.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,36 +1,34 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from copy import deepcopy
-from typing import List
-from typing import Set
+from typing import List, Set
 
 import numpy as np
 import tensorflow as tf
 
 from nncf import NNCFConfig
 from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.scopes import check_scopes_in_graph
 from nncf.common.scopes import should_consider_scope
 from nncf.common.sparsity.schedulers import SPARSITY_SCHEDULERS
 from nncf.common.sparsity.schedulers import SparsityScheduler
 from nncf.common.sparsity.statistics import RBSparsityStatistics
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.schemata.defaults import SPARSITY_INIT
 from nncf.config.schemata.defaults import SPARSITY_LEVEL_SETTING_MODE
 from nncf.tensorflow.algorithm_selector import TF_COMPRESSION_ALGORITHMS
 from nncf.tensorflow.api.compression import TFCompressionAlgorithmBuilder
 from nncf.tensorflow.graph.converter import TFModelConverterFactory
 from nncf.tensorflow.graph.transformations.commands import TFInsertionCommand
@@ -41,19 +39,19 @@
 from nncf.tensorflow.sparsity.base_algorithm import SPARSITY_LAYER_METATYPES
 from nncf.tensorflow.sparsity.base_algorithm import BaseSparsityController
 from nncf.tensorflow.sparsity.collector import TFSparseModelStatisticsCollector
 from nncf.tensorflow.sparsity.rb.loss import SparseLoss
 from nncf.tensorflow.sparsity.rb.operation import RBSparsifyingWeight
 
 
-@TF_COMPRESSION_ALGORITHMS.register('rb_sparsity')
+@TF_COMPRESSION_ALGORITHMS.register("rb_sparsity")
 class RBSparsityBuilder(TFCompressionAlgorithmBuilder):
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
-        self.ignored_scopes = self._algo_config.get('ignored_scopes', [])
+        self.ignored_scopes = self._algo_config.get("ignored_scopes", [])
         self._op_names = []
 
     def get_transformation_layout(self, model: tf.keras.Model) -> TFTransformationLayout:
         converter = TFModelConverterFactory.create(model)
         nncf_graph = converter.convert()
 
         check_scopes_in_graph(nncf_graph, self.ignored_scopes, self.target_scopes)
@@ -65,37 +63,39 @@
         for node in nncf_graph.get_all_nodes():
             if node.is_shared():
                 target_layer_name, _ = get_original_name_and_instance_idx(node.node_name)
                 if target_layer_name in processed_shared_layer_names:
                     continue
                 processed_shared_layer_names.add(target_layer_name)
 
-            if not (node.metatype in SPARSITY_LAYER_METATYPES and
-                    should_consider_scope(node.node_name, ignored_scopes=self.ignored_scopes)):
+            if not (
+                node.metatype in SPARSITY_LAYER_METATYPES
+                and should_consider_scope(node.node_name, ignored_scopes=self.ignored_scopes)
+            ):
                 continue
 
             _, layer_info = converter.get_layer_info_for_node(node.node_name)
             for weight_def in node.metatype.weight_definitions:
-                op_name = self._get_rb_sparsity_operation_name(node.node_name,
-                                                               weight_def.weight_attr_name)
+                op_name = self._get_rb_sparsity_operation_name(node.node_name, weight_def.weight_attr_name)
                 self._op_names.append(op_name)
 
                 transformations.register(
                     TFInsertionCommand(
                         target_point=TFLayerWeight(layer_info.layer_name, weight_def.weight_attr_name),
                         callable_object=RBSparsifyingWeight(op_name),
-                        priority=TransformationPriority.SPARSIFICATION_PRIORITY
-                    ))
+                        priority=TransformationPriority.SPARSIFICATION_PRIORITY,
+                    )
+                )
 
         return transformations
 
     def _get_rb_sparsity_operation_name(self, layer_name: str, weight_attr_name: str) -> str:
-        return f'{layer_name}_{weight_attr_name}_rb_sparsity_weight'
+        return f"{layer_name}_{weight_attr_name}_rb_sparsity_weight"
 
-    def _build_controller(self, model: tf.keras.Model) -> 'RBSparsityController':
+    def _build_controller(self, model: tf.keras.Model) -> "RBSparsityController":
         """
         Simple implementation of building controller without setting builder state and loading controller's one.
         Should be called once the compressed model target_model is fully constructed.
 
         :param model: The model with additional modifications necessary to enable
             algorithm-specific compression during fine-tuning.
         :return: The instance of the `RBSparsityController`.
@@ -103,38 +103,41 @@
 
         return RBSparsityController(model, self.config, self._op_names)
 
     def initialize(self, model: tf.keras.Model) -> None:
         pass
 
 
-@ADAPTIVE_COMPRESSION_CONTROLLERS.register('tf_rb_sparsity')
+@api()
+@ADAPTIVE_COMPRESSION_CONTROLLERS.register("tf_rb_sparsity")
 class RBSparsityController(BaseSparsityController):
+    """
+    Controller class for regularization-based (RB) sparsity in TF.
+    """
+
     def __init__(self, target_model, config: NNCFConfig, op_names: List[str]):
         super().__init__(target_model, op_names)
         algo_config = extract_algo_specific_config(config, "rb_sparsity")
-        sparsity_init = algo_config.get('sparsity_init', SPARSITY_INIT)
-        params = deepcopy(algo_config.get('params', {}))
-        params['sparsity_init'] = sparsity_init
-        sparsity_level_mode = params.get('sparsity_level_setting_mode', SPARSITY_LEVEL_SETTING_MODE)
+        sparsity_init = algo_config.get("sparsity_init", SPARSITY_INIT)
+        params = deepcopy(algo_config.get("params", {}))
+        params["sparsity_init"] = sparsity_init
+        sparsity_level_mode = params.get("sparsity_level_setting_mode", SPARSITY_LEVEL_SETTING_MODE)
 
-        if sparsity_level_mode == 'local':
-            raise NotImplementedError('RB sparsity algorithm do not support local sparsity loss')
+        if sparsity_level_mode == "local":
+            raise NotImplementedError("RB sparsity algorithm do not support local sparsity loss")
 
         target_ops = []
         for wrapped_layer, _, op in get_nncf_operations(self.model, self._op_names):
-            target_ops.append(
-                (op, wrapped_layer.get_operation_weights(op.name))
-            )
+            target_ops.append((op, wrapped_layer.get_operation_weights(op.name)))
 
         self._loss = SparseLoss(target_ops)
-        schedule_type = params.get('schedule', 'exponential')
+        schedule_type = params.get("schedule", "exponential")
 
-        if schedule_type == 'adaptive':
-            raise NotImplementedError('RB sparsity algorithm do not support adaptive scheduler')
+        if schedule_type == "adaptive":
+            raise NotImplementedError("RB sparsity algorithm do not support adaptive scheduler")
 
         scheduler_cls = SPARSITY_SCHEDULERS.get(schedule_type)
         self._scheduler = scheduler_cls(self, params)
         self.set_sparsity_level(sparsity_init)
 
     @property
     def scheduler(self) -> SparsityScheduler:
@@ -164,15 +167,15 @@
         mean_sparse_prob = 1.0 - (sparse_prob_sum / num_weights)
 
         target_sparsity_level = self.scheduler.current_sparsity_level
 
         stats = RBSparsityStatistics(model_stats, target_sparsity_level, mean_sparse_prob)
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('rb_sparsity', stats)
+        nncf_stats.register("rb_sparsity", stats)
         return nncf_stats
 
     @property
     def compression_rate(self) -> float:
         return self._loss.target_sparsity_rate
 
     @compression_rate.setter
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/rb/functions.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/rb/functions.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,23 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
-from nncf.tensorflow.functions import logit, st_threshold
+from nncf.tensorflow.functions import logit
+from nncf.tensorflow.functions import st_threshold
 
 
 def binary_mask(mask):
     return tf.round(tf.math.sigmoid(mask))
 
 
 def st_binary_mask(mask):
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/rb/loss.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/rb/loss.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,21 +1,19 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict, Any
+from typing import Any, Dict
 
 import tensorflow as tf
 
 from nncf.api.compression import CompressionLoss
 
 
 class SparseLoss(CompressionLoss):
@@ -23,58 +21,56 @@
         super().__init__()
         self._target_ops = target_ops
         self.target = tf.Variable(target, trainable=False)
         self.p = p
         self.disabled = tf.Variable(False, trainable=False)
 
     def disable(self):
-        tf.cond(tf.cast(self.disabled, tf.bool),
-                lambda: None,
-                self._disable)
+        tf.cond(tf.cast(self.disabled, tf.bool), lambda: None, self._disable)
 
     def _disable(self):
         self.disabled.assign(True)
 
         for op, op_weights in self._target_ops:
             op.freeze(op_weights)
 
     def calculate(self, *args, **kwargs):
-        return tf.cond(tf.cast(self.disabled, tf.bool),
-                       lambda: tf.constant(0.),
-                       self._calculate)
+        return tf.cond(tf.cast(self.disabled, tf.bool), lambda: tf.constant(0.0), self._calculate)
 
     def _calculate(self):
         params = tf.constant(0)
-        loss = tf.constant(0.)
+        loss = tf.constant(0.0)
         for op, op_weights in self._target_ops:
             tf.debugging.assert_equal(
-                op.get_trainable_weight(op_weights), tf.constant(True),
-                'Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss')
+                op.get_trainable_weight(op_weights),
+                tf.constant(True),
+                "Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss",
+            )
             mask = op.get_mask(op_weights)
             params = params + tf.size(mask)
             loss = loss + op.loss(op_weights)
 
         params = tf.cast(params, tf.float32)
         return tf.reshape(tf.math.pow(((loss / params - self.target) / self.p), 2), shape=[])
 
     @property
     def target_sparsity_rate(self):
         eager_target = tf.keras.backend.eval(self.target)
-        rate = 1. - eager_target
+        rate = 1.0 - eager_target
         if rate < 0 or rate > 1:
-            raise ValueError('Target is not within range [0, 1]')
+            raise ValueError("Target is not within range [0, 1]")
         return rate
 
     def set_target_sparsity_loss(self, sparsity_level):
         self.target.assign(1 - sparsity_level)
 
     def load_state(self, state: Dict[str, Any]) -> None:
-        self.target.assign(state['target'])
-        self.disabled.assign(state['disabled'])
-        self.p = state['p']
+        self.target.assign(state["target"])
+        self.disabled.assign(state["disabled"])
+        self.p = state["p"]
 
     def get_state(self) -> Dict[str, Any]:
         return {
-            'target': float(tf.keras.backend.eval(self.target)),
-            'disabled': bool(tf.keras.backend.eval(tf.cast(self.disabled, tf.bool))),
-            'p': self.p
+            "target": float(tf.keras.backend.eval(self.target)),
+            "disabled": bool(tf.keras.backend.eval(tf.cast(self.disabled, tf.bool))),
+            "p": self.p,
         }
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/rb/operation.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/rb/operation.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import tensorflow as tf
 import numpy as np
+import tensorflow as tf
 
+from nncf.tensorflow import tf_internals
 from nncf.tensorflow.functions import logit
 from nncf.tensorflow.layers.custom_objects import NNCF_CUSTOM_OBJECTS
 from nncf.tensorflow.layers.operation import InputType
 from nncf.tensorflow.layers.operation import NNCFOperation
-from nncf.tensorflow.sparsity.magnitude.functions import apply_mask
-from nncf.tensorflow.sparsity.rb.functions import calc_rb_binary_mask, st_binary_mask, binary_mask
 from nncf.tensorflow.layers.wrapper import NNCFWrapper
-from nncf.tensorflow import tf_internals
+from nncf.tensorflow.sparsity.magnitude.functions import apply_mask
+from nncf.tensorflow.sparsity.rb.functions import binary_mask
+from nncf.tensorflow.sparsity.rb.functions import calc_rb_binary_mask
+from nncf.tensorflow.sparsity.rb.functions import st_binary_mask
 
 
 @NNCF_CUSTOM_OBJECTS.register()
 class RBSparsifyingWeight(NNCFOperation):
     def __init__(self, name: str, eps: float = 1e-6):
         """
         :param name: Model scope unique operation name.
@@ -40,110 +40,109 @@
         :param input_shape: Shape of weights which needs to be sparsifyed.
         :param input_type: Type of operation input, must be InputType.WEIGHTS.
         :param name: Name of weight attribute which needs to be sparsifyed.
         :param layer: Layer which needs to be sparsifyed.
         """
         if input_type is not InputType.WEIGHTS:
             raise ValueError(
-                'RB Sparsity mask operation could not be applied '
-                'to input of the layer: {}'.format(layer.name))
+                "RB Sparsity mask operation could not be applied to input of the layer: {}".format(layer.name)
+            )
 
         mask = layer.add_weight(
-            name + '_mask',
+            name + "_mask",
             shape=input_shape,
             initializer=tf.keras.initializers.Constant(logit(0.99)),
             trainable=True,
-            aggregation=tf.VariableAggregation.MEAN)
+            aggregation=tf.VariableAggregation.MEAN,
+        )
 
         trainable = layer.add_weight(
-            name + '_trainable',
-            initializer=tf.keras.initializers.Constant(True),
-            trainable=False,
-            dtype=tf.bool)
+            name + "_trainable", initializer=tf.keras.initializers.Constant(True), trainable=False, dtype=tf.bool
+        )
 
         seed = layer.add_weight(
-            name + '_seed',
+            name + "_seed",
             shape=(2,),
-            initializer=tf.keras.initializers.Constant(
-                            np.random.randint(size=(2,), low=-2**31, high=2**31-1)),
+            initializer=tf.keras.initializers.Constant(np.random.randint(size=(2,), low=-(2**31), high=2**31 - 1)),
             trainable=False,
-            dtype=tf.int32)
+            dtype=tf.int32,
+        )
 
         return {
-            'mask': mask,
-            'trainable': trainable,
-            'seed': seed,
+            "mask": mask,
+            "trainable": trainable,
+            "seed": seed,
         }
 
     def call(self, inputs, weights, training: tf.constant):
         """
         Apply rb sparsity mask to given weights.
 
         :param inputs: Target weights to sparsify.
         :param weights: Operation weights contains
             `mask` and param `trainable`.
         :param training: True if operation called in training mode
             else False
         """
         true_fn = lambda: apply_mask(inputs, self._calc_rb_binary_mask(weights))
-        false_fn = lambda: apply_mask(inputs, binary_mask(weights['mask']))
-        return tf_internals.smart_cond(training,
-                                       true_fn=lambda: tf_internals.smart_cond(weights['trainable'],
-                                                                               true_fn=true_fn, false_fn=false_fn),
-                                       false_fn=false_fn)
+        false_fn = lambda: apply_mask(inputs, binary_mask(weights["mask"]))
+        return tf_internals.smart_cond(
+            training,
+            true_fn=lambda: tf_internals.smart_cond(weights["trainable"], true_fn=true_fn, false_fn=false_fn),
+            false_fn=false_fn,
+        )
 
     def _calc_rb_binary_mask(self, op_weights):
-        new_seed = tf.random.stateless_uniform(
-                       (2,), seed=op_weights['seed'], minval=-2**31, maxval=2**31-1)
+        new_seed = tf.random.stateless_uniform((2,), seed=op_weights["seed"], minval=-(2**31), maxval=2**31 - 1)
         new_seed = tf.cast(new_seed, tf.int32)
-        op_weights['seed'].assign(new_seed)
-        return calc_rb_binary_mask(op_weights['mask'], op_weights['seed'], self.eps)
+        op_weights["seed"].assign(new_seed)
+        return calc_rb_binary_mask(op_weights["mask"], op_weights["seed"], self.eps)
 
     def freeze(self, op_weights):
         """
         Freeze rb mask from operation weights.
 
         :param op_weights: Operation weights.
         """
-        op_weights['trainable'].assign(False)
+        op_weights["trainable"].assign(False)
 
     @staticmethod
     def loss(op_weights):
         """
         Return count of non zero weight in mask.
 
         :param op_weights: Operation weights.
         """
-        return tf.reduce_sum(st_binary_mask(op_weights['mask']))
+        return tf.reduce_sum(st_binary_mask(op_weights["mask"]))
 
     @staticmethod
     def get_mask(op_weights):
         """
         Return mask weight from operation weights.
 
         :param op_weights: Operation weights.
         """
-        return op_weights['mask']
+        return op_weights["mask"]
 
     @staticmethod
     def get_binary_mask(op_weights):
         """
         Returns binary mask from weights of the operation.
 
         :param op_weights: Weights of the operaton.
         :return: Binary mask.
         """
-        return binary_mask(op_weights['mask'])
+        return binary_mask(op_weights["mask"])
 
     @staticmethod
     def get_trainable_weight(op_weights):
         """
         Return trainable weight from operation weights.
 
         :param op_weights: Operation weights.
         """
-        return op_weights['trainable']
+        return op_weights["trainable"]
 
     def get_config(self):
         config = super().get_config()
-        config['eps'] = self.eps
+        config["eps"] = self.eps
         return config
```

### Comparing `nncf-2.4.0/nncf/tensorflow/sparsity/utils.py` & `nncf-2.5.0/nncf/tensorflow/sparsity/utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,51 +1,47 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 import tensorflow as tf
 
 from nncf.tensorflow.graph.model_transformer import TFModelTransformer
 from nncf.tensorflow.graph.transformations.commands import TFOperationWithWeights
 from nncf.tensorflow.graph.transformations.commands import TFRemovalCommand
 from nncf.tensorflow.graph.transformations.layout import TFTransformationLayout
 from nncf.tensorflow.graph.utils import get_nncf_operations
-from nncf.tensorflow.layers.wrapper import NNCFWrapper
 from nncf.tensorflow.layers.operation import NNCFOperation
+from nncf.tensorflow.layers.wrapper import NNCFWrapper
 
 
 def strip_model_from_masks(model: tf.keras.Model, op_names: List[str]) -> tf.keras.Model:
     if not isinstance(model, tf.keras.Model):
-        raise ValueError(f'Expected model to be a `tf.keras.Model` instance but got: {type(model)}')
+        raise ValueError(f"Expected model to be a `tf.keras.Model` instance but got: {type(model)}")
 
     transformations = TFTransformationLayout()
     for wrapped_layer, weight_attr, op in get_nncf_operations(model, op_names):
         apply_mask(wrapped_layer, weight_attr, op)
         transformations.register(
             TFRemovalCommand(
                 target_point=TFOperationWithWeights(
-                    wrapped_layer.name,
-                    weights_attr_name=weight_attr,
-                    operation_name=op.name)
-            ))
+                    wrapped_layer.name, weights_attr_name=weight_attr, operation_name=op.name
+                )
+            )
+        )
 
     return TFModelTransformer(model).transform(transformations)
 
 
 def apply_mask(wrapped_layer: NNCFWrapper, weight_attr: str, op: NNCFOperation) -> None:
     layer_weight = wrapped_layer.layer_weights[weight_attr]
     op_weights = wrapped_layer.get_operation_weights(op.name)
-    layer_weight.assign(
-        op(layer_weight, op_weights, False)
-    )
+    layer_weight.assign(op(layer_weight, op_weights, False))
     wrapped_layer.set_layer_weight(weight_attr, layer_weight)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/tensor.py` & `nncf-2.5.0/nncf/tensorflow/tensor.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
 from nncf.common.tensor import NNCFTensor
 
 
 class TFNNCFTensor(NNCFTensor):
```

### Comparing `nncf-2.4.0/nncf/tensorflow/tensor_statistics/collectors.py` & `nncf-2.5.0/nncf/tensorflow/tensor_statistics/collectors.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,88 +1,114 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Union, Deque, List
+from typing import Any, Callable, Deque, List, Optional, Union
 
 import numpy as np
 import tensorflow as tf
 
 from nncf.common.tensor import NNCFTensor
 from nncf.common.tensor import TensorElementsType
+from nncf.common.tensor_statistics.collectors import MeanMinMaxStatisticCollector
+from nncf.common.tensor_statistics.collectors import MeanPercentileStatisticCollector
 from nncf.common.tensor_statistics.collectors import MedianMADStatisticCollector
+from nncf.common.tensor_statistics.collectors import MinMaxStatisticCollector
+from nncf.common.tensor_statistics.collectors import MixedMinMaxStatisticCollector
 from nncf.common.tensor_statistics.collectors import NNCFCollectorTensorProcessor
 from nncf.common.tensor_statistics.collectors import PercentileStatisticCollector
-from nncf.common.tensor_statistics.collectors import MeanPercentileStatisticCollector
-from nncf.common.tensor_statistics.collectors import MixedMinMaxStatisticCollector
-from nncf.common.tensor_statistics.collectors import MeanMinMaxStatisticCollector
-from nncf.common.tensor_statistics.collectors import MinMaxStatisticCollector
 from nncf.common.tensor_statistics.reduction import np_percentile_reduce_like
+from nncf.tensorflow.tensor import TFNNCFTensor
+from nncf.tensorflow.tensor_statistics.reduction import convert_rs_to_pt_type
+from nncf.tensorflow.tensor_statistics.statistics import TFMedianMADTensorStatistic
 from nncf.tensorflow.tensor_statistics.statistics import TFMinMaxTensorStatistic
 from nncf.tensorflow.tensor_statistics.statistics import TFPercentileTensorStatistic
-from nncf.tensorflow.tensor_statistics.statistics import TFMedianMADTensorStatistic
-from nncf.tensorflow.tensor_statistics.reduction import convert_rs_to_pt_type
-from nncf.tensorflow.tensor import TFNNCFTensor
 
 
 class TFNNCFCollectorTensorProcessor(NNCFCollectorTensorProcessor):
     """
     A realization of the processing methods for TFNNCFTensors.
     """
 
     @staticmethod
-    def reduce_min(x: NNCFTensor, axis: Union[int, tuple, list]) -> NNCFTensor:
-        return TFNNCFTensor(tf.reduce_min(x.tensor, axis=axis))
+    def reduce_min(x: NNCFTensor, axis: Union[int, tuple, list], keepdims: bool = False) -> NNCFTensor:
+        return TFNNCFTensor(tf.reduce_min(x.tensor, axis=axis, keepdims=keepdims))
 
     @staticmethod
-    def reduce_max(x: NNCFTensor, axis: Union[int, tuple, list]) -> NNCFTensor:
-        return TFNNCFTensor(tf.reduce_max(x.tensor, axis=axis))
+    def reduce_max(x: NNCFTensor, axis: Union[int, tuple, list], keepdims: bool = False) -> NNCFTensor:
+        return TFNNCFTensor(tf.reduce_max(x.tensor, axis=axis, keepdims=keepdims))
 
     @staticmethod
     def abs(x: NNCFTensor) -> NNCFTensor:
         return TFNNCFTensor(tf.math.abs(x.tensor))
 
     @staticmethod
     def min(x1: tf.Tensor, x2: tf.Tensor) -> NNCFTensor:
         return TFNNCFTensor(tf.math.minimum(x1.tensor, x2.tensor))
 
     @staticmethod
     def max(x1: tf.Tensor, x2: tf.Tensor) -> NNCFTensor:
         return TFNNCFTensor(tf.math.maximum(x1.tensor, x2.tensor))
 
     @staticmethod
-    def mean(x: NNCFTensor, axis: Union[int, tuple, list]) -> NNCFTensor:
-        return TFNNCFTensor(tf.math.reduce_mean(x.tensor, axis=axis))
+    def mean(x: NNCFTensor, axis: Union[int, tuple, list], keepdims=False) -> NNCFTensor:
+        return TFNNCFTensor(tf.math.reduce_mean(x.tensor, axis=axis, keepdims=keepdims))
+
+    @staticmethod
+    def median(x: NNCFTensor, axis: Union[int, tuple, list], keepdims=False) -> NNCFTensor:
+        raise NotImplementedError()
+
+    @staticmethod
+    def masked_mean(x: NNCFTensor, axis: Union[int, tuple, list], mask: NNCFTensor, keepdims=False) -> NNCFTensor:
+        raise NotImplementedError()
+
+    @staticmethod
+    def masked_median(x: NNCFTensor, axis: Union[int, tuple, list], mask: NNCFTensor, keepdims=False) -> NNCFTensor:
+        raise NotImplementedError()
 
     @staticmethod
     def stack(x: Union[List[tf.Tensor], Deque[tf.Tensor]], axis: int = 0) -> NNCFTensor:
         x = [t.tensor for t in x]
         return TFNNCFTensor(tf.stack(x, axis=axis))
 
     @staticmethod
     def unstack(x: NNCFTensor, axis: int = 0) -> List[NNCFTensor]:
         tensor = x.tensor
-        if list(tensor.shape) == []: #pylint: disable=C1803
+        if list(tensor.shape) == []:  # pylint: disable=C1803
             tensor = tf.expand_dims(tensor, 0)
         tensor_list = tf.unstack(tensor, axis=axis)
         return [TFNNCFTensor(t) for t in tensor_list]
 
     @staticmethod
     def sum(tensor: NNCFTensor) -> TensorElementsType:
         return tf.reduce_sum(tensor.tensor).numpy()
 
+    @staticmethod
+    def quantile(
+        tensor: NNCFTensor, quantile: Union[float, List[float]], axis: Union[int, tuple, list], keepdims: bool = False
+    ) -> List[NNCFTensor]:
+        raise NotImplementedError()
+
+    @staticmethod
+    def mean_per_channel(x: NNCFTensor, axis: int) -> NNCFTensor:
+        raise NotImplementedError()
+
+    @classmethod
+    def no_outliers_map(
+        cls, x: NNCFTensor, fn: Callable[[NNCFTensor, Optional[int]], Any], axis: int = 0, alpha: float = 0.01
+    ):
+        raise NotImplementedError()
+
 
 class TFMinMaxStatisticCollector(MinMaxStatisticCollector):
     @staticmethod
     def _get_processor() -> NNCFCollectorTensorProcessor:
         return TFNNCFCollectorTensorProcessor()
 
     def _register_input(self, x: tf.Tensor):
```

### Comparing `nncf-2.4.0/nncf/tensorflow/tensor_statistics/reduction.py` & `nncf-2.5.0/nncf/tensorflow/tensor_statistics/reduction.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,51 +1,49 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Union, Tuple
+from typing import Tuple, Union
 
 import tensorflow as tf
 
-from nncf.tensorflow.layers.data_layout import get_weight_shape
 from nncf.common.tensor_statistics.collectors import ReductionShape
+from nncf.tensorflow.layers.data_layout import get_weight_shape
 
 
 def get_axes(ndims: int, per_channel: bool, channel_axes: Union[int, list, tuple]) -> list:
     axes = list(range(ndims))
     if per_channel:
         for val in channel_axes:
             val = (ndims + val) % ndims
             axes.remove(val)
     return axes
 
 
-def get_reduction_shape_activations(layer: tf.keras.layers.Layer,
-                                    channel_axes: Union[int, tuple, list],
-                                    use_per_sample_stats: bool) -> ReductionShape:
+def get_reduction_shape_activations(
+    layer: tf.keras.layers.Layer, channel_axes: Union[int, tuple, list], use_per_sample_stats: bool
+) -> ReductionShape:
     ndims = len(layer.get_input_shape_at(0))
     channel_axes_ = channel_axes if isinstance(channel_axes, (list, tuple)) else [channel_axes]
     reduction_shape = get_axes(ndims, layer.per_channel, channel_axes_)
     if use_per_sample_stats:
         reduction_shape = reduction_shape[1:]
     return tuple(reduction_shape)
 
 
-def get_reduction_shape_weights(layer: tf.keras.layers.Layer,
-                                weight_attr: str, channel_axes: Union[int, tuple, list],
-                                per_channel: bool) -> ReductionShape:
+def get_reduction_shape_weights(
+    layer: tf.keras.layers.Layer, weight_attr: str, channel_axes: Union[int, tuple, list], per_channel: bool
+) -> ReductionShape:
     weight_shape = get_weight_shape(layer, weight_attr)
     ndims = len(weight_shape)
     channel_axes_ = channel_axes if isinstance(channel_axes, (list, tuple)) else [channel_axes]
     reduction_shape = get_axes(ndims, per_channel, channel_axes_)
     return tuple(reduction_shape)
```

### Comparing `nncf-2.4.0/nncf/tensorflow/tensor_statistics/statistics.py` & `nncf-2.5.0/nncf/tensorflow/tensor_statistics/statistics.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import tensorflow as tf
 
-from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic, TensorStatistic
 from nncf.common.tensor_statistics.statistics import MedianMADTensorStatistic
+from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
 from nncf.common.tensor_statistics.statistics import PercentileTensorStatistic
+from nncf.common.tensor_statistics.statistics import TensorStatistic
 
 
 class TFMinMaxTensorStatistic(MinMaxTensorStatistic):
     @staticmethod
     def tensor_eq(tensor1: tf.Tensor, tensor2: tf.Tensor, rtol=1e-6) -> bool:
         return bool(tf.experimental.numpy.allclose(tensor1, tensor2, rtol=rtol))
 
@@ -38,17 +37,20 @@
 
 def tf_convert_stat_to_min_max_tensor_stat(statistic: TensorStatistic) -> TFMinMaxTensorStatistic:
     if isinstance(statistic, TFMinMaxTensorStatistic):
         return statistic
     if isinstance(statistic, TFMedianMADTensorStatistic):
         # Using three-sigma approach to estimate min and max
         # Constant factor depends on the distribution form - assuming normal and the factor is 1.4826
-        return TFMinMaxTensorStatistic(statistic.median_values - 3 * 1.4826230 * statistic.mad_values,
-                                     statistic.median_values + 3 * 1.4826230 * statistic.mad_values)
+        return TFMinMaxTensorStatistic(
+            statistic.median_values - 3 * 1.4826230 * statistic.mad_values,
+            statistic.median_values + 3 * 1.4826230 * statistic.mad_values,
+        )
     if isinstance(statistic, TFPercentileTensorStatistic):
         if len(statistic.percentile_vs_values_dict.keys()) < 2:
             raise ValueError("Cannot create a min-max statistic for less than 2 percentile values")
         min_pct = min(statistic.percentile_vs_values_dict.keys())
         max_pct = max(statistic.percentile_vs_values_dict.keys())
-        return TFMinMaxTensorStatistic(statistic.percentile_vs_values_dict[min_pct],
-                                     statistic.percentile_vs_values_dict[max_pct])
+        return TFMinMaxTensorStatistic(
+            statistic.percentile_vs_values_dict[min_pct], statistic.percentile_vs_values_dict[max_pct]
+        )
     raise ValueError("Unknown TensorStatistic to generate min-max stat from!")
```

### Comparing `nncf-2.4.0/nncf/tensorflow/tf_internals.py` & `nncf-2.5.0/nncf/tensorflow/tf_internals.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,35 +1,33 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
- # pylint: disable=unused-import
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# pylint: disable=unused-import
 
 from packaging import version
-
-from nncf.tensorflow import tensorflow_version
-
 from tensorflow.python.eager import context as eager_context
 
+from nncf.tensorflow import tensorflow_version
 
-if version.parse(tensorflow_version) < version.parse('2.6'):
+if version.parse(tensorflow_version) < version.parse("2.6"):
+    from tensorflow.python.keras import backend
     from tensorflow.python.keras import engine as keras_engine
-    from tensorflow.python.keras import backend, layers
-    from tensorflow.python.keras.utils.control_flow_util import smart_cond
-    from tensorflow.python.keras.engine.keras_tensor import KerasTensor
+    from tensorflow.python.keras import layers
     from tensorflow.python.keras.applications import imagenet_utils
+    from tensorflow.python.keras.engine.keras_tensor import KerasTensor
     from tensorflow.python.keras.layers import Rescaling
+    from tensorflow.python.keras.utils.control_flow_util import smart_cond
 else:
+    from keras import backend
     from keras import engine as keras_engine
-    from keras import backend, layers
-    from keras.utils.control_flow_util import smart_cond
-    from keras.engine.keras_tensor import KerasTensor
+    from keras import layers
     from keras.applications import imagenet_utils
+    from keras.engine.keras_tensor import KerasTensor
     from keras.layers import Rescaling
+    from keras.utils.control_flow_util import smart_cond
```

### Comparing `nncf-2.4.0/nncf/tensorflow/utils/hook_handle.py` & `nncf-2.5.0/nncf/tensorflow/utils/hook_handle.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,27 +1,26 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import weakref
 
 
 class HookHandle:
     """
     A handle to remove a hook
     """
+
     id = 0
 
     def __init__(self, hooks_registry):
         self.hooks_registry_ref = weakref.ref(hooks_registry)
         self.hook_id = HookHandle.id
         HookHandle.id = HookHandle.id + 1
```

### Comparing `nncf-2.4.0/nncf/tensorflow/utils/node.py` & `nncf-2.5.0/nncf/openvino/tensor.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,23 +1,31 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-import re
-
-from nncf.tensorflow.graph.utils import get_original_name_and_instance_idx
-
-
-def is_ignored(node_name, ignored_scopes):
-    original_name, _ = get_original_name_and_instance_idx(node_name)
-    return any(re.fullmatch(ignored.replace('{re}', ''), original_name) if ignored.startswith('{re}')
-               else ignored == original_name
-               for ignored in ignored_scopes)
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import numpy as np
+
+from nncf.common.tensor import NNCFTensor
+from nncf.parameters import TargetDevice
+
+
+class OVNNCFTensor(NNCFTensor):
+    """
+    A realisation of OpenVINO tensor wrapper for common NNCF algorithms.
+    """
+
+    def __init__(self, tensor: np.ndarray):
+        super().__init__(tensor)
+
+    @property
+    def device(self):
+        return TargetDevice.CPU.value
+
+    def is_empty(self) -> bool:
+        return self.tensor.size == 0
```

### Comparing `nncf-2.4.0/nncf/tensorflow/utils/scopes_handle.py` & `nncf-2.5.0/nncf/tensorflow/utils/scopes_handle.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,21 +1,20 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import re
 from typing import List
 
 
 def should_consider_scope(scope_str: str, target_scopes: List[str], ignored_scopes: List[str]):
     # TODO: rewrite and add target_scopes handling
-    return all(not re.fullmatch(ignored.replace('{re}', ''), scope_str) if ignored.startswith('{re}')
-               else scope_str != ignored
-               for ignored in ignored_scopes)
+    return all(
+        not re.fullmatch(ignored.replace("{re}", ""), scope_str) if ignored.startswith("{re}") else scope_str != ignored
+        for ignored in ignored_scopes
+    )
```

### Comparing `nncf-2.4.0/nncf/tensorflow/utils/state.py` & `nncf-2.5.0/nncf/tensorflow/utils/state.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,20 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict, Any
 import json
+from typing import Any, Dict
 
 import tensorflow as tf
 
 from nncf.common.compression import BaseCompressionAlgorithmController
 
 
 class TFCompressionState(tf.train.experimental.PythonState):
@@ -71,16 +69,17 @@
         Returns the compression state which was extracted from the checkpoint.
 
         :return: The compression state.
         """
         return self._state
 
     def serialize(self) -> str:
-        raise NotImplementedError('Use an instance of the `TFCompressionState` class to '
-                                  'serialize the compression state.')
+        raise NotImplementedError(
+            "Use an instance of the `TFCompressionState` class to serialize the compression state."
+        )
 
     def deserialize(self, string_value: str) -> None:
         """
         Callback to deserialize the compression state.
 
         :param string_value: A serialized compression state.
         """
```

### Comparing `nncf-2.4.0/nncf/torch/__init__.py` & `nncf-2.5.0/nncf/torch/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,27 +1,38 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# isort: off
+# pylint: skip-file
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Base subpackage for NNCF PyTorch functionality.
 """
 from nncf import nncf_logger
 from nncf.common.logging.logger import warn_bkc_version_mismatch
-# pylint: skip-file
+
 from nncf.version import BKC_TORCH_VERSION
 
 import torch
 from pkg_resources import parse_version
-torch_version = parse_version(torch.__version__).base_version
+
+try:
+    _torch_version = torch.__version__
+    torch_version = parse_version(_torch_version).base_version
+except:
+    nncf_logger.debug("Could not parse torch version")
+    _torch_version = "0.0.0"
+    torch_version = parse_version(_torch_version).base_version
+
 if parse_version(BKC_TORCH_VERSION).base_version != torch_version:
     warn_bkc_version_mismatch("torch", BKC_TORCH_VERSION, torch.__version__)
 
 
 # Required for correct COMPRESSION_ALGORITHMS registry functioning
 from nncf.torch.binarization import algo as binarization_algo
 from nncf.torch.quantization import algo as quantization_algo
@@ -38,14 +49,15 @@
 from nncf.torch.model_creation import create_compressed_model
 from nncf.torch.checkpoint_loading import load_state
 from nncf.torch.initialization import register_default_init_args
 from nncf.torch.layers import register_module
 from nncf.torch.dynamic_graph.patch_pytorch import register_operator
 from nncf.torch.dynamic_graph.io_handling import nncf_model_input
 from nncf.torch.dynamic_graph.io_handling import nncf_model_output
+from nncf.torch.dynamic_graph.context import disable_tracing
 from nncf.torch.dynamic_graph.context import no_nncf_trace
 from nncf.torch.dynamic_graph.context import forward_nncf_trace
 
 # NNCF relies on tracing PyTorch operations. Each code that uses NNCF
 # should be executed with PyTorch operators wrapped via a call to "patch_torch_operators",
 # so this call is moved to package __init__ to ensure this.
 from nncf.torch.dynamic_graph.patch_pytorch import patch_torch_operators
```

### Comparing `nncf-2.4.0/nncf/torch/accuracy_aware_training/runner.py` & `nncf-2.5.0/nncf/torch/accuracy_aware_training/runner.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,30 +1,29 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import os.path as osp
+from typing import Callable, Dict
 
 import torch
 from torch.optim.lr_scheduler import ReduceLROnPlateau
 from torchvision.transforms import ToTensor
 
-from nncf.torch.checkpoint_loading import load_state
-from nncf.torch.accuracy_aware_training.utils import is_main_process
 from nncf.common.accuracy_aware_training.runner import BaseAccuracyAwareTrainingRunner
 from nncf.common.accuracy_aware_training.runner import BaseAdaptiveCompressionLevelTrainingRunner
+from nncf.torch.accuracy_aware_training.utils import is_main_process
+from nncf.torch.checkpoint_loading import load_state
 
 try:
     from torch.utils.tensorboard import SummaryWriter
 
     TENSORBOARD_AVAILABLE = True
 except ImportError:
     TENSORBOARD_AVAILABLE = False
@@ -40,23 +39,14 @@
         if not is_main_process():
             return
         # Only the main process should initialize and create a log directory, other processes don't use it
         super().initialize_logging(log_dir, tensorboard_writer)
         if self._tensorboard_writer is None and TENSORBOARD_AVAILABLE:
             self._tensorboard_writer = SummaryWriter(self._log_dir)
 
-    def retrieve_uncompressed_model_accuracy(self, model):
-        if hasattr(model, 'original_model_accuracy') or hasattr(model.module, 'original_model_accuracy'):
-            if isinstance(model, (torch.nn.DataParallel, torch.nn.parallel.DistributedDataParallel)):
-                self.uncompressed_model_accuracy = model.module.original_model_accuracy
-            else:
-                self.uncompressed_model_accuracy = model.original_model_accuracy
-        else:
-            raise RuntimeError('Original model does not contain the pre-calculated reference metric value')
-
     def validate(self, model):
         with torch.no_grad():
             self.current_val_metric_value = self._validate_fn(model, epoch=self.cumulative_epoch_count)
         is_best = (not self.is_higher_metric_better) != (self.current_val_metric_value > self.best_val_metric_value)
         if is_best:
             self.best_val_metric_value = self.current_val_metric_value
         return self.current_val_metric_value
@@ -64,43 +54,46 @@
     def dump_statistics(self, model, compression_controller):
         if not is_main_process():
             return
         super().dump_statistics(model, compression_controller)
 
     def update_learning_rate(self):
         if self._update_learning_rate_fn is not None:
-            self._update_learning_rate_fn(self.lr_scheduler,
-                                          self.training_epoch_count,
-                                          self.current_val_metric_value,
-                                          self.current_loss)
+            self._update_learning_rate_fn(
+                self.lr_scheduler, self.training_epoch_count, self.current_val_metric_value, self.current_loss
+            )
         else:
             if self.lr_scheduler is not None and self.lr_updates_needed:
                 self.lr_scheduler.step(
-                    self.training_epoch_count if not isinstance(self.lr_scheduler, ReduceLROnPlateau)
-                    else self.best_val_metric_value)
+                    self.training_epoch_count
+                    if not isinstance(self.lr_scheduler, ReduceLROnPlateau)
+                    else self.best_val_metric_value
+                )
 
     def reset_training(self):
         self.configure_optimizers()
 
         optimizers = self.optimizer if isinstance(self.optimizer, (tuple, list)) else [self.optimizer]
         for optimizer in optimizers:
             for param_group in optimizer.param_groups:
-                param_group['lr'] *= self.base_lr_reduction_factor_during_search
+                param_group["lr"] *= self.base_lr_reduction_factor_during_search
 
         lr_schedulers = self.lr_scheduler if isinstance(self.lr_scheduler, (tuple, list)) else [self.lr_scheduler]
         for lr_scheduler in lr_schedulers:
             if lr_scheduler is None:
                 continue
-            for attr_name in ['base_lrs', 'init_lr']:
+            for attr_name in ["base_lrs", "init_lr"]:
                 if hasattr(lr_scheduler, attr_name):
                     setattr(
                         lr_scheduler,
                         attr_name,
-                        [base_lr * self.base_lr_reduction_factor_during_search
-                         for base_lr in getattr(lr_scheduler, attr_name)]
+                        [
+                            base_lr * self.base_lr_reduction_factor_during_search
+                            for base_lr in getattr(lr_scheduler, attr_name)
+                        ],
                     )
 
         self.training_epoch_count = 0
         self.best_val_metric_value = 0
         self.current_val_metric_value = 0
 
     def dump_checkpoint(self, model, compression_controller):
@@ -112,55 +105,70 @@
         if not is_main_process():
             return
         super().load_best_checkpoint(model)
 
     def _save_checkpoint(self, model, compression_controller, checkpoint_path):
         optimizers = self.optimizer if isinstance(self.optimizer, (tuple, list)) else [self.optimizer]
         checkpoint = {
-            'epoch': self.cumulative_epoch_count + 1,
-            'state_dict': model.state_dict(),
-            'compression_state': compression_controller.get_compression_state(),
-            'best_metric_val': self.best_val_metric_value,
-            'current_val_metric_value': self.current_val_metric_value,
-            'optimizer': [optimizer.state_dict() for optimizer in optimizers],
+            "epoch": self.cumulative_epoch_count + 1,
+            "state_dict": model.state_dict(),
+            "compression_state": compression_controller.get_compression_state(),
+            "best_metric_val": self.best_val_metric_value,
+            "current_val_metric_value": self.current_val_metric_value,
+            "optimizer": [optimizer.state_dict() for optimizer in optimizers],
         }
         torch.save(checkpoint, checkpoint_path)
 
     def _load_checkpoint(self, model, checkpoint_path):
         if self._load_checkpoint_fn is not None:
             self._load_checkpoint_fn(model, checkpoint_path)
         else:
-            resuming_checkpoint = torch.load(checkpoint_path, map_location='cpu')
-            resuming_model_state_dict = resuming_checkpoint.get('state_dict', resuming_checkpoint)
+            resuming_checkpoint = torch.load(checkpoint_path, map_location="cpu")
+            resuming_model_state_dict = resuming_checkpoint.get("state_dict", resuming_checkpoint)
             load_state(model, resuming_model_state_dict, is_resume=True)
 
     def _make_checkpoint_path(self, is_best, compression_rate=None):
-        extension = '.pth'
+        extension = ".pth"
         return osp.join(self._checkpoint_save_dir, f'acc_aware_checkpoint_{"best" if is_best else "last"}{extension}')
 
     def add_tensorboard_scalar(self, key, data, step):
         if is_main_process():
             if self.verbose and self._tensorboard_writer is not None:
                 self._tensorboard_writer.add_scalar(key, data, step)
 
     def add_tensorboard_image(self, key, data, step):
         if is_main_process():
             if self.verbose and self._tensorboard_writer is not None:
                 self._tensorboard_writer.add_image(key, ToTensor()(data), step)
 
 
-class PTAdaptiveCompressionLevelTrainingRunner(BaseAdaptiveCompressionLevelTrainingRunner,
-                                               PTAccuracyAwareTrainingRunner):
-    def __init__(self, accuracy_aware_training_params, verbose=True, dump_checkpoints=True, lr_updates_needed=True,
-                 minimal_compression_rate=0.0, maximal_compression_rate=0.95):
-        super().__init__(accuracy_aware_training_params, verbose, dump_checkpoints, lr_updates_needed,
-                         minimal_compression_rate=minimal_compression_rate,
-                         maximal_compression_rate=maximal_compression_rate)
+class PTAdaptiveCompressionLevelTrainingRunner(
+    BaseAdaptiveCompressionLevelTrainingRunner, PTAccuracyAwareTrainingRunner
+):
+    def __init__(
+        self,
+        accuracy_aware_training_params: Dict,
+        uncompressed_model_accuracy: float,
+        verbose: bool = True,
+        dump_checkpoints: bool = True,
+        lr_updates_needed: bool = True,
+        minimal_compression_rate: float = 0.0,
+        maximal_compression_rate: float = 0.95,
+    ):
+        super().__init__(
+            accuracy_aware_training_params,
+            uncompressed_model_accuracy,
+            verbose,
+            dump_checkpoints,
+            lr_updates_needed,
+            minimal_compression_rate=minimal_compression_rate,
+            maximal_compression_rate=maximal_compression_rate,
+        )
 
     def _make_checkpoint_path(self, is_best, compression_rate=None):
-        extension = '.pth'
-        base_path = osp.join(self._checkpoint_save_dir, 'acc_aware_checkpoint')
+        extension = ".pth"
+        base_path = osp.join(self._checkpoint_save_dir, "acc_aware_checkpoint")
         if is_best:
             if compression_rate is None:
-                raise ValueError('Compression rate cannot be None')
-            return f'{base_path}_best_{compression_rate:.3f}{extension}'
-        return f'{base_path}_last{extension}'
+                raise ValueError("Compression rate cannot be None")
+            return f"{base_path}_best_{compression_rate:.3f}{extension}"
+        return f"{base_path}_last{extension}"
```

### Comparing `nncf-2.4.0/nncf/torch/accuracy_aware_training/utils.py` & `nncf-2.5.0/nncf/torch/accuracy_aware_training/utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from torch import distributed as dist
 
 
 def is_dist_avail_and_initialized():
     if not dist.is_available():
         return False
```

### Comparing `nncf-2.4.0/nncf/torch/algo_selector.py` & `nncf-2.5.0/nncf/torch/algo_selector.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,41 +1,36 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-# pylint:disable=relative-beyond-top-level
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Dict
 
 import torch
 
-from nncf.torch.graph.transformations.layout import PTTransformationLayout
-from nncf.torch.nncf_network import NNCFNetwork
-
-from nncf.api.compression import CompressionStage
 from nncf.api.compression import CompressionScheduler
-from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
-from nncf.torch.compression_method_api import PTCompressionAlgorithmController
-
-from nncf.torch.compression_method_api import PTCompressionLoss
+from nncf.api.compression import CompressionStage
 from nncf.common.compression import NO_COMPRESSION_ALGORITHM_NAME
 from nncf.common.schedulers import StubCompressionScheduler
-from nncf.common.utils.registry import Registry
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.backend import copy_model
+from nncf.common.utils.registry import Registry
+from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
+from nncf.torch.compression_method_api import PTCompressionAlgorithmController
+from nncf.torch.compression_method_api import PTCompressionLoss
+from nncf.torch.graph.transformations.layout import PTTransformationLayout
+from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.utils import get_model_device
 
-PT_COMPRESSION_ALGORITHMS = Registry('compression algorithm', add_name_as_attr=True)
+PT_COMPRESSION_ALGORITHMS = Registry("compression algorithm", add_name_as_attr=True)
 
 
 class ZeroCompressionLoss(PTCompressionLoss):
     def __init__(self, device: torch.device):
         super().__init__()
         self._device = device
 
@@ -82,7 +77,13 @@
 
     @property
     def scheduler(self) -> CompressionScheduler:
         return self._scheduler
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         return NNCFStatistics()
+
+    def strip(self, do_copy: bool = True) -> NNCFNetwork:
+        model = self.model
+        if do_copy:
+            model = copy_model(self.model)
+        return model
```

### Comparing `nncf-2.4.0/nncf/torch/batchnorm_adaptation.py` & `nncf-2.5.0/nncf/torch/batchnorm_adaptation.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithmImpl
 from nncf.torch.initialization import DataLoaderBNAdaptationRunner
 
 
 class PTBatchnormAdaptationAlgorithmImpl(BatchnormAdaptationAlgorithmImpl):
     """
```

### Comparing `nncf-2.4.0/nncf/torch/binarization/algo.py` & `nncf-2.5.0/nncf/torch/binarization/algo.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,41 +1,38 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import OrderedDict
-from typing import Callable
-from typing import List
+from typing import Callable, List
 
 from texttable import Texttable
 from torch import nn
 
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationPriority
-from nncf.common.statistics import NNCFStatistics
 from nncf.common.logging import nncf_logger
+from nncf.common.statistics import NNCFStatistics
 from nncf.config import NNCFConfig
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.schemata.defaults import BINARIZATION_MODE
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 from nncf.torch.algo_selector import ZeroCompressionLoss
+from nncf.torch.binarization.layers import BINARIZATION_MODULES
 from nncf.torch.binarization.layers import ActivationBinarizationScaleThreshold
 from nncf.torch.binarization.layers import ActivationBinarizer
-from nncf.torch.binarization.layers import BINARIZATION_MODULES
 from nncf.torch.binarization.layers import BaseBinarizer
 from nncf.torch.binarization.layers import WeightBinarizer
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
@@ -43,68 +40,72 @@
 from nncf.torch.module_operations import UpdateInputs
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.algo import QuantizationControllerBase
 from nncf.torch.quantization.schedulers import QUANTIZATION_SCHEDULERS
 from nncf.torch.utils import get_model_device
 
 
-@PT_COMPRESSION_ALGORITHMS.register('binarization')
+@PT_COMPRESSION_ALGORITHMS.register("binarization")
 class BinarizationBuilder(PTCompressionAlgorithmBuilder):
     def __init__(self, config, should_init: bool = True):
         super().__init__(config, should_init)
-        self.mode = self._algo_config.get('mode', BINARIZATION_MODE)
+        self.mode = self._algo_config.get("mode", BINARIZATION_MODE)
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         layout = PTTransformationLayout()
         commands = self._binarize_weights_and_module_inputs(target_model)
         for command in commands:
             layout.register(command)
         return layout
 
     def __create_binarize_module(self):
         return BINARIZATION_MODULES.get(self.mode)()
 
     def _nncf_module_types_to_compress(self) -> List[str]:
-        return [NNCFConv2d.__name__, ]
+        return [
+            NNCFConv2d.__name__,
+        ]
 
     def _binarize_weights_and_module_inputs(self, target_model: NNCFNetwork) -> List[PTInsertionCommand]:
         device = get_model_device(target_model)
 
-        module_nodes = target_model.get_weighted_original_graph_nodes(
-            nncf_module_names=self.compressed_nncf_module_names)
+        module_nodes = target_model.nncf.get_weighted_original_graph_nodes(
+            nncf_module_names=self.compressed_nncf_module_names
+        )
 
         insertion_commands = []
         for module_node in module_nodes:
             if not self._should_consider_scope(module_node.node_name):
                 nncf_logger.info(f"Ignored adding binarizers in scope: {module_node.node_name}")
                 continue
 
             nncf_logger.debug(f"Adding weight binarizer in scope: {module_node.node_name}")
             op_weights = self.__create_binarize_module().to(device)
 
             nncf_logger.debug(f"Adding activation binarizer in scope: {module_node.node_name}")
 
             compression_lr_multiplier = self.config.get_redefinable_global_param_value_for_algo(
-                'compression_lr_multiplier',
-                self.name)
+                "compression_lr_multiplier", self.name
+            )
 
-            op_inputs = UpdateInputs(ActivationBinarizationScaleThreshold(
-                module_node.layer_attributes.get_weight_shape(),
-                compression_lr_multiplier=compression_lr_multiplier
-            )).to(device)
-
-            ip_w = PTTargetPoint(TargetType.OPERATION_WITH_WEIGHTS,
-                                 target_node_name=module_node.node_name)
-            insertion_commands.append(PTInsertionCommand(ip_w, op_weights,
-                                                         TransformationPriority.QUANTIZATION_PRIORITY))
-
-            ip_i = PTTargetPoint(TargetType.PRE_LAYER_OPERATION,
-                                 target_node_name=module_node.node_name, input_port_id=0)
-            insertion_commands.append(PTInsertionCommand(ip_i, op_inputs,
-                                                         TransformationPriority.QUANTIZATION_PRIORITY))
+            op_inputs = UpdateInputs(
+                ActivationBinarizationScaleThreshold(
+                    module_node.layer_attributes.get_weight_shape(), compression_lr_multiplier=compression_lr_multiplier
+                )
+            ).to(device)
+
+            ip_w = PTTargetPoint(TargetType.OPERATION_WITH_WEIGHTS, target_node_name=module_node.node_name)
+            insertion_commands.append(
+                PTInsertionCommand(ip_w, op_weights, TransformationPriority.QUANTIZATION_PRIORITY)
+            )
+
+            ip_i = PTTargetPoint(
+                TargetType.PRE_LAYER_OPERATION, target_node_name=module_node.node_name, input_port_id=0
+            )
+            insertion_commands.append(PTInsertionCommand(ip_i, op_inputs, TransformationPriority.QUANTIZATION_PRIORITY))
         return insertion_commands
 
     def _build_controller(self, model: NNCFNetwork) -> PTCompressionAlgorithmController:
         return BinarizationController(model, self.config)
 
     def initialize(self, model: NNCFNetwork) -> None:
         pass
@@ -114,28 +115,30 @@
     def __init__(self, target_model: NNCFNetwork, config: NNCFConfig):
         super().__init__(target_model)
 
         self._loss = ZeroCompressionLoss(get_model_device(target_model))
         scheduler_cls = QUANTIZATION_SCHEDULERS.get("staged")
         algo_config = extract_algo_specific_config(config, "binarization")
         self._scheduler = scheduler_cls(self, algo_config.get("params", {}))
-        from nncf.torch.utils import is_main_process #pylint: disable=cyclic-import
+        from nncf.torch.utils import is_main_process  # pylint: disable=cyclic-import
+
         if is_main_process():
             self._compute_and_display_flops_binarization_rate()
 
     @property
     def loss(self) -> CompressionLoss:
         return self._loss
 
     @property
     def scheduler(self) -> CompressionScheduler:
         return self._scheduler
 
-    def _set_binarization_status(self, condition_fn: Callable[[BaseBinarizer], bool],
-                                 apply_fn: Callable[[BaseBinarizer], None]):
+    def _set_binarization_status(
+        self, condition_fn: Callable[[BaseBinarizer], bool], apply_fn: Callable[[BaseBinarizer], None]
+    ):
         if self._model is not None:
             for _, m in self._model.named_modules():
                 if condition_fn(m):
                     apply_fn(m)
 
     def enable_activation_quantization(self):
         self._set_binarization_status(lambda x: isinstance(x, ActivationBinarizer), lambda x: x.enable())
@@ -179,15 +182,15 @@
                     return
                 ops_dict[name] = (name_type, ops_count, isinstance(self, NNCFConv2d))
 
             return compute_flops_hook
 
         hook_list = [m.register_forward_hook(get_hook(n)) for n, m in net.named_modules()]
 
-        net.do_dummy_forward(force_eval=True)
+        net.nncf.do_dummy_forward(force_eval=True)
 
         for h in hook_list:
             h.remove()
 
         # restore all parameters that can be corrupted due forward pass
         for n, v in state_dict.items():
             state_dict[n].data.copy_(weight_list[n].data)
@@ -204,15 +207,15 @@
         header = ["Layer name", "Layer type", "Binarized", "MAC count", "MAC share"]
         table_data = [header]
 
         for layer_name, (layer_type, ops, is_binarized) in ops_dict.items():
             drow = {h: 0 for h in header}
             drow["Layer name"] = layer_name
             drow["Layer type"] = layer_type
-            drow["Binarized"] = 'Y' if is_binarized else 'N'
+            drow["Binarized"] = "Y" if is_binarized else "N"
             drow["MAC count"] = f"{ops * 1e-9:.3f}G"
             drow["MAC share"] = f"{ops / ops_total * 100:2.1f}%"
             row = [drow[h] for h in header]
             table_data.append(row)
 
         table.add_rows(table_data)
         nncf_logger.info(table.draw())
```

### Comparing `nncf-2.4.0/nncf/torch/binarization/binarize_functions.py` & `nncf-2.5.0/nncf/torch/binarization/binarize_functions.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,41 +1,26 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Any
 
 import torch
+from torch.onnx.symbolic_helper import _is_constant  # pylint:disable=protected-access
 
 from nncf.common.logging import nncf_logger
-from nncf.torch.utils import add_domain
 from nncf.torch.binarization.extensions import BinarizedFunctionsCUDA
-
-from torch import _C  # pylint:disable=protected-access
-
-
-def _is_value(x: Any) -> bool:
-    return isinstance(x, _C.Value)
-
-# Implementation is copy-pasted from torch.onnx.symbolic_helper.
-# It's need to support torch < 1.9, since there's no such function in such versions of torch.
-def _is_constant(value: Any) -> bool:
-    return not _is_value(value) or value.node().kind() in {
-        "onnx::Constant",
-        "prim::Constant",
-    }
+from nncf.torch.utils import add_domain
 
 
 def _unsqueeze_helper(g, input_, axes_i):
     # Unsqueeze handling for different opsets inspired by torch.onnx.symbolic_helper._unsqueeze_helper
     # The original unsqueeze_helper cannot be used in 1.13 since it references
     # an `.opset` attribute on the `g` argument which is not there. The original intent
     # of this in pytorch was to allow accessing opset info in the symbolic fn's
@@ -49,80 +34,82 @@
         axes = g.op("Constant", value_t=torch.tensor(axes_i, dtype=torch.long))
         return g.op("Unsqueeze", input_, axes)
     return g.op("Unsqueeze", input_, axes_i=axes_i[0])
 
 
 # pylint:disable=abstract-method
 class XNORBinarizeFn(torch.autograd.Function):
-    """ Binarizes x into `scale` * { +1; -1}, where +1 or -1 are chosen based
-        on whether the x element value is >0 or <0. `scale` is determined as mean of absolute
-        values, per input channel (0-th dimension of x). """
+    """Binarizes x into `scale` * { +1; -1}, where +1 or -1 are chosen based
+    on whether the x element value is >0 or <0. `scale` is determined as mean of absolute
+    values, per input channel (0-th dimension of x)."""
+
     @staticmethod
     def symbolic(g, x):
-        zero = g.constant(0, [1], 'float')
+        zero = g.constant(0, [1], "float")
         zero = _unsqueeze_helper(g, zero, [1, 2, 3])
         scale = g.op("Abs", x)
         scale = g.op("ReduceMean", scale, axes_i=[1, 2, 3])
         scale_neg = g.op("Neg", scale)
         return g.op(add_domain("FakeQuantize"), x, zero, zero, scale_neg, scale, levels_i=2)
 
     @staticmethod
     def forward(ctx, x):
         if x.is_cuda:
             output = BinarizedFunctionsCUDA.get("WeightBinarize_forward")(x, True)
         else:
             # Current CPU kernel implementations do not improve performance
             norm = x.abs().mean([1, 2, 3], keepdim=True)
-            sign = ((x > 0).type(x.dtype) * 2 - 1)
+            sign = (x > 0).type(x.dtype) * 2 - 1
             output = sign * norm
             return output
         return output
 
     @staticmethod
     def backward(ctx: Any, *grad_outputs: Any) -> Any:
         return grad_outputs[0]
 
 
 # pylint:disable=abstract-method
 class DOREFABinarizeFn(torch.autograd.Function):
-    """ Binarizes x into `scale` * { +1; -1}, where +1 or -1 are chosen based
-        on whether the x element value is >0 or <0. `scale` is determined as mean of absolute
-        values of the entire x tensor. """
+    """Binarizes x into `scale` * { +1; -1}, where +1 or -1 are chosen based
+    on whether the x element value is >0 or <0. `scale` is determined as mean of absolute
+    values of the entire x tensor."""
+
     @staticmethod
     def symbolic(g, x):
-        zero = g.constant(0, [1], 'float')
+        zero = g.constant(0, [1], "float")
         zero = _unsqueeze_helper(g, zero, [1, 2, 3])
         scale = g.op("Abs", x)
         scale = g.op("ReduceMean", scale, axes_i=[0, 1, 2, 3])
         scale_neg = g.op("Neg", scale)
         return g.op(add_domain("FakeQuantize"), x, zero, zero, scale_neg, scale, levels_i=2)
 
     @staticmethod
     def forward(ctx, x):
         if x.is_cuda:
             output = BinarizedFunctionsCUDA.get("WeightBinarize_forward")(x, False)
         else:
             # Current CPU kernel implementations do not improve performance
             norm = x.abs().mean()
-            sign = ((x > 0).type(x.dtype) * 2 - 1)
+            sign = (x > 0).type(x.dtype) * 2 - 1
             output_flat = sign * norm
             return output_flat.view_as(x)
         return output
 
     @staticmethod
     def backward(ctx: Any, *grad_outputs: Any) -> Any:
         return grad_outputs[0]
 
 
 # Activation binarization function
 # pylint:disable=abstract-method
 class ActivationBinarizationScaleThresholdFn(torch.autograd.Function):
     @staticmethod
     def symbolic(g, x, scale, threshold):
-        zero = g.constant(0, [1], 'float')
+        zero = g.constant(0, [1], "float")
         zero = _unsqueeze_helper(g, zero, [0, 2, 3])
         threshold = g.op("Mul", threshold, scale)
         scale = _unsqueeze_helper(g, scale, [0, 2, 3])
         return g.op(add_domain("FakeQuantize"), x, threshold, threshold, zero, scale, levels_i=2)
 
     @staticmethod
     def forward(ctx, input_, scale, threshold):
@@ -146,17 +133,16 @@
                 nncf_logger.debug("grad_output is not contiguous!")
                 grad_output = grad_output.contiguous()
 
         input_, scale, output = ctx.saved_variables
 
         if input_.is_cuda:
             grad_input, grad_scale, grad_threshold = BinarizedFunctionsCUDA.get("ActivationBinarize_backward")(
-                grad_output,
-                input_,
-                scale, output)
+                grad_output, input_, scale, output
+            )
         else:
             # Current CPU kernel implementations do not improve performance
             # grad_input, grad_scale, grad_threshold = BinarizedFunctionsCPU.ActivationBinarize_backward(grad_output,
             #                                                                                           input_,
             #                                                                                           scale, output)
             # calc gradient for input
             mask_lower = (input_ <= scale).type(input_.dtype)
```

### Comparing `nncf-2.4.0/nncf/torch/binarization/extensions.py` & `nncf-2.5.0/nncf/torch/binarization/extensions.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,99 +1,108 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import os.path
 import subprocess
 
 import torch
 
 from nncf import nncf_logger
-from nncf.torch.binarization.reference import ReferenceBinarizedFunctions
-from nncf.torch.extensions import CudaNotAvailableStub, ExtensionsType, ExtensionLoader, EXTENSIONS
 from nncf.definitions import NNCF_PACKAGE_ROOT_DIR
+from nncf.torch.binarization.reference import ReferenceBinarizedFunctions
+from nncf.torch.extensions import EXTENSIONS
+from nncf.torch.extensions import CudaNotAvailableStub
+from nncf.torch.extensions import ExtensionLoader
 from nncf.torch.extensions import ExtensionNamespace
+from nncf.torch.extensions import ExtensionsType
 
 BASE_EXT_DIR = os.path.join(NNCF_PACKAGE_ROOT_DIR, "torch/extensions/src/binarization")
 
 EXT_INCLUDE_DIRS = [
     os.path.join(NNCF_PACKAGE_ROOT_DIR, "torch/extensions/include"),
 ]
 
 CPU_EXT_SRC_LIST = [
     os.path.join(BASE_EXT_DIR, "cpu/functions_cpu.cpp"),
-    os.path.join(NNCF_PACKAGE_ROOT_DIR, "torch/extensions/src/common/cpu/tensor_funcs.cpp")
+    os.path.join(NNCF_PACKAGE_ROOT_DIR, "torch/extensions/src/common/cpu/tensor_funcs.cpp"),
 ]
 
 CUDA_EXT_SRC_LIST = [
     os.path.join(BASE_EXT_DIR, "cuda/functions_cuda.cpp"),
-    os.path.join(BASE_EXT_DIR, "cuda/functions_cuda_impl.cu")
+    os.path.join(BASE_EXT_DIR, "cuda/functions_cuda_impl.cu"),
 ]
 
 
 @EXTENSIONS.register()
 class BinarizedFunctionsCPULoader(ExtensionLoader):
     @classmethod
     def name(cls) -> str:
-        return 'binarized_functions_cpu'
+        return "binarized_functions_cpu"
 
     @classmethod
     def extension_type(cls):
         return ExtensionsType.CPU
 
     @classmethod
     def load(cls):
         try:
-            retval = torch.utils.cpp_extension.load(cls.name(),
-                          CPU_EXT_SRC_LIST,
-                          extra_include_paths=EXT_INCLUDE_DIRS,
-                          build_directory=cls.get_build_dir(),
-                          verbose=False)
+            retval = torch.utils.cpp_extension.load(
+                cls.name(),
+                CPU_EXT_SRC_LIST,
+                extra_include_paths=EXT_INCLUDE_DIRS,
+                build_directory=cls.get_build_dir(),
+                verbose=False,
+            )
         except Exception as e:  # pylint:disable=broad-except
-            nncf_logger.warning(f"Could not compile CPU binarization extensions. "
-                                f"Falling back on torch native operations - "
-                                f"CPU binarization fine-tuning may be slower than expected.\n"
-                                f"Reason: {str(e)}")
+            nncf_logger.warning(
+                f"Could not compile CPU binarization extensions. "
+                f"Falling back on torch native operations - "
+                f"CPU binarization fine-tuning may be slower than expected.\n"
+                f"Reason: {str(e)}"
+            )
             retval = ReferenceBinarizedFunctions
         return retval
 
 
 @EXTENSIONS.register()
 class BinarizedFunctionsCUDALoader(ExtensionLoader):
     @classmethod
     def name(cls) -> str:
-        return 'binarized_functions_cuda'
+        return "binarized_functions_cuda"
 
     @classmethod
     def extension_type(cls):
         return ExtensionsType.CUDA
 
     @classmethod
     def load(cls):
         try:
-            return torch.utils.cpp_extension.load(cls.name(),
-                        CUDA_EXT_SRC_LIST,
-                        extra_include_paths=EXT_INCLUDE_DIRS,
-                        build_directory=cls.get_build_dir(),
-                        verbose=False)
+            return torch.utils.cpp_extension.load(
+                cls.name(),
+                CUDA_EXT_SRC_LIST,
+                extra_include_paths=EXT_INCLUDE_DIRS,
+                build_directory=cls.get_build_dir(),
+                verbose=False,
+            )
         except (subprocess.CalledProcessError, OSError, RuntimeError) as e:
             assert torch.cuda.is_available()
-            raise RuntimeError("CUDA is available for PyTorch, but NNCF could not compile "
-                               "GPU binarization extensions. Make sure that you have installed CUDA development "
-                               "tools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for "
-                               "guidance) and that 'nvcc' is available on your system's PATH variable.\n") from e
+            raise RuntimeError(
+                "CUDA is available for PyTorch, but NNCF could not compile "
+                "GPU binarization extensions. Make sure that you have installed CUDA development "
+                "tools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for "
+                "guidance) and that 'nvcc' is available on your system's PATH variable.\n"
+            ) from e
 
 
 BinarizedFunctionsCPU = ExtensionNamespace(BinarizedFunctionsCPULoader())
 
 if torch.cuda.is_available():
     BinarizedFunctionsCUDA = ExtensionNamespace(BinarizedFunctionsCUDALoader())
 else:
```

### Comparing `nncf-2.4.0/nncf/torch/binarization/layers.py` & `nncf-2.5.0/nncf/torch/binarization/layers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,44 +1,43 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import torch
 from torch import nn
 
-from nncf.torch.dynamic_graph.patch_pytorch import register_operator
-from nncf.torch.layer_utils import COMPRESSION_MODULES, CompressionParameter
+from nncf.common.logging import nncf_logger
 from nncf.common.utils.registry import Registry
-from nncf.torch.quantization.layers import get_per_channel_scale_shape
-from nncf.torch.binarization.binarize_functions import XNORBinarizeFn, DOREFABinarizeFn
 from nncf.torch.binarization.binarize_functions import ActivationBinarizationScaleThresholdFn
+from nncf.torch.binarization.binarize_functions import DOREFABinarizeFn
+from nncf.torch.binarization.binarize_functions import XNORBinarizeFn
+from nncf.torch.dynamic_graph.patch_pytorch import register_operator
+from nncf.torch.layer_utils import COMPRESSION_MODULES
+from nncf.torch.layer_utils import CompressionParameter
+from nncf.torch.quantization.layers import get_per_channel_scale_shape
 
-from nncf.common.logging import nncf_logger
-
-BINARIZATION_MODULES = Registry('binarization_modules')
+BINARIZATION_MODULES = Registry("binarization_modules")
 
 
 class BinarizationMode:
     XNOR = "xnor"
     DOREFA = "dorefa"
 
 
 class BaseBinarizer(nn.Module):
     def __init__(self, enabled=False):
         super().__init__()
-        self.register_buffer('enabled', torch.IntTensor([0]))
+        self.register_buffer("enabled", torch.IntTensor([0]))
         if enabled:
             self.enable()
 
     def forward(self, x):
         if self.is_enabled():
             return self.binarize(x)
         return x
@@ -51,14 +50,15 @@
 
     def disable(self):
         self.enabled[0] = 0
 
     def is_enabled(self):
         return self.enabled[0] == 1
 
+
 class WeightBinarizer(BaseBinarizer):
     def binarize(self, x):
         raise NotImplementedError
 
 
 class ActivationBinarizer(BaseBinarizer):
     def binarize(self, x):
@@ -92,24 +92,26 @@
 # Activation binarization module
 class ActivationBinarizationScaleThreshold(ActivationBinarizer):
     def __init__(self, input_shape, enabled=False, compression_lr_multiplier=None, desc=""):
         super().__init__(enabled)
 
         self.input_shape = input_shape
 
-        self.scale = CompressionParameter(torch.Tensor([0]), requires_grad=enabled,
-                                          compression_lr_multiplier=compression_lr_multiplier)
+        self.scale = CompressionParameter(
+            torch.Tensor([0]), requires_grad=enabled, compression_lr_multiplier=compression_lr_multiplier
+        )
         self.scale.data.zero_()
 
         # Need scale_initialized as buffer for it to appear in the model state dict
-        self.register_buffer('scale_initialized', torch.IntTensor([0]))
+        self.register_buffer("scale_initialized", torch.IntTensor([0]))
 
         threshold_shape = get_per_channel_scale_shape(self.input_shape, is_weights=False)
-        self.threshold = CompressionParameter(torch.ones(threshold_shape), requires_grad=enabled,
-                                              compression_lr_multiplier=compression_lr_multiplier)
+        self.threshold = CompressionParameter(
+            torch.ones(threshold_shape), requires_grad=enabled, compression_lr_multiplier=compression_lr_multiplier
+        )
         self.threshold.data.zero_()
         self.bin = activation_bin_scale_threshold_op
 
     @property
     def is_scale_initialized(self):
         return self.scale_initialized[0] == 1
 
@@ -117,15 +119,15 @@
     def is_scale_initialized(self, value: bool):
         self.scale_initialized[0] = 1 if value else 0
 
     def binarize(self, x):
         if self.training and not self.is_scale_initialized:
             # init scale using nonbinarized activation statistics
             d = x.detach().data.contiguous().view(-1)
-            top_num = max(1, round(d.shape[0]*0.001))
+            top_num = max(1, round(d.shape[0] * 0.001))
             topk_res = d.topk(top_num)
             scale = topk_res[0].min()
             nncf_logger.debug(f"Binarized activation scale set to: {scale.item()}")
             self.scale.data[:] = scale.log()
             self.is_scale_initialized = True
 
         x = self.bin(x, self.scale.exp(), self.threshold.sigmoid())
```

### Comparing `nncf-2.4.0/nncf/torch/binarization/reference.py` & `nncf-2.5.0/nncf/torch/binarization/reference.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,30 @@
-"""
- Copyright (c) 2022 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2022 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 from enum import Enum
 from typing import TypeVar
 
 import numpy as np
 import torch
 
-GeneralizedTensor = TypeVar('GeneralizedTensor', torch.Tensor, np.ndarray)
+GeneralizedTensor = TypeVar("GeneralizedTensor", torch.Tensor, np.ndarray)
 
 
 class ReferenceBackendType(Enum):
-    NUMPY = 'numpy'
-    TORCH = 'torch'
+    NUMPY = "numpy"
+    TORCH = "torch"
 
 
 class ReferenceBase:
     def __init__(self, backend_type: ReferenceBackendType):
         if backend_type is ReferenceBackendType.NUMPY:
             self.backend = np
         elif backend_type is ReferenceBackendType.TORCH:
@@ -33,27 +32,27 @@
         else:
             raise RuntimeError("Unknown backend for ReferenceQuantize")
 
 
 class ReferenceXNORBinarize(ReferenceBase):
     def forward(self, x: GeneralizedTensor) -> GeneralizedTensor:
         norm = self.backend.abs(x).mean((1, 2, 3), keepdims=True)
-        sign = ((x > 0).astype(x.dtype) * 2 - 1)
+        sign = (x > 0).astype(x.dtype) * 2 - 1
         output = sign * norm
         return output
 
     @staticmethod
     def backward(grad_output: GeneralizedTensor) -> GeneralizedTensor:
         return grad_output
 
 
 class ReferenceDOREFABinarize(ReferenceBase):
     def forward(self, x: GeneralizedTensor) -> GeneralizedTensor:
         norm = self.backend.abs(x).mean()
-        sign = ((x > 0).astype(x.dtype) * 2 - 1)
+        sign = (x > 0).astype(x.dtype) * 2 - 1
         return sign * norm
 
     @staticmethod
     def backward(grad_output: GeneralizedTensor) -> GeneralizedTensor:
         return grad_output
 
 
@@ -64,15 +63,14 @@
         shape[1] = x.shape[1]
         t = threshold * scale
         output = (x > t).astype(x.dtype) * scale
         return output
 
     @staticmethod
     def backward(grad_output, x, scale, output):
-
         # calc gradient for input
         mask_lower = (x <= scale).astype(x.dtype)
         grad_input = grad_output * (x >= 0).astype(x.dtype) * mask_lower
 
         # calc gradient for scale
         err = (output - x) / scale
         grad_scale = grad_output * (mask_lower * err + (1 - mask_lower))
```

### Comparing `nncf-2.4.0/nncf/torch/checkpoint_loading.py` & `nncf-2.5.0/nncf/torch/checkpoint_loading.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,31 +1,32 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import re
 from enum import Enum
 from typing import Dict, List, Set, Tuple
 
 import torch
 
+from nncf.common.deprecation import warning_deprecated
 from nncf.common.logging import nncf_logger
-from nncf.common.logging.logger import warning_deprecated
+from nncf.common.utils.api_marker import api
 
 
-def load_state(model: torch.nn.Module, state_dict_to_load: dict, is_resume: bool = False,
-               keys_to_ignore: List[str] = None) -> int:
+@api(canonical_alias="nncf.torch.load_state")
+def load_state(
+    model: torch.nn.Module, state_dict_to_load: dict, is_resume: bool = False, keys_to_ignore: List[str] = None
+) -> int:
     """
     Used to load a checkpoint containing a compressed model into an NNCFNetwork object, but can
     be used for any PyTorch module as well. Will do matching of state_dict_to_load parameters to
     the model's state_dict parameters while discarding irrelevant prefixes added during wrapping
     in NNCFNetwork or DataParallel/DistributedDataParallel objects, and load the matched parameters
     from the state_dict_to_load into the model's state dict.
     :param model: The target module for the state_dict_to_load to be loaded to.
@@ -40,15 +41,16 @@
     with compression algorithms applied to evaluate the model.
     :param keys_to_ignore: A list of parameter names that should be skipped from matching process.
     :return: The number of state_dict_to_load entries successfully matched and loaded into model.
     """
 
     model_state_dict = model.state_dict()
 
-    from nncf.torch.utils import maybe_convert_legacy_names_in_model_state #pylint: disable=cyclic-import
+    from nncf.torch.utils import maybe_convert_legacy_names_in_model_state  # pylint: disable=cyclic-import
+
     maybe_convert_legacy_names_in_model_state(state_dict_to_load)
     key_matcher = KeyMatcher(is_resume, state_dict_to_load, model_state_dict, keys_to_ignore)
     new_dict = key_matcher.run()
     num_loaded_params = len(new_dict)
     key_matcher.handle_problematic_keys()
     nncf_logger.info(f"Loaded {num_loaded_params}/{len(model_state_dict.items())} parameters")
 
@@ -73,41 +75,41 @@
 
 # If optional parameter is missed in a checkpoint, it can be loaded without an error in a strict mode.
 # New parameters can be introduced for the model without breaking backward compatibility with old checkpoint.
 OPTIONAL_PARAMETERS_REGISTRY = ParametersRegistry()
 
 
 class ProcessedKeyStatus(Enum):
-    """ Status of matching checkpoint key with model keys """
-    MATCHED = 'Matched'
-    MISSING = 'Missing'
-    UNEXPECTED = 'Unexpected'
-    SIZE_MISMATCHED = 'Size mismatched'
-    SKIPPED = 'Skipped'
+    """Status of matching checkpoint key with model keys"""
+
+    MATCHED = "Matched"
+    MISSING = "Missing"
+    UNEXPECTED = "Unexpected"
+    SIZE_MISMATCHED = "Size mismatched"
+    SKIPPED = "Skipped"
 
 
 class ProcessedKeys:
-    """ Contains checkpoint keys with their status of matching with model keys """
+    """Contains checkpoint keys with their status of matching with model keys"""
 
     def __init__(self):
         self._keys = {}  # type: Dict[ProcessedKeyStatus, Set[str]]
         for key_status in ProcessedKeyStatus:
             self._keys[key_status] = set()
 
     def add_key(self, key: str, status: ProcessedKeyStatus):
         self._keys[status].add(key)
 
     def extend_keys(self, keys: List[str], status: ProcessedKeyStatus):
         self._keys[status].update(keys)
 
-    def add_skipped_and_missing_keys(self,
-                                     model_state_dict: Dict[str, torch.Tensor]):
+    def add_skipped_and_missing_keys(self, model_state_dict: Dict[str, torch.Tensor]):
         all_processed_keys = []
         optional_param_names = OPTIONAL_PARAMETERS_REGISTRY.get_parameters_names()
-        params_to_skip = tuple('.' + name for name in optional_param_names)
+        params_to_skip = tuple("." + name for name in optional_param_names)
         for keys in self._keys.values():
             all_processed_keys.extend(keys)
 
         for key in model_state_dict.keys():
             if key not in all_processed_keys:
                 if key.endswith(params_to_skip) or key in optional_param_names:
                     self.add_key(key, ProcessedKeyStatus.SKIPPED)
@@ -124,35 +126,33 @@
         non optional parameters only or when not all parameters from state_dict_to_load match.
         :param is_resume: Determines the behavior when the function cannot do a successful parameter match when loading.
         :param are_all_loaded_params_matched: whether all parameters to load match with model parameters
         """
         error_msgs = []
 
         def add_error_msg(name, keys_):
-            error_msgs.insert(
-                0, '{} key(s):\n{}. '.format(name,
-                                             ',\n'.join('\t\t"{}"'.format(k) for k in keys_)))
+            error_msgs.insert(0, "{} key(s):\n{}. ".format(name, ",\n".join('\t\t"{}"'.format(k) for k in keys_)))
 
         for key_status, keys in self._keys.items():
             is_missing = key_status == ProcessedKeyStatus.MISSING
             erroneous = key_status in (ProcessedKeyStatus.SIZE_MISMATCHED, ProcessedKeyStatus.UNEXPECTED)
             if keys and (erroneous or is_missing and (is_resume or not are_all_loaded_params_matched)):
                 add_error_msg(key_status.value, keys)
         if error_msgs:
-            error_msg = 'Error(s) when loading model parameters:\n\t{}'.format("\n\t".join(error_msgs))
+            error_msg = "Error(s) when loading model parameters:\n\t{}".format("\n\t".join(error_msgs))
             if is_resume:
                 raise RuntimeError(error_msg)
             nncf_logger.error(error_msg)
 
 
 class NormalizedKeys:
     """
-        Contains normalized form of parameters. It helps to discard irrelevant prefixes added during wrapping in
-        NNCFNetwork or DataParallel/DistributedDataParallel objects, to handle legacy parameters' names and to match
-        unified compression parameters from the separate ones.
+    Contains normalized form of parameters. It helps to discard irrelevant prefixes added during wrapping in
+    NNCFNetwork or DataParallel/DistributedDataParallel objects, to handle legacy parameters' names and to match
+    unified compression parameters from the separate ones.
     """
 
     def __init__(self, keys: List[str], keys_to_ignore: List[str]):
         self._unique_normalized_key_vs_orig_key_map = {}
         self.is_unified_group_detected = False
         self.has_legacy_storage_keys = False
         unique_clipped_key_vs_orig_key_map, ignored_keys = self._clip_keys_without_collisions(keys, keys_to_ignore)
@@ -165,17 +165,17 @@
 
     def __iter__(self):
         return iter(self._unique_normalized_key_vs_orig_key_map)
 
     def get_orig_key(self, normalized_key: str):
         return self._unique_normalized_key_vs_orig_key_map[normalized_key]
 
-    def _normalize_keys_without_collisions(self,
-                                           unique_clipped_key_vs_orig_key_map: Dict[str, str],
-                                           keys_to_ignore: List[str]) -> List[str]:
+    def _normalize_keys_without_collisions(
+        self, unique_clipped_key_vs_orig_key_map: Dict[str, str], keys_to_ignore: List[str]
+    ) -> List[str]:
         ignored_keys = []
         normalized_key_vs_clipped_key_list_map = {}
         for clipped_key in unique_clipped_key_vs_orig_key_map:
             replaced_keys = self._key_replacer(clipped_key)
             if len(replaced_keys) > 1:
                 self.is_unified_group_detected = True
 
@@ -224,85 +224,89 @@
                 for orig_key in list_orig_keys:
                     unique_clipped_key_vs_orig_key_map[orig_key] = orig_key
         return unique_clipped_key_vs_orig_key_map, ignored_keys
 
     @staticmethod
     def _key_clipper(key: str) -> str:
         new_key = key
-        from nncf.torch.nncf_network import MODEL_WRAPPED_BY_NNCF_ATTR_NAME #pylint: disable=cyclic-import
-        clip_patterns = [MODEL_WRAPPED_BY_NNCF_ATTR_NAME + '.', 'module.', '|OUTPUT', '|INPUT']
+        from nncf.torch.nncf_network import LEGACY_MODEL_WRAPPED_BY_NNCF_ATTR_NAME  # pylint: disable=cyclic-import
+
+        clip_patterns = [LEGACY_MODEL_WRAPPED_BY_NNCF_ATTR_NAME + ".", "module.", "|OUTPUT", "|INPUT", "_nncf."]
         for pattern in clip_patterns:
-            new_key = new_key.replace(pattern, '')
+            new_key = new_key.replace(pattern, "")
         return new_key
 
     def _key_replacer(self, key: str) -> List[str]:
         new_key = key
 
-        match = re.search('(pre_ops|post_ops)\\.(\\d+?)\\.op', key)
-        new_key = new_key if not match else new_key.replace(match.group(), 'operation')
+        match = re.search("(pre_ops|post_ops)\\.(\\d+?)\\.op", key)
+        new_key = new_key if not match else new_key.replace(match.group(), "operation")
 
         new_key, did_replace = self._replace_legacy_act_quantizer_storage_name(new_key)
         if did_replace:
             self.has_legacy_storage_keys = True
 
         result = self._split_unified_parameters(new_key)
         if len(result) > 1:
             self.is_unified_group_detected = True
         return result
 
     @staticmethod
     def _split_unified_parameters(new_key: str) -> List[str]:
-        """ covers unified activation quantizers case, e.g.
-                external_quantizers.RELU_0;RELU_1;RELU_2.op
-            Result of this function is full names of individual parameters:
-                external_quantizers.RELU_2.op
-                external_quantizers.RELU_1.op
-                external_quantizers.RELU_0.op
-            It's utilized to match parameters from checkpoints without unified operations to not start training
-            compression from scratch, but instead initialize group of parameters by one of the matched individual one.
-            Returns original key if there's no ';' and operation doesn't start with EXTERNAL_QUANTIZERS_STORAGE_NAME
+        """covers unified activation quantizers case, e.g.
+            external_quantizers.RELU_0;RELU_1;RELU_2.op
+        Result of this function is full names of individual parameters:
+            external_quantizers.RELU_2.op
+            external_quantizers.RELU_1.op
+            external_quantizers.RELU_0.op
+        It's utilized to match parameters from checkpoints without unified operations to not start training
+        compression from scratch, but instead initialize group of parameters by one of the matched individual one.
+        Returns original key if there's no ';' and operation doesn't start with EXTERNAL_QUANTIZERS_STORAGE_NAME
         """
         result = [new_key]
-        from nncf.torch.nncf_network import EXTERNAL_QUANTIZERS_STORAGE_NAME #pylint: disable=cyclic-import
-        if ';' in new_key and new_key.startswith(EXTERNAL_QUANTIZERS_STORAGE_NAME):
-            group_of_keys = new_key.split(';')
+        from nncf.torch.nncf_network import CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX  # pylint: disable=cyclic-import
+
+        if ";" in new_key and new_key.startswith(CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX):
+            group_of_keys = new_key.split(";")
             last_key = group_of_keys[-1]
-            common_op = last_key.split('.')[-1]
-            result = [
-                group_of_keys[0] + '.' + common_op,
-                EXTERNAL_QUANTIZERS_STORAGE_NAME + '.' + last_key
-            ]
+            common_op = last_key.split(".")[-1]
+            result = [group_of_keys[0] + "." + common_op, CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX + "." + last_key]
             for key in group_of_keys[1:-1]:
-                result.append(EXTERNAL_QUANTIZERS_STORAGE_NAME + '.' + key + '.' + common_op)
+                result.append(CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX + "." + key + "." + common_op)
         return result
 
     @staticmethod
     def _replace_legacy_act_quantizer_storage_name(checkpoint_key: str) -> Tuple[str, bool]:
         did_replace = False
-        splits = checkpoint_key.split('.')
-        from nncf.torch.nncf_network import LEGACY_ACT_STORAGE_NAME #pylint: disable=cyclic-import
-        from nncf.torch.nncf_network import EXTERNAL_QUANTIZERS_STORAGE_NAME #pylint: disable=cyclic-import
-        if splits[0] == LEGACY_ACT_STORAGE_NAME:
+        splits = checkpoint_key.split(".")
+        from nncf.torch.nncf_network import CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX  # pylint: disable=cyclic-import
+        from nncf.torch.nncf_network import LEGACY_EXTERNAL_QUANTIZERS_STORAGE_PREFIX  # pylint: disable=cyclic-import
+
+        if splits[0] == LEGACY_EXTERNAL_QUANTIZERS_STORAGE_PREFIX:
             did_replace = True
-            splits[0] = EXTERNAL_QUANTIZERS_STORAGE_NAME
-        reconstructed_key = '.'.join(splits)
+            splits[0] = CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX
+        reconstructed_key = ".".join(splits)
         return reconstructed_key, did_replace
 
 
 class KeyMatcher:
     """
     Matches state_dict_to_load parameters to the model's state_dict parameters while discarding irrelevant prefixes
     added during wrapping in NNCFNetwork or DataParallel/DistributedDataParallel objects, skipping registered optional
     parameters, handling legacy parameters' names, ignoring the order of pre/post operations, matching unified
     compression parameters from the separate ones and forms the model state dict with matched parameters.
     """
 
-    def __init__(self, is_resume: bool,
-                 state_dict_to_load: Dict[str, torch.Tensor], model_state_dict: Dict[str, torch.Tensor],
-                 ignored_keys: List[str] = None):
+    def __init__(
+        self,
+        is_resume: bool,
+        state_dict_to_load: Dict[str, torch.Tensor],
+        model_state_dict: Dict[str, torch.Tensor],
+        ignored_keys: List[str] = None,
+    ):
         """
         :param state_dict_to_load: A state dict containing the parameters to be loaded into the model.
         :param ignored_keys: list of parameters to skip from matching process on loading.
         """
         self._is_resume = is_resume
         self.state_dict_to_load = state_dict_to_load
 
@@ -312,81 +316,101 @@
         self._num_params_to_load = len(state_dict_to_load.items())
         self.ignored_keys = ignored_keys if ignored_keys else []
 
     def run(self) -> Dict[str, torch.Tensor]:
         """
         :return: the model state dict with matched parameters
         """
-        normalized_model_keys = NormalizedKeys(list(self.model_state_dict.keys()),
-                                               keys_to_ignore=self.ignored_keys)
-        normalized_keys_to_load = NormalizedKeys(list(self.state_dict_to_load.keys()),
-                                                 keys_to_ignore=self.ignored_keys)
+        normalized_model_keys = NormalizedKeys(list(self.model_state_dict.keys()), keys_to_ignore=self.ignored_keys)
+        normalized_keys_to_load = NormalizedKeys(list(self.state_dict_to_load.keys()), keys_to_ignore=self.ignored_keys)
 
         has_version_agnostic_names = False
-        cross_match_key_map = self._cross_match_version_agnostic_names(list(normalized_keys_to_load),
-                                                                       list(normalized_model_keys))
+        cross_match_key_map = self._cross_match_version_agnostic_names(
+            list(normalized_keys_to_load), list(normalized_model_keys)
+        )
 
         for matched_checkpoint_key, matched_model_key in cross_match_key_map.items():
             if matched_checkpoint_key != matched_model_key:
                 has_version_agnostic_names = True
 
         if has_version_agnostic_names:
-            warning_deprecated('Legacy NNCF-enabled .pth checkpoint has been loaded! '
-                                'The version-agnostic `RELU` operator name entries in the state dict '
-                               'have been deprecated. '
-                                'The loader will try to match these entries to the correspoindig `relu` and `relu_` op '
-                                'names. The newly exported checkpoints will be adjusted to the new format.')
+            warning_deprecated(
+                "Legacy NNCF-enabled .pth checkpoint has been loaded! "
+                "The version-agnostic `RELU` operator name entries in the state dict "
+                "have been deprecated. "
+                "The loader will try to match these entries to the correspoindig `relu` and `relu_` op "
+                "names. The newly exported checkpoints will be adjusted to the new format."
+            )
 
         if normalized_keys_to_load.has_legacy_storage_keys:
-            warning_deprecated('Legacy NNCF-enabled .pth checkpoint has been loaded! '
-                               'The "activation_quantizers" storage key is replaced with '
-                               '"external_quantizers" in newer versions of NNCF, and support '
-                               'for the legacy storage key will be dropped in a future release. '
-                               'This checkpoint will be loaded; update your checkpoint file by saving this model\'s'
-                               'checkpoint file again.')
+            from nncf.torch.nncf_network import CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX
+            from nncf.torch.nncf_network import LEGACY_EXTERNAL_QUANTIZERS_STORAGE_PREFIX
+
+            warning_deprecated(
+                f"Legacy NNCF-enabled .pth checkpoint has been loaded! "
+                f"The {LEGACY_EXTERNAL_QUANTIZERS_STORAGE_PREFIX} storage key is replaced with "
+                f"{CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX} in newer versions of NNCF, and support "
+                f"for the legacy storage key will be dropped in a future release. "
+                f"This checkpoint will be loaded; update your checkpoint file by saving this model's"
+                f"checkpoint file again."
+            )
 
         if normalized_model_keys.is_unified_group_detected and not normalized_keys_to_load.is_unified_group_detected:
             nncf_logger.warning(
-                'Unified parameters are detected in the compressed model, but all parameters are independent '
-                'and separate in the loading checkpoint. The unified parameters will be initialized by one of'
-                'the corresponding separate parameter in the checkpoint. That may slightly degrade the '
-                'accuracy, but should allow to not start training compression from scratch with unified '
-                'params.')
+                "Unified parameters are detected in the compressed model, but all parameters are independent "
+                "and separate in the loading checkpoint. The unified parameters will be initialized by one of"
+                "the corresponding separate parameter in the checkpoint. That may slightly degrade the "
+                "accuracy, but should allow to not start training compression from scratch with unified "
+                "params."
+            )
         ignored_keys = normalized_model_keys.ignored_orig_keys + normalized_keys_to_load.ignored_orig_keys
         self._processed_keys.extend_keys(ignored_keys, ProcessedKeyStatus.SKIPPED)
         if ignored_keys:
-            ignored_keys_str = '\n'.join(set(ignored_keys))
+            ignored_keys_str = "\n".join(set(ignored_keys))
             nncf_logger.warning(
-                f"Following parameters were skipped from matching checkpoint's keys:\n{ignored_keys_str}")
+                f"Following parameters were skipped from matching checkpoint's keys:\n{ignored_keys_str}"
+            )
 
+        loaded_prefixless_keys = False
         for normalized_key_to_load in normalized_keys_to_load:
             key_to_load = normalized_keys_to_load.get_orig_key(normalized_key_to_load)
-            normalized_key_to_load = cross_match_key_map.get(normalized_key_to_load,
-                                                             normalized_key_to_load)
+            normalized_key_to_load = cross_match_key_map.get(normalized_key_to_load, normalized_key_to_load)
             if normalized_key_to_load in normalized_model_keys:
                 model_key = normalized_model_keys.get_orig_key(normalized_key_to_load)
+                if "_nncf." + key_to_load == model_key:
+                    loaded_prefixless_keys = True
                 value_to_load = self.state_dict_to_load[key_to_load]
                 size_of_value_to_load = value_to_load.size()
                 size_of_model_value = self.model_state_dict[model_key].size()
                 if size_of_value_to_load == size_of_model_value:
                     self._new_dict[model_key] = value_to_load
                     self._processed_keys.add_key(model_key, ProcessedKeyStatus.MATCHED)
                 else:
                     nncf_logger.warning(
                         f"Different size of value of '{model_key}' "
-                        f"in resuming dictionary ({size_of_value_to_load}) and in model ({size_of_model_value})")
+                        f"in resuming dictionary ({size_of_value_to_load}) and in model ({size_of_model_value})"
+                    )
                     self._processed_keys.add_key(model_key, ProcessedKeyStatus.SIZE_MISMATCHED)
             else:
                 self._processed_keys.add_key(key_to_load, ProcessedKeyStatus.UNEXPECTED)
         self._processed_keys.add_skipped_and_missing_keys(self.model_state_dict)
+        if loaded_prefixless_keys:
+            warning_deprecated(
+                "Legacy NNCF-enabled .pth checkpoint has been loaded! "
+                'Some storage keys in the loaded checkpoint should now have a "_nncf." prefix,'
+                "support for the legacy storage key will be dropped in a future release. "
+                "This checkpoint will be loaded; update your checkpoint file by saving this model's"
+                "checkpoint file again."
+            )
         return self._new_dict
 
     @staticmethod
-    def _cross_match_version_agnostic_names(normalized_keys_to_load: List[str],
-                                            normalized_model_keys: List[str]) -> Dict[str, str]:
+    def _cross_match_version_agnostic_names(
+        normalized_keys_to_load: List[str], normalized_model_keys: List[str]
+    ) -> Dict[str, str]:
         """
         Handles the situation where the normalized_keys_to_load contain legacy version-agnostic names
         of operations, such as `RELU`.
 
         :param normalized_keys_to_load: A list of keys in the checkpoint, potentially with version-agnostic names
         :param normalized_model_keys: A list of keys in the model, without version-agnostic names.
         :return: A mapping of the checkpoint key to a model key that matches version-agnostic names with their
@@ -401,21 +425,21 @@
                 matches_for_curr_agnostic_op_name = []
                 has_specific_op_name = False
                 for specific_op_name in specific_op_name_set:
                     # Have to take care not to replace the matches to the class names
                     # The op names in existing checkpoint can only appear in external quantizers,
                     # i.e. external_quantizers.ResNet/ReLU[relu]/relu_0.signed_tensor, so composing a regex to match
                     # for that
-                    slash_split_str = model_key.split('/')
+                    slash_split_str = model_key.split("/")
                     last_portion = slash_split_str[-1]
                     if specific_op_name in last_portion:
                         last_portion = last_portion.replace(specific_op_name, agnostic_op_name, 1)
                         has_specific_op_name = True
                     slash_split_str[-1] = last_portion
-                    agnostic_version_of_model_key = '/'.join(slash_split_str)
+                    agnostic_version_of_model_key = "/".join(slash_split_str)
                     processed_agnostic_version_of_model_key = agnostic_version_of_model_key
                     if processed_agnostic_version_of_model_key in processed_keys_to_load:
                         idx = processed_keys_to_load.index(processed_agnostic_version_of_model_key)
                         matches_for_curr_agnostic_op_name.append(normalized_keys_to_load[idx])
 
                 if not has_specific_op_name:
                     if matches_for_curr_agnostic_op_name:
@@ -423,17 +447,19 @@
                         retval[checkpoint_matched_key] = model_key
                 elif len(matches_for_curr_agnostic_op_name) == 1:
                     checkpoint_matched_key = next(iter(matches_for_curr_agnostic_op_name))
                     retval[checkpoint_matched_key] = model_key
                 elif len(matches_for_curr_agnostic_op_name) == 0:
                     nncf_logger.debug(f"Failed to match a version-specific key: {model_key}")
                 elif len(matches_for_curr_agnostic_op_name) > 1:
-                    nncf_logger.debug(f"More than one match for the version specific key: {model_key}\n"
-                                      f"Matches:\n"
-                                      f"{', '.join(matches_for_curr_agnostic_op_name)}")
+                    nncf_logger.debug(
+                        f"More than one match for the version specific key: {model_key}\n"
+                        f"Matches:\n"
+                        f"{', '.join(matches_for_curr_agnostic_op_name)}"
+                    )
 
         return retval
 
     def handle_problematic_keys(self):
         """
         Reports about errors during the matching state_dict_to_load parameters to the model's state_dict ones.
         """
```

### Comparing `nncf-2.4.0/nncf/torch/composite_compression.py` & `nncf-2.5.0/nncf/torch/composite_compression.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,17 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import TypeVar
 
 import torch.nn
 
 from nncf import NNCFConfig
 from nncf.common.composite_compression import CompositeCompressionAlgorithmBuilder
 from nncf.common.composite_compression import CompositeCompressionAlgorithmController
@@ -24,37 +21,37 @@
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.compression_method_api import PTCompressionLoss
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.nncf_network import PTModelTransformer
 
-TModel = TypeVar('TModel')
+TModel = TypeVar("TModel")
 
 
 class PTCompositeCompressionLoss(CompositeCompressionLoss, PTCompressionLoss):
     def __init__(self):
         super().__init__()
         self._child_losses = torch.nn.ModuleList()
 
     @property
     def child_losses(self) -> torch.nn.ModuleList:
         return self._child_losses
 
 
-class PTCompositeCompressionAlgorithmBuilder(
-        CompositeCompressionAlgorithmBuilder, PTCompressionAlgorithmBuilder):
+class PTCompositeCompressionAlgorithmBuilder(CompositeCompressionAlgorithmBuilder, PTCompressionAlgorithmBuilder):
     def __init__(self, config: NNCFConfig, should_init: bool = True):
-
         super().__init__(config, should_init)
 
         algo_names = extract_algorithm_names(config)
         if len(algo_names) < 2:
-            raise RuntimeError('Composite algorithm builder must be supplied with a config with more than one '
-                               'compression algo specified!')
+            raise RuntimeError(
+                "Composite algorithm builder must be supplied with a config with more than one "
+                "compression algo specified!"
+            )
         for algo_name in algo_names:
             algo_builder = PT_COMPRESSION_ALGORITHMS.get(algo_name)
             self._child_builders.append(algo_builder(config, should_init=should_init))
 
     def __bool__(self):
         return bool(self.child_builders)
 
@@ -105,15 +102,16 @@
                 builder.initialize(model)
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         pass  # Higher-level get_transformation_layout is overridden, no need to define this
 
 
 class PTCompositeCompressionAlgorithmController(
-    CompositeCompressionAlgorithmController, PTCompressionAlgorithmController):
+    CompositeCompressionAlgorithmController, PTCompressionAlgorithmController
+):
     def __init__(self, target_model: TModel):
         super().__init__(target_model)
         self._loss = PTCompositeCompressionLoss()
 
     def distributed(self):
         for ctrl in self.child_ctrls:
             ctrl.distributed()
```

### Comparing `nncf-2.4.0/nncf/torch/debug.py` & `nncf-2.5.0/nncf/torch/debug.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import List, Dict
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import functools
+from typing import Dict, List
 
 from torch.nn import Module
+
 from nncf.common.logging import nncf_logger
 
 
 class CallCountTracker:
     def __init__(self, name):
         self.name = name
         self.call_counts = {}
@@ -58,33 +58,34 @@
         raise NotImplementedError
 
     def init_actual(self, owner_model):
         raise NotImplementedError
 
 
 def debuggable_forward(forward_func):
-    def decorated(self, *args, **kwargs):
-        if self.debug_interface is not None:
-            self.debug_interface.pre_forward_actions(module=self)
+    @functools.wraps(forward_func)
+    def decorated(self: "NNCFNetwork", *args, **kwargs):
+        if hasattr(self, "nncf") and self.nncf.debug_interface is not None:
+            self.nncf.debug_interface.pre_forward_actions(module=self)
         retval = forward_func(self, *args, **kwargs)
-        if self.debug_interface is not None:
-            self.debug_interface.post_forward_actions(module=self)
+        if hasattr(self, "nncf") and self.nncf.debug_interface is not None:
+            self.nncf.debug_interface.post_forward_actions(module=self)
         return retval
 
     return decorated
 
 
 class CombinedDebugInterface(DebugInterface):
     def __init__(self):
         self._interfaces = []  # type: List[DebugInterface]
 
-    def add_interface(self, interface: 'DebugInterface'):
+    def add_interface(self, interface: "DebugInterface"):
         self._interfaces.append(interface)
 
-    def init_actual(self, owner_model: 'NNCFNetwork'):
+    def init_actual(self, owner_model: "NNCFNetwork"):
         for interface in self._interfaces:
             interface.init_actual(owner_model)
 
     def pre_forward_actions(self, module: Module):
         for interface in self._interfaces:
             interface.pre_forward_actions(module)
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/context.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/context.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,70 +1,93 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import threading
+import weakref
 from collections import deque
 from contextlib import contextmanager
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Optional
+from typing import Callable, Dict, List, Optional
 
 import torch
 
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.debug import is_debug
+from nncf.common.utils.patcher import PATCHER
 from nncf.torch.dynamic_graph.graph import DynamicGraph
 from nncf.torch.dynamic_graph.graph import DynamicGraphNode
 from nncf.torch.dynamic_graph.graph import DynamicGraphNodeParameters
 from nncf.torch.dynamic_graph.op_input_processing import OperatorInput
 from nncf.torch.dynamic_graph.operation_address import OperationAddress
 from nncf.torch.dynamic_graph.scope import Scope
 from nncf.torch.dynamic_graph.scope import ScopeElement
 from nncf.torch.dynamic_graph.trace_tensor import TensorMeta
 
-_CURRENT_CONTEXT = None
+
+class ThreadLocalGlobalContext(threading.local):
+    def __init__(self):
+        super().__init__()
+        self.context = None
+
+
+_CURRENT_CONTEXT = ThreadLocalGlobalContext()
 
 
 class PreHookId:
-    def __init__(self, op_address: OperationAddress,
-                 input_port_id: int):
+    def __init__(self, op_address: OperationAddress, input_port_id: int):
         self.op_address = op_address
         self.input_port_id = input_port_id
 
     def __eq__(self, other):
         return self.__dict__ == other.__dict__
 
     def __str__(self):
         return str(self.op_address) + "|INPUT{}".format(self.input_port_id)
 
     def __hash__(self):
         return hash(str(self))
 
+
+class TracingThreadLocals(threading.local):
+    def __init__(self):
+        super().__init__()
+        self.reset()
+
+    def reset(self):
+        self.scopes = []
+        self.module_call_stack = []
+        self.in_operator = False
+        self.num_nested_hooks = 0
+        self.base_module_replica = None
+        self.operator_counters = {}
+        self.node_call_tracker = {}
+        self.traced_tensor_weakrefs = []
+
+
 class CopySafeThreadingVars:
-    """ A class holding variables that are related to threading and
+    """A class holding variables that are related to threading and
     thus impossible to deepcopy. The deepcopy will simply return a
     new object without copying, but won't fail."""
+
     def __init__(self):
-        self.thread_local = threading.local()
+        self.thread_local = TracingThreadLocals()
         self.cond = threading.Condition()
 
     def __deepcopy__(self, memo):
         return CopySafeThreadingVars()
 
+
 # pylint: disable=too-many-public-methods
 class TracingContext:
     def __init__(self):
         self.graph = DynamicGraph()
 
         self._save_context = None
         self._post_hooks = {}
@@ -90,64 +113,93 @@
         self.start_node_name_of_skipped_block = []
         self.end_node_name_of_skipped_block = []
         self.active_block_indexes = None
         self.tensor_cache = None
         self._ordinals_ids = None
 
     def __enter__(self):
-        global _CURRENT_CONTEXT
-        self._save_context = _CURRENT_CONTEXT
-        _CURRENT_CONTEXT = self
+        # For DataParallel, this relies on having the same compressed context for
+        # all replicas. Otherwise we will have data races on setting and reading the global _CURRENT_CONTEXT
+        # variable, which will in turn lead to DP-specific runtime errors such as
+        # "'_thread._local' object has no attribute 'scopes'"
+        self._save_context = get_current_context()
+        set_current_context(self)
         self._reset_thread_local()
         if is_debug():
             self.reset_node_call_counters()
 
         return self
 
     def __exit__(self, *args):
+        if self._save_context is not self:  # NNCFNetwork.rebuild_graph() uses the compressed context nested in self
+            for traced_tensor_weakref in self._threading.thread_local.traced_tensor_weakrefs:
+                tt = traced_tensor_weakref()
+                if tt is not None:
+                    tt.nncf_expire()
+
         self._reset_thread_local()
 
-        global _CURRENT_CONTEXT
-        _CURRENT_CONTEXT = self._save_context
+        set_current_context(self._save_context)
         self._save_context = None
 
-    def find_operator_node(self, tensor_metas: List[Optional[TensorMeta]],
-                           op_address: OperationAddress) -> Optional[DynamicGraphNode]:
+    def find_operator_node(
+        self, tensor_metas: List[Optional[TensorMeta]], op_address: OperationAddress
+    ) -> Optional[DynamicGraphNode]:
         with self._threading.cond:
             self._n_instances_searching_graph += 1
 
         node = self.graph.find_node(op_address, tensor_metas, self._input_comparators_per_scope)
 
         with self._threading.cond:
             self._n_instances_searching_graph -= 1
             self._threading.cond.notify_all()
         return node
 
     def register_global_buffer(self, name: str, buffer):
         self.global_buffer_store[name] = buffer
 
-    def maybe_add_node(self,
-                       inputs: OperatorInput,
-                       tensor_metas: List[Optional[TensorMeta]],
-                       op_address: OperationAddress,
-                       module_attrs: BaseLayerAttributes = None,
-                       ignored_algorithms: List[str] = None,
-                       is_called_inside_nncf_module: bool = False) -> Optional[DynamicGraphNode]:
+    def register_traced_tensor(self, tt: "TracedTensor"):
+        """
+        Registers a weak reference to a traced tensor in the context so that in case
+        the block under context retains a reference to an intermediate tensor somewhere,
+        the context can mark this traced tensor reference as "expired" tracing-wise upon context
+        exit.
+        :param tt: A TracedTensor to be registered.
+        """
+        wr = weakref.ref(tt)
+        self._threading.thread_local.traced_tensor_weakrefs.append(wr)
+
+    def maybe_add_node(
+        self,
+        inputs: OperatorInput,
+        tensor_metas: List[Optional[TensorMeta]],
+        op_address: OperationAddress,
+        module_attrs: BaseLayerAttributes = None,
+        ignored_algorithms: List[str] = None,
+        is_called_inside_nncf_module: bool = False,
+    ) -> Optional[DynamicGraphNode]:
         if not self._may_add_nodes:
             return None
         with self._threading.cond:
             while self._n_instances_searching_graph > 0:
                 self._threading.cond.wait()
             # Another thread may have added a node inside this block,
             # so we need to check again if a node is already added.
             node = self.graph.find_node(op_address, tensor_metas, self._input_comparators_per_scope)
             if node is None:
+                mid = id(self.get_current_module())
                 node = self.graph.add_node(
-                    op_address, tensor_metas, self._input_comparators_per_scope, inputs,
-                    DynamicGraphNodeParameters(module_attrs, ignored_algorithms, is_called_inside_nncf_module))
+                    op_address,
+                    tensor_metas,
+                    self._input_comparators_per_scope,
+                    inputs,
+                    DynamicGraphNodeParameters(
+                        module_attrs, ignored_algorithms, is_called_inside_nncf_module, calling_module_id=mid
+                    ),
+                )
         return node
 
     def get_caller_context(self, operator_name: str) -> OperationAddress:
         """
         Designed to work in the following way - for each scope the context will track the number of the calls to the
         operators with the name operator_name (call_order). The counter values are preserved until reset by a
         corresponding member function of the context, which must be called after each model iteration - this is
@@ -156,17 +208,15 @@
         function calls (either on their own or inside a `for` cycle), and at the same moment allow the checkpoints to
         be loaded if the model had changed in the meantime in a way that does not impact the major function call
         order (e.g. if comments were added to the .py file with the model)
         """
 
         call_order = self.get_operator_call_count_in_scope(operator_name, self.scope)
 
-        op_address = OperationAddress(operator_name,
-                                      self.scope,
-                                      call_order)
+        op_address = OperationAddress(operator_name, self.scope, call_order)
         return op_address
 
     def reset_scope_operator_call_counters(self):
         """
         Must be called after each "forward" operation of the model that is made
         within this context
         """
@@ -187,37 +237,35 @@
         key = self._get_operator_counter_key(operator_name, scope)
         if key in self._threading.thread_local.operator_counters:
             return self._threading.thread_local.operator_counters[key]
         return 0
 
     def reset_operator_call_count_in_scope(self, scope):
         scoped_op_name = str(scope)
-        for key in self._threading.thread_local.operator_counters.keys():
+        for key in self._threading.thread_local.operator_counters:
             if scoped_op_name in key:
                 self._threading.thread_local.operator_counters[key] = 0
 
     def push_scope(self, called_module: torch.nn.Module):
         relative_scopes_list = self._get_scope_relative_to_last_registered_module_call(called_module)
         self.module_call_stack.append(called_module)
         self.relative_scopes_stack.append(relative_scopes_list)
 
     def pop_scope(self):
         self.relative_scopes_stack.pop()
         self.module_call_stack.pop()
 
-    def register_pre_hooks(self, fn_list: List[Callable], op_address: OperationAddress,
-                           input_port_id: int):
+    def register_pre_hooks(self, fn_list: List[Callable], op_address: OperationAddress, input_port_id: int):
         pre_hook_id = PreHookId(op_address, input_port_id)
         if pre_hook_id in self._pre_hooks:
             raise KeyError("Pre hook for context {} is already registered".format(str(pre_hook_id)))
         self._pre_hooks[pre_hook_id] = fn_list
 
-    def execute_pre_hooks(self, op_address: OperationAddress,
-                          op_inputs: OperatorInput) -> OperatorInput:
-        in_op = getattr(self, 'in_operator', False)
+    def execute_pre_hooks(self, op_address: OperationAddress, op_inputs: OperatorInput) -> OperatorInput:
+        in_op = getattr(self, "in_operator", False)
         self.in_operator = False
         self._threading.thread_local.num_nested_hooks += 1
 
         pre_hook_ids_for_curr_op = [x for x in self._pre_hooks if x.op_address == op_address]
         pre_hook_ids_for_curr_op = sorted(pre_hook_ids_for_curr_op, key=lambda x: x.input_port_id)
         for pre_hook_id in pre_hook_ids_for_curr_op:
             hook_list_for_current_input_port = self._pre_hooks[pre_hook_id]
@@ -230,15 +278,15 @@
 
     def register_post_hooks(self, fn_list: List[Callable], op_address: OperationAddress):
         if op_address in self._post_hooks:
             raise KeyError("Post hook for context {} is already registered".format(str(op_address)))
         self._post_hooks[op_address] = fn_list
 
     def execute_post_hooks(self, op_address: OperationAddress, outputs):
-        in_op = getattr(self, 'in_operator', False)
+        in_op = getattr(self, "in_operator", False)
         self.in_operator = False
         self._threading.thread_local.num_nested_hooks += 1
         if op_address in self._post_hooks:
             for hook in self._post_hooks[op_address]:
                 outputs = hook(outputs)
         self._threading.thread_local.num_nested_hooks -= 1
         self.in_operator = in_op
@@ -266,16 +314,15 @@
 
     def enable_node_additions(self):
         self._may_add_nodes = True
 
     def disable_node_additions(self):
         self._may_add_nodes = False
 
-    def add_node_comparators(self, scopes_to_apply: List[str],
-                             node_input_comparator: 'TensorMetaComparator' = None):
+    def add_node_comparators(self, scopes_to_apply: List[str], node_input_comparator: "TensorMetaComparator" = None):
         self._input_comparators_per_scope.append((node_input_comparator, scopes_to_apply))
 
     @property
     def base_module_thread_local_replica(self) -> torch.nn.Module:
         return self._threading.thread_local.base_module_replica
 
     @base_module_thread_local_replica.setter
@@ -311,23 +358,15 @@
         if not self.graph.is_graph_with_iteration_modules():
             self._trace_dynamic_graph = False
 
     def enable_trace_dynamic_graph(self):
         self._trace_dynamic_graph = True
 
     def _reset_thread_local(self):
-        tl = self._threading.thread_local
-        tl.scopes = []
-        tl.module_call_stack = []
-        tl.in_operator = False
-        tl.num_nested_hooks = 0
-        tl.base_module_replica = None
-        tl.operator_counters = {}
-        tl.node_call_tracker = {}
-
+        self._threading.thread_local.reset()
 
     def register_node_call(self, node: DynamicGraphNode):
         if node.node_id in self._threading.thread_local.node_call_tracker:
             self._threading.thread_local.node_call_tracker[node.node_id] += 1
         else:
             self._threading.thread_local.node_call_tracker[node.node_id] = 1
 
@@ -337,24 +376,32 @@
 
     def get_node_call_counter_dict(self):
         return self._threading.thread_local.node_call_tracker
 
     def _get_scope_relative_to_last_registered_module_call(self, module) -> Scope:
         module_class = module.__class__.__name__
         if not self.module_call_stack:
-            return Scope([ScopeElement(module_class), ])
+            return Scope(
+                [
+                    ScopeElement(module_class),
+                ]
+            )
         q = deque([(tuple(), self.module_call_stack[-1])])
         while q:
             scope_parts, top = q.popleft()
             if module is top:
                 return Scope(list(scope_parts))
             for name, child in top.named_children():
                 scope_element = ScopeElement(child.__class__.__name__, name)
                 q.append((scope_parts + (scope_element,), child))
-        return Scope([ScopeElement(module_class), ])
+        return Scope(
+            [
+                ScopeElement(module_class),
+            ]
+        )
 
     @property
     def scope(self) -> Scope:
         stack_copy = self.relative_scopes_stack.copy()
         scope_el_list = []
         for relative_scope in stack_copy:
             for scope_element in relative_scope.scope_elements:
@@ -370,42 +417,62 @@
             self.end_node_name_of_skipped_block = []
         self.active_block_indexes = block_indexes
         if self.elastic_depth and len(block_indexes) > 0:
             for block_index in block_indexes:
                 self.start_node_name_of_skipped_block.append(self.skipped_blocks[block_index].start_node_name)
                 self.end_node_name_of_skipped_block.append(self.skipped_blocks[block_index].end_node_name)
 
-    def set_elastic_blocks(self, blocks: List['BuildingBlock'] = None):
+    def set_elastic_blocks(self, blocks: List["BuildingBlock"] = None):
         if blocks is not None:
             if isinstance(blocks, list):
                 if len(blocks) == 0:
                     self.skipped_blocks = []
                 elif isinstance(blocks[0], str):
                     self.skipped_blocks = [blocks]
                 else:
                     self.skipped_blocks = blocks
 
 
+def set_current_context(c: TracingContext):
+    _CURRENT_CONTEXT.context = c
+
+
+@api(canonical_alias="nncf.torch.no_nncf_trace")
 @contextmanager
 def no_nncf_trace():
     ctx = get_current_context()
     if ctx is not None and ctx.is_tracing:
         ctx.disable_tracing()
         yield
         ctx.enable_tracing()
     else:
         yield
 
 
+@api(canonical_alias="nncf.torch.forward_nncf_trace")
 @contextmanager
 def forward_nncf_trace():
     ctx = get_current_context()
     if ctx is not None and not ctx.is_forwarding:
         ctx.enable_forwarding()
         yield
         ctx.disable_forwarding()
     else:
         yield
 
 
 def get_current_context() -> TracingContext:
-    return _CURRENT_CONTEXT
+    return _CURRENT_CONTEXT.context
+
+
+@api(canonical_alias="nncf.torch.disable_tracing")
+def disable_tracing(method):
+    """
+    Patch a method so that it will be executed within no_nncf_trace context
+    :param method: A method to patch.
+    """
+
+    def no_nncf_trace_wrapper(self, fn, *args, **kwargs):  # pylint: disable=unused-argument
+        with no_nncf_trace():
+            return fn(*args, **kwargs)
+
+    PATCHER.patch(method, no_nncf_trace_wrapper)
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/graph.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/graph.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,38 +1,31 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import Counter
-from typing import Any
-from typing import Dict
-from typing import Generator
-from typing import List
-from typing import Optional
-from typing import Tuple
+from typing import Any, Dict, Generator, List, Optional, Tuple
 
 import networkx as nx
 import networkx.algorithms.isomorphism as iso
 from torch import Tensor
 
 from nncf.common.graph import Dtype
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.logging import nncf_logger
+from nncf.torch.dynamic_graph.operation_address import OperationAddress
 from nncf.torch.dynamic_graph.scope import Scope
 from nncf.torch.dynamic_graph.trace_tensor import TensorMeta
 from nncf.torch.dynamic_graph.trace_tensor import TracedTensor
-from nncf.torch.dynamic_graph.operation_address import OperationAddress
 
 
 class TensorMetaComparator:
     def __call__(self, lhs: TensorMeta, rhs: TensorMeta) -> bool:
         raise NotImplementedError
 
 
@@ -48,37 +41,43 @@
 
 class ShapeOnlyTensorMetaComparator(TensorMetaComparator):
     def __call__(self, lhs: TensorMeta, rhs: TensorMeta) -> bool:
         return lhs.shape[1:] == rhs.shape[1:]
 
 
 class InputsMatcher:
-    def __call__(self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta],
-                 tm_comparators: List[TensorMetaComparator]) -> bool:
+    def __call__(
+        self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta], tm_comparators: List[TensorMetaComparator]
+    ) -> bool:
         raise NotImplementedError
 
 
 class FirstInputsMatcher(InputsMatcher):
-    def __call__(self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta],
-                 tm_comparators: List[TensorMetaComparator]) -> bool:
+    def __call__(
+        self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta], tm_comparators: List[TensorMetaComparator]
+    ) -> bool:
         if not node_inputs or not real_inputs:
             return False
 
         if not node_inputs[0] or not real_inputs[0]:
             return False
 
         for tm_comparator in tm_comparators:
             if not tm_comparator(node_inputs[0], real_inputs[0]):
                 return False
         return True
 
 
 class DefaultInputsMatcher(InputsMatcher):
-    def __call__(self, saved_inputs: List[TensorMeta], actual_inputs: List[TensorMeta],
-                 tm_comparators: List[TensorMetaComparator]) -> bool:
+    def __call__(
+        self,
+        saved_inputs: List[TensorMeta],
+        actual_inputs: List[TensorMeta],
+        tm_comparators: List[TensorMetaComparator],
+    ) -> bool:
         if saved_inputs is None and actual_inputs:
             return False
 
         matched_with_unexpected_tensors = False
         for saved_input, actual_input in zip(saved_inputs, actual_inputs):
             if saved_input is None and actual_input is None:
                 continue
@@ -96,63 +95,64 @@
             for tm_comparator in tm_comparators:
                 if not tm_comparator(saved_input, actual_input):
                     return False
         if matched_with_unexpected_tensors:
             nncf_logger.debug(
                 f"Had to match a node to an op which has tensors at positions where there were "
                 f"no tensors at graph building time:\n"
-                f"Node input metas: {saved_inputs}, but op input metas: {actual_inputs}")
+                f"Node input metas: {saved_inputs}, but op input metas: {actual_inputs}"
+            )
         return True
 
 
 class OperationExecutionContext:
     """
     Information that allows to uniquely identify an operation inside the NNCF graph,
     i.e. determine whether an execution of the operator inside the module has already been
     registered as a node in the graph or not (in the latter case a new node would have to
     be created
     """
 
-    def __init__(self,
-                 operator_name: str,
-                 scope_in_model: Scope,
-                 call_order: int,
-                 tensor_metas: List[TensorMeta],
-                 tm_comparators: List[TensorMetaComparator] = None,
-                 input_matcher: InputsMatcher = None):
+    def __init__(
+        self,
+        operator_name: str,
+        scope_in_model: Scope,
+        call_order: int,
+        tensor_metas: List[TensorMeta],
+        tm_comparators: List[TensorMetaComparator] = None,
+        input_matcher: InputsMatcher = None,
+    ):
         self.op_address = OperationAddress(operator_name, scope_in_model, call_order)
         # This should be a list with a length equal to the number of inputs.
         # "None" values in this list correspond to non-tensor input nodes.
         self.tensor_metas = tensor_metas
-        self.tm_comparators = tm_comparators if tm_comparators else [
-            DefaultTensorMetaComparator()]
+        self.tm_comparators = tm_comparators if tm_comparators else [DefaultTensorMetaComparator()]
         self.input_matcher = input_matcher if input_matcher else DefaultInputsMatcher()
 
     def __eq__(self, other):
         return self.op_address == other.op_address and Counter(self.tensor_metas) == Counter(other.tensor_metas)
 
-    def matches_saved_inputs_from(self, other: 'OperationExecutionContext'):
+    def matches_saved_inputs_from(self, other: "OperationExecutionContext"):
         # WARNING: not commutative
-        return self.op_address == other.op_address and self.input_matcher(other.tensor_metas,
-                                                                          self.tensor_metas,
-                                                                          self.tm_comparators)
+        return self.op_address == other.op_address and self.input_matcher(
+            other.tensor_metas, self.tensor_metas, self.tm_comparators
+        )
 
     def __hash__(self):
-        return hash((self.operator_name, tuple(self.scope_in_model), self.call_order,
-                     tuple(self.tensor_metas)))
+        return hash((self.operator_name, tuple(self.scope_in_model), self.call_order, tuple(self.tensor_metas)))
 
     def __str__(self):
         input_info_str = ""
         for meta in self.tensor_metas:
             if meta is None:
                 input_info_str += "N;"
             else:
                 input_info_str += str(meta) + ";"
 
-        return super().__str__() + '(' + input_info_str + ')'
+        return super().__str__() + "(" + input_info_str + ")"
 
     @property
     def operator_name(self):
         return self.op_address.operator_name
 
     @property
     def scope_in_model(self) -> Scope:
@@ -160,98 +160,122 @@
 
     @property
     def call_order(self):
         return self.op_address.call_order
 
 
 class DynamicGraphNodeParameters:
-    def __init__(self, layer_attributes: BaseLayerAttributes,
-                 ignored_algorithms: List[str],
-                 is_called_inside_nncf_module: bool):
+    def __init__(
+        self,
+        layer_attributes: BaseLayerAttributes,
+        ignored_algorithms: List[str],
+        is_called_inside_nncf_module: bool,
+        calling_module_id: int,
+    ):
         self.layer_attributes = layer_attributes
         self.ignored_algorithms = ignored_algorithms
         self.is_called_inside_nncf_module = is_called_inside_nncf_module
+        self.calling_module_id = calling_module_id
 
 
 class DynamicGraphNode:
-    def __init__(self, node_id: int, node_key: str, layer_attributes: BaseLayerAttributes,
-                 op_exec_context: OperationExecutionContext, ignored_algorithms: List[str],
-                 is_called_inside_nncf_module: bool, is_in_iteration_scope: bool):
+    def __init__(
+        self,
+        node_id: int,
+        node_key: str,
+        layer_attributes: BaseLayerAttributes,
+        op_exec_context: OperationExecutionContext,
+        calling_module_id: int,
+        ignored_algorithms: List[str],
+        is_called_inside_nncf_module: bool,
+        is_in_iteration_scope: bool,
+    ):
         self.node_id = node_id
         self.node_key = node_key
         self.layer_attributes = layer_attributes
         self.op_exec_context = op_exec_context
+        self.calling_module_id = calling_module_id
         self.ignored_algorithms = ignored_algorithms
         self.is_called_inside_nncf_module = is_called_inside_nncf_module
         self.is_in_iteration_scope = is_in_iteration_scope
 
     @classmethod
-    def build_from_nx_node(cls, nx_node: Dict[str, Any]) -> 'DynamicGraphNode':
-        return cls(node_id=nx_node[DynamicGraph.ID_NODE_ATTR],
-                   node_key=nx_node[DynamicGraph.KEY_NODE_ATTR],
-                   layer_attributes=nx_node.get(DynamicGraph.LAYER_ATTRIBUTES),
-                   op_exec_context=nx_node[DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR],
-                   ignored_algorithms=nx_node[DynamicGraph.IGNORED_ALGOS_NODE_ATTR],
-                   is_called_inside_nncf_module=nx_node[DynamicGraph.IS_CALLED_INSIDE_NNCF_MODULE],
-                   is_in_iteration_scope=nx_node[DynamicGraph.IS_IN_ITERATION_SCOPE_NODE_ATTR])
+    def build_from_nx_node(cls, nx_node: Dict[str, Any]) -> "DynamicGraphNode":
+        return cls(
+            node_id=nx_node[DynamicGraph.ID_NODE_ATTR],
+            node_key=nx_node[DynamicGraph.KEY_NODE_ATTR],
+            layer_attributes=nx_node.get(DynamicGraph.LAYER_ATTRIBUTES),
+            op_exec_context=nx_node[DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR],
+            ignored_algorithms=nx_node[DynamicGraph.IGNORED_ALGOS_NODE_ATTR],
+            is_called_inside_nncf_module=nx_node[DynamicGraph.IS_CALLED_INSIDE_NNCF_MODULE],
+            is_in_iteration_scope=nx_node[DynamicGraph.IS_IN_ITERATION_SCOPE_NODE_ATTR],
+            calling_module_id=nx_node[DynamicGraph.CALLING_MODULE_ID],
+        )
 
-    def __eq__(self, other: 'DynamicGraphNode') -> bool:
+    def __eq__(self, other: "DynamicGraphNode") -> bool:
         return self.__dict__ == other.__dict__
 
     def __str__(self):
         return self.node_key
 
 
 class DynamicGraphEdge:
-    def __init__(self, from_node_id: int, to_node_id: int,
-                 activation_shape: List[int], input_port_id: int, output_port_id: int,
-                 dtype: Dtype):
+    def __init__(
+        self,
+        from_node_id: int,
+        to_node_id: int,
+        activation_shape: List[int],
+        input_port_id: int,
+        output_port_id: int,
+        dtype: Dtype,
+    ):
         self.from_node_id = from_node_id
         self.to_node_id = to_node_id
         self.activation_shape = activation_shape
         self.input_port_id = input_port_id
         self.output_port_id = output_port_id
         self.dtype = dtype
 
     @classmethod
-    def build_between_two_nx_nodes(cls,
-                                   from_nx_node: Dict[str, Any],
-                                   to_nx_node: Dict[str, Any],
-                                   nx_edge: Dict[str, Any]) -> 'DynamicGraphEdge':
+    def build_between_two_nx_nodes(
+        cls, from_nx_node: Dict[str, Any], to_nx_node: Dict[str, Any], nx_edge: Dict[str, Any]
+    ) -> "DynamicGraphEdge":
         from_node_id = from_nx_node[DynamicGraph.ID_NODE_ATTR]
         to_node_id = to_nx_node[DynamicGraph.ID_NODE_ATTR]
         return DynamicGraphEdge(
             from_node_id=from_node_id,
             to_node_id=to_node_id,
             activation_shape=nx_edge[DynamicGraph.ACTIVATION_SHAPE_EDGE_ATTR],
             input_port_id=nx_edge[DynamicGraph.INPUT_PORT_ID_EDGE_ATTR],
             output_port_id=nx_edge[DynamicGraph.OUTPUT_PORT_ID_EDGE_ATTR],
-            dtype=nx_edge[DynamicGraph.ACTIVATION_DTYPE_EDGE_ATTR]
+            dtype=nx_edge[DynamicGraph.ACTIVATION_DTYPE_EDGE_ATTR],
         )
 
 
 class DefaultScopeNodeMatcher:
     def __init__(self, node_id_to_key_dict, nx_graph):
         self._node_id_to_key_dict = node_id_to_key_dict
         self._nx_graph = nx_graph
         self._inputless_nodes = {}  # type: Dict[str, DynamicGraphNode]
 
     def get_node_by_id(self, node_id):
         return self._nx_graph.nodes[self._node_id_to_key_dict[node_id]]
 
-    def _find_nodes_with_matching_context_among_inputless(self, op_exec_context: OperationExecutionContext) \
-            -> Dict[str, DynamicGraphNode]:
+    def _find_nodes_with_matching_context_among_inputless(
+        self, op_exec_context: OperationExecutionContext
+    ) -> Dict[str, DynamicGraphNode]:
         node_candidates = {}
         for nx_node_key, node in self._inputless_nodes.items():
             if op_exec_context.matches_saved_inputs_from(node.op_exec_context):
                 node_candidates[nx_node_key] = node
         return node_candidates
 
-    def _find_nodes_with_matching_context_and_inputs(self, op_exec_context: OperationExecutionContext) \
-            -> Dict[str, DynamicGraphNode]:
+    def _find_nodes_with_matching_context_and_inputs(
+        self, op_exec_context: OperationExecutionContext
+    ) -> Dict[str, DynamicGraphNode]:
         nx_node_candidates = {}
         for info in op_exec_context.tensor_metas:
             if info is None or info.creator_id is None:
                 continue
             creator_id = info.creator_id
             for successor_node_key in self._nx_graph.successors(self._node_id_to_key_dict[creator_id]):
                 successor_node = self._nx_graph.nodes[successor_node_key]
@@ -260,30 +284,35 @@
 
         node_candidates = {}  # type: Dict[str, DynamicGraphNode]
         for nx_node_key, nx_node_dict in nx_node_candidates.items():
             node_candidates[nx_node_key] = DynamicGraphNode.build_from_nx_node(nx_node_dict)
 
         return node_candidates
 
-    def add_node(self, op_exec_context: OperationExecutionContext, inputs,
-                 node_parameters: DynamicGraphNodeParameters,
-                 is_in_iteration_scope: bool = False) -> DynamicGraphNode:
+    def add_node(
+        self,
+        op_exec_context: OperationExecutionContext,
+        inputs,
+        node_parameters: DynamicGraphNodeParameters,
+        is_in_iteration_scope: bool = False,
+    ) -> DynamicGraphNode:
         node_id = len(self._node_id_to_key_dict)
 
         name_parts = (str(op_exec_context.scope_in_model), op_exec_context.operator_name)
-        node_key = '{idx} {uri}'.format(uri='/'.join(name_parts), idx=node_id)
+        node_key = "{idx} {uri}".format(uri="/".join(name_parts), idx=node_id)
 
         nncf_logger.debug(f"New node added to NNCF graph: {node_key}")
 
         self._node_id_to_key_dict[node_id] = node_key
         attrs = {
             DynamicGraph.ID_NODE_ATTR: node_id,
             DynamicGraph.KEY_NODE_ATTR: node_key,
             DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR: op_exec_context,
-            DynamicGraph.IS_IN_ITERATION_SCOPE_NODE_ATTR: is_in_iteration_scope
+            DynamicGraph.IS_IN_ITERATION_SCOPE_NODE_ATTR: is_in_iteration_scope,
+            DynamicGraph.CALLING_MODULE_ID: node_parameters.calling_module_id,
         }
         if node_parameters.layer_attributes is not None:
             attrs[DynamicGraph.LAYER_ATTRIBUTES] = node_parameters.layer_attributes
 
         if node_parameters.ignored_algorithms is not None:
             attrs[DynamicGraph.IGNORED_ALGOS_NODE_ATTR] = node_parameters.ignored_algorithms
         else:
@@ -308,22 +337,24 @@
         node = DynamicGraphNode.build_from_nx_node(nx_node_dict)
 
         if not has_traced_inputs:
             self._inputless_nodes[node_key] = node
 
         return node
 
-    def find_node(self, op_address: OperationAddress,
-                  tensor_metas: List[TensorMeta],
-                  tm_comparators: List[TensorMetaComparator]) -> DynamicGraphNode:
-        op_exec_context = OperationExecutionContext(op_address.operator_name,
-                                                    op_address.scope_in_model,
-                                                    op_address.call_order,
-                                                    tensor_metas,
-                                                    tm_comparators=tm_comparators)
+    def find_node(
+        self, op_address: OperationAddress, tensor_metas: List[TensorMeta], tm_comparators: List[TensorMetaComparator]
+    ) -> DynamicGraphNode:
+        op_exec_context = OperationExecutionContext(
+            op_address.operator_name,
+            op_address.scope_in_model,
+            op_address.call_order,
+            tensor_metas,
+            tm_comparators=tm_comparators,
+        )
         node_candidates = self._find_nodes_with_matching_context_and_inputs(op_exec_context)
         if not node_candidates:
             node_candidates = self._find_nodes_with_matching_context_among_inputless(op_exec_context)
 
         node_candidates = list(node_candidates.values())
         result = None
         if len(node_candidates) == 1:
@@ -336,15 +367,15 @@
 
 
 class IterationScopeNodeMatcher(DefaultScopeNodeMatcher):
     def __init__(self, node_id_to_key_dict, nx_graph):
         super().__init__(node_id_to_key_dict, nx_graph)
         self._first_iteration_nodes = {}  # type: {str: {str: DynamicGraphNode}}
 
-    def save_first_iteration_node(self, inputs: 'OperatorInput', node: DynamicGraphNode):
+    def save_first_iteration_node(self, inputs: "OperatorInput", node: DynamicGraphNode):
         """
         It finds and saves "starting" points of iteration for further matching with them on next iteration,
         instead of adding new nodes for each iteration. "Starting" points of iteration are nodes
             * that have at least one input node, which is outside of iteration scope
             * or whose all inputs are not TracedTensor
         """
         op_exec_context = node.op_exec_context
@@ -378,53 +409,62 @@
                         has_input_outside_iteration = True
 
                 if len(untraced_tensor_inputs) == (len(inputs) - len(non_tensor_inputs)):
                     has_input_outside_iteration = True
                 if has_input_outside_iteration:
                     node_name = str(op_exec_context.op_address)
                     first_nodes[node_name] = node
-                    nncf_logger.debug(f'Found first iteration node: {name} in scope: {iter_scope}')
+                    nncf_logger.debug(f"Found first iteration node: {name} in scope: {iter_scope}")
 
-    def add_node(self, op_exec_context: OperationExecutionContext, inputs,
-                 node_parameters: DynamicGraphNodeParameters,
-                 is_in_iteration_scope: bool = True) -> DynamicGraphNode:
+    def add_node(
+        self,
+        op_exec_context: OperationExecutionContext,
+        inputs,
+        node_parameters: DynamicGraphNodeParameters,
+        is_in_iteration_scope: bool = True,
+    ) -> DynamicGraphNode:
         node = super().add_node(op_exec_context, inputs, node_parameters, is_in_iteration_scope=True)
         self.save_first_iteration_node(inputs, node)
         return node
 
-    def find_node(self,
-                  op_address: OperationAddress,
-                  tensor_metas: List[TensorMeta],
-                  tm_comparators: List[TensorMetaComparator]) -> Optional[DynamicGraphNode]:
+    def find_node(
+        self, op_address: OperationAddress, tensor_metas: List[TensorMeta], tm_comparators: List[TensorMetaComparator]
+    ) -> Optional[DynamicGraphNode]:
         iter_scopes = op_address.scope_in_model.get_iteration_scopes()
         # compare meta information about first input nodes during the matching. During the iteration some nodes may
         # change number of inputs, e.g. on concat of hidden outputs
         input_matcher = FirstInputsMatcher()
-        op_exec_context = OperationExecutionContext(op_address.operator_name,
-                                                    op_address.scope_in_model,
-                                                    op_address.call_order,
-                                                    tensor_metas,
-                                                    input_matcher=input_matcher,
-                                                    tm_comparators=tm_comparators)
+        op_exec_context = OperationExecutionContext(
+            op_address.operator_name,
+            op_address.scope_in_model,
+            op_address.call_order,
+            tensor_metas,
+            input_matcher=input_matcher,
+            tm_comparators=tm_comparators,
+        )
         node_candidates = self._find_nodes_with_matching_context_and_inputs(op_exec_context)
         if not node_candidates:
-            op_exec_context = OperationExecutionContext(op_address.operator_name,
-                                                        op_address.scope_in_model,
-                                                        op_address.call_order,
-                                                        tensor_metas,
-                                                        tm_comparators=tm_comparators)
+            op_exec_context = OperationExecutionContext(
+                op_address.operator_name,
+                op_address.scope_in_model,
+                op_address.call_order,
+                tensor_metas,
+                tm_comparators=tm_comparators,
+            )
             node_candidates = self._find_nodes_with_matching_context_among_inputless(op_exec_context)
             if not node_candidates and iter_scopes:
                 # ignore information about node creator and index of input
                 comparators = tm_comparators + [ShapeOnlyTensorMetaComparator()]
-                op_exec_context = OperationExecutionContext(op_address.operator_name,
-                                                            op_address.scope_in_model,
-                                                            op_address.call_order,
-                                                            tensor_metas,
-                                                            tm_comparators=comparators)
+                op_exec_context = OperationExecutionContext(
+                    op_address.operator_name,
+                    op_address.scope_in_model,
+                    op_address.call_order,
+                    tensor_metas,
+                    tm_comparators=comparators,
+                )
                 # match with starting points of iteration
                 iter_nodes = self._match_first_iteration_nodes(op_exec_context, iter_scopes)
                 for node_key, node in iter_nodes.items():
                     node_candidates[node_key] = node
 
         node_candidates = list(node_candidates.values())
         result = None
@@ -457,112 +497,137 @@
         self.base_matcher = DefaultScopeNodeMatcher(node_id_to_key_dict, nx_graph)
         self.iteration_matcher = IterationScopeNodeMatcher(node_id_to_key_dict, nx_graph)
 
     # TODO: optimize by matching exact module type
     @staticmethod
     def _within_iteration(scope: Scope):
         scope_name = str(scope)
-        from nncf.torch.layers import ITERATION_MODULES #pylint: disable=cyclic-import
+        from nncf.torch.layers import ITERATION_MODULES  # pylint: disable=cyclic-import
+
         for iter_scope in ITERATION_MODULES.registry_dict:
             if iter_scope in scope_name:
                 return True
         return False
 
     def choose_matcher(self, op_address: OperationAddress) -> DefaultScopeNodeMatcher:
         if self._within_iteration(op_address.scope_in_model):
             return self.iteration_matcher
         return self.base_matcher
 
     @staticmethod
-    def choose_tm_comparators(op_address: OperationAddress,
-                              input_comparators_per_scope:
-                              List[Tuple[TensorMetaComparator, List[str]]]) -> List[TensorMetaComparator]:
+    def choose_tm_comparators(
+        op_address: OperationAddress, input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]]
+    ) -> List[TensorMetaComparator]:
         result = []
         for pairs in input_comparators_per_scope:
             comparator, scopes = pairs
             for scope in scopes:
                 if scope in str(op_address):
                     result.append(comparator)
         return result
 
-    def find_node(self, op_address: OperationAddress,
-                  tensor_metas: List[TensorMeta],
-                  input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]]) -> DynamicGraphNode:
+    def find_node(
+        self,
+        op_address: OperationAddress,
+        tensor_metas: List[TensorMeta],
+        input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]],
+    ) -> DynamicGraphNode:
         matcher = self.choose_matcher(op_address)
         comparators = self.choose_tm_comparators(op_address, input_comparators_per_scope)
         return matcher.find_node(op_address, tensor_metas, comparators)
 
-    def add_node(self, op_address: OperationAddress,
-                 tensor_metas: List[TensorMeta],
-                 tm_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]],
-                 inputs,
-                 node_parameters: DynamicGraphNodeParameters) -> DynamicGraphNode:
+    def add_node(
+        self,
+        op_address: OperationAddress,
+        tensor_metas: List[TensorMeta],
+        tm_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]],
+        inputs,
+        node_parameters: DynamicGraphNodeParameters,
+    ) -> DynamicGraphNode:
         matcher = self.choose_matcher(op_address)
         tm_comparators = self.choose_tm_comparators(op_address, tm_comparators_per_scope)
-        op_exec_context = OperationExecutionContext(op_address.operator_name,
-                                                    op_address.scope_in_model,
-                                                    op_address.call_order,
-                                                    tensor_metas,
-                                                    tm_comparators=tm_comparators)
+        op_exec_context = OperationExecutionContext(
+            op_address.operator_name,
+            op_address.scope_in_model,
+            op_address.call_order,
+            tensor_metas,
+            tm_comparators=tm_comparators,
+        )
 
         return matcher.add_node(op_exec_context, inputs, node_parameters)
 
 
 class DynamicGraph:
     """
     The class for representing a graph dynamically built during a PyTorch model's `forward` method execution
     within a nncf.torch.dynamic_graph.context.TracingContext. This graph may change from a forward call to a
     forward call if the execution paths of the model change between the calls - this sets DynamicGraph apart from
     NNCFGraph which is a static representation of the model's structure. The DynamicGraph has limited support for
     RNN tracing and is rather suited to regular DNN tracing.
     """
-    ID_NODE_ATTR = 'id'
-    KEY_NODE_ATTR = 'key'
-    LAYER_ATTRIBUTES = 'layer_attributes'
-    OP_EXEC_CONTEXT_NODE_ATTR = 'op_exec_context'
-    ACTIVATION_SHAPE_EDGE_ATTR = 'activation_shape'
-    ACTIVATION_DTYPE_EDGE_ATTR = 'activation_dtype'
-    INPUT_PORT_ID_EDGE_ATTR = 'input_port_id'
-    OUTPUT_PORT_ID_EDGE_ATTR = 'output_port_id'
-    IGNORED_ALGOS_NODE_ATTR = 'ignored_algos'
-    IS_CALLED_INSIDE_NNCF_MODULE = 'is_called_inside_nncf_module'
-    IS_IN_ITERATION_SCOPE_NODE_ATTR = 'is_in_iteration_scope'
+
+    ID_NODE_ATTR = "id"
+    KEY_NODE_ATTR = "key"
+    LAYER_ATTRIBUTES = "layer_attributes"
+    OP_EXEC_CONTEXT_NODE_ATTR = "op_exec_context"
+    ACTIVATION_SHAPE_EDGE_ATTR = "activation_shape"
+    ACTIVATION_DTYPE_EDGE_ATTR = "activation_dtype"
+    INPUT_PORT_ID_EDGE_ATTR = "input_port_id"
+    OUTPUT_PORT_ID_EDGE_ATTR = "output_port_id"
+    IGNORED_ALGOS_NODE_ATTR = "ignored_algos"
+    IS_CALLED_INSIDE_NNCF_MODULE = "is_called_inside_nncf_module"
+    IS_IN_ITERATION_SCOPE_NODE_ATTR = "is_in_iteration_scope"
+    CALLING_MODULE_ID = "calling_module_id"
 
     def __init__(self):
         self._nx_graph = nx.DiGraph()
         self._node_id_to_key_dict = {}
         self.match_manager = NodeManager(self._node_id_to_key_dict, self._nx_graph)
         self._input_nncf_nodes = []
         self._output_nncf_nodes = []
 
-    def __eq__(self, other: 'DynamicGraph'):
-        nm = iso.categorical_node_match([DynamicGraph.ID_NODE_ATTR,
-                                         DynamicGraph.KEY_NODE_ATTR,
-                                         DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR,
-                                         DynamicGraph.LAYER_ATTRIBUTES], [None, None, None])
-        em = iso.categorical_edge_match([DynamicGraph.ACTIVATION_SHAPE_EDGE_ATTR,
-                                         DynamicGraph.INPUT_PORT_ID_EDGE_ATTR], [None, None])
+    def __eq__(self, other: "DynamicGraph"):
+        nm = iso.categorical_node_match(
+            [
+                DynamicGraph.ID_NODE_ATTR,
+                DynamicGraph.KEY_NODE_ATTR,
+                DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR,
+                DynamicGraph.LAYER_ATTRIBUTES,
+                DynamicGraph.CALLING_MODULE_ID,
+            ],
+            [None, None, None, None, None],
+        )
+        em = iso.categorical_edge_match(
+            [DynamicGraph.ACTIVATION_SHAPE_EDGE_ATTR, DynamicGraph.INPUT_PORT_ID_EDGE_ATTR], [None, None]
+        )
         return nx.is_isomorphic(self._nx_graph, other._nx_graph, node_match=nm, edge_match=em)
 
-    def find_node(self,
-                  op_address: OperationAddress,
-                  tensor_metas: List[TensorMeta],
-                  input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]]) -> DynamicGraphNode:
+    def find_node(
+        self,
+        op_address: OperationAddress,
+        tensor_metas: List[TensorMeta],
+        input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]],
+    ) -> DynamicGraphNode:
         return self.match_manager.find_node(op_address, tensor_metas, input_comparators_per_scope)
 
-    def add_node(self, op_address: OperationAddress,
-                 tensor_metas: List[TensorMeta],
-                 input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]],
-                 inputs,
-                 node_parameters: DynamicGraphNodeParameters) -> DynamicGraphNode:
-        node = self.match_manager.add_node(op_address, tensor_metas, input_comparators_per_scope,
-                                           inputs, node_parameters)
+    def add_node(
+        self,
+        op_address: OperationAddress,
+        tensor_metas: List[TensorMeta],
+        input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]],
+        inputs,
+        node_parameters: DynamicGraphNodeParameters,
+    ) -> DynamicGraphNode:
+        node = self.match_manager.add_node(
+            op_address, tensor_metas, input_comparators_per_scope, inputs, node_parameters
+        )
+
+        from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME  # pylint: disable=cyclic-import
+        from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME  # pylint: disable=cyclic-import
 
-        from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME #pylint: disable=cyclic-import
-        from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME #pylint: disable=cyclic-import
         if node.op_exec_context.operator_name == MODEL_INPUT_OP_NAME:
             self._input_nncf_nodes.append(node)
 
         if node.op_exec_context.operator_name == MODEL_OUTPUT_OP_NAME:
             self._output_nncf_nodes.append(node)
         return node
 
@@ -583,16 +648,15 @@
         return all_nodes
 
     def get_all_edges(self) -> Generator[DynamicGraphEdge, None, None]:
         """
         Generates all edges in the graph
         """
         for from_nx_node_key, to_nx_node_key in self._nx_graph.in_edges:
-            yield self._get_edge(self._get_node_by_key(from_nx_node_key),
-                                 self._get_node_by_key(to_nx_node_key))
+            yield self._get_edge(self._get_node_by_key(from_nx_node_key), self._get_node_by_key(to_nx_node_key))
 
     def get_input_edges(self, node: DynamicGraphNode) -> List[DynamicGraphEdge]:
         """
         Returns edges of input tensors with description sorted by input port ID.
 
         :param node: Consumer node.
         :return: List of input edges for the node sorted by input port ID.
@@ -623,8 +687,8 @@
         from_nx_node = self._nx_graph.nodes[from_node.node_key]
         to_nx_node = self._nx_graph.nodes[to_node.node_key]
         return DynamicGraphEdge.build_between_two_nx_nodes(from_nx_node, to_nx_node, nx_edge)
 
     def _get_nx_edge(self, node_u: DynamicGraphNode, node_v: DynamicGraphNode):
         nx_node_u = self._nx_graph.nodes[self._node_id_to_key_dict[node_u.node_id]]
         nx_node_v = self._nx_graph.nodes[self._node_id_to_key_dict[node_v.node_id]]
-        return self._nx_graph.edges[nx_node_u['key'], nx_node_v['key']]
+        return self._nx_graph.edges[nx_node_u["key"], nx_node_v["key"]]
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/graph_tracer.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/graph_tracer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,22 +1,20 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import OrderedDict
-from typing import Callable, Any, List, Optional
 from copy import deepcopy
+from typing import Any, Callable, List, Optional
 
 import torch
 
 from nncf.torch.dynamic_graph.graph import DynamicGraph
 from nncf.torch.utils import get_model_device
 
 
@@ -57,23 +55,29 @@
 
 
 def create_input_infos(config) -> Optional[List[ModelInputInfo]]:
     input_infos = config.get("input_info")
     if input_infos is None:
         return input_infos
     if isinstance(input_infos, dict):
-        return [ModelInputInfo(input_infos.get("sample_size"),
-                               input_infos.get("type"),
-                               input_infos.get("keyword"),
-                               input_infos.get("filler")), ]
+        return [
+            ModelInputInfo(
+                input_infos.get("sample_size"),
+                input_infos.get("type"),
+                input_infos.get("keyword"),
+                input_infos.get("filler"),
+            ),
+        ]
     if isinstance(input_infos, list):
-        return [ModelInputInfo(info_dict.get("sample_size"),
-                               info_dict.get("type"),
-                               info_dict.get("keyword"),
-                               info_dict.get("filler")) for info_dict in input_infos]
+        return [
+            ModelInputInfo(
+                info_dict.get("sample_size"), info_dict.get("type"), info_dict.get("keyword"), info_dict.get("filler")
+            )
+            for info_dict in input_infos
+        ]
     raise RuntimeError("Invalid input_infos specified in config - should be either dict or list of dicts")
 
 
 def create_mock_tensor(input_info: ModelInputInfo, device: str):
     args = {"size": input_info.shape, "dtype": input_info.type, "device": device}
     if input_info.filler == ModelInputInfo.FILLER_TYPE_ZEROS:
         return torch.zeros(**args)
@@ -84,54 +88,56 @@
     raise RuntimeError
 
 
 class GraphTracer:
     def __init__(self, custom_forward_fn: Callable[[torch.nn.Module], Any]):
         self.custom_forward_fn = custom_forward_fn
 
-    def trace_graph(self, model: torch.nn.Module, context_to_use: Optional['TracingContext'] = None,
-                    as_eval: bool = False) -> DynamicGraph:
+    def trace_graph(
+        self, model: torch.nn.Module, context_to_use: Optional["TracingContext"] = None, as_eval: bool = False
+    ) -> DynamicGraph:
         sd = deepcopy(model.state_dict())
 
-        from nncf.torch.dynamic_graph.context import TracingContext #pylint: disable=cyclic-import
+        from nncf.torch.dynamic_graph.context import TracingContext  # pylint: disable=cyclic-import
+
         if context_to_use is None:
             context_to_use = TracingContext()
 
         context_to_use.enable_trace_dynamic_graph()
-        from nncf.torch.utils import training_mode_switcher #pylint: disable=cyclic-import
+        from nncf.torch.utils import training_mode_switcher  # pylint: disable=cyclic-import
+
         with context_to_use as _ctx:
             _ctx.base_module_thread_local_replica = model
             with torch.no_grad():
                 if as_eval:
                     with training_mode_switcher(model, is_training=False):
                         self.custom_forward_fn(model)
                 else:
                     self.custom_forward_fn(model)
         model.load_state_dict(sd)
 
-        if isinstance(model, PostGraphBuildActing):
-            model.post_build_graph_actions()
         context_to_use.disable_trace_dynamic_graph()
         return context_to_use.graph
 
 
-class PostGraphBuildActing:
-    def post_build_graph_actions(self):
-        pass
-
-
-def create_dummy_forward_fn(input_infos: List[ModelInputInfo], with_input_tracing=False,
-                            wrap_inputs_fn=None,
-                            wrap_outputs_fn=None,
-                            with_output_tracing=False):
-
+def create_dummy_forward_fn(
+    input_infos: List[ModelInputInfo],
+    with_input_tracing=False,
+    wrap_inputs_fn=None,
+    wrap_outputs_fn=None,
+    with_output_tracing=False,
+):
     def default_dummy_forward_fn(model):
-        from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_inputs_with_objwalk #pylint: disable=cyclic-import
-        from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_outputs_with_objwalk #pylint: disable=cyclic-import
-        from nncf.torch.dynamic_graph.io_handling import replicate_same_tensors #pylint: disable=cyclic-import
+        from nncf.torch.dynamic_graph.io_handling import replicate_same_tensors  # pylint: disable=cyclic-import
+        from nncf.torch.dynamic_graph.io_handling import (
+            wrap_nncf_model_inputs_with_objwalk,  # pylint: disable=cyclic-import
+        )
+        from nncf.torch.dynamic_graph.io_handling import (
+            wrap_nncf_model_outputs_with_objwalk,  # pylint: disable=cyclic-import
+        )
 
         device = get_model_device(model)
         args_list = [create_mock_tensor(info, device) for info in input_infos if info.keyword is None]
         kwargs = OrderedDict()
         for info in input_infos:
             if info.keyword is not None:
                 kwargs[info.keyword] = create_mock_tensor(info, device)
@@ -149,8 +155,9 @@
         retval = model(*args, **kwargs)
         if with_output_tracing:
             retval = replicate_same_tensors(retval)
             if wrap_outputs_fn is not None:
                 return wrap_outputs_fn(retval)
             return wrap_nncf_model_outputs_with_objwalk(retval)
         return retval
+
     return default_dummy_forward_fn
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/io_handling.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/io_handling.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,40 +1,46 @@
 from collections import OrderedDict
-from inspect import Signature, Parameter
-from typing import Any
-from typing import List
+from inspect import Parameter
+from inspect import Signature
+from typing import Any, List
 
 import torch
 
 from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
 from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME
-from nncf.torch.dynamic_graph.patch_pytorch import register_operator
-from nncf.torch.dynamic_graph.graph_tracer import ModelInputInfo, create_mock_tensor
-from nncf.torch.utils import get_model_device
-from nncf.torch.utils import is_tensor, is_traced_tensor
-from nncf.torch.nested_objects_traversal import objwalk
 from nncf.common.logging import nncf_logger
+from nncf.common.utils.api_marker import api
 from nncf.torch.dynamic_graph.context import forward_nncf_trace
+from nncf.torch.dynamic_graph.graph_tracer import ModelInputInfo
+from nncf.torch.dynamic_graph.graph_tracer import create_mock_tensor
+from nncf.torch.dynamic_graph.patch_pytorch import register_operator
+from nncf.torch.nested_objects_traversal import objwalk
+from nncf.torch.utils import get_model_device
+from nncf.torch.utils import is_tensor
+from nncf.torch.utils import is_traced_tensor
 
 
+@api(canonical_alias="nncf.torch.nncf_model_input")
 @register_operator(name=MODEL_INPUT_OP_NAME)
-def nncf_model_input(tensor: 'torch.Tensor'):
+def nncf_model_input(tensor: "torch.Tensor"):
     return tensor
 
 
+@api(canonical_alias="nncf.torch.nncf_model_output")
 @register_operator(name=MODEL_OUTPUT_OP_NAME)
-def nncf_model_output(tensor: 'torch.Tensor'):
+def nncf_model_output(tensor: "torch.Tensor"):
     return tensor
 
 
 def wrap_nncf_model_inputs_with_objwalk(model_args, model_kwargs):
     model_args = objwalk(model_args, is_tensor, nncf_model_input)
     model_kwargs = objwalk(model_kwargs, is_tensor, nncf_model_input)
     return model_args, model_kwargs
 
+
 def wrap_nncf_model_outputs_with_objwalk(model_outputs):
     model_outputs = objwalk(model_outputs, is_traced_tensor, nncf_model_output)
     return model_outputs
 
 
 def replicate_same_tensors(obj: Any) -> Any:
     """
@@ -48,32 +54,39 @@
     def replicate_fn(tensor: torch.Tensor) -> torch.Tensor:
         tensor_object_id = id(tensor)
         if tensor_object_id in observed_tensor_object_ids:
             with forward_nncf_trace():
                 return tensor.clone()
         observed_tensor_object_ids.add(tensor_object_id)
         return tensor
+
     obj = objwalk(obj, is_tensor, replicate_fn)
     return obj
 
 
 class InputInfoWrapManager:
-    INPUTS_MISMATCH_WARNING_TEXT = "Compression with regards to this input may occur incorrectly. Make sure " \
-                                   "you call the compressed model with inputs that correspond to what NNCF was " \
-                                   "configured to expect (either via NNCF config's input_infos, or custom" \
-                                   "dummy_forward_fn/wrap_inputs_fn parameters), or that you know what you are " \
-                                   "doing. This warning will not be shown again."
-    ARGS_INPUTS_MISMATCH_FORMAT_STRING = "Inputs mismatch - could not find arg with idx {} in NNCF-wrapped model " \
-                                         "input args! " + INPUTS_MISMATCH_WARNING_TEXT
-    KWARGS_INPUTS_MISMATCH_FORMAT_STRING = "Inputs mismatch - could not find kwarg '{}' in NNCF-wrapped model input " \
-                                           "kwargs! " + INPUTS_MISMATCH_WARNING_TEXT
-
-    def __init__(self, input_infos: List[ModelInputInfo],
-                 fwd_signature: Signature,
-                 module_ref_for_device: torch.nn.Module = None):
+    INPUTS_MISMATCH_WARNING_TEXT = (
+        "Compression with regards to this input may occur incorrectly. Make sure "
+        "you call the compressed model with inputs that correspond to what NNCF was "
+        "configured to expect (either via NNCF config's input_infos, or custom"
+        "dummy_forward_fn/wrap_inputs_fn parameters), or that you know what you are "
+        "doing. This warning will not be shown again."
+    )
+    ARGS_INPUTS_MISMATCH_FORMAT_STRING = (
+        "Inputs mismatch - could not find arg with idx {} in NNCF-wrapped model "
+        "input args! " + INPUTS_MISMATCH_WARNING_TEXT
+    )
+    KWARGS_INPUTS_MISMATCH_FORMAT_STRING = (
+        "Inputs mismatch - could not find kwarg '{}' in NNCF-wrapped model input "
+        "kwargs! " + INPUTS_MISMATCH_WARNING_TEXT
+    )
+
+    def __init__(
+        self, input_infos: List[ModelInputInfo], fwd_signature: Signature, module_ref_for_device: torch.nn.Module = None
+    ):
         self._module_ref_for_device = module_ref_for_device
         arg_iis_list = [ii for ii in input_infos if ii.keyword is None]
         kwarg_iis_list = [(ii.keyword, ii) for ii in input_infos if ii.keyword is not None]
         kwarg_iis = OrderedDict()
         arg_iis = tuple(arg_iis_list)
         for kw, ii in kwarg_iis_list:
             kwarg_iis[kw] = ii
@@ -86,35 +99,39 @@
         self._device = device
 
     def wrap_inputs(self, model_args, model_kwargs):
         bound_model_params = self._fwd_signature.bind(*model_args, **model_kwargs)
         for param_name in self._fwd_params_to_input_infos_odict:
             param_kind = self._fwd_signature.parameters[param_name].kind
             if param_kind is Parameter.VAR_POSITIONAL or param_kind is Parameter.VAR_KEYWORD:
-                nncf_logger.warning("An input_info tensor was bound to a *args or **kwargs variadic parameter in the"
-                                    "forward's signature! This is currently unsupported by NNCF. Input compression may "
-                                    "be incorrect.")
+                nncf_logger.warning(
+                    "An input_info tensor was bound to a *args or **kwargs variadic parameter in the"
+                    "forward's signature! This is currently unsupported by NNCF. Input compression may "
+                    "be incorrect."
+                )
                 # Currently won't support input info mapping to *args or **kwargs-mapped parameters
                 continue
 
             if param_name not in bound_model_params.arguments:
-                nncf_logger.warning("A call to a compressed model's forward occurred without one of the params"
-                                    "specified in input_infos! Input compression may be incorrect. Trying to recover "
-                                    "by wrapping the default value for the parameter.")
+                nncf_logger.warning(
+                    "A call to a compressed model's forward occurred without one of the params"
+                    "specified in input_infos! Input compression may be incorrect. Trying to recover "
+                    "by wrapping the default value for the parameter."
+                )
                 bound_model_params.apply_defaults()
 
             potential_tensor = bound_model_params.arguments[param_name]
             if potential_tensor is not None:
                 bound_model_params.arguments[param_name] = nncf_model_input(bound_model_params.arguments[param_name])
             else:
                 # Default was None - cannot wrap as-is. Will wrap a dummy tensor as specified in
                 # input infos - will preserve the call order of nncf_model_input nodes,
                 # and the post-hooks for the input node will execute. The result won't go anywhere, though.
                 nncf_logger.info(f"Wrapping a dummy tensor for input {param_name}")
                 info_for_missing_input = self._fwd_params_to_input_infos_odict[param_name]
-                device = 'cuda'
+                device = "cuda"
                 if self._module_ref_for_device is not None:
                     device = get_model_device(self._module_ref_for_device)
                 dummy_tensor = create_mock_tensor(info_for_missing_input, device)
                 _ = nncf_model_input(dummy_tensor)
 
         return bound_model_params.args, bound_model_params.kwargs
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/operation_address.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/operation_address.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,41 +1,38 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.torch.dynamic_graph.scope import Scope
 
 
 class OperationAddress:
     def __init__(self, operator_name: str, scope_in_model: Scope, call_order: int):
         self.operator_name = operator_name
         self.scope_in_model = scope_in_model
         self.call_order = call_order
 
-    def __eq__(self, other: 'OperationAddress'):
-        return isinstance(other, OperationAddress) and \
-               (self.operator_name == other.operator_name) and \
-               (self.scope_in_model == other.scope_in_model) and \
-               (self.call_order == other.call_order)
+    def __eq__(self, other: "OperationAddress"):
+        return (
+            isinstance(other, OperationAddress)
+            and (self.operator_name == other.operator_name)
+            and (self.scope_in_model == other.scope_in_model)
+            and (self.call_order == other.call_order)
+        )
 
     def __str__(self):
-        return str(self.scope_in_model) + '/' + \
-               self.operator_name + "_" + str(self.call_order)
+        return str(self.scope_in_model) + "/" + self.operator_name + "_" + str(self.call_order)
 
     def __hash__(self):
         return hash((self.operator_name, self.scope_in_model, self.call_order))
 
     @staticmethod
     def from_str(s: str):
-        scope_and_op, _, call_order_str = s.rpartition('_')
-        scope_str, _, op_name = scope_and_op.rpartition('/')
-        return OperationAddress(op_name,
-                                Scope.from_str(scope_str),
-                                int(call_order_str))
+        scope_and_op, _, call_order_str = s.rpartition("_")
+        scope_str, _, op_name = scope_and_op.rpartition("/")
+        return OperationAddress(op_name, Scope.from_str(scope_str), int(call_order_str))
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/patch_pytorch.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/patch_pytorch.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,46 +1,38 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from enum import Enum
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+import functools
+import inspect
 from typing import List
 
 import torch
 import torch.utils.cpp_extension
+from torch._jit_internal import createResolutionCallbackFromFrame
+from torch.jit import is_tracing
 from torch.nn import DataParallel
 from torch.nn.parallel import DistributedDataParallel
 
 from nncf import nncf_logger
+from nncf.common.utils.api_marker import api
+from nncf.torch.dynamic_graph.structs import NamespaceTarget
 from nncf.torch.dynamic_graph.trace_tensor import TracedTensor
 from nncf.torch.dynamic_graph.wrappers import ignore_scope
 from nncf.torch.dynamic_graph.wrappers import wrap_module_call
 from nncf.torch.dynamic_graph.wrappers import wrap_operator
 
 
-class NamespaceTarget(Enum):
-    """
-    NamespaceTarget stores modules from which patched operators were obtained.
-    """
-    TORCH_NN_FUNCTIONAL = 'torch.nn.functional'
-    TORCH_TENSOR = 'torch.tensor'
-    TORCH = 'torch'
-    EXTERNAL = 'external_function'
-
-
 def get_namespace_to_patch(namespace_target: NamespaceTarget) -> object:
     if namespace_target == NamespaceTarget.TORCH_NN_FUNCTIONAL:
         return torch.nn.functional
     if namespace_target == NamespaceTarget.TORCH_TENSOR:
         return TracedTensor
     if namespace_target == NamespaceTarget.TORCH:
         return torch
@@ -70,89 +62,232 @@
         """
         self.name = name
         self.operator_namespace = operator_namespace
         self.skip_trace = skip_trace
 
 
 class FunctionsToPatchWithoutTracing:
-    TENSOR_CREATING_FUNCTIONS = ['arange', 'as_subclass', 'as_tensor', 'copysign', 'copysign_', 'detach', 'detach_',
-                                 'empty', 'ones', 'ones_like', 'rad2deg', 'rad2deg_', 'rand', 'randn', 'randn_like',
-                                 'tensor', 'zeros']
-    TENSOR_UTILITY_FUNCTIONS = ['all', 'allclose', 'any', 'assert_int_or_pair',
-                                'backward', 'broadcast_to', 'cpu', 'cuda', 'data_ptr', 'dequantize', 'dim',
-                                'handle_torch_function', 'has_names', 'has_torch_function', 'has_torch_function_unary',
-                                'has_torch_function_variadic', 'is_contiguous', 'item', 'names', 'numel', 'numpy',
-                                'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale',
-                                'q_zero_point', 'qr', 'qscheme', 'random_', 'record_stream', 'refine_names',
-                                'register_hook', 'rename', 'rename_', 'shape', 'size', 'sort', 'storage',
-                                'storage_offset', 'stride', 'to']
+    TENSOR_CREATING_FUNCTIONS = [
+        "arange",
+        "as_subclass",
+        "as_tensor",
+        "copysign",
+        "copysign_",
+        "detach",
+        "detach_",
+        "empty",
+        "ones",
+        "ones_like",
+        "rad2deg",
+        "rad2deg_",
+        "rand",
+        "randn",
+        "randn_like",
+        "tensor",
+        "zeros",
+    ]
+    TENSOR_UTILITY_FUNCTIONS = [
+        "all",
+        "allclose",
+        "any",
+        "assert_int_or_pair",
+        "backward",
+        "broadcast_to",
+        "cpu",
+        "cuda",
+        "data_ptr",
+        "dequantize",
+        "dim",
+        "handle_torch_function",
+        "has_names",
+        "has_torch_function",
+        "has_torch_function_unary",
+        "has_torch_function_variadic",
+        "is_contiguous",
+        "item",
+        "names",
+        "numel",
+        "numpy",
+        "q_per_channel_axis",
+        "q_per_channel_scales",
+        "q_per_channel_zero_points",
+        "q_scale",
+        "q_zero_point",
+        "qr",
+        "qscheme",
+        "random_",
+        "record_stream",
+        "refine_names",
+        "register_hook",
+        "rename",
+        "rename_",
+        "shape",
+        "size",
+        "sort",
+        "storage",
+        "storage_offset",
+        "stride",
+        "to",
+        "get_device",
+    ]
 
     FUNCTIONS_TO_PATCH_WITHOUT_TRACING = TENSOR_CREATING_FUNCTIONS + TENSOR_UTILITY_FUNCTIONS
 
 
 class MagicFunctionsToPatch:
     MAGIC_FUNCTIONS_TO_PATCH = {
-        NamespaceTarget.TORCH_TENSOR: ["__add__", "__iadd__", "__radd__", "__sub__", "__isub__",
-                                       "__rsub__", "__mul__",
-                                       "__imul__", "__rmul__", "__div__", "__idiv__",
-                                       "__truediv__", "__floordiv__",
-                                       "__ifloordiv__", "__rfloordiv__", "__getitem__",
-                                       "__lt__", "__le__", "__gt__",
-                                       "__ge__", "__mod__", "__eq__", "__ne__", "__or__",
-                                       "__xor__", "__and__", "__pow__"]
+        NamespaceTarget.TORCH_TENSOR: [
+            "__add__",
+            "__iadd__",
+            "__radd__",
+            "__sub__",
+            "__isub__",
+            "__rsub__",
+            "__mul__",
+            "__matmul__",
+            "__rmatmul__",
+            "__imul__",
+            "__rmul__",
+            "__div__",
+            "__idiv__",
+            "__truediv__",
+            "__floordiv__",
+            "__ifloordiv__",
+            "__rfloordiv__",
+            "__getitem__",
+            "__lt__",
+            "__le__",
+            "__gt__",
+            "__ge__",
+            "__mod__",
+            "__eq__",
+            "__ne__",
+            "__or__",
+            "__xor__",
+            "__and__",
+            "__pow__",
+        ]
     }
 
 
+@api(canonical_alias="nncf.torch.register_operator")
 def register_operator(name=None):
     def wrap(operator):
         op_name = name
         if op_name is None:
             op_name = operator.__name__
         return wrap_operator(operator, PatchedOperatorInfo(op_name, NamespaceTarget.EXTERNAL))
 
     return wrap
 
     # TODO: Use same wrapper for model.forward() calls
 
 
 def torch_jit_script_wrapper(*args, **kwargs):
     # Torch JIT cannot work with NNCF-modified operators,
-    # so at each import of a @torch.jit.script-decorated
-    # function we need to un-patch the torch operators
-    unpatch_torch_operators()
+    # so at call of torch.jit.script function we need to
+    # un-patch the torch operators
+
+    # If already unpatched, don't perform unpatch/patch
+    apply_unpatch = _OPERATORS_ALREADY_WRAPPED
+    if apply_unpatch:
+        unpatch_torch_operators()
+
+    signature = inspect.signature(_ORIG_JIT_SCRIPT)
+    bound_args = signature.bind(*args, **kwargs).arguments
+    # Process the case when the object-to-script is a class as in the original jit.script logic
+    if inspect.isclass(bound_args["obj"]):
+        # Inserting wrapper alters the call stack, hence we need to change the resolution callback accordingly
+        if "_rcb" not in bound_args:
+            frames_up = bound_args.get("_frames_up", 0)
+            rcb = createResolutionCallbackFromFrame(frames_up + 1)
+            kwargs["_rcb"] = rcb
+        retval = _ORIG_JIT_SCRIPT(*args, **kwargs)
+    else:
+        # For some reason resolution callback may return patched methods, so we wrap it to avoid this
+        if "_rcb" in kwargs:
+            rcb = kwargs["_rcb"]
+
+            def rcb_wrapper(name):
+                value = rcb(name)
+                if hasattr(value, "_original_op"):
+                    value = value._original_op  # pylint: disable=protected-access
+                return value
+
+            kwargs["_rcb"] = rcb_wrapper
+
+        retval = _ORIG_JIT_SCRIPT(*args, **kwargs)
+
+    if apply_unpatch:
+        patch_torch_operators()
 
-    retval = _ORIG_JIT_SCRIPT(*args, **kwargs)
-    patch_torch_operators()
     return retval
 
 
+def torch_jit_trace_make_module_wrapper(*args, **kwargs):
+    apply_unpatch = _OPERATORS_ALREADY_WRAPPED
+    if apply_unpatch:
+        unpatch_torch_operators()
+    retval = _ORIG_JIT_TRACE_MAKE_MODULE(*args, **kwargs)
+    if apply_unpatch:
+        patch_torch_operators()
+    return retval
+
+
+def torch_jit_script_if_tracing(fn):
+    # pylint: disable=protected-access
+    @functools.wraps(fn)
+    def wrapper(*args, **kwargs):
+        if not is_tracing():
+            return fn(*args, **kwargs)
+
+        compiled_fn = torch.jit.script(wrapper.__original_fn)
+        return compiled_fn(*args, **kwargs)
+
+    wrapper.__original_fn = fn
+    wrapper.__script_if_tracing_wrapper = True
+
+    return wrapper
+
+
 class OriginalOpInfo:
     def __init__(self, name: str, namespace, op):
         self.name = name
         self.namespace = namespace
         self.op = op
 
 
 ORIGINAL_OPERATORS = []  # type: List[OriginalOpInfo]
+ORIGINAL_CALL = torch.nn.Module.__call__
 _JIT_ALREADY_WRAPPED = False
 _OPERATORS_ALREADY_WRAPPED = False
 _ORIG_JIT_SCRIPT = None
+_ORIG_JIT_TRACE_MAKE_MODULE = None
 
 
-def patch_torch_jit_script():
+def patch_torch_jit():
     # This import statement is required, otherwise we get a
     # "RuntimeError: undefined value torch" inside the real torch.jit.script
-    # pylint:disable=unused-import,redefined-outer-name,reimported
+    # pylint:disable=unused-import,redefined-outer-name,reimported,protected-access
     import torch
 
-    orig = getattr(torch.jit, "script")
     global _ORIG_JIT_SCRIPT
-    _ORIG_JIT_SCRIPT = orig
+    _ORIG_JIT_SCRIPT = getattr(torch.jit, "script")
     setattr(torch.jit, "script", torch_jit_script_wrapper)
 
+    # Patch torch.jit._trace.make_module() which is called during
+    # torch.jit.trace() call
+    global _ORIG_JIT_TRACE_MAKE_MODULE
+    _ORIG_JIT_TRACE_MAKE_MODULE = getattr(torch.jit._trace, "make_module")
+    setattr(torch.jit._trace, "make_module", torch_jit_trace_make_module_wrapper)
+
+    # Patch torch.jit._script_if_tracing because it references an original
+    # unpatched torch.jit.script and the patching above does not affect it
+    setattr(torch.jit, "_script_if_tracing", torch_jit_script_if_tracing)
+
 
 def patch_namespace_opname(namespace, op_info: PatchedOperatorInfo):
     op_name = op_info.name
     if hasattr(namespace, op_name):
         orig = getattr(namespace, op_name)
         ORIGINAL_OPERATORS.append(OriginalOpInfo(op_name, namespace, orig))
         setattr(namespace, op_name, wrap_operator(orig, op_info))
@@ -164,50 +299,56 @@
     """
     Seeks all attributes from the namespace, then takes only attributes,
     which types are function, builtin, method or method descriptor.
     If 'do_filer' is True, then also removes all private or magic attributes.
     :param namespace: Python module.
     :param do_filter: If True return only public functions, else - otherwise.
     """
-    import inspect
 
     def remove_private_functions(names: List[str]) -> List[str]:
         filtered_names = []
         for name in names:
-            if name.startswith('_'):
+            if name.startswith("_"):
                 continue
             filtered_names.append(name)
         return filtered_names
 
     patched_namespace = get_namespace_to_extract_functions_from(namespace)
     all_torch_function_names = []
     members = inspect.getmembers(patched_namespace)
     for member in members:
-        if inspect.isfunction(member[1]) or inspect.isbuiltin(member[1]) or inspect.ismethod(
-                member[1]) or inspect.ismethoddescriptor(member[1]):
+        if (
+            inspect.isfunction(member[1])
+            or inspect.isbuiltin(member[1])
+            or inspect.ismethod(member[1])
+            or inspect.ismethoddescriptor(member[1])
+        ):
             all_torch_function_names.append(member[0])
     if do_filter:
         filtered_function_names = remove_private_functions(all_torch_function_names)
         return filtered_function_names
     return all_torch_function_names
 
 
 def patch_torch_operators():
     # Only patch torch.jit.script during first patch_torch_operators call
     global _JIT_ALREADY_WRAPPED
     if not _JIT_ALREADY_WRAPPED:
-        patch_torch_jit_script()
+        patch_torch_jit()
         _JIT_ALREADY_WRAPPED = True
 
     # Do not patch operators twice as well
     global _OPERATORS_ALREADY_WRAPPED
     if _OPERATORS_ALREADY_WRAPPED:
         return
     _OPERATORS_ALREADY_WRAPPED = True
 
+    global ORIGINAL_OPERATORS
+    ORIGINAL_OPERATORS = []
+
     functions_to_patch = {}
     for namespace in NamespaceTarget:
         if namespace == NamespaceTarget.EXTERNAL:
             continue
         functions_to_patch[namespace] = get_all_functions_from_namespace(namespace)
 
     ignored_functions = FunctionsToPatchWithoutTracing.FUNCTIONS_TO_PATCH_WITHOUT_TRACING
@@ -221,16 +362,21 @@
                 continue
             new_function_names.append(function_name)
         functions_to_patch[namespace] = new_function_names
 
     # Relu function from torch.nn.functional uses torch.relu or torch.relu_ inside,
     # which we've already wrapped. So we don't have to wrap Relu in torch.nn.functional
     # to be able to differ what torch.relu or torch.relu was called exactly inside Relu
+    functions_to_patch[NamespaceTarget.TORCH_NN_FUNCTIONAL].remove("relu")
 
-    functions_to_patch[NamespaceTarget.TORCH_NN_FUNCTIONAL].remove('relu')
+    # nn.MultiheadAttention uses multi_head_attention_forward from torch.nn.functional inside, which
+    # leads to adding a single node to the graph. In turn, it makes impossible to quantize internals of the multi head
+    # attention which is required for better speed-up. So multi_head_attention_forward is skipped from wrapping and
+    # tracing goes inside and finds internal operations in it: bmm, linear, softmax and etc.
+    functions_to_patch[NamespaceTarget.TORCH_NN_FUNCTIONAL].remove("multi_head_attention_forward")
 
     magic_functions_to_patch = MagicFunctionsToPatch.MAGIC_FUNCTIONS_TO_PATCH
     for namespace, function_names in magic_functions_to_patch.items():
         functions_to_patch[namespace] += function_names
 
     for namespace, function_names in functions_to_patch.items():
         for function_name in function_names:
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/scope.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/scope.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,39 +1,36 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import re
 from copy import deepcopy
 from typing import List
 
 
-
 class ScopeElement:
     def __init__(self, calling_module_class_name: str, calling_field_name: str = None):
         self.calling_module_class_name = calling_module_class_name
         self.calling_field_name = calling_field_name
 
     def __str__(self):
         if self.calling_field_name is None:
             return self.calling_module_class_name
-        return "{cls}[{name}]".format(cls=self.calling_module_class_name,
-                                      name=self.calling_field_name)
+        return "{cls}[{name}]".format(cls=self.calling_module_class_name, name=self.calling_field_name)
 
-    def __eq__(self, other: 'ScopeElement'):
-        return (self.calling_module_class_name == other.calling_module_class_name) and \
-               (self.calling_field_name == other.calling_field_name)
+    def __eq__(self, other: "ScopeElement"):
+        return (self.calling_module_class_name == other.calling_module_class_name) and (
+            self.calling_field_name == other.calling_field_name
+        )
 
     def __hash__(self):
         return hash((self.calling_module_class_name, self.calling_field_name))
 
     @staticmethod
     def from_str(string: str):
         matches = re.search(r"(.*)\[(.*)\]|(.*)", string)
@@ -50,56 +47,60 @@
     def __init__(self, scope_elements: List[ScopeElement] = None):
         if scope_elements is not None:
             self.scope_elements = scope_elements
         else:
             self.scope_elements = []
 
     def __str__(self):
-        return '/'.join([str(scope_el) for scope_el in self.scope_elements])
+        return "/".join([str(scope_el) for scope_el in self.scope_elements])
+
+    def __repr__(self):
+        return str(self)
 
     def __hash__(self):
         return hash(str(self))
 
-    def __eq__(self, other: 'Scope'):
+    def __eq__(self, other: "Scope"):
         return self.scope_elements == other.scope_elements
 
-    def __getitem__(self, key):
+    def __getitem__(self, key) -> ScopeElement:
         return self.scope_elements[key]
 
-    def __contains__(self, item: 'Scope'):
+    def __contains__(self, item: "Scope"):
         """Idiom: ('A/B/C' in 'A/B') == True"""
         if len(self.scope_elements) > len(item.scope_elements):
             return False
         for i, element in enumerate(self.scope_elements):
             if element != item.scope_elements[i]:
                 return False
         return True
 
-    def __add__(self, rhs):
+    def __add__(self, rhs: "Scope") -> "Scope":
         init_list = self.scope_elements + rhs.scope_elements
         return Scope(init_list)
 
-    def copy(self):
+    def copy(self) -> "Scope":
         return Scope(deepcopy(self.scope_elements))
 
     def push(self, scope_element: ScopeElement):
         self.scope_elements.append(scope_element)
 
     def pop(self) -> ScopeElement:
         return self.scope_elements.pop()
 
     @staticmethod
-    def from_str(string: str) -> 'Scope':
+    def from_str(string: str) -> "Scope":
         if string:
-            elts = string.split('/')
+            elts = string.split("/")
         else:
             elts = []
         return Scope([ScopeElement.from_str(s) for s in elts])
 
     def get_iteration_scopes(self) -> List[str]:
         results = []
         scope_name = str(self)
-        from nncf.torch.layers import ITERATION_MODULES #pylint: disable=cyclic-import
+        from nncf.torch.layers import ITERATION_MODULES  # pylint: disable=cyclic-import
+
         for iter_scope in ITERATION_MODULES.registry_dict:
             if iter_scope in scope_name:
                 results.append(iter_scope)
         return results
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/trace_functions.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/trace_functions.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,27 +1,25 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from copy import deepcopy
 from typing import Callable
 
 from torch import Tensor
 
-from nncf.torch.dynamic_graph.trace_tensor import flatten_args
 from nncf.torch.dynamic_graph.trace_tensor import TracedTensor
+from nncf.torch.dynamic_graph.trace_tensor import flatten_args
 
 
 def forward_trace_only(operator: Callable, *args, **kwargs):
     """
     This wrapper override will result in the operator not being added to graph,
     but the result will still have TracedTensors with parent IDs left the same as in input.
     Useful for operators which are not likely to be present in patterns considered for
@@ -31,44 +29,44 @@
 
     result = operator(*args, **kwargs)
 
     fargs = flatten_args(args, kwargs)
     input_traced_tensor_indices = [i for i in range(len(fargs)) if isinstance(fargs[i], TracedTensor)]
 
     if isinstance(result, (list, tuple)):
-        output_tensors_to_be_traced_indices = [i for i in range(len(result)) if
-                                               isinstance(result[i], Tensor)]
+        output_tensors_to_be_traced_indices = [i for i in range(len(result)) if isinstance(result[i], Tensor)]
 
         was_tuple = isinstance(result, tuple)
         result = list(result)
         if len(input_traced_tensor_indices) == 1:
             # Broadcast one and the same creator ID of input to all outputs
             for out_idx in output_tensors_to_be_traced_indices:
                 forwarded_meta = deepcopy(fargs[input_traced_tensor_indices[0]].tensor_meta)
                 if forwarded_meta is not None:
                     forwarded_meta.shape = tuple(result[out_idx].shape)
-                result[out_idx] = TracedTensor.from_torch_tensor(result[out_idx],
-                                                                 forwarded_meta)
+                result[out_idx] = TracedTensor.from_torch_tensor(result[out_idx], forwarded_meta)
         elif len(input_traced_tensor_indices) != len(output_tensors_to_be_traced_indices):
-            raise RuntimeError("Unable to forward trace through operator {} - "
-                               "input and output tensor count mismatch!".format(operator.__name__))
+            raise RuntimeError(
+                "Unable to forward trace through operator {} - "
+                "input and output tensor count mismatch!".format(operator.__name__)
+            )
         else:
             # Assume that output tensor order corresponds to input tensor order
             for in_idx, out_idx in zip(input_traced_tensor_indices, output_tensors_to_be_traced_indices):
                 forwarded_meta = deepcopy(fargs[in_idx].tensor_meta)
                 if forwarded_meta is not None:
                     forwarded_meta.shape = tuple(result[out_idx].shape)
-                result[out_idx] = TracedTensor.from_torch_tensor(result[out_idx],
-                                                                 forwarded_meta)
+                result[out_idx] = TracedTensor.from_torch_tensor(result[out_idx], forwarded_meta)
         if was_tuple:
             result = tuple(result)
     elif len(input_traced_tensor_indices) > 1:
-        raise RuntimeError("Unable to forward trace through operator {} - "
-                           "input and output tensor count mismatch!".format(operator.__name__))
+        raise RuntimeError(
+            "Unable to forward trace through operator {} - "
+            "input and output tensor count mismatch!".format(operator.__name__)
+        )
     elif input_traced_tensor_indices:
         forwarded_meta = deepcopy(fargs[input_traced_tensor_indices[0]].tensor_meta)
         if forwarded_meta is not None:
             forwarded_meta.shape = tuple(result.shape)
-        return TracedTensor.from_torch_tensor(result,
-                                              forwarded_meta)
+        return TracedTensor.from_torch_tensor(result, forwarded_meta)
     # No traced tensors in input, return a usual torch.Tensor as well
     return result
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/trace_tensor.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/trace_tensor.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,39 +1,34 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Iterable
-from typing import List
-from typing import Optional
-from typing import Tuple
-from typing import Union
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Iterable, List, Optional, Tuple, TypeVar, Union
 
 import numpy as np
 import torch
 
+from nncf import nncf_logger
 from nncf.common.graph import Dtype
 
 
 class TensorMeta:
     @staticmethod
-    def default_comparator(lhs: 'TensorMeta', rhs: 'TensorMeta'):
+    def default_comparator(lhs: "TensorMeta", rhs: "TensorMeta"):
         return lhs.index == rhs.index and lhs.creator_id == rhs.creator_id and lhs.shape[1:] == rhs.shape[1:]
 
-    def __init__(self, creator_id: int, index: int, shape: Union[List[int], Tuple[torch.Tensor, ...]],
-                 dtype: Dtype = Dtype.FLOAT):
+    def __init__(
+        self, creator_id: int, index: int, shape: Union[List[int], Tuple[torch.Tensor, ...]], dtype: Dtype = Dtype.FLOAT
+    ):
         """
         :param creator_id: An ID of the node in DynamicGraph that corresponds to an operation that created the
             tensor.
         :param index: The index of this tensor in the creator operation's output.
         :param shape: The shape of the tensor.
         """
         self.creator_id = creator_id
@@ -50,48 +45,62 @@
         return hash((self.creator_id, self.index, self.shape))
 
     def __str__(self):
         return "C{}_I{}_".format(self.creator_id, self.index) + "S" + "x".join([str(s) for s in self.shape])
 
 
 class TracedTensor(torch.Tensor):
-    # pylint: disable=abstract-method
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.tensor_meta = None
+    """
+    When tracing a torch model, intermediate tensors will be dynamically turned into
+    instances of this class to be able to store additional data required for establishing
+    relation between tensor producer and consumer operations.
+    """
 
     @staticmethod
     def from_torch_tensor(tensor, tensor_meta: TensorMeta):
         tensor.tensor_meta = tensor_meta
         tensor.__class__ = TracedTensor
+        # pylint:disable=protected-access
+        tensor._nncf_expired = False
         return tensor
 
-    def as_subclass(self, cls: 'TracedTensor') -> 'TracedTensor':
+    def nncf_expire(self):
+        """
+        Mark the traced tensor as "expired". The tensor's metainformation should
+        then be considered outdated/invalid.
+        """
+        self._nncf_expired = True
+
+    @property
+    def nncf_expired(self) -> bool:
+        return self._nncf_expired
+
+    def as_subclass(self, cls: "TracedTensor") -> "TracedTensor":
         """
         Required for PyTorch 1.7.0 compatibility - the handle_torch_function and __torch_function__
         API in general calls this after a wrapped function call; need to preserve the tensor_meta extensions
         """
 
         return self
 
     # NOTE: This disables the __torch_function__ API altogether when using NNCF.
     # TODO: make NNCF utilize the __torch_function__ API instead.
-    #pylint:disable=protected-access
+    # pylint:disable=protected-access
     if hasattr(torch._C, "_disabled_torch_function_impl"):
         __torch_function__ = torch._C._disabled_torch_function_impl
 
 
 def is_iterable(item):
     non_iterable_types = (str, bytes, bytearray, torch.Tensor, np.ndarray)
     # pylint:disable=isinstance-second-argument-not-valid-type
     return isinstance(item, Iterable) and not isinstance(item, non_iterable_types)
 
 
 def flatten(items):
-    it = items.items() if hasattr(items, 'items') else iter(items)
+    it = items.items() if hasattr(items, "items") else iter(items)
     for item in it:
         if is_iterable(item):
             for i in flatten(item):
                 yield i
         else:
             yield item
 
@@ -102,36 +111,68 @@
 
 def get_dtype(x: torch.Tensor) -> Dtype:
     if x.dtype in [torch.float, torch.float16, torch.float32, torch.float64]:
         return Dtype.FLOAT
     return Dtype.INTEGER
 
 
-def trace_tensors(operator_output, node: 'DynamicGraphNode'):
+TensorOrTupleOrList = TypeVar("TensorOrTupleOrList", List[torch.Tensor], Tuple[torch.Tensor], torch.Tensor)
+
+
+def trace_tensors(
+    operator_output: TensorOrTupleOrList, node: "DynamicGraphNode", ctx: "TracingContext" = None
+) -> TensorOrTupleOrList:
+    """
+    Dynamically turn torch.Tensor instances in `operator_output` into TracedTensor instances. `operator_output` is
+    presumed to be the output of a model operation (function call) associated with `node`.
+    :param operator_output: The output of an NNCF-wrapped function executed in a model object.
+    :param node: A node in DynamicGraph associated with the function that produced `operator_output`
+    :param ctx: If supplied, the resulting tensors will be registered within this TracingContext instance
+    to be marked as expired on context exit, which is required to correctly process situations when a traced model
+    retains intermediate tensor values.
+    :return: Same structure as `operator_output`, but with torch.Tensor entries turned into TracedTensors.
+    """
     if isinstance(operator_output, (list, tuple)):
         output_ = []
         for i, x in enumerate(operator_output):
             meta = None
             if node is not None:
                 meta = TensorMeta(node.node_id, i, x.shape, get_dtype(x))
-            output_.append(TracedTensor.from_torch_tensor(x, meta))
+            tt = TracedTensor.from_torch_tensor(x, meta)
+            if ctx is not None:
+                ctx.register_traced_tensor(tt)
+            output_.append(tt)
         return operator_output.__class__(output_)
     if isinstance(operator_output, torch.Tensor):
         meta = None
         if node is not None:
             meta = TensorMeta(node.node_id, 0, operator_output.shape, get_dtype(operator_output))
-        return TracedTensor.from_torch_tensor(operator_output, meta)
-    raise ValueError("Unknown return type. Can not trace function call")
+        tt = TracedTensor.from_torch_tensor(operator_output, meta)
+        if ctx is not None:
+            ctx.register_traced_tensor(tt)
+        return tt
+    nncf_logger.debug(f"Could not find tensors to trace in operator output: {operator_output}")
+    return operator_output
 
 
-def make_tensor_metas(inputs: 'OperatorInput') -> List[Optional[TensorMeta]]:
+def make_tensor_metas(inputs: "OperatorInput") -> List[Optional[TensorMeta]]:
+    """
+    Produces TensorMeta data for each torch.Tensor or TracedTensor in `inputs`.
+    :param inputs: An OperatorInput representation of input arguments to an operation in the traced model.
+    :return: A list of TensorMeta objects, one for every torch.Tensor or TracedTensor object in `inputs` in the
+    order of item enumeration in `inputs`.
+    """
     tensor_metas = []
     for i, node_input_index_entry in enumerate(inputs):
         node_input = node_input_index_entry.getter()
         if isinstance(node_input, TracedTensor):
-            tensor_metas.append(node_input.tensor_meta)
+            if not node_input.nncf_expired:
+                tensor_metas.append(node_input.tensor_meta)
+            else:
+                meta = TensorMeta(None, i, node_input.shape)
+                tensor_metas.append(meta)
         elif isinstance(node_input, torch.Tensor) and not isinstance(node_input, TracedTensor):
             meta = TensorMeta(None, i, node_input.shape)
             tensor_metas.append(meta)
         else:
             tensor_metas.append(None)
     return tensor_metas
```

### Comparing `nncf-2.4.0/nncf/torch/dynamic_graph/wrappers.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/wrappers.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,104 +1,91 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import functools
 from copy import deepcopy
-from typing import Callable
-from typing import List
-from typing import Tuple
-
-from torch.nn import Conv1d
-from torch.nn import Conv2d
-from torch.nn import Conv3d
-from torch.nn import ConvTranspose1d
-from torch.nn import ConvTranspose2d
-from torch.nn import ConvTranspose3d
+from typing import Callable, List, Tuple
+
 from torch.nn import DataParallel
-from torch.nn import Linear
-from torch.nn import Module as TorchModule
 
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
-from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
-from nncf.common.graph.layer_attributes import GenericWeightedLayerAttributes
-from nncf.common.graph.layer_attributes import GroupNormLayerAttributes
-from nncf.common.graph.layer_attributes import LinearLayerAttributes
-from nncf.common.utils.debug import is_debug
 from nncf.common.logging import nncf_logger
+from nncf.common.utils.debug import is_debug
 from nncf.torch.dynamic_graph.context import TracingContext
 from nncf.torch.dynamic_graph.context import get_current_context
+from nncf.torch.dynamic_graph.layer_attributes_handlers import OP_NAMES_REQUIRING_ATTRS_FROM_ARGS_KWARGS
+from nncf.torch.dynamic_graph.layer_attributes_handlers import get_layer_attributes_from_args_and_kwargs
+from nncf.torch.dynamic_graph.layer_attributes_handlers import get_layer_attributes_from_module
 from nncf.torch.dynamic_graph.op_input_processing import OperatorInput
 from nncf.torch.dynamic_graph.trace_tensor import make_tensor_metas
 from nncf.torch.dynamic_graph.trace_tensor import trace_tensors
 from nncf.torch.layer_utils import _NNCFModuleMixin
 from nncf.torch.layers import ITERATION_MODULES
-from nncf.torch.layers import NNCF_MODULES_DICT
 
 _IGNORED_SCOPES = []
 
 
 def _warn_data_parallel():
-    if getattr(_warn_data_parallel, 'warned_once', False):
+    if getattr(_warn_data_parallel, "warned_once", False):
         return
     _warn_data_parallel.warned_once = True
-    nncf_logger.warning("You are using DataParallel, which may cause significant performance issues with dynamic graph "
-                        "building. Consider using distributed training (DistributedDataParallel) instead.")
+    nncf_logger.warning(
+        "You are using DataParallel, which may cause significant performance issues with dynamic graph "
+        "building. Consider using distributed training (DistributedDataParallel) instead."
+    )
 
 
 def ignore_scope(cls):
     if cls not in _IGNORED_SCOPES:
         _IGNORED_SCOPES.append(cls)
     return cls
 
 
-OP_NAMES_REQUIRING_MODULE_ATTRS = [v.op_func_name for v in NNCF_MODULES_DICT]
-
-
-def wrap_operator(operator, operator_info: 'PatchedOperatorInfo'):
+def wrap_operator(operator, operator_info: "PatchedOperatorInfo"):
     """
     Wraps the input callable object (`operator`) with the functionality that allows the calls to this object
     to be tracked by the currently set global TracingContext. The wrapped functions can be then intercepted,
     their arguments and return values modified arbitrarily and, for functions that correspond to operations on
     tensors in a DNN,  their general position and address in the DNN's model control flow graph can be established.
 
     :param: operator: A callable object to be wrapped.
     :param: operator_info (PatchedOperatorInfo): An informational struct containing the specifics of wrapping
             the `operator` in question.
 
     :return: The wrapped version of `operator` that, without a TracingContext, performs functionally the same as
              the unwrapped version, but within a TracingContext is able to be tracked and hooked.
     """
     # do not wrap function twice
-    _orig_op = getattr(operator, '_original_op', None)
+    _orig_op = getattr(operator, "_original_op", None)
     if _orig_op is not None:
         nncf_logger.debug(f"Operator: {_orig_op.__name__} is already wrapped")
         return operator
 
+    @functools.wraps(operator)
     def wrapped(*args, **kwargs):
         ctx = get_current_context()
-        if not ctx or getattr(ctx, 'in_operator', False) or not ctx.is_tracing:
+        if not ctx or getattr(ctx, "in_operator", False) or not ctx.is_tracing:
             op1 = operator(*args, **kwargs)
             return op1
 
         ctx.in_operator = True
 
         try:
             if operator_info.skip_trace:
                 result = operator(*args, **kwargs)
             elif ctx.is_forwarding:
-                from nncf.torch.dynamic_graph.trace_functions import forward_trace_only #pylint: disable=cyclic-import
+                from nncf.torch.dynamic_graph.trace_functions import forward_trace_only  # pylint: disable=cyclic-import
+
                 result = forward_trace_only(operator, *args, **kwargs)
             else:
                 op_name = operator_info.name
                 op_address = ctx.get_caller_context(op_name)
                 ctx.register_operator_call(op_address.operator_name, op_address.scope_in_model)
 
                 if ctx.elastic_depth and ctx.in_skipped_block:
@@ -107,15 +94,15 @@
                     result = _execute_op(op_address, operator_info, operator, ctx, *args, **kwargs)
 
                 str_op_address = str(op_address)
                 if str_op_address in ctx.end_node_name_of_skipped_block:
                     assert ctx.in_skipped_block is True
                     ctx.in_skipped_block = False
                 if str_op_address in ctx.start_node_name_of_skipped_block:
-                    assert ctx.in_skipped_block is False, 'skipping of overlapping blocks'
+                    assert ctx.in_skipped_block is False, "skipping of overlapping blocks"
                     ctx.in_skipped_block = True
                     ctx.tensor_cache = result
         except:
             # Looks like the __repr__ call made during IDE debug to display tensor contents does not exit properly,
             # but instead throws an exception. This try...except block handles such a situation.
             # Otherwise the context is stuck in the "in_operator == True" state.
             ctx.in_operator = False
@@ -127,17 +114,19 @@
     # pylint: disable=protected-access
     wrapped._original_op = operator
     wrapped._operator_namespace = operator_info.operator_namespace
     return wrapped
 
 
 def wrap_module_call(module_call):
-    from nncf.torch.dynamic_graph.patch_pytorch import ORIGINAL_OPERATORS #pylint: disable=cyclic-import
+    from nncf.torch.dynamic_graph.patch_pytorch import ORIGINAL_OPERATORS  # pylint: disable=cyclic-import
+
     NAMES_ORIGINAL_OPERATORS = [op.name for op in ORIGINAL_OPERATORS]
 
+    @functools.wraps(module_call)
     def wrapped(self, *args, **kwargs):
         ctx = get_current_context()
         if not ctx or self.__class__ in _IGNORED_SCOPES:
             if isinstance(self, DataParallel):
                 _warn_data_parallel()
             return module_call(self, *args, **kwargs)
         ctx.push_scope(self)
@@ -152,106 +141,74 @@
         if ctx.elastic_depth and is_layer_or_op and ctx.in_skipped_block:
             ctx.register_operator_call(op_address.operator_name, op_address.scope_in_model)
             retval = ctx.tensor_cache
             if str_op_address in ctx.end_node_name_of_skipped_block:
                 assert ctx.in_skipped_block is True
                 ctx.in_skipped_block = False
             if str_op_address in ctx.start_node_name_of_skipped_block:
-                assert ctx.in_skipped_block is False, 'skipping of overlapping blocks'
+                assert ctx.in_skipped_block is False, "skipping of overlapping blocks"
                 ctx.in_skipped_block = True
         else:
             retval = module_call(self, *args, **kwargs)
 
         if type(self).__name__ in ITERATION_MODULES.registry_dict:
             ctx.reset_operator_call_count_in_scope(ctx.scope)
         ctx.pop_scope()
         return retval
 
     return wrapped
 
 
-def _execute_op(op_address: 'OperationAddress',
-                operator_info: 'PatchedOperatorInfo',
-                operator: Callable,
-                ctx: 'TracingContext',
-                *args,
-                **kwargs):
+def _execute_op(
+    op_address: "OperationAddress",
+    operator_info: "PatchedOperatorInfo",
+    operator: Callable,
+    ctx: "TracingContext",
+    *args,
+    **kwargs,
+):
     op_name = operator_info.name
 
     op_input = OperatorInput(list(args), kwargs)
     processed_input = ctx.execute_pre_hooks(op_address, op_input)
     args = tuple(processed_input.op_args)
     kwargs = processed_input.op_kwargs
     result = operator(*args, **kwargs)
     node = None
     if isinstance(result, type(NotImplemented)):
         nncf_logger.debug("Operation {} returned NotImplemented".format(op_name))
     elif ctx.trace_dynamic_graph:
         tensor_metas = make_tensor_metas(processed_input)
         node = ctx.find_operator_node(tensor_metas, op_address)
         if node is None:
-            layer_attrs, ignored_algos = _collect_module_attrs_and_ignored_algorithms(ctx, op_name)
+            layer_attrs, ignored_algos = _collect_module_attrs_and_ignored_algorithms(ctx, op_name, args, kwargs)
             is_called_inside_nncf_module = isinstance(ctx.get_current_module(), _NNCFModuleMixin)
-            node = ctx.maybe_add_node(processed_input, tensor_metas, op_address,
-                                      layer_attrs, ignored_algos, is_called_inside_nncf_module)
+            node = ctx.maybe_add_node(
+                processed_input, tensor_metas, op_address, layer_attrs, ignored_algos, is_called_inside_nncf_module
+            )
         if is_debug() and node is not None:
             ctx.register_node_call(node)
 
-    result = trace_tensors(result, node)
+    result = trace_tensors(result, node, ctx)
     result = ctx.execute_post_hooks(op_address, result)
     return result
 
 
-def _collect_module_attrs_and_ignored_algorithms(ctx: TracingContext,
-                                                 op_name: str) -> Tuple[BaseLayerAttributes, List[str]]:
+def _collect_module_attrs_and_ignored_algorithms(
+    ctx: TracingContext, op_name: str, args, kwargs
+) -> Tuple[BaseLayerAttributes, List[str]]:
     layer_attrs = None
     ignored_algos = []
-    if op_name in OP_NAMES_REQUIRING_MODULE_ATTRS:
+    from nncf.torch.graph.operator_metatypes import OP_NAMES_WITH_WEIGHTS  # pylint:disable=cyclic-import
+
+    if op_name in OP_NAMES_WITH_WEIGHTS:
         curr_module = ctx.get_current_module()
         if curr_module is None:
-            raise RuntimeError("Operation {} requires module attributes, "
-                               "but it was executed outside any module".format(op_name))
-        layer_attrs = _get_layer_attributes(curr_module, op_name)
+            raise RuntimeError(
+                f"Operation {op_name} requires module attributes, " f"but it was executed outside any module"
+            )
+        layer_attrs = get_layer_attributes_from_module(curr_module, op_name)
         if isinstance(curr_module, _NNCFModuleMixin):
             ignored_algos = deepcopy(curr_module.ignored_algorithms)
+    elif op_name in OP_NAMES_REQUIRING_ATTRS_FROM_ARGS_KWARGS:
+        layer_attrs = get_layer_attributes_from_args_and_kwargs(op_name, args, kwargs)
     return layer_attrs, ignored_algos
-
-
-def _get_layer_attributes(module: TorchModule, operator_name: str) -> BaseLayerAttributes:
-    if operator_name == "group_norm":
-        return GroupNormLayerAttributes(
-            module.weight.requires_grad,
-            module.num_channels,
-            module.num_groups
-        )
-    # torch.nn.utils.weight_norm replaces weight with weight_g and weight_v
-    is_weight_norm_applied = hasattr(module, 'weight_g') and hasattr(module, 'weight_v')
-    weight_attr = 'weight_g' if is_weight_norm_applied else 'weight'
-    if isinstance(module, (Conv1d, Conv2d, Conv3d)):
-        return ConvolutionLayerAttributes(weight_requires_grad=getattr(module, weight_attr).requires_grad,
-                                          in_channels=module.in_channels,
-                                          out_channels=module.out_channels,
-                                          kernel_size=module.kernel_size,
-                                          stride=module.stride,
-                                          groups=module.groups,
-                                          transpose=False,
-                                          padding_values=module.padding)
-    if isinstance(module, (ConvTranspose1d, ConvTranspose2d, ConvTranspose3d)):
-        return ConvolutionLayerAttributes(weight_requires_grad=getattr(module, weight_attr).requires_grad,
-                                          in_channels=module.in_channels,
-                                          out_channels=module.out_channels,
-                                          kernel_size=module.kernel_size,
-                                          stride=module.stride,
-                                          groups=module.groups,
-                                          transpose=True,
-                                          padding_values=module.padding)
-    if isinstance(module, Linear):
-        return LinearLayerAttributes(weight_requires_grad=getattr(module, weight_attr).requires_grad,
-                                     in_features=module.in_features,
-                                     out_features=module.out_features,
-                                     bias=module.bias is not None)
-    if hasattr(module, 'weight') or is_weight_norm_applied:
-        return GenericWeightedLayerAttributes(weight_requires_grad=getattr(module, weight_attr).requires_grad,
-                                              weight_shape=module.weight.shape)
-
-    return GenericWeightedLayerAttributes(weight_requires_grad=False,
-                                          weight_shape=[1, 1])
```

### Comparing `nncf-2.4.0/nncf/torch/exporter.py` & `nncf-2.5.0/nncf/torch/exporter.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,95 +1,96 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Any
-from typing import Tuple
-from functools import partial
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from copy import copy
+from functools import partial
+from typing import Any, Tuple
+
 import torch
 from torch.onnx import OperatorExportTypes
 
 from nncf.common.exporter import Exporter
 from nncf.common.logging import nncf_logger
 from nncf.telemetry import tracked_function
 from nncf.telemetry.events import NNCF_PT_CATEGORY
 from nncf.torch.dynamic_graph.graph_tracer import create_dummy_forward_fn
 from nncf.torch.dynamic_graph.graph_tracer import create_mock_tensor
 from nncf.torch.nested_objects_traversal import objwalk
-from nncf.torch.utils import is_tensor, get_model_device
+from nncf.torch.utils import get_model_device
+from nncf.torch.utils import is_tensor
 
 
 def generate_input_names_list(num_inputs: int):
-    return [f'input.{idx}' for idx in range(0, num_inputs)]
+    return [f"input.{idx}" for idx in range(0, num_inputs)]
 
 
 def generate_output_names_list(num_outputs: int):
-    return [f'output.{idx}' for idx in range(0, num_outputs)]
+    return [f"output.{idx}" for idx in range(0, num_outputs)]
 
 
 def count_tensors(model_retval: Any) -> int:
     count = 0
+
     def counter_fn(x: torch.Tensor) -> torch.Tensor:
         nonlocal count
         count += 1
         return x
 
     objwalk(model_retval, is_tensor, counter_fn)
     return count
 
 
 class PTExportFormat:
-    ONNX = 'onnx'
+    ONNX = "onnx"
 
 
 class PTExporter(Exporter):
     """
     This class provides export of the compressed model to the ONNX format.
     """
 
     _ONNX_DEFAULT_OPSET = 13
 
-
     @staticmethod
     def parse_format(save_format: str) -> Tuple[str, dict]:
         """
         Parse saving format to a short form and additional arguments.
 
         :param save_format: Saving format.
 
         :return
             str: short form of the save_format
             dict: additional arguments for exporter
         """
         if save_format.startswith(PTExportFormat.ONNX):
-            split_format = save_format.split('_')
+            split_format = save_format.split("_")
             opset = None
 
             if len(split_format) == 1:
                 opset = PTExporter._ONNX_DEFAULT_OPSET
             elif len(split_format) == 2:
                 opset = int(split_format[1])
 
             if opset is not None and opset <= 0:
                 raise ValueError("Incorrect save_format, expected 'onnx' or 'onnx_<opset_version>'.")
 
             if opset != PTExporter._ONNX_DEFAULT_OPSET:
-                nncf_logger.warning(f'Exporting to ONNX opset {opset}, which is not guaranteed to work with NNCF. '
-                                    f'Recommended opset export version is {PTExporter._ONNX_DEFAULT_OPSET}.')
+                nncf_logger.warning(
+                    f"Exporting to ONNX opset {opset}, which is not guaranteed to work with NNCF. "
+                    f"Recommended opset export version is {PTExporter._ONNX_DEFAULT_OPSET}."
+                )
 
-            return PTExportFormat.ONNX, {'opset_version': opset}
+            return PTExportFormat.ONNX, {"opset_version": opset}
         return save_format, {}
 
     @tracked_function(NNCF_PT_CATEGORY, ["save_format"])
     def export_model(self, save_path: str, save_format: str = PTExportFormat.ONNX) -> None:
         """
         Exports the compressed model to the specified format.
 
@@ -97,96 +98,97 @@
         :param save_format: Saving format.
             One of the following:
                 - `onnx` for export to the ONNX format.
                 - `onnx_<opset_version>` for export to the ONNX format with specific opset version.
             The ONNX format will be used if `save_format` is not specified.
         """
 
-        fn_args = {'save_path': save_path}
+        fn_args = {"save_path": save_path}
 
         save_format, extra_args = PTExporter.parse_format(save_format)
         fn_args.update(extra_args)
 
         format_to_export_fn = {
             PTExportFormat.ONNX: self._export_to_onnx,
         }
 
         export_fn = format_to_export_fn.get(save_format)
 
         if export_fn is None:
             available_formats = list(format_to_export_fn.keys())
-            raise ValueError(f'Unsupported saving format: \'{save_format}\'. '
-                             f'Available formats: {available_formats}')
+            raise ValueError(f"Unsupported saving format: '{save_format}'. " f"Available formats: {available_formats}")
 
         export_fn(**fn_args)
 
     def _export_to_onnx(self, save_path: str, opset_version: int) -> None:
         """
         Exports the compressed model to the ONNX format.
 
         :param save_path: The path where the model will be saved.
         """
         original_device = get_model_device(self._model)
         model = self._model.eval().cpu()
         input_tensor_list = []
-        for info in self._model.input_infos:
+        for info in self._model.nncf.input_infos:
             single_batch_info = copy(info)
             input_shape = tuple([1] + list(info.shape)[1:])
             single_batch_info.shape = input_shape
-            input_tensor_list.append(create_mock_tensor(single_batch_info, 'cpu'))
+            input_tensor_list.append(create_mock_tensor(single_batch_info, "cpu"))
 
-        original_forward = model.forward
+        full_arg_forward = model.nncf.get_original_forward()
         args = self._model_args[:-1]
         kwargs = self._model_args[-1]
-        model.forward = partial(model.forward, *args, **kwargs)
-
-        if self._input_names is not None:
-            input_names = self._input_names
-        else:
-            input_names = generate_input_names_list(len(input_tensor_list))
-
-
-        # pylint:disable=unexpected-keyword-arg
-        with torch.no_grad():
-            # Should call this, otherwise the operations executed during export will end up in the graph.
-            model.disable_dynamic_graph_building()
-
-            if self._output_names is not None:
-                output_names = self._output_names
+        partial_forward = partial(full_arg_forward, *args, **kwargs)
+        with model.nncf.temporary_bound_original_forward(partial_forward):
+            if self._input_names is not None:
+                input_names = self._input_names
             else:
-                # Will have to run a dummy forward call in order to determine the number of outputs.
-                dummy_forward = create_dummy_forward_fn(self._model.input_infos)
-                retval = dummy_forward(self._model)
-                output_names = generate_output_names_list(count_tensors(retval))
+                input_names = generate_input_names_list(len(input_tensor_list))
+            # pylint:disable=unexpected-keyword-arg
+            with torch.no_grad():
+                # Should call this, otherwise the operations executed during export will end up in the graph.
+                model.nncf.disable_dynamic_graph_building()
+
+                if self._output_names is not None:
+                    output_names = self._output_names
+                else:
+                    # Will have to run a dummy forward call in order to determine the number of outputs.
+                    dummy_forward = create_dummy_forward_fn(self._model.nncf.input_infos)
+                    retval = dummy_forward(self._model)
+                    output_names = generate_output_names_list(count_tensors(retval))
 
-            self._torch_export_call(model, input_tensor_list, save_path, input_names, output_names, opset_version)
+                self._torch_export_call(model, input_tensor_list, save_path, input_names, output_names, opset_version)
 
-            model.enable_dynamic_graph_building()
-        model.forward = original_forward
+                model.nncf.enable_dynamic_graph_building()
         model.to(original_device)
 
     def _torch_export_call(self, model, input_tensor_list, save_path, input_names, output_names, opset_version):
         """
         Call of torch.onnx.export function.
         :param model: torch.nn.Module to be exported.
         :param input_tensor_list: the list containing model inputs.
         :param save_path: a string containing a path for saving onnx model.
         :param input_names: Names to be assigned to the input tensors of the model.
         :param output_names: Names to be assigned to the output tensors of the model.
         :param opset_version: the version of the onnx opset.
         """
-        fn = partial(torch.onnx.export,
-                model, tuple(input_tensor_list), save_path,
-                input_names=input_names,
-                output_names=output_names,
-                opset_version=opset_version,
-                training=torch.onnx.TrainingMode.EVAL)
+        fn = partial(
+            torch.onnx.export,
+            model,
+            tuple(input_tensor_list),
+            save_path,
+            input_names=input_names,
+            output_names=output_names,
+            opset_version=opset_version,
+            training=torch.onnx.TrainingMode.EVAL,
+        )
         try:
             fn()
         except torch.onnx.errors.SymbolicValueError:
             # May have failed for reasons of missing and unspecifiable shape inference
             # for quantizer ops in torch==1.13, try to export with a workaround.
             nncf_logger.warning(
                 "Encountered shape inferencing failures during ONNX export. "
                 "The model was exported with a workaround - some of the operations may have been exported using "
-                "the `org.pytorch.aten` domain.")
+                "the `org.pytorch.aten` domain."
+            )
             fn(operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK)
```

### Comparing `nncf-2.4.0/nncf/torch/extensions/__init__.py` & `nncf-2.5.0/nncf/torch/extensions/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 import enum
+from abc import ABC
+from abc import abstractmethod
 from pathlib import Path
 from typing import Callable
 
 import torch
-
-from abc import ABC, abstractmethod
-
 from torch.utils.cpp_extension import _get_build_directory
 
+from nncf.common.logging import nncf_logger
 from nncf.common.logging.logger import extension_is_loading_info_log
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.registry import Registry
-from nncf.common.logging import nncf_logger
 
-EXTENSIONS = Registry('extensions')
+EXTENSIONS = Registry("extensions")
 
 
 class ExtensionsType(enum.Enum):
     CPU = 0
     CUDA = 1
 
+
 def get_build_directory_for_extension(name: str) -> Path:
-    build_dir = Path(_get_build_directory('nncf/' + name, verbose=False)) / torch.__version__
+    build_dir = Path(_get_build_directory("nncf/" + name, verbose=False)) / torch.__version__
     if not build_dir.exists():
         nncf_logger.debug(f"Creating build directory: {str(build_dir)}")
         build_dir.mkdir(parents=True, exist_ok=True)
     return build_dir
 
 
 class ExtensionLoader(ABC):
@@ -48,14 +49,15 @@
         return str(get_build_directory_for_extension(cls.name()))
 
 
 class ExtensionNamespace:
     """
     Provides lazy loading of the underlying extension, i.e. on the first request of a function from the extension.
     """
+
     def __init__(self, loader: ExtensionLoader):
         """
         :param loader: The extension loader.
         """
         self._loaded_namespace = None
         self._loader = loader
 
@@ -76,19 +78,23 @@
 def _force_build_extensions(ext_type: ExtensionsType):
     for class_type in EXTENSIONS.registry_dict.values():
         if class_type.extension_type() != ext_type:
             continue
         class_type.load()
 
 
+@api(canonical_alias="nncf.torch.force_build_cpu_extensions")
 def force_build_cpu_extensions():
     _force_build_extensions(ExtensionsType.CPU)
 
 
+@api(canonical_alias="nncf.torch.force_build_cuda_extensions")
 def force_build_cuda_extensions():
     _force_build_extensions(ExtensionsType.CUDA)
 
 
 class CudaNotAvailableStub:
     def __getattr__(self, item):
-        raise RuntimeError(f"CUDA is not available on this machine. Check that the machine has a GPU and a proper "
-                           f"driver supporting CUDA {torch.version.cuda} is installed.")
+        raise RuntimeError(
+            f"CUDA is not available on this machine. Check that the machine has a GPU and a proper "
+            f"driver supporting CUDA {torch.version.cuda} is installed."
+        )
```

### Comparing `nncf-2.4.0/nncf/torch/extensions/include/common_cuda_defs.cuh` & `nncf-2.5.0/nncf/torch/extensions/include/common_cuda_defs.cuh`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/extensions/include/common_cuda_funcs.cuh` & `nncf-2.5.0/nncf/torch/extensions/include/common_cuda_funcs.cuh`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/extensions/src/binarization/cpu/functions_cpu.cpp` & `nncf-2.5.0/nncf/torch/extensions/src/binarization/cpu/functions_cpu.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda.cpp` & `nncf-2.5.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda_impl.cu` & `nncf-2.5.0/nncf/torch/extensions/src/binarization/cuda/functions_cuda_impl.cu`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/extensions/src/common/cpu/tensor_funcs.cpp` & `nncf-2.5.0/nncf/torch/extensions/src/common/cpu/tensor_funcs.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/extensions/src/quantization/cpu/functions_cpu.cpp` & `nncf-2.5.0/nncf/torch/extensions/src/quantization/cpu/functions_cpu.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp` & `nncf-2.5.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu` & `nncf-2.5.0/nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/functions.py` & `nncf-2.5.0/nncf/torch/functions.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Any
 
 import torch
 
 
 def clamp(x, low, high):
     return torch.max(torch.min(x, high), low)
```

### Comparing `nncf-2.4.0/nncf/torch/graph/graph.py` & `nncf-2.5.0/nncf/torch/graph/graph.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Dict
-from typing import List
-from typing import Tuple
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Dict, List, Tuple
+
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.torch.dynamic_graph.scope import Scope
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 
 
@@ -40,20 +37,18 @@
             assert port_id not in retval
             retval[port_id] = edge_attr_dict[NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR]
         return retval
 
     def get_input_shape_for_insertion_point(self, insertion_point: PTTargetPoint) -> Tuple[int]:
         target_node_name = insertion_point.target_node_name
         if insertion_point.input_port_id is not None:
-            quantizer_input_shape = self.get_input_shapes_for_node(
-                target_node_name)[insertion_point.input_port_id]
+            quantizer_input_shape = self.get_input_shapes_for_node(target_node_name)[insertion_point.input_port_id]
         else:
             # Tailored for post-hook quantization and first output quantization only
-            quantizer_input_shape = self.get_output_shapes_for_node(
-                target_node_name)[0]
+            quantizer_input_shape = self.get_output_shapes_for_node(target_node_name)[0]
         return quantizer_input_shape
 
     def get_op_nodes_in_scope(self, scope: Scope) -> List[NNCFNode]:
         matching_graph_op_nodes = []
         for scope_str, nodes_in_module in self._layer_name_vs_shared_nodes.items():
             module_scope = Scope.from_str(scope_str)
             if module_scope in scope:
```

### Comparing `nncf-2.4.0/nncf/torch/graph/operator_metatypes.py` & `nncf-2.5.0/nncf/torch/graph/operator_metatypes.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,68 +1,66 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Type
-from typing import TypeVar
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Dict, List, Optional, Type, TypeVar
 
 from nncf.common.graph.definitions import NNCFGraphNodeType
 from nncf.common.graph.layer_attributes import BaseLayerAttributes
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.operator_metatypes import INPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OUTPUT_NOOP_METATYPES
 from nncf.common.graph.operator_metatypes import OperatorMetatype
 from nncf.common.graph.operator_metatypes import OperatorMetatypeRegistry
 from nncf.common.hardware.opset import HWConfigOpName
 from nncf.torch.dynamic_graph.graph import DynamicGraph
-from nncf.torch.dynamic_graph.patch_pytorch import NamespaceTarget
+from nncf.torch.dynamic_graph.structs import NamespaceTarget
 
-ModuleAttributes = TypeVar('ModuleAttributes', bound=BaseLayerAttributes)
+ModuleAttributes = TypeVar("ModuleAttributes", bound=BaseLayerAttributes)
 
 PT_OPERATOR_METATYPES = OperatorMetatypeRegistry("operator_metatypes")
 
 
+# pylint: disable=too-many-lines
 class PTOperatorMetatype(OperatorMetatype):
     """
     Base class for grouping PyTorch operators based on their semantic meaning.
     Each derived class represents a single semantic group - for example, AddMetatype would
     group together '__iadd__', '__add__' and '__radd__' operations which all define nodewise
     tensor addition.
     Derived classes also specify which PyTorch functions in which modules should be patched
     so that the entire group of operations is visible in the internal graph.
     Grouping also allows efficient application of HW specifics to compression of
     certain operation groups.
     """
+
     # Names of functions registered as operators via @register_operator to be associated
     # with this metatype
     external_op_names = []  # type: List[str]
 
     # Names of functions from 'torch.nn.function', 'torch.tensor' and 'torch' modules respectively,
     # which are associated with this metatype.
-    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: [],
-                                NamespaceTarget.TORCH_TENSOR: [],
-                                NamespaceTarget.TORCH: []}  # type: Dict[NamespaceTarget, List[str]]
+    module_to_function_names = {
+        NamespaceTarget.TORCH_NN_FUNCTIONAL: [],
+        NamespaceTarget.TORCH_TENSOR: [],
+        NamespaceTarget.TORCH: [],
+    }  # type: Dict[NamespaceTarget, List[str]]
 
     subtypes = []  # type: List[Type[PTOperatorMetatype]]
 
     @classmethod
-    def get_subtypes(cls) -> List[Type['PTOperatorMetatype']]:
+    def get_subtypes(cls) -> List[Type["PTOperatorMetatype"]]:
         return cls.subtypes.copy()
 
     @classmethod
     def get_all_namespace_to_function_names(cls) -> Dict[NamespaceTarget, List[str]]:
         output = dict(cls.module_to_function_names)
         output[NamespaceTarget.EXTERNAL] = cls.external_op_names
         return output
@@ -70,70 +68,65 @@
     @classmethod
     def get_all_aliases(cls) -> List[str]:
         output = set()
         for _, function_names in cls.module_to_function_names.items():
             output = output.union(function_names)
         if cls.external_op_names is not None:
             output = output.union(cls.external_op_names)
-        return output
+        return list(output)
 
     @classmethod
-    def determine_subtype(cls,
-                          layer_attributes: Optional[BaseLayerAttributes] = None,
-                          function_args=None,
-                          functions_kwargs=None) -> Optional['PTOperatorSubtype']:
+    def determine_subtype(
+        cls, layer_attributes: Optional[BaseLayerAttributes] = None, function_args=None, functions_kwargs=None
+    ) -> Optional["PTOperatorSubtype"]:
         matches = []
         for subtype in cls.get_subtypes():
-            if subtype.matches(layer_attributes,
-                               function_args,
-                               functions_kwargs):
+            if subtype.matches(layer_attributes, function_args, functions_kwargs):
                 matches.append(subtype)
-        assert len(matches) <= 1, "Multiple subtypes match operator call " \
-                                  "- cannot determine single subtype."
+        assert len(matches) <= 1, "Multiple subtypes match operator call - cannot determine single subtype."
         if not matches:
             return None
 
         subtype = matches[0]
-        nested_subtype = subtype.determine_subtype(layer_attributes, function_args,
-                                                   functions_kwargs)
+        nested_subtype = subtype.determine_subtype(layer_attributes, function_args, functions_kwargs)
         if nested_subtype:
             return nested_subtype
         return subtype
 
 
 class PTOperatorSubtype(PTOperatorMetatype):
     """
     Exact specialization of PTOperatorMetatype that can only be determined via operator argument
     inspection or owning module attribute inspection, and that may have specialized compression method
     configuration other than the one used for general operations having the type of PTOperatorMetatype.
     """
 
     @classmethod
-    def matches(cls, layer_attributes: Optional[BaseLayerAttributes] = None,
-                function_args=None,
-                functions_kwargs=None) -> bool:
+    def matches(
+        cls, layer_attributes: Optional[BaseLayerAttributes] = None, function_args=None, functions_kwargs=None
+    ) -> bool:
         raise NotImplementedError
 
 
 class PTModuleOperatorSubtype(PTOperatorSubtype):
     @classmethod
-    def matches(cls, layer_attributes: Optional[BaseLayerAttributes] = None,
-                function_args=None,
-                functions_kwargs=None) -> bool:
+    def matches(
+        cls, layer_attributes: Optional[BaseLayerAttributes] = None, function_args=None, functions_kwargs=None
+    ) -> bool:
         key = DynamicGraph.IS_CALLED_INSIDE_NNCF_MODULE
         if functions_kwargs is None or key not in functions_kwargs:
             return False
         return functions_kwargs[key]
 
 
 class PTDepthwiseConvOperatorSubtype(PTOperatorSubtype):
     @classmethod
-    def matches(cls, layer_attributes: Optional[BaseLayerAttributes] = None,
-                function_args=None,
-                functions_kwargs=None) -> bool:
+    def matches(
+        cls, layer_attributes: Optional[BaseLayerAttributes] = None, function_args=None, functions_kwargs=None
+    ) -> bool:
         if not isinstance(layer_attributes, ConvolutionLayerAttributes):
             return False
         if layer_attributes.groups == layer_attributes.in_channels and layer_attributes.in_channels > 1:
             return True
         return False
 
 
@@ -152,321 +145,274 @@
 
 
 @PT_OPERATOR_METATYPES.register()
 @NOOP_METATYPES.register()
 class PTNoopMetatype(PTOperatorMetatype):
     name = "noop"
     external_op_names = [name]
-    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: [],
-                                NamespaceTarget.TORCH_TENSOR: [],
-                                NamespaceTarget.TORCH: ["contiguous", "clone"]}
+    module_to_function_names = {
+        NamespaceTarget.TORCH_NN_FUNCTIONAL: [],
+        NamespaceTarget.TORCH_TENSOR: [],
+        NamespaceTarget.TORCH: ["contiguous", "clone"],
+    }
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTDepthwiseConv1dSubtype(PTDepthwiseConvOperatorSubtype):
     name = "Conv1DOp"
     hw_config_name = [HWConfigOpName.DEPTHWISECONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv1d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv1d"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleConv1dMetatype(PTModuleOperatorSubtype):
     name = "Conv1DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv1d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv1d"]}
     subtypes = [PTDepthwiseConv1dSubtype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTConv1dMetatype(PTOperatorMetatype):
     name = "Conv1DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv1d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv1d"]}
     subtypes = [PTModuleConv1dMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTDepthwiseConv2dSubtype(PTDepthwiseConvOperatorSubtype):
     name = "Conv2DOp"
     hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv2d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv2d"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleConv2dMetatype(PTModuleOperatorSubtype):
     name = "Conv2DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv2d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv2d"]}
     subtypes = [PTDepthwiseConv2dSubtype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTConv2dMetatype(PTOperatorMetatype):
     name = "Conv2DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv2d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv2d"]}
     subtypes = [PTModuleConv2dMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTDepthwiseConv3dSubtype(PTDepthwiseConvOperatorSubtype):
     name = "Conv3DOp"
     hw_config_names = [HWConfigOpName.DEPTHWISECONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv3d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv3d"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleConv3dMetatype(PTModuleOperatorSubtype):
     name = "Conv3DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv3d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv3d"]}
     subtypes = [PTDepthwiseConv3dSubtype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTConv3dMetatype(PTOperatorMetatype):
     name = "Conv3DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv3d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv3d"]}
     subtypes = [PTModuleConv3dMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleConvTranspose1dMetatype(PTModuleOperatorSubtype):
     name = "ConvTranspose1DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose1d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose1d"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTConvTranspose1dMetatype(PTOperatorMetatype):
     name = "ConvTranspose1DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose1d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose1d"]}
     subtypes = [PTModuleConvTranspose1dMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleConvTranspose2dMetatype(PTModuleOperatorSubtype):
     name = "ConvTranspose2DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose2d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose2d"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTConvTranspose2dMetatype(PTOperatorMetatype):
     name = "ConvTranspose2DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose2d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose2d"]}
     subtypes = [PTModuleConvTranspose2dMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleConvTranspose3dMetatype(PTModuleOperatorSubtype):
     name = "ConvTranspose3DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose3d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose3d"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTConvTranspose3dMetatype(PTOperatorMetatype):
     name = "ConvTranspose3DOp"
     hw_config_names = [HWConfigOpName.CONVOLUTION]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose3d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["conv_transpose3d"]}
     subtypes = [PTModuleConvTranspose3dMetatype]
 
+
+@PT_OPERATOR_METATYPES.register()
+class PTModuleDeformConv2dMetatype(PTModuleOperatorSubtype):
+    name = "DeformConv2dOp"
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["deform_conv2d"]}
+
+
+@PT_OPERATOR_METATYPES.register()
+class PTDeformConv2dMetatype(PTOperatorMetatype):
+    name = "DeformConv2dOp"
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["deform_conv2d"]}
+    subtypes = [PTModuleDeformConv2dMetatype]
+
+
 @PT_OPERATOR_METATYPES.register()
 class PTModuleLinearMetatype(PTModuleOperatorSubtype):
     name = "LinearOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["linear"],
-        NamespaceTarget.TORCH: ["addmm"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["linear"], NamespaceTarget.TORCH: ["addmm"]}
     hw_config_names = [HWConfigOpName.MATMUL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLinearMetatype(PTOperatorMetatype):
     name = "LinearOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["linear"],
-        NamespaceTarget.TORCH: ["addmm"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["linear"], NamespaceTarget.TORCH: ["addmm"]}
     hw_config_names = [HWConfigOpName.MATMUL]
     subtypes = [PTModuleLinearMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTHardTanhMetatype(PTOperatorMetatype):
     name = "HardTanhOP"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["hardtanh"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["hardtanh"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTHardSwishMetatype(PTOperatorMetatype):
     name = "HardSwishOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["hardswish"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["hardswish"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTHardSigmoidMetatype(PTOperatorMetatype):
     name = "HardSigmoidOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["hardsigmoid"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["hardsigmoid"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTTanhMetatype(PTOperatorMetatype):
     name = "TanhOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["tanh"],
-        NamespaceTarget.TORCH: ["tanh"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["tanh"], NamespaceTarget.TORCH: ["tanh"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTELUMetatype(PTOperatorMetatype):
     name = "EluOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["elu", "elu_"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["elu", "elu_"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTPRELUMetatype(PTOperatorMetatype):
     name = "PReluOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["prelu"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["prelu"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLeakyRELUMetatype(PTOperatorMetatype):
     name = "LeakyReluOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["leaky_relu"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["leaky_relu"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleLayerNormMetatype(PTModuleOperatorSubtype):
     name = "LayerNormOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["layer_norm"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["layer_norm"]}
     hw_config_names = [HWConfigOpName.MVN]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLayerNormMetatype(PTOperatorMetatype):
     name = "LayerNormOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["layer_norm"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["layer_norm"]}
     hw_config_names = [HWConfigOpName.MVN]
     subtypes = [PTModuleLayerNormMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleGroupNormMetatype(PTModuleOperatorSubtype):
     name = "GroupNormOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["group_norm"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["group_norm"]}
     hw_config_names = [HWConfigOpName.MVN]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTGroupNormMetatype(PTOperatorMetatype):
     name = "GroupNormOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["group_norm"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["group_norm"]}
     hw_config_names = [HWConfigOpName.MVN]
     subtypes = [PTModuleGroupNormMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTGELUMetatype(PTOperatorMetatype):
     name = "GeluOp"
     hw_config_names = [HWConfigOpName.GELU]
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["gelu"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["gelu"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTSILUMetatype(PTOperatorMetatype):
     name = "SiluOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["silu"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["silu"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTSigmoidMetatype(PTOperatorMetatype):
     name = "SigmoidOp"
     module_to_function_names = {
         NamespaceTarget.TORCH_NN_FUNCTIONAL: ["sigmoid"],
         NamespaceTarget.TORCH_TENSOR: ["sigmoid"],
-        NamespaceTarget.TORCH: ["sigmoid"]
+        NamespaceTarget.TORCH: ["sigmoid"],
     }
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTAddMetatype(PTOperatorMetatype):
     name = "AddOp"
     module_to_function_names = {
         NamespaceTarget.TORCH_TENSOR: ["add", "__add__", "__iadd__", "__radd__"],
-        NamespaceTarget.TORCH: ["add"]
+        NamespaceTarget.TORCH: ["add"],
     }
     hw_config_names = [HWConfigOpName.ADD]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTSubMetatype(PTOperatorMetatype):
     name = "SubOp"
     module_to_function_names = {
         NamespaceTarget.TORCH_TENSOR: ["sub", "__sub__", "__isub__", "__rsub__"],
-        NamespaceTarget.TORCH: ["sub"]
+        NamespaceTarget.TORCH: ["sub"],
     }
     hw_config_names = [HWConfigOpName.SUBTRACT]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTMulMetatype(PTOperatorMetatype):
     name = "MulOp"
@@ -478,15 +424,15 @@
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTDivMetatype(PTOperatorMetatype):
     name = "DivOp"
     module_to_function_names = {
         NamespaceTarget.TORCH_TENSOR: ["__div__", "__idiv__", "__truediv__"],
-        NamespaceTarget.TORCH: ["div"]
+        NamespaceTarget.TORCH: ["div"],
     }
     hw_config_names = [HWConfigOpName.DIVIDE]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTFloorDivMetatype(PTOperatorMetatype):
     name = "FloordivOp"
@@ -500,420 +446,378 @@
     name = "ExpOp"
     module_to_function_names = {
         NamespaceTarget.TORCH: ["exp"],
     }
 
 
 @PT_OPERATOR_METATYPES.register()
+class PTLogMetatype(PTOperatorMetatype):
+    name = "LogOp"
+    module_to_function_names = {
+        NamespaceTarget.TORCH: ["log"],
+    }
+
+
+@PT_OPERATOR_METATYPES.register()
+class PTAbsMetatype(PTOperatorMetatype):
+    name = "AbsOp"
+    module_to_function_names = {
+        NamespaceTarget.TORCH: ["abs"],
+    }
+
+
+@PT_OPERATOR_METATYPES.register()
 class PTErfMetatype(PTOperatorMetatype):
     name = "ErfOp"
     module_to_function_names = {
         NamespaceTarget.TORCH: ["erf"],
     }
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTMatMulMetatype(PTOperatorMetatype):
     name = "MatMulOp"
     module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["matmul"],
+        NamespaceTarget.TORCH_TENSOR: ["matmul", "__matmul__"],
         NamespaceTarget.TORCH: ["matmul", "bmm", "mm"],
     }
     hw_config_names = [HWConfigOpName.MATMUL]
 
 
 @PT_OPERATOR_METATYPES.register()
+class PTBaddBmmMetatype(PTOperatorMetatype):
+    name = "MatMulOp"
+    module_to_function_names = {NamespaceTarget.TORCH: ["baddbmm"]}
+    hw_config_names = [HWConfigOpName.MATMUL]
+    # 0-th arg to the baddbmm is basically a (b)ias to be (add)ed to the (bmm) operation,
+    # presuming that most runtime implementations will fuse the bias addition into the matrix multiplication
+    # and therefore won't quantize the bias input, as this would break the hardware-fused pattern.
+    ignored_input_ports: List[int] = [0]
+
+
+@PT_OPERATOR_METATYPES.register()
 class PTMeanMetatype(PTOperatorMetatype):
     name = "MeanOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["mean"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["mean"]}
     hw_config_names = [HWConfigOpName.REDUCEMEAN]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTRoundMetatype(PTOperatorMetatype):
     name = "RoundOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["round"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["round"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTDropoutMetatype(PTOperatorMetatype):
     name = "DropoutOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["dropout"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["dropout"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTThresholdMetatype(PTOperatorMetatype):
     name = "ThresholdOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["threshold"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["threshold"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleBatchNormMetatype(PTModuleOperatorSubtype):
     name = "BatchNormOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["batch_norm"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["batch_norm"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTBatchNormMetatype(PTOperatorMetatype):
     name = "BatchNormOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["batch_norm"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["batch_norm"]}
     subtypes = [PTModuleBatchNormMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTAvgPool2dMetatype(PTOperatorMetatype):
     name = "AvgPool2DOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["avg_pool2d", "adaptive_avg_pool2d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["avg_pool2d", "adaptive_avg_pool2d"]}
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTAvgPool3dMetatype(PTOperatorMetatype):
     name = "AvgPool3DOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["avg_pool3d", "adaptive_avg_pool3d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["avg_pool3d", "adaptive_avg_pool3d"]}
     hw_config_names = [HWConfigOpName.AVGPOOL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTMaxPool2dMetatype(PTOperatorMetatype):
     name = "MaxPool2DOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["max_pool2d", "adaptive_max_pool2d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["max_pool2d", "adaptive_max_pool2d"]}
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTMaxPool3dMetatype(PTOperatorMetatype):
     name = "MaxPool3DOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["max_pool3d", "adaptive_max_pool3d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["max_pool3d", "adaptive_max_pool3d"]}
     hw_config_names = [HWConfigOpName.MAXPOOL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTMaxUnpool3dMetatype(PTOperatorMetatype):
     name = "MaxUnPool3DOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["max_unpool3d"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["max_unpool3d"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTPadMetatype(PTOperatorMetatype):
     name = "PadOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["pad"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["pad"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTCatMetatype(PTOperatorMetatype):
     name = "CatOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH: ["cat", "stack"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH: ["cat", "stack"]}
     hw_config_names = [HWConfigOpName.CONCAT]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTRELUMetatype(PTOperatorMetatype):
     name = "ReluOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH: ["relu", "relu_"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH: ["relu", "relu_"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTRELU6Metatype(PTOperatorMetatype):
     name = "Relu6Op"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["relu6"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["relu6"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTMaxMetatype(PTOperatorMetatype):
     name = "MaxOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH: ["max"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH: ["max"]}
     hw_config_names = [HWConfigOpName.MAXIMUM, HWConfigOpName.REDUCEMAX]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTMinMetatype(PTOperatorMetatype):
     name = "MinOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH: ["min"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH: ["min"]}
     hw_config_names = [HWConfigOpName.MINIMUM]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTTransposeMetatype(PTOperatorMetatype):
     name = "TransposeOp"
     module_to_function_names = {
         NamespaceTarget.TORCH_TENSOR: ["transpose", "permute", "transpose_"],
-        NamespaceTarget.TORCH: ["transpose"]
+        NamespaceTarget.TORCH: ["transpose"],
     }
     hw_config_names = [HWConfigOpName.TRANSPOSE]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTGatherMetatype(PTOperatorMetatype):
     name = "GatherOp"
     module_to_function_names = {
         NamespaceTarget.TORCH_TENSOR: ["index_select", "__getitem__"],
-        NamespaceTarget.TORCH: ["gather", "index_select", "where"]
+        NamespaceTarget.TORCH: ["gather", "index_select", "where"],
     }
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTScatterMetatype(PTOperatorMetatype):
     name = "ScatterOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["scatter", "masked_fill", "masked_fill_"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["scatter", "masked_fill", "masked_fill_"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTReshapeMetatype(PTOperatorMetatype):
     name = "ReshapeOp"
     module_to_function_names = {
         NamespaceTarget.TORCH_TENSOR: ["reshape", "view", "flatten", "squeeze", "unsqueeze"],
-        NamespaceTarget.TORCH: ["squeeze", "flatten", "unsqueeze"]
+        NamespaceTarget.TORCH: ["squeeze", "flatten", "unsqueeze"],
     }
-    hw_config_names = [HWConfigOpName.RESHAPE, HWConfigOpName.SQUEEZE,
-                       HWConfigOpName.UNSQUEEZE, HWConfigOpName.FLATTEN]
+    hw_config_names = [HWConfigOpName.RESHAPE, HWConfigOpName.SQUEEZE, HWConfigOpName.UNSQUEEZE, HWConfigOpName.FLATTEN]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTSplitMetatype(PTOperatorMetatype):
     name = "SplitOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["split", "chunk"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["split", "chunk", "unbind"]}
     hw_config_names = [HWConfigOpName.SPLIT, HWConfigOpName.CHUNK]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTExpandMetatype(PTOperatorMetatype):
     name = "ExpandOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["expand"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["expand"]}
+
+
+@PT_OPERATOR_METATYPES.register()
+class PTExpandAsMetatype(PTOperatorMetatype):
+    name = "ExpandAsOp"
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["expand_as"]}
 
 
 # Non-quantizable ops
 @PT_OPERATOR_METATYPES.register()
 class PTModuleEmbeddingMetatype(PTModuleOperatorSubtype):
     name = "EmbeddingOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["embedding"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["embedding"]}
     hw_config_names = [HWConfigOpName.EMBEDDING]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTEmbeddingMetatype(PTOperatorMetatype):
     name = "EmbeddingOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["embedding"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["embedding"]}
     hw_config_names = [HWConfigOpName.EMBEDDING]
     subtypes = [PTModuleEmbeddingMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModuleEmbeddingBagMetatype(PTModuleOperatorSubtype):
     name = "EmbeddingBagOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["embedding_bag"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["embedding_bag"]}
     hw_config_names = [HWConfigOpName.EMBEDDINGBAG]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTEmbeddingBagMetatype(PTOperatorMetatype):
     name = "EmbeddingBagOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["embedding_bag"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["embedding_bag"]}
     hw_config_names = [HWConfigOpName.EMBEDDINGBAG]
     subtypes = [PTModuleEmbeddingBagMetatype]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTSoftmaxMetatype(PTOperatorMetatype):
     name = "SoftmaxOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["softmax"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["softmax"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLessMetatype(PTOperatorMetatype):
     name = "LessOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__lt__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__lt__"]}
     hw_config_names = [HWConfigOpName.LESS]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLessEqualMetatype(PTOperatorMetatype):
     name = "LessEqualOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__le__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__le__"]}
     hw_config_names = [HWConfigOpName.LESSEQUAL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTGreaterMetatype(PTOperatorMetatype):
     name = "GreaterOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__gt__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__gt__"]}
     hw_config_names = [HWConfigOpName.GREATER]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTGreaterEqualMetatype(PTOperatorMetatype):
     name = "GreaterEqualOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__ge__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__ge__"]}
     hw_config_names = [HWConfigOpName.GREATEREQUAL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTModMetatype(PTOperatorMetatype):
     name = "ModOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__mod__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__mod__"]}
     hw_config_names = [HWConfigOpName.FLOORMOD]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTEqualsMetatype(PTOperatorMetatype):
     name = "EqualsOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__eq__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__eq__"]}
     hw_config_names = [HWConfigOpName.EQUAL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTNotEqualMetatype(PTOperatorMetatype):
     name = "NotEqualOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__ne__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__ne__"]}
     hw_config_names = [HWConfigOpName.NOTEQUAL]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLogicalOrMetatype(PTOperatorMetatype):
     name = "LogicalOrOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__or__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__or__"]}
     hw_config_names = [HWConfigOpName.LOGICALOR]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLogicalXorMetatype(PTOperatorMetatype):
     name = "LogicalXorOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__xor__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__xor__"]}
     hw_config_names = [HWConfigOpName.LOGICALXOR]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLogicalAndMetatype(PTOperatorMetatype):
     name = "LogicalAndOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__and__"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__and__"]}
     hw_config_names = [HWConfigOpName.LOGICALAND]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTLogicalNotMetatype(PTOperatorMetatype):
     name = "LogicalNotOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["logical_not_"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["logical_not_"]}
     hw_config_names = [HWConfigOpName.LOGICALNOT]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTPowerMetatype(PTOperatorMetatype):
     name = "PowerOp"
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["__pow__", "pow"], NamespaceTarget.TORCH: ["pow"]}
+    hw_config_names = [HWConfigOpName.POWER]
+
+
+@PT_OPERATOR_METATYPES.register()
+class PTSqrtMetatype(PTOperatorMetatype):
+    name = "SqrtOp"
     module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["__pow__", "pow", "sqrt", "sqrt_"],
-        NamespaceTarget.TORCH: ["pow", "sqrt", "sqrt_"]
+        NamespaceTarget.TORCH_TENSOR: ["sqrt", "sqrt_"],
+        NamespaceTarget.TORCH: ["sqrt", "sqrt_"],
     }
     hw_config_names = [HWConfigOpName.POWER]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTInterpolateMetatype(PTOperatorMetatype):
     name = "InterpolateOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["interpolate"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["interpolate"]}
     hw_config_names = [HWConfigOpName.INTERPOLATE]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTRepeatMetatype(PTOperatorMetatype):
     name = "RepeatOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH: ["repeat_interleave"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH: ["repeat_interleave"]}
     hw_config_names = [HWConfigOpName.TILE]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTPixelShuffleMetatype(PTOperatorMetatype):
     name = "PixelShuffleOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_NN_FUNCTIONAL: ["pixel_shuffle"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ["pixel_shuffle"]}
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTSumMetatype(PTOperatorMetatype):
     name = "SumOp"
-    module_to_function_names = {
-        NamespaceTarget.TORCH_TENSOR: ["sum"],
-        NamespaceTarget.TORCH: ["sum"]
-    }
+    module_to_function_names = {NamespaceTarget.TORCH_TENSOR: ["sum"], NamespaceTarget.TORCH: ["sum"]}
     hw_config_names = [HWConfigOpName.REDUCESUM]
 
 
 @PT_OPERATOR_METATYPES.register()
 class PTReduceL2(PTOperatorMetatype):
     name = "ReduceL2"
     module_to_function_names = {
@@ -944,7 +848,9 @@
     PTModuleLayerNormMetatype,
     PTModuleConvTranspose1dMetatype,
     PTModuleConvTranspose2dMetatype,
     PTModuleConvTranspose3dMetatype,
     PTModuleEmbeddingMetatype,
     PTModuleEmbeddingBagMetatype,
 ]
+
+OP_NAMES_WITH_WEIGHTS = [x for meta in OPERATORS_WITH_WEIGHTS_METATYPES for x in meta.get_all_aliases()]
```

### Comparing `nncf-2.4.0/nncf/torch/graph/transformations/commands.py` & `nncf-2.5.0/nncf/torch/graph/transformations/commands.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-from typing import Any
-from typing import Callable
-from typing import Dict
+from typing import Any, Callable, Dict
 
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationCommand
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.graph.transformations.commands import TransformationType
 
 
 class PTTargetPointStateNames:
-    TARGET_NODE_NAME = 'target_node_name'
-    INPUT_PORT = 'input_port_id'
-    TARGET_TYPE = 'target_type'
+    TARGET_NODE_NAME = "target_node_name"
+    INPUT_PORT = "input_port_id"
+    TARGET_TYPE = "target_type"
 
 
 class PTTargetPoint(TargetPoint):
-    _OPERATION_TYPES = [TargetType.PRE_LAYER_OPERATION,
-                        TargetType.POST_LAYER_OPERATION,
-                        TargetType.OPERATION_WITH_WEIGHTS]
-    _HOOK_TYPES = [TargetType.OPERATOR_PRE_HOOK,
-                   TargetType.OPERATOR_POST_HOOK]
+    _OPERATION_TYPES = [
+        TargetType.PRE_LAYER_OPERATION,
+        TargetType.POST_LAYER_OPERATION,
+        TargetType.OPERATION_WITH_WEIGHTS,
+    ]
+    _HOOK_TYPES = [TargetType.OPERATOR_PRE_HOOK, TargetType.OPERATOR_POST_HOOK]
 
     _state_names = PTTargetPointStateNames
 
-    def __init__(self, target_type: TargetType, target_node_name: NNCFNodeName,
-                 *,
-                 input_port_id: int = None):
+    def __init__(self, target_type: TargetType, target_node_name: NNCFNodeName, *, input_port_id: int = None):
         super().__init__(target_type)
         self.target_node_name = target_node_name
         self.target_type = target_type
         if self.target_type not in self._OPERATION_TYPES + self._HOOK_TYPES:
             raise NotImplementedError("Unsupported target type: {}".format(target_type))
 
         self.input_port_id = input_port_id
 
-    def __eq__(self, other: 'PTTargetPoint'):
-        return isinstance(other, PTTargetPoint) and \
-               self.target_type == other.target_type and self.target_node_name == other.target_node_name
+    def __eq__(self, other: "PTTargetPoint"):
+        return (
+            isinstance(other, PTTargetPoint)
+            and self.target_type == other.target_type
+            and self.target_node_name == other.target_node_name
+        )
 
     def __str__(self):
         prefix = str(self.target_type)
         retval = prefix
         if self.target_type in self._OPERATION_TYPES:
             retval += " {}".format(self.target_node_name)
         elif self.target_type in self._HOOK_TYPES:
@@ -57,36 +57,42 @@
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
-        return {self._state_names.TARGET_TYPE: self.target_type.get_state(),
-                 self._state_names.INPUT_PORT: self.input_port_id,
-                 self._state_names.TARGET_NODE_NAME: self.target_node_name}
+        return {
+            self._state_names.TARGET_TYPE: self.target_type.get_state(),
+            self._state_names.INPUT_PORT: self.input_port_id,
+            self._state_names.TARGET_NODE_NAME: self.target_node_name,
+        }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'PTTargetPoint':
+    def from_state(cls, state: Dict[str, Any]) -> "PTTargetPoint":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         kwargs = {
             cls._state_names.TARGET_TYPE: TargetType.from_state(state[cls._state_names.TARGET_TYPE]),
             cls._state_names.INPUT_PORT: state[cls._state_names.INPUT_PORT],
-            cls._state_names.TARGET_NODE_NAME: state[cls._state_names.TARGET_NODE_NAME]
+            cls._state_names.TARGET_NODE_NAME: state[cls._state_names.TARGET_NODE_NAME],
         }
         return cls(**kwargs)
 
 
 class PTInsertionCommand(TransformationCommand):
-    def __init__(self, point: PTTargetPoint, fn: Callable,
-                 priority: TransformationPriority = TransformationPriority.DEFAULT_PRIORITY):
+    def __init__(
+        self,
+        point: PTTargetPoint,
+        fn: Callable,
+        priority: TransformationPriority = TransformationPriority.DEFAULT_PRIORITY,
+    ):
         super().__init__(TransformationType.INSERT, point)
         self.fn = fn  # type: Callable
         self.priority = priority  # type: TransformationPriority
 
-    def union(self, other: 'TransformationCommand') -> 'TransformationCommand':
+    def union(self, other: "TransformationCommand") -> "TransformationCommand":
         # TODO: keep all TransformationCommands atomic, refactor TransformationLayout instead
         raise NotImplementedError()
```

### Comparing `nncf-2.4.0/nncf/torch/initialization.py` & `nncf-2.5.0/nncf/torch/initialization.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 import math
 from contextlib import contextmanager
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import Optional
-from typing import Tuple
+from functools import partial
+from typing import Any, Callable, Dict, Optional, Tuple
 
 import torch
-from functools import partial
 from torch.nn.modules.loss import _Loss
 from torch.utils.data import DataLoader
 
 from nncf.common.initialization.dataloader import NNCFDataLoader
 from nncf.common.logging import nncf_logger
 from nncf.common.logging.progress_bar import ProgressBar
+from nncf.common.utils.api_marker import api
 from nncf.config.structures import BNAdaptationInitArgs
 from nncf.config.structures import ModelEvaluationArgs
 from nncf.config.structures import QuantizationRangeInitArgs
 from nncf.torch.nested_objects_traversal import objwalk
-from nncf.torch.structures import AutoQPrecisionInitArgs, LeGRInitArgs, DistributedCallbacksArgs
+from nncf.torch.structures import AutoQPrecisionInitArgs
+from nncf.torch.structures import DistributedCallbacksArgs
+from nncf.torch.structures import LeGRInitArgs
 from nncf.torch.structures import QuantizationPrecisionInitArgs
+from nncf.torch.utils import default_distributed_unwrapper
+from nncf.torch.utils import default_distributed_wrapper
 from nncf.torch.utils import get_model_device
-from nncf.torch.utils import is_tensor, default_distributed_wrapper, default_distributed_unwrapper
+from nncf.torch.utils import is_tensor
 
 
 class PTInitializingDataLoader(NNCFDataLoader):
     """
     This class wraps the torch.utils.data.DataLoader class,
     and defines methods to parse the general data loader output to
     separate the input to the compressed model and the ground truth target
@@ -58,33 +59,37 @@
         :param dataloader_output - the (args, kwargs) tuple returned by the __next__ method.
         """
 
         raise NotImplementedError
 
 
 class DefaultInitializingDataLoader(PTInitializingDataLoader):
-
     def get_inputs(self, dataloader_output: Any) -> Tuple[Tuple, Dict]:
         return (dataloader_output[0],), {}
 
     def get_target(self, dataloader_output: Any):
         return dataloader_output[1]
 
 
 def wrap_dataloader_for_init(data_loader) -> PTInitializingDataLoader:
     if not isinstance(data_loader, PTInitializingDataLoader):
         loaded_item = next(iter(data_loader))
         if isinstance(loaded_item, (tuple, list)) and len(loaded_item) == 2:
             return DefaultInitializingDataLoader(data_loader)
-        raise NotImplementedError("By default it is assumed that the data loader used for initialize "
-                                  "produces a tuple/list of (*model_input*, *ground_truth*) and that no special "
-                                  "forward arguments have to be set during init. If this is not the case, then instead "
-                                  "of your regular data loader you need to pass a specialized version of "
-                                  "PTInitializingDataLoader that returns a general (args, kwargs) tuple for your "
-                                  "model to be called with at each __next__ call.")
+        raise NotImplementedError(
+            "Could not deduce the forward arguments from the initializing dataloader output.\n"
+            "By default it is assumed that the data loader used for initialize "
+            "produces a tuple/list of (*model_input*, *ground_truth*) and that no special "
+            "forward arguments have to be set during init. If this is not the case, then instead "
+            "of your regular data loader you need to pass a specialized version of "
+            "PTInitializingDataLoader that returns a general (args, kwargs) tuple for your "
+            "model to be called with at each __next__ call.\n"
+            "See https://github.com/openvinotoolkit/nncf/blob/develop/docs/FAQ.md#pt_init_dataloader for "
+            "an example of how to do this this in your code."
+        )
     return data_loader
 
 
 class PartialDataLoader:
     def __init__(self, regular_data_loader: DataLoader, iter_ratio=1.0):
         if iter_ratio < 0.0 or iter_ratio > 1.0:
             raise ValueError("iter_ratio must be within 0 to 1 range")
@@ -109,21 +114,21 @@
         return self._stop_id
 
 
 class DataLoaderBaseRunner:
     def __init__(self, model, init_device: Optional[str]):
         self.model = model
         self.init_device = init_device
-        self.progressbar_description = 'Algorithm initialization'
+        self.progressbar_description = "Algorithm initialization"
 
     def _run_model_inference(self, data_loader, num_init_steps, device):
         for i, loaded_item in ProgressBar(
-                enumerate(data_loader),
-                total=num_init_steps,
-                desc=self.progressbar_description,
+            enumerate(data_loader),
+            total=num_init_steps,
+            desc=self.progressbar_description,
         ):
             if num_init_steps is not None and i >= num_init_steps:
                 break
             args_kwargs_tuple = data_loader.get_inputs(loaded_item)
             self._infer_batch(args_kwargs_tuple, device)
 
     def _infer_batch(self, args_kwargs_tuple, device):
@@ -161,24 +166,29 @@
     def _apply_initializers(self):
         pass
 
 
 class DataLoaderBNAdaptationRunner(DataLoaderBaseRunner):
     def __init__(self, model, init_device: str):
         super().__init__(model, init_device)
-        self.progressbar_description = 'BatchNorm statistics adaptation'
+        self.progressbar_description = "BatchNorm statistics adaptation"
         self.original_momenta_values = {}
         self.original_training_state = {}
 
     @staticmethod
     def _apply_to_batchnorms(func):
         def func_apply_to_bns(module):
-            if isinstance(module, (torch.nn.modules.batchnorm.BatchNorm1d,
-                                   torch.nn.modules.batchnorm.BatchNorm2d,
-                                   torch.nn.modules.batchnorm.BatchNorm3d)):
+            if isinstance(
+                module,
+                (
+                    torch.nn.modules.batchnorm.BatchNorm1d,
+                    torch.nn.modules.batchnorm.BatchNorm2d,
+                    torch.nn.modules.batchnorm.BatchNorm3d,
+                ),
+            ):
                 func(module)
 
         return func_apply_to_bns
 
     @contextmanager
     def _bn_training_state_switcher(self) -> None:
         def save_original_bn_training_state(module: torch.nn.Module):
@@ -196,17 +206,15 @@
             yield
         finally:
             self.model.apply(self._apply_to_batchnorms(restore_original_bn_training_state))
 
     def _run_model_inference(self, data_loader, num_init_steps, device):
         with self._bn_training_state_switcher():
             for i, loaded_item in ProgressBar(
-                    enumerate(data_loader),
-                    total=num_init_steps,
-                    desc=self.progressbar_description
+                enumerate(data_loader), total=num_init_steps, desc=self.progressbar_description
             ):
                 if num_init_steps is not None and i >= num_init_steps:
                     break
                 args_kwargs_tuple = data_loader.get_inputs(loaded_item)
                 self._infer_batch(args_kwargs_tuple, device)
 
     def _prepare_initialization(self):
@@ -216,66 +224,83 @@
         pass
 
 
 def default_criterion_fn(outputs: Any, target: Any, criterion: Any) -> torch.Tensor:
     return criterion(outputs, target)
 
 
-def register_default_init_args(nncf_config: 'NNCFConfig',
-                               train_loader: torch.utils.data.DataLoader,
-                               criterion: _Loss = None,
-                               criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor] = None,
-                               train_steps_fn: Callable[[torch.utils.data.DataLoader, torch.nn.Module,
-                                                         torch.optim.Optimizer, 'CompressionAlgorithmController',
-                                                         Optional[int]], type(None)] = None,
-                               validate_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader],
-                                                     Tuple[float, float]] = None,
-                               val_loader: torch.utils.data.DataLoader = None,
-                               autoq_eval_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader], float] = None,
-                               model_eval_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader], float] = None,
-                               distributed_callbacks: Tuple[Callable, Callable] = None,
-                               execution_parameters: 'ExecutionParameters' = None,
-                               legr_train_optimizer: torch.optim.Optimizer = None,
-                               device: str = None, ) -> 'NNCFConfig':
-    nncf_config.register_extra_structs([QuantizationRangeInitArgs(data_loader=wrap_dataloader_for_init(train_loader),
-                                                                  device=device),
-                                        BNAdaptationInitArgs(data_loader=wrap_dataloader_for_init(train_loader),
-                                                             device=device),
-
-                                        ])
+@api(canonical_alias="nncf.torch.register_default_init_args")
+def register_default_init_args(
+    nncf_config: "NNCFConfig",
+    train_loader: torch.utils.data.DataLoader,
+    criterion: _Loss = None,
+    criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor] = None,
+    train_steps_fn: Callable[
+        [
+            torch.utils.data.DataLoader,
+            torch.nn.Module,
+            torch.optim.Optimizer,
+            "CompressionAlgorithmController",
+            Optional[int],
+        ],
+        type(None),
+    ] = None,
+    validate_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader], Tuple[float, float]] = None,
+    val_loader: torch.utils.data.DataLoader = None,
+    autoq_eval_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader], float] = None,
+    model_eval_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader], float] = None,
+    distributed_callbacks: Tuple[Callable, Callable] = None,
+    execution_parameters: "ExecutionParameters" = None,
+    legr_train_optimizer: torch.optim.Optimizer = None,
+    device: str = None,
+) -> "NNCFConfig":
+    nncf_config.register_extra_structs(
+        [
+            QuantizationRangeInitArgs(data_loader=wrap_dataloader_for_init(train_loader), device=device),
+            BNAdaptationInitArgs(data_loader=wrap_dataloader_for_init(train_loader), device=device),
+        ]
+    )
     if train_loader and train_steps_fn and val_loader and validate_fn:
-        nncf_config.register_extra_structs([LeGRInitArgs(
-            train_loader=train_loader,
-            train_fn=train_steps_fn,
-            val_loader=val_loader,
-            val_fn=validate_fn,
-            train_optimizer=legr_train_optimizer,
-            nncf_config=nncf_config,
-        )])
+        nncf_config.register_extra_structs(
+            [
+                LeGRInitArgs(
+                    train_loader=train_loader,
+                    train_fn=train_steps_fn,
+                    val_loader=val_loader,
+                    val_fn=validate_fn,
+                    train_optimizer=legr_train_optimizer,
+                    nncf_config=nncf_config,
+                )
+            ]
+        )
 
     if criterion is not None:
         if not criterion_fn:
             criterion_fn = default_criterion_fn
-        nncf_config.register_extra_structs([QuantizationPrecisionInitArgs(criterion_fn=criterion_fn,
-                                                                          criterion=criterion,
-                                                                          data_loader=train_loader,
-                                                                          device=device)])
+        nncf_config.register_extra_structs(
+            [
+                QuantizationPrecisionInitArgs(
+                    criterion_fn=criterion_fn, criterion=criterion, data_loader=train_loader, device=device
+                )
+            ]
+        )
 
     if autoq_eval_fn is not None:
         if not val_loader:
             val_loader = train_loader
-        nncf_config.register_extra_structs([AutoQPrecisionInitArgs(data_loader=val_loader,
-                                                                   eval_fn=autoq_eval_fn,
-                                                                   nncf_config=nncf_config)])
+        nncf_config.register_extra_structs(
+            [AutoQPrecisionInitArgs(data_loader=val_loader, eval_fn=autoq_eval_fn, nncf_config=nncf_config)]
+        )
 
     if model_eval_fn is not None:
         nncf_config.register_extra_structs([ModelEvaluationArgs(eval_fn=model_eval_fn)])
 
     if distributed_callbacks is None:
-        distributed_callbacks = (partial(default_distributed_wrapper,
-                                         execution_parameters=execution_parameters),
-                                 default_distributed_unwrapper)
+        distributed_callbacks = (
+            partial(default_distributed_wrapper, execution_parameters=execution_parameters),
+            default_distributed_unwrapper,
+        )
     else:
-        nncf_logger.info('Utilizing user-provided distributed training wrappers.')
+        nncf_logger.info("Utilizing user-provided distributed training wrappers.")
 
     nncf_config.register_extra_structs([DistributedCallbacksArgs(*distributed_callbacks)])
     return nncf_config
```

### Comparing `nncf-2.4.0/nncf/torch/knowledge_distillation/algo.py` & `nncf-2.5.0/nncf/torch/knowledge_distillation/algo.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,72 +1,84 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Contains builder and controller class definitions for the knowledge distillation.
 """
 
 from copy import deepcopy
 
 from torch import nn
 
+from nncf import NNCFConfig
+from nncf.api.compression import CompressionLoss
+from nncf.api.compression import CompressionScheduler
+from nncf.api.compression import CompressionStage
 from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
 from nncf.config.schemata.defaults import KNOWLEDGE_DISTILLATION_SCALE
 from nncf.config.schemata.defaults import KNOWLEDGE_DISTILLATION_TEMPERATURE
+from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
+from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
+from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
-from nncf import NNCFConfig
 from nncf.torch.knowledge_distillation.knowledge_distillation_loss import KnowledgeDistillationLoss
 from nncf.torch.nncf_network import NNCFNetwork
-from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
-from nncf.torch.compression_method_api import PTCompressionAlgorithmController
-from nncf.api.compression import CompressionLoss, CompressionScheduler, CompressionStage
-from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 
 
-@PT_COMPRESSION_ALGORITHMS.register('knowledge_distillation')
+@PT_COMPRESSION_ALGORITHMS.register("knowledge_distillation")
 class KnowledgeDistillationBuilder(PTCompressionAlgorithmBuilder):
     def __init__(self, config: NNCFConfig, should_init: bool = True):
         super().__init__(config, should_init)
-        self.kd_type = self._algo_config.get('type')
-        self.scale = self._algo_config.get('scale', KNOWLEDGE_DISTILLATION_SCALE)
-        self.temperature = self._algo_config.get('temperature', KNOWLEDGE_DISTILLATION_TEMPERATURE)
-        if 'temperature' in self._algo_config.keys() and self.kd_type == 'mse':
+        self.kd_type = self._algo_config.get("type")
+        self.scale = self._algo_config.get("scale", KNOWLEDGE_DISTILLATION_SCALE)
+        self.temperature = self._algo_config.get("temperature", KNOWLEDGE_DISTILLATION_TEMPERATURE)
+        if "temperature" in self._algo_config.keys() and self.kd_type == "mse":
             raise ValueError("Temperature shouldn't be stated for MSE Loss (softmax only feature)")
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
-        self.original_model = deepcopy(target_model.nncf_module)
+        self.original_model = deepcopy(target_model).nncf.get_clean_shallow_copy()
         for param in self.original_model.parameters():
             param.requires_grad = False
         return PTTransformationLayout()
 
     def _build_controller(self, model):
         return KnowledgeDistillationController(model, self.original_model, self.kd_type, self.scale, self.temperature)
 
     def initialize(self, model: NNCFNetwork) -> None:
         pass
 
 
+@api()
 class KnowledgeDistillationController(PTCompressionAlgorithmController):
-    def __init__(self, target_model: NNCFNetwork, original_model: nn.Module, kd_type: str, scale: float,
-                 temperature: float):
+    """
+    Controller for the knowledge distillation in PT.
+    """
+
+    def __init__(
+        self, target_model: NNCFNetwork, original_model: nn.Module, kd_type: str, scale: float, temperature: float
+    ):
         super().__init__(target_model)
         original_model.train()
         self._scheduler = BaseCompressionScheduler()
-        self._loss = KnowledgeDistillationLoss(target_model=target_model,
-                                               original_model=original_model,
-                                               kd_type=kd_type,
-                                               scale=scale,
-                                               temperature=temperature)
+        self._loss = KnowledgeDistillationLoss(
+            target_model=target_model,
+            original_model=original_model,
+            kd_type=kd_type,
+            scale=scale,
+            temperature=temperature,
+        )
 
     def compression_stage(self) -> CompressionStage:
         """
         Returns level of compression. Should be used on saving best checkpoints to distinguish between
         uncompressed, partially compressed and fully compressed models.
         """
         return CompressionStage.FULLY_COMPRESSED
```

### Comparing `nncf-2.4.0/nncf/torch/knowledge_distillation/knowledge_distillation_handler.py` & `nncf-2.5.0/nncf/torch/knowledge_distillation/knowledge_distillation_handler.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 import torch
 from torch import nn
 
 from nncf.torch.dynamic_graph.context import TracingContext
@@ -22,44 +20,49 @@
 class KnowledgeDistillationLossHandler(nn.Module):
     """
     Encapsulates knowledge distillation logic. Controls knowledge distillation loss calculation. Notice that knowledge
     distillation loss is computed between results of original model and compressed model inferences only with latest
     inputs. And storages loss values in context at storage_device for further access. Such complex method of storage
     is required for DataParallel model replication logic.
     """
-    KD_LOSS_STORAGE_NAME = 'kd_loss'
-    KD_STORAGE_DEVICE = 'kd_storage_device'
 
-    def __init__(self, context: TracingContext, kd_original_model: nn.Module, calculate_kd_loss_fn,
-                 storage_device: torch.device):
+    KD_LOSS_STORAGE_NAME = "kd_loss"
+    KD_STORAGE_DEVICE = "kd_storage_device"
+
+    def __init__(
+        self, context: TracingContext, kd_original_model: nn.Module, calculate_kd_loss_fn, storage_device: torch.device
+    ):
         super().__init__()
         self._compressed_context = context
         self._kd_original_model = kd_original_model
         self._calculate_kd_loss_fn = calculate_kd_loss_fn
         self._compressed_context.register_global_buffer(self.KD_LOSS_STORAGE_NAME, [])
         self._compressed_context.register_global_buffer(self.KD_STORAGE_DEVICE, storage_device)
 
     def zero_kd_loss(self):
         """
-            Frees storage space for further next iteration loss value storage.
+        Frees storage space for further next iteration loss value storage.
         """
         self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME] = []
 
     def get_kd_loss(self) -> List[torch.Tensor]:
         if len(self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME]) == 0:
             return [torch.zeros([], device=self._compressed_context.global_buffer_store[self.KD_STORAGE_DEVICE])]
         return self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME]
 
-    def forward(self, inputs, *args, **kwargs):
+    def forward(self, compressed_model_outputs, *args, **kwargs):
         """
         Infers kd original model with latest NNCFNetwork forward inputs (*args, **kwargs) and computes distillation loss
         between results of kd original model forward and compressed model forward (inputs). Then stores loss values
         in context at storage device.
 
-        :param inputs: Results of compressed model forward used for knowledge distillation loss calculations.
+        :param compressed_model_outputs: Results of compressed model forward used for
+        knowledge distillation loss calculations.
         """
         self.zero_kd_loss()
         with torch.no_grad():
-            kd_outputs = self._kd_original_model(*args, **kwargs)
-        kd_loss = self._calculate_kd_loss_fn(inputs, kd_outputs)
-        self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME].append(kd_loss.to(
-            self._compressed_context.global_buffer_store[self.KD_STORAGE_DEVICE]))
+            original_model_outputs = self._kd_original_model(*args, **kwargs)
+        kd_loss = self._calculate_kd_loss_fn(compressed_model_outputs, original_model_outputs)
+        if kd_loss is not None:
+            self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME].append(
+                kd_loss.to(self._compressed_context.global_buffer_store[self.KD_STORAGE_DEVICE])
+            )
```

### Comparing `nncf-2.4.0/nncf/torch/knowledge_distillation/knowledge_distillation_loss.py` & `nncf-2.5.0/nncf/torch/knowledge_distillation/knowledge_distillation_loss.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,71 +1,88 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
-from functools import reduce, partial
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from functools import partial
+from functools import reduce
+from typing import Any, Callable, Optional
 
 import torch
 from torch import nn
 
-from nncf.torch.nncf_network import NNCFNetwork
+from nncf.common.logging import nncf_logger
 from nncf.torch.compression_method_api import PTCompressionLoss
 from nncf.torch.nested_objects_traversal import NestedObjectIndex
-from nncf.common.logging import nncf_logger
+from nncf.torch.nncf_network import NNCFNetwork
 
 
 class KnowledgeDistillationLoss(PTCompressionLoss):
     """
     Doesn't not directly compute knowledge distillation loss values but has access to them through
     KnowledgeDistillationLossHandler. Notice that knowledge distillation loss is computed between results of original
     model and compressed model inferences with latest inputs. Provides KnowledgeDistillationLossHandler with kd original
     model (to distill from), storage device and function to calculate knowledge distillation loss.
     """
-    def __init__(self, target_model: NNCFNetwork, original_model: nn.Module, kd_type: str, scale: float,
-                 temperature: float):
+
+    def __init__(
+        self, target_model: NNCFNetwork, original_model: nn.Module, kd_type: str, scale: float, temperature: float
+    ):
         super().__init__()
         original_model.train()
-        if kd_type == 'softmax':
+        if kd_type == "softmax":
+
             def kd_loss_fn(teacher_output: torch.Tensor, student_output: torch.Tensor):
                 if len(student_output.shape) != 2 or len(teacher_output.shape) != 2:
                     nncf_logger.debug(
                         f"Incompatible number of dimensions of the model output tensor for softmax KD "
                         f"(student - {student_output.shape}, "
                         f"teacher - {teacher_output.shape}, "
-                        f"number of dims for both should be == 2) - ignoring!")
+                        f"number of dims for both should be == 2) - ignoring!"
+                    )
                     return torch.zeros([1]).to(student_output.device)
-                return scale * -(nn.functional.log_softmax(student_output / temperature, dim=1) *
-                                 nn.functional.softmax(teacher_output / temperature, dim=1)).mean() \
-                       * (student_output.shape[1] * temperature * temperature)
+                return (
+                    scale
+                    * -(
+                        nn.functional.log_softmax(student_output / temperature, dim=1)
+                        * nn.functional.softmax(teacher_output / temperature, dim=1)
+                    ).mean()
+                    * (student_output.shape[1] * temperature * temperature)
+                )
+
         else:
+
             def kd_loss_fn(teacher_output: torch.Tensor, student_output: torch.Tensor):
                 mse = torch.nn.MSELoss()
                 if len(teacher_output.shape) < 2:
                     nncf_logger.debug(
                         f"Incompatible number of dimensions of the model output tensor for MSE KD "
                         f"(student - {student_output.shape}, "
                         f"teacher - {teacher_output.shape}, "
-                        f"number of dims {len(student_output.shape)} should be > 1) (most likely loss) - ignoring!")
+                        f"number of dims {len(student_output.shape)} should be > 1) (most likely loss) - ignoring!"
+                    )
                     return torch.zeros([1]).to(student_output.device)
                 return scale * mse(teacher_output, student_output)
-        self._kd_loss_handler = target_model.create_knowledge_distillation_loss_handler(original_model, partial(
-            KnowledgeDistillationLoss._calculate,
-            kd_loss_fn=kd_loss_fn))
+
+        self._kd_loss_handler = target_model.nncf.create_knowledge_distillation_loss_handler(
+            original_model, partial(KnowledgeDistillationLoss._calculate, kd_loss_fn=kd_loss_fn)
+        )
 
     @staticmethod
-    def _calculate(compressed_model_outputs, orig_model_outputs, kd_loss_fn) -> torch.Tensor:
+    def _calculate(
+        compressed_model_outputs: Any,
+        orig_model_outputs: Any,
+        kd_loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
+    ) -> Optional[torch.Tensor]:
         """
         Calculates knowledge distillation loss value from compressed_model_outputs and orig_model_outputs. First uses
         nested_object_paths_generator to unpack input containers and numerate contents inside them.
         Than checks compressed_model_outputs unpacked container for loss tensors (requires_grad=True)
         and maps extracted structure of loss tensors to orig_model_outputs.
         Finally computes knowledge distillation loss with extracted loss tensors.
 
@@ -74,32 +91,55 @@
         :param orig_model_outputs: Output tensors of original model (used for distillation) can be any type of
             container with deterministic traversal.
         :return: knowledge distillation loss value
         """
 
         compressed_model_outputs_nested_obj_indexing = NestedObjectIndex([compressed_model_outputs])
         orig_model_outputs_nested_obj_indexing = NestedObjectIndex([orig_model_outputs])
-        compressed_model_loss_outputs_nested_obj_indexing = list(filter(
-            lambda x: KnowledgeDistillationLoss._is_loss(x.getter()),
-            compressed_model_outputs_nested_obj_indexing.get_flat_nested_obj_indexing()))
-        compressed_model_loss_outputs = list(map(lambda x: x.getter(),
-                                                 compressed_model_loss_outputs_nested_obj_indexing))
+        compressed_model_loss_outputs_nested_obj_indexing = list(
+            filter(
+                lambda x: KnowledgeDistillationLoss._is_loss(x.getter()),
+                compressed_model_outputs_nested_obj_indexing.get_flat_nested_obj_indexing(),
+            )
+        )
+        compressed_model_loss_outputs = list(
+            map(lambda x: x.getter(), compressed_model_loss_outputs_nested_obj_indexing)
+        )
 
         def match_fn(obj):
             for x in compressed_model_loss_outputs_nested_obj_indexing:
                 if x.path == obj.path:
                     return True
             return False
 
-        orig_model_loss_outputs = list(map(lambda x: x.getter(), filter(
-            match_fn, orig_model_outputs_nested_obj_indexing.get_flat_nested_obj_indexing())))
+        orig_model_loss_outputs = list(
+            map(
+                lambda x: x.getter(),
+                filter(match_fn, orig_model_outputs_nested_obj_indexing.get_flat_nested_obj_indexing()),
+            )
+        )
+
+        if len(orig_model_loss_outputs) != len(compressed_model_loss_outputs):
+            nncf_logger.warning(
+                f"KD: mismatch in the number of detected loss tensors in return value between original "
+                f"and compressed models;\n"
+                f"original has {len(orig_model_loss_outputs)} loss tensors,\n"
+                f"compressed has {len(compressed_model_loss_outputs)}"
+            )
+        if not orig_model_loss_outputs:
+            nncf_logger.warning("KD: no loss outputs detected in original model, knowledge distillation not possible")
+            return None
+        if not compressed_model_loss_outputs:
+            nncf_logger.warning("KD: no loss outputs detected in compressed model, knowledge distillation not possible")
+            return None
         return reduce(
             lambda kd_loss, loss_tensors: kd_loss + kd_loss_fn(loss_tensors[0], loss_tensors[1]),
             zip(orig_model_loss_outputs, compressed_model_loss_outputs),
-            torch.zeros([], device=orig_model_loss_outputs[0].device))
+            torch.zeros([], device=orig_model_loss_outputs[0].device),
+        )
 
     @staticmethod
     def _is_loss(obj):
         if not isinstance(obj, torch.Tensor):
             return False
         if obj.requires_grad:
             return True
```

### Comparing `nncf-2.4.0/nncf/torch/layer_utils.py` & `nncf-2.5.0/nncf/torch/layer_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import torch
 from torch import nn
 
 from nncf.common.utils.registry import Registry
 
-COMPRESSION_MODULES = Registry('compression modules')
+COMPRESSION_MODULES = Registry("compression modules")
 
 
 class ProxyModule:
     def __init__(self, module):
         self._module = module
 
     def __getattr__(self, name):
```

### Comparing `nncf-2.4.0/nncf/torch/layers.py` & `nncf-2.5.0/nncf/torch/layers.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,52 +1,48 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Dict
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import math
 import numbers
-from typing import Optional
-from typing import Tuple
+from typing import Dict, Optional, Tuple
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 from torch.nn import init
 from torch.nn.utils.rnn import PackedSequence
 from torch.nn.utils.weight_norm import WeightNorm
 
 from nncf import nncf_logger
-from nncf.torch.dynamic_graph.context import forward_nncf_trace
-from nncf.torch.utils import no_jit_trace
-from nncf.torch.checkpoint_loading import OPTIONAL_PARAMETERS_REGISTRY
 from nncf.common.graph.layer_attributes import GenericWeightedLayerAttributes
+from nncf.common.utils.api_marker import api
 from nncf.common.utils.registry import Registry
+from nncf.torch.checkpoint_loading import OPTIONAL_PARAMETERS_REGISTRY
+from nncf.torch.dynamic_graph.context import forward_nncf_trace
 from nncf.torch.layer_utils import _NNCFModuleMixin
+from nncf.torch.utils import no_jit_trace
 
 
 def dict_update(src: Dict, dst: Dict, recursive: bool = True):
     for name, value in src.items():
         if recursive and name in dst and isinstance(value, dict):
             dict_update(value, dst[name], recursive)
         else:
             dst[name] = value
 
 
 def maybe_reapply_weight_norm(src: torch.nn.Module, dst: torch.nn.Module) -> torch.nn.Module:
-    #pylint:disable=protected-access
+    # pylint:disable=protected-access
     for k, hook in src._forward_pre_hooks.items():
         if isinstance(hook, WeightNorm):
             # The code below presumes that the `hook` object does not
             # contain internal references to the module it was set up on
             # (i.e. to the `src`) and takes the module to act on as a parameter.
             # This is the case for the `WeightNorm` hook.
             hook.remove(dst)
@@ -66,39 +62,53 @@
 class NNCFConv1d(_NNCFModuleMixin, nn.Conv1d):
     op_func_name = "conv1d"
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.Conv1d.__name__
         nncf_conv = NNCFConv1d(
-            module.in_channels, module.out_channels, module.kernel_size, module.stride,
-            module.padding, module.dilation, module.groups, hasattr(module, 'bias')
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            hasattr(module, "bias"),
         )
         nncf_conv = align_module_internals(module, nncf_conv)
         return nncf_conv
 
 
 class NNCFConvTranspose1d(_NNCFModuleMixin, nn.ConvTranspose1d):
     op_func_name = "conv_transpose1d"
     target_weight_dim_for_compression = 1
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.ConvTranspose1d.__name__
-        args = [module.in_channels, module.out_channels, module.kernel_size, module.stride,
-                module.padding, module.output_padding, module.groups, hasattr(module, 'bias'),
-                module.dilation]
-        if hasattr(module, 'padding_mode'):
+        args = [
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.output_padding,
+            module.groups,
+            hasattr(module, "bias"),
+            module.dilation,
+        ]
+        if hasattr(module, "padding_mode"):
             args.append(module.padding_mode)
         nncf_conv_transpose1d = NNCFConvTranspose1d(*args)
         nncf_conv_transpose1d = align_module_internals(module, nncf_conv_transpose1d)
         return nncf_conv_transpose1d
 
 
-NNCF_PADDING_VALUE_ATTR_NAME = 'nncf_padding_value'
+NNCF_PADDING_VALUE_ATTR_NAME = "nncf_padding_value"
 OPTIONAL_PARAMETERS_REGISTRY.register(NNCF_PADDING_VALUE_ATTR_NAME)
 
 
 class NNCFConv2d(_NNCFModuleMixin, nn.Conv2d):
     op_func_name = "conv2d"
 
     def __init__(
@@ -107,45 +117,51 @@
         out_channels: int,
         kernel_size,
         stride=1,
         padding=0,
         dilation=1,
         groups: int = 1,
         bias: bool = True,
-        padding_mode: str = 'zeros'
+        padding_mode: str = "zeros",
     ):
-        super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups,
-                         bias, padding_mode)
+        super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)
         self.register_buffer(NNCF_PADDING_VALUE_ATTR_NAME, torch.zeros([1]))
 
     def get_padding_value_ref(self):
         return getattr(self, NNCF_PADDING_VALUE_ATTR_NAME)
 
     def _set_padding_value(self, value):
         setattr(self, NNCF_PADDING_VALUE_ATTR_NAME, value)
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.Conv2d.__name__
         nncf_conv = NNCFConv2d(
-            module.in_channels, module.out_channels, module.kernel_size, module.stride,
-            module.padding, module.dilation, module.groups, hasattr(module, 'bias')
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            hasattr(module, "bias"),
         )
         nncf_conv = align_module_internals(module, nncf_conv)
         return nncf_conv
 
     # override class attribute of _NNCFModuleMixin
     def _custom_forward_fn(self, input_):
         proxy_padding_value = getattr(self, NNCF_PADDING_VALUE_ATTR_NAME)  # hack to get value from ProxyModule
         proxy_weight = self.weight
         proxy_bias = self.bias
         proxy_padding = self.padding
         proxy_num_groups = self.groups
-        return self._conv_forward_proxy(input_,
-                                  proxy_weight, proxy_bias, proxy_padding_value, proxy_padding, proxy_num_groups)
+        return self._conv_forward_proxy(
+            input_, proxy_weight, proxy_bias, proxy_padding_value, proxy_padding, proxy_num_groups
+        )
 
     def _conv_forward_proxy(self, input_, weight, bias, padding_value, padding, num_groups):
         with no_jit_trace():
             padding_val = padding_value.item()
         self.get_padding_value_ref().data.fill_(padding_val)
         self.groups = num_groups
 
@@ -154,201 +170,248 @@
 
             This can be used to translate padding arg used by Conv and Pooling modules
             to the ones used by `F.pad`.
             """
             return tuple(x for x in reversed(t) for _ in range(n))
 
         reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)
-        if self.padding_mode != 'zeros':
-            return F.conv2d(F.pad(input_, reversed_padding_repeated_twice, mode=self.padding_mode,
-                                  value=padding_val),
-                            weight, bias, self.stride,
-                            (0, 0), self.dilation, self.groups)
+        if self.padding_mode != "zeros":
+            return F.conv2d(
+                F.pad(input_, reversed_padding_repeated_twice, mode=self.padding_mode, value=padding_val),
+                weight,
+                bias,
+                self.stride,
+                (0, 0),
+                self.dilation,
+                self.groups,
+            )
         if padding_val == 0:
             return F.conv2d(input_, weight, bias, self.stride, padding, self.dilation, self.groups)
-        return F.conv2d(F.pad(input_, reversed_padding_repeated_twice, value=padding_val),
-                        weight, bias, self.stride,
-                        (0, 0), self.dilation, self.groups)
+        return F.conv2d(
+            F.pad(input_, reversed_padding_repeated_twice, value=padding_val),
+            weight,
+            bias,
+            self.stride,
+            (0, 0),
+            self.dilation,
+            self.groups,
+        )
 
 
 class NNCFLinear(_NNCFModuleMixin, nn.Linear):
     op_func_name = "linear"
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.Linear.__name__
 
-        nncf_linear = NNCFLinear(module.in_features, module.out_features, hasattr(module, 'bias'))
+        nncf_linear = NNCFLinear(module.in_features, module.out_features, hasattr(module, "bias"))
         nncf_linear = align_module_internals(module, nncf_linear)
         return nncf_linear
 
 
 class NNCFBatchNorm1d(_NNCFModuleMixin, nn.BatchNorm1d):
     op_func_name = "batch_norm"
-    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']
+    ignored_algorithms = ["magnitude_sparsity", "rb_sparsity", "const_sparsity", "quantization"]
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.BatchNorm1d.__name__
 
         nncf_bn = NNCFBatchNorm1d(module.num_features)
         nncf_bn = align_module_internals(module, nncf_bn)
         return nncf_bn
 
 
 class NNCFBatchNorm2d(_NNCFModuleMixin, nn.BatchNorm2d):
     op_func_name = "batch_norm"
-    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']
+    ignored_algorithms = ["magnitude_sparsity", "rb_sparsity", "const_sparsity", "quantization"]
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.BatchNorm2d.__name__
 
         nncf_bn = NNCFBatchNorm2d(module.num_features)
         nncf_bn = align_module_internals(module, nncf_bn)
         return nncf_bn
 
 
 class NNCFBatchNorm3d(_NNCFModuleMixin, nn.BatchNorm3d):
     op_func_name = "batch_norm"
-    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']
+    ignored_algorithms = ["magnitude_sparsity", "rb_sparsity", "const_sparsity", "quantization"]
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.BatchNorm3d.__name__
 
         nncf_bn = NNCFBatchNorm3d(module.num_features)
         nncf_bn = align_module_internals(module, nncf_bn)
         return nncf_bn
 
 
 class NNCFGroupNorm(_NNCFModuleMixin, nn.GroupNorm):
     op_func_name = "group_norm"
-    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']
+    ignored_algorithms = ["magnitude_sparsity", "rb_sparsity", "const_sparsity", "quantization"]
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.GroupNorm.__name__
 
-        nncf_bn = NNCFGroupNorm(num_groups=module.num_groups,
-                                num_channels=module.num_channels,
-                                eps=module.eps,
-                                affine=module.affine)
+        nncf_bn = NNCFGroupNorm(
+            num_groups=module.num_groups, num_channels=module.num_channels, eps=module.eps, affine=module.affine
+        )
         nncf_bn = align_module_internals(module, nncf_bn)
         return nncf_bn
 
 
 class NNCFLayerNorm(_NNCFModuleMixin, nn.LayerNorm):
     op_func_name = "layer_norm"
-    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']
+    ignored_algorithms = ["magnitude_sparsity", "rb_sparsity", "const_sparsity", "quantization"]
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.LayerNorm.__name__
 
-        nncf_ln = NNCFLayerNorm(module.normalized_shape, hasattr(module, 'eps'))
+        nncf_ln = NNCFLayerNorm(module.normalized_shape, hasattr(module, "eps"))
         nncf_ln = align_module_internals(module, nncf_ln)
         return nncf_ln
 
 
 class NNCFConvTranspose2d(_NNCFModuleMixin, nn.ConvTranspose2d):
     op_func_name = "conv_transpose2d"
     target_weight_dim_for_compression = 1
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.ConvTranspose2d.__name__
-        args = [module.in_channels, module.out_channels, module.kernel_size, module.stride,
-                module.padding, module.output_padding, module.groups, hasattr(module, 'bias'),
-                module.dilation]
-        if hasattr(module, 'padding_mode'):
+        args = [
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.output_padding,
+            module.groups,
+            hasattr(module, "bias"),
+            module.dilation,
+        ]
+        if hasattr(module, "padding_mode"):
             args.append(module.padding_mode)
         nncf_conv_transpose2d = NNCFConvTranspose2d(*args)
         nncf_conv_transpose2d = align_module_internals(module, nncf_conv_transpose2d)
         return nncf_conv_transpose2d
 
 
 class NNCFConv3d(_NNCFModuleMixin, nn.Conv3d):
     op_func_name = "conv3d"
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.Conv3d.__name__
 
         nncf_conv3d = NNCFConv3d(
-            module.in_channels, module.out_channels, module.kernel_size, module.stride,
-            module.padding, module.dilation, module.groups, hasattr(module, 'bias')
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            hasattr(module, "bias"),
         )
         nncf_conv3d = align_module_internals(module, nncf_conv3d)
         return nncf_conv3d
 
+
 class NNCFConvTranspose3d(_NNCFModuleMixin, nn.ConvTranspose3d):
     op_func_name = "conv_transpose3d"
     target_weight_dim_for_compression = 1
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.ConvTranspose3d.__name__
-        args = [module.in_channels, module.out_channels, module.kernel_size, module.stride,
-                module.padding, module.output_padding, module.groups, hasattr(module, 'bias'),
-                module.dilation]
-        if hasattr(module, 'padding_mode'):
+        args = [
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.output_padding,
+            module.groups,
+            hasattr(module, "bias"),
+            module.dilation,
+        ]
+        if hasattr(module, "padding_mode"):
             args.append(module.padding_mode)
         nncf_conv_transpose3d = NNCFConvTranspose3d(*args)
         nncf_conv_transpose3d = align_module_internals(module, nncf_conv_transpose3d)
         return nncf_conv_transpose3d
 
 
 class NNCFEmbedding(_NNCFModuleMixin, nn.Embedding):
     op_func_name = "embedding"
 
     # Note that this does not require activation quantization because it's basically a lookup.
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.Embedding.__name__
 
-        args = [module.num_embeddings, module.embedding_dim, module.padding_idx,
-                module.max_norm, module.norm_type, module.scale_grad_by_freq,
-                module.sparse, module.weight]
+        args = [
+            module.num_embeddings,
+            module.embedding_dim,
+            module.padding_idx,
+            module.max_norm,
+            module.norm_type,
+            module.scale_grad_by_freq,
+            module.sparse,
+            module.weight,
+        ]
         nncf_embedding = NNCFEmbedding(*args)
         nncf_embedding = align_module_internals(module, nncf_embedding)
         return nncf_embedding
 
 
-
 class NNCFEmbeddingBag(_NNCFModuleMixin, nn.EmbeddingBag):
     op_func_name = "embedding_bag"
 
     @staticmethod
     def from_module(module):
         assert module.__class__.__name__ == nn.EmbeddingBag.__name__
 
-        args = [module.num_embeddings, module.embedding_dim,
-                module.max_norm, module.norm_type, module.scale_grad_by_freq,
-                module.mode, module.sparse, module.weight,
-                module.include_last_offset]
+        args = [
+            module.num_embeddings,
+            module.embedding_dim,
+            module.max_norm,
+            module.norm_type,
+            module.scale_grad_by_freq,
+            module.mode,
+            module.sparse,
+            module.weight,
+            module.include_last_offset,
+        ]
         nncf_embedding_bag = NNCFEmbeddingBag(*args)
         nncf_embedding_bag = align_module_internals(module, nncf_embedding_bag)
         return nncf_embedding_bag
 
+
 NNCF_MODULES_DICT = {
     NNCFConv1d: nn.Conv1d,
     NNCFConv2d: nn.Conv2d,
     NNCFConv3d: nn.Conv3d,
     NNCFLinear: nn.Linear,
     NNCFBatchNorm1d: nn.BatchNorm1d,
     NNCFBatchNorm2d: nn.BatchNorm2d,
     NNCFBatchNorm3d: nn.BatchNorm3d,
     NNCFGroupNorm: nn.GroupNorm,
     NNCFLayerNorm: nn.LayerNorm,
     NNCFConvTranspose1d: nn.ConvTranspose1d,
     NNCFConvTranspose2d: nn.ConvTranspose2d,
     NNCFConvTranspose3d: nn.ConvTranspose3d,
     NNCFEmbedding: nn.Embedding,
-    NNCFEmbeddingBag: nn.EmbeddingBag
+    NNCFEmbeddingBag: nn.EmbeddingBag,
 }
 
 NNCF_MODULES_MAP = {k.__name__: v.__name__ for k, v in NNCF_MODULES_DICT.items()}
 NNCF_MODULES = list(NNCF_MODULES_MAP.keys())
 NNCF_MODULES_OP_NAMES = [k.op_func_name for k, _ in NNCF_MODULES_DICT.items()]
 
 NNCF_CONV_MODULES_DICT = {
@@ -377,27 +440,29 @@
     NNCFConvTranspose1d: nn.ConvTranspose1d,
     NNCFConvTranspose2d: nn.ConvTranspose2d,
     NNCFConvTranspose3d: nn.ConvTranspose3d,
 }
 NNCF_PRUNING_MODULES_MAP = {k.__name__: v.__name__ for k, v in NNCF_CONV_MODULES_DICT.items()}
 NNCF_PRUNING_MODULES = list(NNCF_CONV_MODULES_MAP.keys())
 
-UNWRAPPED_USER_MODULES = Registry('user_modules')
+UNWRAPPED_USER_MODULES = Registry("user_modules")
 NNCF_WRAPPED_USER_MODULES_DICT = {}
 
 
+@api(canonical_alias="nncf.torch.register_module")
 def register_module(*quantizable_field_names: str, ignored_algorithms: list = None):
     # quantizable_field_names will work for `weight` attributes only. Should later extend to registering
     # customly named attributes if it becomes necessary
     def wrap(cls):
         UNWRAPPED_USER_MODULES.registry_dict[cls.__name__] = cls
-        nncf_wrapped_module_class_name = 'NNCFUser{}'.format(cls.__name__)
+        nncf_wrapped_module_class_name = "NNCFUser{}".format(cls.__name__)
         NNCF_WRAPPED_USER_MODULES_DICT[cls] = type(nncf_wrapped_module_class_name, (_NNCFModuleMixin, cls), {})
-        get_base_attributes_fn = lambda self: GenericWeightedLayerAttributes(self.weight.requires_grad,
-                                                                             self.weight.shape)
+        get_base_attributes_fn = lambda self: GenericWeightedLayerAttributes(
+            self.weight.requires_grad, self.weight.shape
+        )
         setattr(NNCF_WRAPPED_USER_MODULES_DICT[cls], "get_weight_shape", get_base_attributes_fn)
         if ignored_algorithms:
             setattr(NNCF_WRAPPED_USER_MODULES_DICT[cls], "ignored_algorithms", ignored_algorithms)
         return cls
 
     return wrap
 
@@ -407,15 +472,15 @@
     assert user_class.__name__ in UNWRAPPED_USER_MODULES.registry_dict
     module.__class__ = NNCF_WRAPPED_USER_MODULES_DICT[user_class]
     _NNCFModuleMixin.add_mixin_fields(module)
     return module
 
 
 class RNNCellBaseNNCF(nn.Module):
-    __constants__ = ['input_size', 'hidden_size', 'bias']
+    __constants__ = ["input_size", "hidden_size", "bias"]
 
     def __init__(self, input_size, hidden_size, bias, num_chunks):
         super().__init__()
         self.input_size = input_size
         self.hidden_size = hidden_size
         self.bias = bias
         linear_ih = nn.Linear(input_size, num_chunks * hidden_size, self.bias)
@@ -424,49 +489,53 @@
         self.weight_hh = linear_hh.weight
         self.bias_ih = linear_ih.bias
         self.bias_hh = linear_hh.bias
         self.linear_list = [linear_ih, linear_hh]
         self.reset_parameters()
 
     def extra_repr(self):
-        s = '{input_size}, {hidden_size}'
-        if 'bias' in self.__dict__ and self.bias is not True:
-            s += ', bias={bias}'
-        if 'nonlinearity' in self.__dict__ and self.nonlinearity != "tanh":
-            s += ', nonlinearity={nonlinearity}'
+        s = "{input_size}, {hidden_size}"
+        if "bias" in self.__dict__ and self.bias is not True:
+            s += ", bias={bias}"
+        if "nonlinearity" in self.__dict__ and self.nonlinearity != "tanh":
+            s += ", nonlinearity={nonlinearity}"
         return s.format(**self.__dict__)
 
     def check_forward_input(self, input_):
         if input_.size(1) != self.input_size:
             raise RuntimeError(
-                "input_ has inconsistent input_size: got {}, expected {}".format(
-                    input_.size(1), self.input_size))
+                "input_ has inconsistent input_size: got {}, expected {}".format(input_.size(1), self.input_size)
+            )
 
-    def check_forward_hidden(self, input_, hx, hidden_label=''):
+    def check_forward_hidden(self, input_, hx, hidden_label=""):
         # type: (Tensor, Tensor, str) -> None
         if input_.size(0) != hx.size(0):
             raise RuntimeError(
                 "Input batch size {} doesn't match hidden{} batch size {}".format(
-                    input_.size(0), hidden_label, hx.size(0)))
+                    input_.size(0), hidden_label, hx.size(0)
+                )
+            )
 
         if hx.size(1) != self.hidden_size:
             raise RuntimeError(
                 "hidden{} has inconsistent hidden_size: got {}, expected {}".format(
-                    hidden_label, hx.size(1), self.hidden_size))
+                    hidden_label, hx.size(1), self.hidden_size
+                )
+            )
 
     def reset_parameters(self):
         stdv = 1.0 / math.sqrt(self.hidden_size)
         for weight in self.parameters():
             init.uniform_(weight, -stdv, stdv)
 
     def forward(self, input_, hidden):
         raise NotImplementedError
 
 
-ITERATION_MODULES = Registry('iteration_modules')
+ITERATION_MODULES = Registry("iteration_modules")
 
 
 @ITERATION_MODULES.register()
 class LSTMCellForwardNNCF(nn.Module):
     def __init__(self, input_linear, hidden_linear):
         super().__init__()
         self.input_linear = input_linear
@@ -495,16 +564,16 @@
         self.cell = LSTMCellForwardNNCF(self.linear_list[0], self.linear_list[1])
 
     def forward(self, input_, hidden=None):
         self.check_forward_input(input_)
         if hidden is None:
             zeros = torch.zeros(input_.size(0), self.hidden_size, dtype=input_.dtype, device=input_.device)
             hidden = (zeros, zeros)
-        self.check_forward_hidden(input_, hidden[0], '[0]')
-        self.check_forward_hidden(input_, hidden[1], '[1]')
+        self.check_forward_hidden(input_, hidden[0], "[0]")
+        self.check_forward_hidden(input_, hidden[1], "[1]")
 
         return self.cell(input_, hidden)
 
 
 @ITERATION_MODULES.register()
 class StackedRNN(nn.Module):
     class StackedRNNResetPoint(nn.Module):
@@ -544,19 +613,18 @@
             if self.dropout != 0 and i < self.num_layers - 1:
                 input_ = F.dropout(input_, p=self.dropout, training=self.training, inplace=False)
 
         if self.lstm:
             next_h, next_c = zip(*next_hidden)
             next_hidden = (
                 torch.cat(next_h, 0).view(self.total_layers, *next_h[0].size()),
-                torch.cat(next_c, 0).view(self.total_layers, *next_c[0].size())
+                torch.cat(next_c, 0).view(self.total_layers, *next_c[0].size()),
             )
         else:
-            next_hidden = torch.cat(next_hidden, 0).view(
-                self.total_layers, *next_hidden[0].size())
+            next_hidden = torch.cat(next_hidden, 0).view(self.total_layers, *next_hidden[0].size())
         return next_hidden, input_
 
 
 @ITERATION_MODULES.register()
 class Recurrent(nn.Module):
     def __init__(self, cell, reverse=False):
         super().__init__()
@@ -599,18 +667,18 @@
         input_offset = 0
         last_batch_size = batch_sizes[0]
         hiddens = []
         flat_hidden = not isinstance(hidden, tuple)
         if flat_hidden:
             hidden = (hidden,)
         with forward_nncf_trace():
-            #pylint:disable=unnecessary-comprehension
+            # pylint:disable=unnecessary-comprehension
             batch_size_elements = [b for b in batch_sizes]
         for batch_size in batch_size_elements:
-            step_input = input_[input_offset:input_offset + batch_size]
+            step_input = input_[input_offset : input_offset + batch_size]
             input_offset += batch_size
 
             bs_decrease = last_batch_size - batch_size
             if bs_decrease > 0:
                 hidden_len = len(hidden)
                 hidden_offset_elts = []
                 hidden_offset_elts_reversed = []
@@ -651,20 +719,20 @@
         input_offset = input_.size(0)
         last_batch_size = batch_sizes[-1]
         initial_hidden = hidden
         flat_hidden = not isinstance(hidden, tuple)
         if flat_hidden:
             hidden = (hidden,)
             initial_hidden = (initial_hidden,)
-        hidden = tuple(h[:batch_sizes[-1]] for h in hidden)
+        hidden = tuple(h[: batch_sizes[-1]] for h in hidden)
         for batch_size in reversed(batch_sizes):
             inc = batch_size - last_batch_size
             hidden = self.ReverseResetPoint()(batch_size, hidden, inc, initial_hidden, last_batch_size)
             last_batch_size = batch_size
-            step_input = input_[input_offset - batch_size:input_offset]
+            step_input = input_[input_offset - batch_size : input_offset]
             input_offset -= batch_size
 
             if flat_hidden:
                 hidden = (self.cell(step_input, hidden[0]),)
             else:
                 hidden = self.cell(step_input, hidden)
             output.append(hidden[0])
@@ -680,47 +748,59 @@
         """
         Intentionally wrap concat undef if condition as a separate module
         to prevent adding new node to nncf graph on each iteration
         """
 
         def forward(self, batch_size, hidden, inc, initial_hidden, last_batch_size):
             if inc > 0:
-                hidden = tuple(torch.cat((h, ih[last_batch_size:batch_size]), 0)
-                               for h, ih in zip(hidden, initial_hidden))
+                hidden = tuple(
+                    torch.cat((h, ih[last_batch_size:batch_size]), 0) for h, ih in zip(hidden, initial_hidden)
+                )
             return hidden
 
 
 class NNCF_RNN(nn.Module):
     """Common class for RNN modules. Currently, LSTM is supported only"""
 
-    def __init__(self, mode='LSTM', input_size=1, hidden_size=1, num_layers=1, batch_first=False,
-                 dropout=0, bidirectional=False, bias=True):
+    def __init__(
+        self,
+        mode="LSTM",
+        input_size=1,
+        hidden_size=1,
+        num_layers=1,
+        batch_first=False,
+        dropout=0,
+        bidirectional=False,
+        bias=True,
+    ):
         super().__init__()
         self.mode = mode
         self.input_size = input_size
         self.hidden_size = hidden_size
         self.num_layers = num_layers
         self.bias = bias
         self.batch_first = batch_first
         self.dropout = dropout
         self.bidirectional = bidirectional
         self.num_directions = 2 if bidirectional else 1
 
-        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \
-            isinstance(dropout, bool):
-            raise ValueError("dropout should be a number in range [0, 1] "
-                             "representing the probability of an element being "
-                             "zeroed")
+        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or isinstance(dropout, bool):
+            raise ValueError(
+                "dropout should be a number in range [0, 1] "
+                "representing the probability of an element being "
+                "zeroed"
+            )
         if dropout > 0 and num_layers == 1:
             nncf_logger.debug(
                 f"dropout option adds dropout after all but last recurrent layer, "
                 f"so non-zero dropout expects num_layers greater than 1, "
-                f"but got dropout={dropout} and num_layers={num_layers}")
+                f"but got dropout={dropout} and num_layers={num_layers}"
+            )
 
-        if mode == 'LSTM':
+        if mode == "LSTM":
             gate_size = 4 * hidden_size
             self.cell_type = LSTMCellForwardNNCF
         else:
             # elif mode == 'GRU':
             #     gate_size = 3 * hidden_size
             # elif mode == 'RNN_TANH':
             #     gate_size = hidden_size
@@ -734,18 +814,18 @@
         for layer in range(num_layers):
             for direction in range(self.num_directions):
                 layer_input_size = input_size if layer == 0 else hidden_size * self.num_directions
                 linear_ih = nn.Linear(layer_input_size, gate_size, bias)
                 linear_hh = nn.Linear(hidden_size, gate_size, bias)
                 self.cells.append(self.cell_type(linear_ih, linear_hh))
                 params = (linear_ih.weight, linear_hh.weight, linear_ih.bias, linear_hh.bias)
-                suffix = '_reverse' if direction == 1 else ''
-                weight_names = ['weight_ih_l{}{}', 'weight_hh_l{}{}']
+                suffix = "_reverse" if direction == 1 else ""
+                weight_names = ["weight_ih_l{}{}", "weight_hh_l{}{}"]
                 if bias:
-                    weight_names += ['bias_ih_l{}{}', 'bias_hh_l{}{}']
+                    weight_names += ["bias_ih_l{}{}", "bias_hh_l{}{}"]
                 weight_names = [x.format(layer, suffix) for x in weight_names]
                 for name, param in zip(weight_names, params):
                     setattr(self, name, param)
                 self._all_weights.append(weight_names)
 
         self.reset_parameters()
         self.variable_length = True
@@ -758,53 +838,50 @@
             rec_factory = Recurrent
         inners = []
         for layer_idx in range(self.num_layers):
             idx = layer_idx * self.num_directions
             if self.bidirectional:
                 layer_inners = [rec_factory(cells[idx]), rec_factory(cells[idx + 1], reverse=True)]
             else:
-                layer_inners = [rec_factory(cells[idx]), ]
+                layer_inners = [
+                    rec_factory(cells[idx]),
+                ]
             inners.extend(layer_inners)
-        return StackedRNN(inners,
-                          self.num_layers,
-                          (self.mode == 'LSTM'),
-                          dropout=self.dropout)
+        return StackedRNN(inners, self.num_layers, (self.mode == "LSTM"), dropout=self.dropout)
 
     def check_forward_args(self, input_, hidden, batch_sizes):
         is_input_packed = batch_sizes is not None
         expected_input_dim = 2 if is_input_packed else 3
         if input_.dim() != expected_input_dim:
-            raise RuntimeError(
-                'input_ must have {} dimensions, got {}'.format(
-                    expected_input_dim, input_.dim()))
+            raise RuntimeError("input_ must have {} dimensions, got {}".format(expected_input_dim, input_.dim()))
         if self.input_size != input_.size(-1):
             raise RuntimeError(
-                'input_.size(-1) must be equal to input_size. Expected {}, got {}'.format(
-                    self.input_size, input_.size(-1)))
+                "input_.size(-1) must be equal to input_size. Expected {}, got {}".format(
+                    self.input_size, input_.size(-1)
+                )
+            )
 
         if is_input_packed:
             mini_batch = int(batch_sizes[0])
         else:
             mini_batch = input_.size(0) if self.batch_first else input_.size(1)
 
         expected_hidden_size = (mini_batch, self.hidden_size)
 
-        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):
+        def check_hidden_size(hx, expected_hidden_size, msg="Expected hidden size {}, got {}"):
             expected_size = self.num_layers * self.num_directions
             if expected_size != len(hx):
-                raise RuntimeError('Expected number of hidden states {}, got {}'.format(expected_size, len(hx)))
+                raise RuntimeError("Expected number of hidden states {}, got {}".format(expected_size, len(hx)))
             for element in hx:
                 if tuple(element.size()) != expected_hidden_size:
                     raise RuntimeError(msg.format(expected_hidden_size, tuple(element.size())))
 
-        if self.mode == 'LSTM':
-            check_hidden_size(hidden[0], expected_hidden_size,
-                              'Expected hidden[0] size {}, got {}')
-            check_hidden_size(hidden[1], expected_hidden_size,
-                              'Expected hidden[1] size {}, got {}')
+        if self.mode == "LSTM":
+            check_hidden_size(hidden[0], expected_hidden_size, "Expected hidden[0] size {}, got {}")
+            check_hidden_size(hidden[1], expected_hidden_size, "Expected hidden[1] size {}, got {}")
         else:
             check_hidden_size(hidden, expected_hidden_size)
 
     def reset_parameters(self):
         stdv = 1.0 / math.sqrt(self.hidden_size)
         for weight in self.parameters():
             init.uniform_(weight, -stdv, stdv)
@@ -847,19 +924,22 @@
             max_batch_size = int(batch_sizes[0])
         else:
             batch_sizes = None
             max_batch_size = input_.size(0) if self.batch_first else input_.size(1)
 
         if hidden is None:
             num_directions = 2 if self.bidirectional else 1
-            hidden = torch.zeros(self.num_layers * num_directions,
-                                 max_batch_size, self.hidden_size,
-                                 requires_grad=False,
-                                 device=input_.device)
-            if self.mode == 'LSTM':
+            hidden = torch.zeros(
+                self.num_layers * num_directions,
+                max_batch_size,
+                self.hidden_size,
+                requires_grad=False,
+                device=input_.device,
+            )
+            if self.mode == "LSTM":
                 hidden = (hidden, hidden)
         else:
             # Each batch of the hidden state should match the input sequence that
             # the user believes he/she is passing in.
             hidden = self.prepare_hidden(hidden, sorted_indices)
 
         self.check_forward_args(input_, hidden, batch_sizes)
```

### Comparing `nncf-2.4.0/nncf/torch/model_creation.py` & `nncf-2.5.0/nncf/torch/model_creation.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,217 +7,228 @@
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 from os import path as osp
-from typing import Any
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
+from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import torch
 from torch.distributed import barrier
 from torch.nn import Module
 
 from nncf.api.compression import CompressionAlgorithmController
 from nncf.common.compression import BaseCompressionAlgorithmController as BaseController
-from nncf.common.utils.debug import set_debug_log_dir
 from nncf.common.logging import nncf_logger
+from nncf.common.utils.api_marker import api
+from nncf.common.utils.debug import set_debug_log_dir
 from nncf.config import NNCFConfig
 from nncf.config.extractors import extract_algorithm_names
-from nncf.config.structures import ModelEvaluationArgs
 from nncf.config.telemetry_extractors import CompressionStartedFromConfig
-from nncf.config.utils import is_accuracy_aware_training
 from nncf.telemetry import tracked_function
 from nncf.telemetry.events import NNCF_PT_CATEGORY
-from nncf.torch.algo_selector import NoCompressionAlgorithmBuilder
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
+from nncf.torch.algo_selector import NoCompressionAlgorithmBuilder
 from nncf.torch.composite_compression import PTCompositeCompressionAlgorithmBuilder
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.dynamic_graph.graph_tracer import create_input_infos
 from nncf.torch.nncf_network import NNCFNetwork
+
+# pylint:disable=too-many-branches
 from nncf.torch.utils import is_dist_avail_and_initialized
 from nncf.torch.utils import is_main_process
-# pylint:disable=too-many-branches
 from nncf.torch.utils import maybe_convert_legacy_names_in_compress_state
+from nncf.torch.utils import training_mode_switcher
 
 
-@tracked_function(NNCF_PT_CATEGORY, [CompressionStartedFromConfig(argname="config"), ])
-def create_compressed_model(model: Module,
-                            config: NNCFConfig,
-                            compression_state: Optional[Dict[str, Any]] = None,
-                            dummy_forward_fn: Callable[[Module], Any] = None,
-                            wrap_inputs_fn: Callable[[Tuple, Dict], Tuple[Tuple, Dict]] = None,
-                            wrap_outputs_fn: Callable[[Tuple, Dict], Tuple[Tuple, Dict]] = None,
-                            dump_graphs=True) \
-    -> Tuple[CompressionAlgorithmController, NNCFNetwork]:
+@api(canonical_alias="nncf.torch.create_compressed_model")
+@tracked_function(
+    NNCF_PT_CATEGORY,
+    [
+        CompressionStartedFromConfig(argname="config"),
+    ],
+)
+def create_compressed_model(
+    model: Module,
+    config: NNCFConfig,
+    compression_state: Optional[Dict[str, Any]] = None,
+    dummy_forward_fn: Callable[[Module], Any] = None,
+    wrap_inputs_fn: Callable[[Tuple, Dict], Tuple[Tuple, Dict]] = None,
+    wrap_outputs_fn: Callable[[Tuple, Dict], Tuple[Tuple, Dict]] = None,
+    dump_graphs=True,
+) -> Tuple[CompressionAlgorithmController, NNCFNetwork]:
     """
     The main function used to produce a model ready for compression fine-tuning from an original PyTorch
     model and a configuration object.
     dummy_forward_fn
     :param model: The original model. Should have its parameters already loaded from a checkpoint or another
-    source.
+        source.
     :param config: A configuration object used to determine the exact compression modifications to be applied
-    to the model
+        to the model
+    :type config: nncf.NNCFConfig
     :param compression_state: representation of the entire compression state to unambiguously restore
-    the compressed model. Includes builder and controller states.
+        the compressed model. Includes builder and controller states.
     :param dummy_forward_fn: if supplied, will be used instead of a *forward* function call to build
-    the internal graph representation via tracing. Specifying this is useful when the original training pipeline
-    has special formats of data loader output or has additional *forward* arguments other than input tensors.
-    Otherwise, the *forward* call of the model during graph tracing will be made with mock tensors according
-    to the shape specified in the config object. The dummy_forward_fn code MUST contain calls to nncf.nncf_model_input
-    functions made with each compressed model input tensor in the underlying model's args/kwargs tuple, and these
-    calls should be exactly the same as in the wrap_inputs_fn function code (see below); if dummy_forward_fn is
-    specified, then wrap_inputs_fn also must be specified.
+        the internal graph representation via tracing. Specifying this is useful when the original training pipeline
+        has special formats of data loader output or has additional *forward* arguments other than input tensors.
+        Otherwise, the *forward* call of the model during graph tracing will be made with mock tensors according
+        to the shape specified in the config object. The dummy_forward_fn code MUST contain calls to
+        nncf.nncf_model_input functions made with each compressed model input tensor in the underlying model's
+        args/kwargs tuple, and these calls should be exactly the same as in the wrap_inputs_fn function code
+        (see below); if dummy_forward_fn is specified, then wrap_inputs_fn also must be specified.
     :param wrap_inputs_fn: if supplied, will be used on the module's input arguments during a regular, non-dummy
-    forward call before passing the inputs to the underlying compressed model. This is required if the model's input
-    tensors that are important for compression are not supplied as arguments to the model's forward call directly, but
-    instead are located in a container (such as list), and the model receives the container as an argument.
-    wrap_inputs_fn should take as input two arguments - the tuple of positional arguments to the underlying
-    model's forward call, and a dict of keyword arguments to the same. The function should wrap each tensor among the
-    supplied model's args and kwargs that is important for compression (e.g. quantization) with an nncf.nncf_model_input
-    function, which is a no-operation function and marks the tensors as inputs to be traced by NNCF in the internal
-    graph representation. Output is the tuple of (args, kwargs), where args and kwargs are the same as were supplied in
-    input, but each tensor in the original input. Must be specified if dummy_forward_fn is specified.
+        forward call before passing the inputs to the underlying compressed model. This is required if the model's
+        input tensors that are important for compression are not supplied as arguments to the model's forward call
+        directly, but instead are located in a container (such as list), and the model receives the container as an
+        argument. wrap_inputs_fn should take as input two arguments - the tuple of positional arguments to the
+        underlying model's forward call, and a dict of keyword arguments to the same. The function should wrap each
+        tensor among nncf.nncf_model_input function, which is a no-operation function and marks the tensors as inputs
+        to be traced by NNCF in the internal graph representation. Output is the tuple of (args, kwargs), where args
+        and kwargs are the same as were supplied in input, but each tensor in the original input. Must be specified
+        if dummy_forward_fn is specified.
     :param wrap_outputs_fn: same as `wrap_inputs_fn`, but applies to model outputs
     :param dump_graphs: Whether to dump the internal graph representation of the
-    original and compressed models in the .dot format into the log directory.
+        original and compressed models in the .dot format into the log directory.
     :return: A controller for the compression algorithm (or algorithms, in which case the controller
-    is an instance of CompositeCompressionController) and the model ready for compression parameter training wrapped
-    as an object of NNCFNetwork."""
+        is an instance of CompositeCompressionController) and the model ready for compression parameter training wrapped
+        as an object of NNCFNetwork.
+    """
 
     set_debug_log_dir(config.get("log_dir", "."))
 
-    is_legacy_model_state_dict = compression_state is not None and \
-                                 BaseController.BUILDER_STATE not in compression_state and \
-                                 BaseController.CONTROLLER_STATE not in compression_state
+    is_legacy_model_state_dict = (
+        compression_state is not None
+        and BaseController.BUILDER_STATE not in compression_state
+        and BaseController.CONTROLLER_STATE not in compression_state
+    )
     maybe_convert_legacy_names_in_compress_state(compression_state)
 
     should_init = compression_state is None
 
     nncf_network = create_nncf_network(model, config, dummy_forward_fn, wrap_inputs_fn, wrap_outputs_fn)
 
     if dump_graphs and is_main_process():
-        nncf_network.get_graph().visualize_graph(osp.join(config.get("log_dir", "."), "original_graph.dot"))
-
+        nncf_network.nncf.get_graph().visualize_graph(osp.join(config.get("log_dir", "."), "original_graph.dot"))
     builder = create_compression_algorithm_builder(config, should_init)
 
     is_state_loadable = not is_legacy_model_state_dict and compression_state is not None
     if is_state_loadable:
         builder.load_state(compression_state[BaseController.BUILDER_STATE])
     compressed_model = builder.apply_to(nncf_network)
     compression_ctrl = builder.build_controller(compressed_model)
+
     if is_state_loadable:
         compression_ctrl.load_state(compression_state[BaseController.CONTROLLER_STATE])
 
+    compressed_model.nncf.set_compression_controller(compression_ctrl)
+
     # Required to ensure that the model leaving create_compressed_model has correct compressed graph.
     # In particular, this is currently required for correct functioning of RNNs.
-    compressed_model.rebuild_graph()
+    compressed_model.nncf.rebuild_graph()
 
     try:
         if is_legacy_model_state_dict:
             from nncf.torch import load_state  # pylint: disable=cyclic-import
-            state_dict_to_load = compression_state.get('state_dict', compression_state)
+
+            state_dict_to_load = compression_state.get("state_dict", compression_state)
             load_state(compressed_model, state_dict_to_load, is_resume=True)
     finally:
         if dump_graphs and is_main_process():
-            compressed_model_graph = compressed_model.get_graph()
+            compressed_model_graph = compressed_model.nncf.get_graph()
             compressed_model_graph.visualize_graph(osp.join(config.get("log_dir", "."), "compressed_graph.dot"))
 
     synchronize_all_processes_in_distributed_mode()
     return compression_ctrl, compressed_model
 
 
-def create_nncf_network(model,
-                        config: NNCFConfig,
-                        dummy_forward_fn: Callable[[Module], Any] = None,
-                        wrap_inputs_fn: Callable[[Tuple, Dict], Tuple[Tuple, Dict]] = None,
-                        wrap_outputs_fn: Callable[[Tuple, Dict], Tuple[Tuple, Dict]] = None) -> NNCFNetwork:
+def create_nncf_network(
+    model: torch.nn.Module,
+    config: NNCFConfig,
+    dummy_forward_fn: Callable[[Module], Any] = None,
+    wrap_inputs_fn: Callable = None,
+    wrap_outputs_fn: Callable = None,
+) -> NNCFNetwork:
     """
     The main function used to produce a model ready for adding compression from an original PyTorch
     model and a configuration object.
-    dummy_forward_fn
+
     :param model: The original model. Should have its parameters already loaded from a checkpoint or another
-    source.
+        source.
     :param config: A configuration object used to determine the exact compression modifications to be applied
-    to the model
+        to the model
     :param dummy_forward_fn: if supplied, will be used instead of a *forward* function call to build
-    the internal graph representation via tracing. Specifying this is useful when the original training pipeline
-    has special formats of data loader output or has additional *forward* arguments other than input tensors.
-    Otherwise, the *forward* call of the model during graph tracing will be made with mock tensors according
-    to the shape specified in the config object. The dummy_forward_fn code MUST contain calls to nncf.nncf_model_input
-    functions made with each compressed model input tensor in the underlying model's args/kwargs tuple, and these
-    calls should be exactly the same as in the wrap_inputs_fn function code (see below); if dummy_forward_fn is
-    specified, then wrap_inputs_fn also must be specified.
+        the internal graph representation via tracing. Specifying this is useful when the original training pipeline
+        has special formats of data loader output or has additional *forward* arguments other than input tensors.
+        Otherwise, the *forward* call of the model during graph tracing will be made with mock tensors according
+        to the shape specified in the config object. The dummy_forward_fn code MUST contain calls to
+        nncf.nncf_model_input
+        functions made with each compressed model input tensor in the underlying model's args/kwargs tuple, and these
+        calls should be exactly the same as in the wrap_inputs_fn function code (see below); if dummy_forward_fn is
+        specified, then wrap_inputs_fn also must be specified.
     :param wrap_inputs_fn: if supplied, will be used on the module's input arguments during a regular, non-dummy
-    forward call before passing the inputs to the underlying compressed model. This is required if the model's input
-    tensors that are important for compression are not supplied as arguments to the model's forward call directly, but
-    instead are located in a container (such as list), and the model receives the container as an argument.
-    wrap_inputs_fn should take as input two arguments - the tuple of positional arguments to the underlying
-    model's forward call, and a dict of keyword arguments to the same. The function should wrap each tensor among the
-    supplied model's args and kwargs that is important for compression (e.g. quantization) with an nncf.nncf_model_input
-    function, which is a no-operation function and marks the tensors as inputs to be traced by NNCF in the internal
-    graph representation. Output is the tuple of (args, kwargs), where args and kwargs are the same as were supplied in
-    input, but each tensor in the original input. Must be specified if dummy_forward_fn is specified.
+        forward call before passing the inputs to the underlying compressed model. This is required if the model's input
+        tensors that are important for compression are not supplied as arguments to the model's forward call directly,
+        but instead are located in a container (such as list), and the model receives the container as an argument.
+        wrap_inputs_fn should take as input two arguments - the tuple of positional arguments to the underlying
+        model's forward call, and a dict of keyword arguments to the same. The function should wrap each tensor among
+        the supplied model's args and kwargs that is important for compression (e.g. quantization) with an
+        nncf.nncf_model_input function, which is a no-operation function and marks the tensors as inputs to be traced
+        by NNCF in the internal graph representation. Output is the tuple of (args, kwargs), where args and kwargs are
+        the same as were supplied in input, but each tensor in the original input. Must be specified if
+        dummy_forward_fn is specified.
     :param wrap_outputs_fn: if supplied, will be used on the module's output during a regular, non-dummy forward call.
-    :return: A model wrapped by NNCFNetwork, which is ready for adding compression. """
+    :return: A model wrapped by NNCFNetwork, which is ready for adding compression."""
 
     if dummy_forward_fn is not None and wrap_inputs_fn is None:
         raise ValueError(
             "A custom dummy forward function was specified, but the corresponding input wrapping function "
             "was not. In case a custom dummy forward function is specified for purposes of NNCF graph "
             "building, then the wrap_inputs_fn parameter MUST also be specified and be consistent with "
-            "the input wrapping done in dummy_forward_fn.")
+            "the input wrapping done in dummy_forward_fn."
+        )
 
-    # Compress model that will be deployed for the inference on target device. No need to compress parts of the
-    # model that are used on training stage only (e.g. AuxLogits of Inception-v3 model) or unused modules with weights.
-    # As a consequence, no need to care about spoiling BN statistics, as they're disabled in eval mode.
-    model.eval()
-
-    input_info_list = create_input_infos(config)
-    scopes_without_shape_matching = config.get('scopes_without_shape_matching', [])
-    ignored_scopes = config.get('ignored_scopes')
-    target_scopes = config.get('target_scopes')
-
-    original_model_accuracy = None
-    if is_accuracy_aware_training(config):
-        if config.has_extra_struct(ModelEvaluationArgs):
-            evaluation_args = config.get_extra_struct(ModelEvaluationArgs)
-            with torch.no_grad():
-                original_model_accuracy = evaluation_args.eval_fn(model)
-                nncf_logger.info(f"Uncompressed model accuracy = {original_model_accuracy}")
-
-    nncf_network = NNCFNetwork(model, input_infos=input_info_list,
-                               dummy_forward_fn=dummy_forward_fn,
-                               wrap_inputs_fn=wrap_inputs_fn,
-                               wrap_outputs_fn=wrap_outputs_fn,
-                               ignored_scopes=ignored_scopes,
-                               target_scopes=target_scopes,
-                               scopes_without_shape_matching=scopes_without_shape_matching,
-                               original_model_accuracy=original_model_accuracy)
+    # Preserve `.training`/`.requires_grad` state since we will be building NNCFNetwork in `.eval` mode
+    with training_mode_switcher(model, is_training=False):
+        # Compress model that will be deployed for the inference on target device. No need to compress parts of the
+        # model that are used on training stage only (e.g. AuxLogits of Inception-v3 model) or unused modules with
+        # weights. As a consequence, no need to care about spoiling BN statistics, as they're disabled in eval mode.
+
+        input_info_list = create_input_infos(config)
+        scopes_without_shape_matching = config.get("scopes_without_shape_matching", [])
+        ignored_scopes = config.get("ignored_scopes")
+        target_scopes = config.get("target_scopes")
+
+        nncf_network = NNCFNetwork(
+            model,
+            input_infos=input_info_list,
+            dummy_forward_fn=dummy_forward_fn,
+            wrap_inputs_fn=wrap_inputs_fn,
+            wrap_outputs_fn=wrap_outputs_fn,
+            ignored_scopes=ignored_scopes,
+            target_scopes=target_scopes,
+            scopes_without_shape_matching=scopes_without_shape_matching,
+        )
 
-    nncf_network.get_tracing_context().disable_trace_dynamic_graph()
+        nncf_network.nncf.get_tracing_context().disable_trace_dynamic_graph()
 
     synchronize_all_processes_in_distributed_mode()
     return nncf_network
 
 
 def synchronize_all_processes_in_distributed_mode():
     if is_dist_avail_and_initialized():
         try:
             barrier()
         # Exception can be raised during running barrier
         # if the backend not in the supported list https://pytorch.org/docs/stable/distributed.html
         except RuntimeError as err:
-            nncf_logger.warning("Training pipeline spawned an error while "
-                                "synchronizing distributed training processes:")
+            nncf_logger.warning(
+                "Training pipeline spawned an error while synchronizing distributed training processes:"
+            )
             nncf_logger.warning(err)
             nncf_logger.warning("Desynchronization of distributed processes may occur.")
 
 
 def create_compression_algorithm_builder(config: NNCFConfig, should_init=True) -> PTCompressionAlgorithmBuilder:
     """
     Create compression algorithm builders by a given list of algorithm names.
@@ -228,17 +239,17 @@
     the training parameters of the model during model building.
     :return: compression algorithm builder
     """
     algo_names = extract_algorithm_names(config)
     return create_compression_algorithm_builder_from_algo_names(algo_names, config, should_init)
 
 
-def create_compression_algorithm_builder_from_algo_names(algo_names: List[str],
-                                                         config: NNCFConfig,
-                                                         should_init: bool) -> PTCompressionAlgorithmBuilder:
+def create_compression_algorithm_builder_from_algo_names(
+    algo_names: List[str], config: NNCFConfig, should_init: bool
+) -> PTCompressionAlgorithmBuilder:
     """
     Create compression algorithm builders by a given list of algorithm names.
 
     :param algo_names: list of algorithm names
     :param config: A configuration object used to determine the exact compression modifications to be applied
     to the model
     :param should_init: The flag indicates that the generated compression builder will initialize (True) or not (False)
```

### Comparing `nncf-2.4.0/nncf/torch/module_operations.py` & `nncf-2.5.0/nncf/torch/module_operations.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,18 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Callable
-from typing import List
-from typing import Optional
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Callable, List, Optional
 
 from torch import nn
 
 from nncf.torch.layers import NNCF_PADDING_VALUE_ATTR_NAME
 
 
 class BaseOp(nn.Module):
@@ -33,40 +29,43 @@
 
 
 class UpdateInputs(BaseOp):
     """
     A module which updates inputs for a module
     fed to forward method call by operand call.
     """
+
     def __call__(self, _, inputs):
         return super().__call__(*inputs)
 
 
 class UpdateParameter(BaseOp):
     """
     A module which updates the attribute by a given of a module
     fed to forward method call by operand call.
     """
+
     def __init__(self, param_name, op):
         super().__init__(op)
         self._param_name = param_name
 
     def __call__(self, module, _):
         if not hasattr(module, self._param_name):
-            raise TypeError('{} should have {} attribute'.format(type(module), self._param_name))
+            raise TypeError("{} should have {} attribute".format(type(module), self._param_name))
         value = getattr(module, self._param_name)
         result = super().__call__(value)
         setattr(module, self._param_name, result)
 
 
 class UpdateWeight(UpdateParameter):
     """
     A module which updates `weight` attributes of a module
     fed to forward method call by operand call.
     """
+
     def __init__(self, op):
         super().__init__("weight", op)
 
 
 class UpdateParameterList(BaseOp):
     """
     A module which updates attributes by a given list of names of a module fed to
@@ -83,15 +82,15 @@
     def __call__(self, module, _):
         param_values = []
         for param_name, is_optional in zip(self._param_names, self._is_optional_list):
             if not hasattr(module, param_name):
                 if is_optional:
                     param_values.append(None)
                     continue
-                raise TypeError('{} should have {} attribute'.format(type(module), param_name))
+                raise TypeError("{} should have {} attribute".format(type(module), param_name))
             param_values.append(getattr(module, param_name))
         updated_kwargs = dict(zip(self._param_names, param_values))
         updated_values = super().__call__(**updated_kwargs)
 
         for param_name, updated_value in zip(self._param_names, updated_values):
             setattr(module, param_name, updated_value)
 
@@ -108,54 +107,60 @@
 
 class UpdateWeightAndOptionalBias(UpdateParameterList):
     """
     A module which updates `weight` and optionally `bias` attributes of a module
     fed to forward method call by operand call. If the module doesn't have bias attribute, None will be passed instead
     of it.
     """
+
     def __init__(self, op):
         super().__init__(["weight", "bias"], op, [False, True])
 
 
 class UpdatePaddingValue(UpdateParameter):
     """
     A module which updates `nncf_padding` attributes of a module
     fed to forward method call by operand call. Eventually, that will be used to apply a custom padding value.
     """
+
     def __init__(self, op):
         super().__init__(NNCF_PADDING_VALUE_ATTR_NAME, op)
 
 
 class UpdateNumGroups(UpdateParameter):
     """
     A module which updates `groups` attribute of a module
     fed to forward method call by operand call.
     """
+
     def __init__(self, op):
-        super().__init__('groups', op)
+        super().__init__("groups", op)
 
 
 class UpdatePadding(UpdateParameter):
     """
     A module which updates `padding` attribute of a module
     fed to forward method call by operand call.
     """
+
     def __init__(self, op):
         super().__init__("padding", op)
 
 
 class UpdateBatchNormParams(UpdateParameterList):
     """
     A module which updates attribute of batch norm module
     fed to forward method call by operand call.
     """
+
     def __init__(self, op):
         super().__init__(["weight", "bias", "running_mean", "running_var"], op)
 
 
 class UpdateLayerNormParams(UpdateParameterList):
     """
     A module which updates attribute of layer norm module
     fed to forward method call by operand call.
     """
+
     def __init__(self, op):
         super().__init__(["weight", "bias", "normalized_shape"], op)
```

### Comparing `nncf-2.4.0/nncf/torch/nested_objects_traversal.py` & `nncf-2.5.0/nncf/torch/nested_objects_traversal.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,33 +1,26 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from functools import partial
-from typing import Callable, Mapping, Sequence, Set, List, Type, Any, Dict
-from typing import Tuple
-from typing import Union
-
+from typing import Any, Callable, Dict, List, Mapping, Sequence, Set, Tuple, Type, Union
 
 string_types = (str, bytes)
-iteritems = lambda mapping: getattr(mapping, 'iteritems', mapping.items)()
+iteritems = lambda mapping: getattr(mapping, "iteritems", mapping.items)()
 
 
-def to_tuple(lst: List,
-             named_tuple_class: Type = None,
-             named_tuple_fields: List[str] = None) -> Tuple:
+def to_tuple(lst: List, named_tuple_class: Type = None, named_tuple_fields: List[str] = None) -> Tuple:
     # Able to produce namedtuples if a corresponding parameter is given
     if named_tuple_fields is None:
         return tuple(lst)
     return named_tuple_class(*lst)
 
 
 def is_tuple(obj) -> bool:
@@ -36,15 +29,15 @@
 
 def is_named_tuple(obj) -> bool:
     return is_tuple(obj) and (obj.__class__ != tuple)
 
 
 def maybe_get_iterator(obj):
     it = None
-        # pylint:disable=isinstance-second-argument-not-valid-type
+    # pylint:disable=isinstance-second-argument-not-valid-type
     if isinstance(obj, Mapping):
         it = iteritems
         # pylint:disable=isinstance-second-argument-not-valid-type
     elif isinstance(obj, (Sequence, Set)) and not isinstance(obj, string_types):
         it = enumerate
     return it
 
@@ -86,36 +79,34 @@
                 current_level_getters = []
                 current_level_setters = []
                 for idx, iterval in enumerate(iterator(obj)):
                     path_component, value = iterval
                     current_level_getters.append(partial(obj.__getitem__, path_component))
                     if not isinstance(obj, tuple):
                         # `range` objects, for instance, have no __setitem__ and should be disregarded
-                        if hasattr(obj, '__setitem__'):
+                        if hasattr(obj, "__setitem__"):
                             current_level_setters.append(partial(obj.__setitem__, path_component))
                         else:
                             current_level_setters.append(None)
                     else:
                         current_level_setters.append(TupleRebuildingSetter(idx, obj, previous_level_setter))
 
                 for idx, iterval in enumerate(iterator(obj)):
                     path_component, value = iterval
-                    retval = NestedObjectIndex._nested_object_paths_generator(value, out_entries_list,
-                                                                         path + (path_component,), memo,
-                                                                         current_level_setters[idx])
+                    retval = NestedObjectIndex._nested_object_paths_generator(
+                        value, out_entries_list, path + (path_component,), memo, current_level_setters[idx]
+                    )
                     was_leaf = retval[1]
                     if was_leaf:
                         leaf_entry_path = retval
                         # getter = partial(obj.__getitem__, path_component)
                         getter = current_level_getters[idx]
                         setter = current_level_setters[idx]
                         if setter is not None:  # see note above about non-settable objects
-                            out_entries_list.append(InputIndexEntry(leaf_entry_path,
-                                                                    getter,
-                                                                    setter))
+                            out_entries_list.append(InputIndexEntry(leaf_entry_path, getter, setter))
 
                 memo.remove(id(obj))
             is_leaf = False
             return path, is_leaf
 
         is_leaf = True
         return path, is_leaf
@@ -125,24 +116,24 @@
 
 
 def objwalk(obj, unary_predicate: Callable[[Any], bool], apply_fn: Callable, memo=None):
     """
     Walks through the indexable container hierarchy of obj and replaces all sub-objects matching a criterion
     with the result of a given function application.
     """
-    #pylint:disable=too-many-nested-blocks
-    #pylint:disable=too-many-branches
+    # pylint:disable=too-many-nested-blocks
+    # pylint:disable=too-many-branches
     if memo is None:
         memo = set()
 
     named_tuple_class = None
     named_tuple_fields = None
     if is_named_tuple(obj):
         named_tuple_class = obj.__class__
-        #pylint:disable=protected-access
+        # pylint:disable=protected-access
         named_tuple_fields = obj._fields
 
     was_tuple = is_tuple(obj)
     if was_tuple:
         obj = list(obj)
 
     iterator = maybe_get_iterator(obj)
```

### Comparing `nncf-2.4.0/nncf/torch/nncf_network.py` & `nncf-2.5.0/nncf/torch/nncf_network.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,116 +1,85 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# pylint: disable=too-many-lines
+import functools
 import inspect
+import types
 from collections import OrderedDict
+from contextlib import contextmanager
+from copy import deepcopy
 from enum import Enum
 from enum import IntEnum
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
-from typing import TypeVar
+from typing import Callable, Dict, Iterator, List, Optional, Tuple, TypeVar
 
-import functools
 import torch
-from copy import deepcopy
 from torch import nn
 
-from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
-from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME
+from nncf import nncf_logger
+from nncf.common.deprecation import warning_deprecated
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
+from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
+from nncf.common.graph.definitions import MODEL_OUTPUT_OP_NAME
 from nncf.common.graph.model_transformer import ModelTransformer
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.transformations.commands import TransformationPriority
 from nncf.common.insertion_point_graph import InsertionPointGraph
 from nncf.common.insertion_point_graph import PostHookInsertionPoint
 from nncf.common.insertion_point_graph import PreHookInsertionPoint
-from nncf.common.logging import nncf_logger
+from nncf.common.utils.debug import is_debug
 from nncf.torch.debug import CombinedDebugInterface
 from nncf.torch.debug import debuggable_forward
-from nncf.common.utils.debug import is_debug
 from nncf.torch.dynamic_graph.context import TracingContext
 from nncf.torch.dynamic_graph.graph import DynamicGraph
 from nncf.torch.dynamic_graph.graph import ShapeIgnoringTensorMetaComparator
 from nncf.torch.dynamic_graph.graph_tracer import GraphTracer
 from nncf.torch.dynamic_graph.graph_tracer import ModelInputInfo
-from nncf.torch.dynamic_graph.graph_tracer import PostGraphBuildActing
 from nncf.torch.dynamic_graph.graph_tracer import create_dummy_forward_fn
 from nncf.torch.dynamic_graph.io_handling import InputInfoWrapManager
 from nncf.torch.dynamic_graph.io_handling import replicate_same_tensors
 from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_outputs_with_objwalk
 from nncf.torch.dynamic_graph.operation_address import OperationAddress
-from nncf.torch.dynamic_graph.patch_pytorch import ignore_scope
+from nncf.torch.dynamic_graph.patch_pytorch import ORIGINAL_CALL
 from nncf.torch.dynamic_graph.scope import Scope
+from nncf.torch.dynamic_graph.scope_access import get_module_by_scope
 from nncf.torch.dynamic_graph.trace_tensor import TracedTensor
-from nncf.torch.dynamic_graph.transform_graph import replace_modules_by_nncf_modules
+from nncf.torch.dynamic_graph.wrappers import wrap_module_call
 from nncf.torch.graph.graph import PTNNCFGraph
 from nncf.torch.graph.graph_builder import GraphBuilder
 from nncf.torch.graph.graph_builder import GraphConverter
 from nncf.torch.graph.operator_metatypes import OPERATORS_WITH_WEIGHTS_METATYPES
 from nncf.torch.graph.operator_metatypes import PTSplitMetatype
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.knowledge_distillation.knowledge_distillation_handler import KnowledgeDistillationLossHandler
 from nncf.torch.layer_utils import _NNCFModuleMixin
-from nncf.torch.layers import NNCF_MODULES
-from nncf.torch.layers import NNCF_WRAPPED_USER_MODULES_DICT
 from nncf.torch.module_operations import UpdateWeight
-from nncf.torch.quantization.layers import QUANTIZATION_MODULES
+from nncf.torch.nested_objects_traversal import objwalk
+from nncf.torch.nncf_module_replacement import replace_modules_by_nncf_modules
 from nncf.torch.utils import compute_FLOPs_hook
 from nncf.torch.utils import get_all_modules_by_type
 from nncf.torch.utils import get_model_device
-from nncf.torch.utils import get_state_dict_names_with_modules
-from nncf.torch.nested_objects_traversal import objwalk
-
-MODEL_WRAPPED_BY_NNCF_ATTR_NAME = 'nncf_module'
-LEGACY_ACT_STORAGE_NAME = "activation_quantizers"
-EXTERNAL_QUANTIZERS_STORAGE_NAME = "external_quantizers"
-
-Module = TypeVar('Module', bound=nn.Module)
+from nncf.torch.utils import training_mode_switcher
 
+LEGACY_MODEL_WRAPPED_BY_NNCF_ATTR_NAME = "nncf_module"
+LEGACY_EXTERNAL_QUANTIZERS_STORAGE_PREFIX = "external_quantizers"
 
-class ExtraCompressionModuleType(Enum):
-    EXTERNAL_QUANTIZER = 0
-
-
-class LoadStateListener:
-    """
-        Resets the initialization flags (`initialized`) for all quantization modules on `load_state_dict` call.
-        These flags are used to update not loaded params (from checkpoint or model's state)
-        on initialization stage of algorithm.
-        Flags reset is required on each call of `load_state_dict`, because internal method (`build_graph`)
-        restores model state by calling this method.
-    """
-
-    def __init__(self, model: 'NNCFNetwork', all_quantizations: Dict[str, torch.nn.Module]):
-        # pylint: disable=protected-access
-        self.hook = model._register_load_state_dict_pre_hook(
-            functools.partial(self.hook_fn, quantize_modules=list(all_quantizations.values())))
-
-    def hook_fn(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs,
-                quantize_modules: List[torch.nn.Module]):
-        for module in quantize_modules:
-            module.initialized = False
+EXTERNAL_QUANTIZERS_STORAGE_NAME = "external_quantizers"
+CURRENT_EXTERNAL_QUANTIZERS_STORAGE_PREFIX = "_nncf." + EXTERNAL_QUANTIZERS_STORAGE_NAME
 
-    def close(self):
-        self.hook.remove()
+Module = TypeVar("Module", bound=nn.Module)
 
 
 class PTInsertionType(IntEnum):
     NNCF_MODULE_PRE_OP = 0
     NNCF_MODULE_POST_OP = 1
     OPERATOR_PRE_HOOK = 2
     OPERATOR_POST_HOOK = 3
@@ -118,269 +87,348 @@
 
 class PTInsertionPoint:
     TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT = {
         TargetType.PRE_LAYER_OPERATION: PTInsertionType.NNCF_MODULE_PRE_OP,
         TargetType.POST_LAYER_OPERATION: PTInsertionType.NNCF_MODULE_POST_OP,
         TargetType.OPERATION_WITH_WEIGHTS: PTInsertionType.NNCF_MODULE_PRE_OP,
         TargetType.OPERATOR_PRE_HOOK: PTInsertionType.OPERATOR_PRE_HOOK,
-        TargetType.OPERATOR_POST_HOOK: PTInsertionType.OPERATOR_POST_HOOK
+        TargetType.OPERATOR_POST_HOOK: PTInsertionType.OPERATOR_POST_HOOK,
     }
 
     def _get_pt_insertion_type(self, target_type: TargetType) -> PTInsertionType:
-        if not isinstance(target_type, TargetType) or\
-            target_type not in PTInsertionPoint.TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT:
+        if (
+            not isinstance(target_type, TargetType)
+            or target_type not in PTInsertionPoint.TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT
+        ):
             raise RuntimeError("Unsupported target type for PyTorch: {}".format(target_type))
         return PTInsertionPoint.TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT[target_type]
 
-    def __init__(self, target_type: TargetType, op_address: OperationAddress,
-                 input_port_id: int = None):
+    def __init__(self, target_type: TargetType, op_address: OperationAddress, input_port_id: int = None):
         self.insertion_type = self._get_pt_insertion_type(target_type)
         self.op_address = op_address
         self.module_scope = op_address.scope_in_model
         self.input_port_id = input_port_id
 
-    def __eq__(self, other: 'PTInsertionPoint'):
-        return self.insertion_type == other.insertion_type and \
-               self.op_address == other.op_address and \
-               self.module_scope == other.module_scope and \
-               self.input_port_id == other.input_port_id
+    def __eq__(self, other: "PTInsertionPoint"):
+        return (
+            self.insertion_type == other.insertion_type
+            and self.op_address == other.op_address
+            and self.module_scope == other.module_scope
+            and self.input_port_id == other.input_port_id
+        )
 
     def __str__(self):
-        return ' '.join([str(v) for v in self.__dict__.values()])
+        return " ".join([str(v) for v in self.__dict__.values()])
 
     def __hash__(self):
         return hash(str(self))
 
-# pylint: disable=too-many-public-methods
 
+class ExtraCompressionModuleType(Enum):
+    EXTERNAL_QUANTIZER = 0
+
+
+class NNCFNetworkInterface(torch.nn.Module):
+    """
+    The single object that is added to the original model object as an attribute to provide a namespace for
+    NNCF-specific method calls and a torch.nn.Module-like storage for compression parameters. Since this is a
+    Module stored in a Module, all trainable parameters of the NNCFInterface will be registered for optimization
+    in the same manner as the original model parameters, and will also be eligible for state_dict-powered persistence
+    when saving/loading checkpoints
+    """
 
-@ignore_scope
-class NNCFNetwork(nn.Module, PostGraphBuildActing):
-    MODEL_STATE_VERSION_ATTR = '_nncf_model_state_version'
+    # pylint:disable=too-many-public-methods
+    MODEL_STATE_VERSION_ATTR = "_nncf_model_state_version"
     MODEL_STATE_VERSION = 1
 
-    def __init__(self, module, input_infos: List[ModelInputInfo],
-                 dummy_forward_fn=None, wrap_inputs_fn=None, scopes_without_shape_matching=None,
-                 ignored_scopes=None, target_scopes=None, reset: bool = False, wrap_outputs_fn=None,
-                 original_model_accuracy=None):
+    def forward(self):
+        """
+        The module only serves a storage and namespacing purpose, forward functionality is not implemented.
+        """
+        raise NotImplementedError("Calling `forward` on NNCFInterface is prohibited.")
+
+    def get_original_forward(self) -> Callable:
+        """
+        Returns the forward function of the original model, unmodified by NNCF. The returned function will
+        have its 0-th implicit `self` argument bound to the model object.
+        """
+        return functools.partial(self._original_unbound_forward, self._model_ref)
+
+    @contextmanager
+    def temporary_bound_original_forward(self, bound_forward: Callable):
+        """
+        Context manager for temporary replacement of the underlying original model forward function. NNCF
+        works by doing additional operations before and after the original object's forward call, and this context
+        manager allows to temporarily run the compressed model object as if it had another original forward method.
+        The signature of the new forward method must be the same w.r.t. the original forward method in terms of
+        activation tensors.
+        :param bound_forward: A callable which will be used to temporary replace the original forward call. Must be
+        a bound method, e.g. the `self` argument had already been set to the same model object where the forward call
+        must be replaced.
+        """
+        prev_bound_forward = self._bound_original_forward
+        self._bound_original_forward = bound_forward
+        yield
+        self._bound_original_forward = prev_bound_forward
+
+    def get_original_unbound_forward(self) -> Callable:
+        """
+        Returns the forward function of the original model, unmodified by NNCF. The returned function will preserve
+        its 0-th implicit `self` argument without binding it to the model object.
+        """
+        return self._original_unbound_forward
+
+    def set_original_unbound_forward(self, fwd_fn: Callable):
+        """
+        Allows to set the function that is treated by NNCF as "original" model forward to another function.
+        :param fwd_fn: The new original forward function. The signature w.r.t. activation tensors must be the same,
+        and the function must leave its 0-th `self` argument unbound.
+        """
+        self._custom_original_unbound_forward = fwd_fn
+
+    def reset_original_unbound_forward(self):
+        """
+        Reset the forward which was set with set_original_unbound_forward() method.
+        After this NNCF will fall back to the unbound forward of the original model.
+        """
+        self._custom_original_unbound_forward = None
+
+    def __init__(
+        self,
+        model: torch.nn.Module,
+        input_infos: List[ModelInputInfo] = None,
+        dummy_forward_fn: Callable = None,
+        wrap_inputs_fn: Callable = None,
+        scopes_without_shape_matching: List[str] = None,
+        ignored_scopes: List[str] = None,
+        target_scopes: List[str] = None,
+        wrap_outputs_fn: Callable = None,
+    ):
         super().__init__()
-        self._set_nncf_wrapped_model(module)
-        self._forward_signature = inspect.signature(module.forward)
-        self.input_infos = input_infos
 
-        self._original_model_accuracy = original_model_accuracy
+        # Need this in order not to register owning module as sub-module of NNCFInterface and thus
+        # avoid circular references
+        object.__setattr__(self, "__model_ref", model)
+
+        if isinstance(model, NNCFNetwork):
+            # Got an NNCFNetwork already, probably during shallow copying.
+            self._original_class = model.nncf._original_class
+            self._bound_original_forward = model.nncf._bound_original_forward
+            self._custom_original_unbound_forward = model.nncf._custom_original_unbound_forward
+        else:
+            self._original_class = model.__class__
+            self._bound_original_forward = None
+            self._custom_original_unbound_forward = None
+
+        self._forward_signature = inspect.signature(self.get_original_forward())
+        self._input_infos = input_infos
 
-        self.ignored_scopes = ignored_scopes
-        self.target_scopes = target_scopes
+        self._ignored_scopes = ignored_scopes
+        self._target_scopes = target_scopes
         self._user_dummy_forward_fn = dummy_forward_fn
         self._kd_loss_handler = None
 
-        device = get_model_device(module)
+        device = get_model_device(model)
 
         if wrap_inputs_fn is not None:
             self._wrap_inputs_fn = wrap_inputs_fn
-        else:
-            self.__input_infos_based_input_wrapper = InputInfoWrapManager(self.input_infos,
-                                                                          self._forward_signature,
-                                                                          module_ref_for_device=self)
+        elif self._input_infos is not None:
+            self.__input_infos_based_input_wrapper = InputInfoWrapManager(
+                self._input_infos, self._forward_signature, module_ref_for_device=model
+            )
             self._wrap_inputs_fn = self.__input_infos_based_input_wrapper.wrap_inputs
+        else:
+            raise ValueError("wrap_inputs_fn or input_infos should be passed.")
 
         if wrap_outputs_fn is not None:
             self._wrap_outputs_fn = wrap_outputs_fn
         else:
             self._wrap_outputs_fn = wrap_nncf_model_outputs_with_objwalk
 
-        self._nncf_module_scopes = []  # type: List[Scope]
-        self.scopes_without_shape_matching = scopes_without_shape_matching
+        self._nncf_replaced_modules = {}  # type: Dict[torch.nn.Module, List[Scope]]
+        self._scopes_without_shape_matching = scopes_without_shape_matching
         self.debug_interface = CombinedDebugInterface() if is_debug() else None
         self._extra_module_types = []  # type: List[ExtraCompressionModuleType]
-        # pylint:disable=line-too-long
-        self._insertions_into_original_graph = {}  # type: Dict[PTTargetPoint, List[Tuple[Callable, TransformationPriority]]]
+        self._insertions_into_original_graph = (
+            {}
+        )  # type: Dict[PTTargetPoint, List[Tuple[Callable, TransformationPriority]]]
+
+        _orig_graph_build_forward_fn = self._get_dummy_forward_fn_for_graph_building(
+            with_input_tracing=True, with_output_tracing=True
+        )
 
-        _orig_graph_build_forward_fn = self._get_dummy_forward_fn_for_graph_building(with_input_tracing=True,
-                                                                                     with_output_tracing=True)
-
-        nncf_wrapped_model = self.get_nncf_wrapped_model()
-        eval_only_op_scopes = self._collect_eval_only_op_scopes(nncf_wrapped_model,
-                                                                _orig_graph_build_forward_fn)
+        eval_op_scopes = self._collect_eval_op_scopes(model, _orig_graph_build_forward_fn)
 
         # all modules called in eval mode should be replaced prior to graph building
-        self._replace_modules_by_nncf_modules(device, eval_only_op_scopes, reset)
+        self._replace_modules_by_nncf_modules(model, device, eval_op_scopes)
 
         _orig_context = TracingContext()
 
         _orig_context.add_node_comparators([MODEL_INPUT_OP_NAME], ShapeIgnoringTensorMetaComparator())
         _orig_context.add_node_comparators([MODEL_OUTPUT_OP_NAME], ShapeIgnoringTensorMetaComparator())
-        if self.scopes_without_shape_matching:
-            _orig_context.add_node_comparators(scopes_without_shape_matching,
-                                               ShapeIgnoringTensorMetaComparator())
-
-        self._original_dynamic_graph = GraphTracer(_orig_graph_build_forward_fn).trace_graph(nncf_wrapped_model,
-                                                                                             _orig_context,
-                                                                                             as_eval=True)
-        self._original_graph = GraphConverter.convert(self._original_dynamic_graph,
-                                                      input_infos=self.input_infos)
+        if self._scopes_without_shape_matching:
+            _orig_context.add_node_comparators(scopes_without_shape_matching, ShapeIgnoringTensorMetaComparator())
+
+        if isinstance(model, NNCFNetwork):
+            self._original_dynamic_graph = model.nncf._original_dynamic_graph
+            self._original_graph = model.nncf._original_graph
+        else:
+            self._original_dynamic_graph = GraphTracer(_orig_graph_build_forward_fn).trace_graph(
+                model, _orig_context, as_eval=True
+            )
+            self._original_graph = GraphConverter.convert(self._original_dynamic_graph, input_infos=self._input_infos)
         self._compressed_graph = None  # type: PTNNCFGraph
 
         self._compressed_context = TracingContext()
 
-        self._dummy_forward_fn = self._get_dummy_forward_fn_for_graph_building(with_input_tracing=False,
-                                                                               with_output_tracing=False)
+        self._dummy_forward_fn = self._get_dummy_forward_fn_for_graph_building(
+            with_input_tracing=False, with_output_tracing=False
+        )
         self._in_user_dummy_forward = False
 
         self._compressed_context.add_node_comparators([MODEL_INPUT_OP_NAME], ShapeIgnoringTensorMetaComparator())
         self._compressed_context.add_node_comparators([MODEL_OUTPUT_OP_NAME], ShapeIgnoringTensorMetaComparator())
-        if self.scopes_without_shape_matching:
-            self._compressed_context.add_node_comparators(scopes_without_shape_matching,
-                                                          ShapeIgnoringTensorMetaComparator())
+        if self._scopes_without_shape_matching:
+            self._compressed_context.add_node_comparators(
+                scopes_without_shape_matching, ShapeIgnoringTensorMetaComparator()
+            )
         self._load_listener = None
 
-    @debuggable_forward
-    def forward(self, *args, **kwargs):
-        with self._compressed_context as ctx:  # type: TracingContext
-            ctx.base_module_thread_local_replica = self
-            args, kwargs = replicate_same_tensors((args, kwargs))
-            if not self._in_user_dummy_forward:
-                # If a user supplies own dummy forward, he is responsible for
-                # correctly wrapping inputs inside it as well.
-                args, kwargs = self._strip_traced_tensors(args, kwargs)
-                args, kwargs = self._wrap_inputs_fn(args, kwargs)
-            retval = self.get_nncf_wrapped_model()(*args, **kwargs)
-            retval = replicate_same_tensors(retval)
-            if not self._in_user_dummy_forward:
-                retval = self._wrap_outputs_fn(retval)
+        self.compression_controller = None  # type: PTCompressionAlgorithmController
 
-        if self._kd_loss_handler is not None and self.get_nncf_wrapped_model().training:
-            self._kd_loss_handler(retval, *args, **kwargs)
-        return retval
+    @property
+    def _original_unbound_forward(self):
+        # Notes:
+        # (1) We rely on an "unbound" forward which is the version of the method that has the
+        #   `self` parameter not set, otherwise we will be indirectly capturing a reference to the
+        #   model object in NNCFInterface - this will lead to failures in DataParallel
+        #   because the bound original forward call during NNCFNetwork.forward
+        #   would then call forward on the original non-replica module even if NNCFNetwork itself was
+        #   replicated.
+        # (2) We access the unbound forward from a reference to the original model class instead
+        #   of storing the reference to the unbound forward itself because the original class forward
+        #   may be overridden by some 3rd party logic. For example, during export of mm-based models to ONNX
+        #   using mmdeploy library, the original forward method of the model is temporarily replaced
+        #   during export. Moreover, in such case the forward signature needs to be hidden by a user
+        #   beforehand by wrapping it with a function with (*args, **kwargs) as its arguments.
+        custom_unbound_forward = self._custom_original_unbound_forward
+        return self._original_class.forward if custom_unbound_forward is None else custom_unbound_forward
+
+    @property
+    def _model_ref(self) -> "NNCFNetwork":
+        return object.__getattribute__(self, "__model_ref")
+
+    @property
+    def input_infos(self) -> List[ModelInputInfo]:
+        return deepcopy(self._input_infos)
 
     def _strip_traced_tensors(self, args: Tuple, kwargs: Dict) -> Tuple[Tuple, Dict]:
         """
-            Required to guard against new forward calls on tensors that have already passed
-            through NNCF's forward once and got turned into TracedTensors by reference access.
+        Required to guard against new forward calls on tensors that have already passed
+        through NNCF's forward once and got turned into TracedTensors by reference access.
         """
         is_traced_tensor_predicate = lambda x: isinstance(x, TracedTensor)
 
         def strip_fn(tensor: TracedTensor) -> torch.Tensor:
-            if hasattr(torch.Tensor, 'as_subclass'):
+            if hasattr(torch.Tensor, "as_subclass"):
                 return torch.Tensor.as_subclass(tensor, torch.Tensor)
             # Torch < 1.7.0 fallback
             return torch.tensor(tensor, device=tensor.device, requires_grad=tensor.requires_grad)
 
         args = objwalk(args, is_traced_tensor_predicate, strip_fn)
         kwargs = objwalk(kwargs, is_traced_tensor_predicate, strip_fn)
         return args, kwargs
 
-    def create_knowledge_distillation_loss_handler(self, kd_original_model: nn.Module, calculate_fn)\
-            -> KnowledgeDistillationLossHandler:
+    def create_knowledge_distillation_loss_handler(
+        self, kd_original_model: nn.Module, calculate_fn
+    ) -> KnowledgeDistillationLossHandler:
         """
         Creates KnowledgeDistillationLossHandler instance for enabling Knowledge Distillation feature.
             Also returns created KnowledgeDistillationLossHandler for control over Knowledge Distillation logic.
 
         :param kd_original_model: original non compressed model used for distillation
         :param calculate_fn: function used to parse model outputs and calculate knowledge distillation loss
         :return: KnowledgeDistillationLossHandler instance
         """
-        device = get_model_device(self.get_nncf_wrapped_model())
-        self._kd_loss_handler = KnowledgeDistillationLossHandler(self._compressed_context,
-                                                                 kd_original_model,
-                                                                 calculate_fn,
-                                                                 device)
+        device = get_model_device(self._model_ref)
+        self._kd_loss_handler = KnowledgeDistillationLossHandler(
+            self._compressed_context, kd_original_model, calculate_fn, device
+        )
         return self._kd_loss_handler
 
-    # Cannnot use property syntax here, otherwise the wrapped module will end up
-    # being twice in the same checkpoint with different prefixes
-    def get_nncf_wrapped_model(self):
-        return getattr(self, MODEL_WRAPPED_BY_NNCF_ATTR_NAME)
+    def reset_nncf_modules(self):
+        for scope_list in self.get_nncf_module_scopes():
+            # Can pick any access scope since they all should
+            # point to the same object
+            some_scope = scope_list[0]
+            module = self.get_module_by_scope(some_scope)
+            module.reset()
 
-    def _set_nncf_wrapped_model(self, value):
-        setattr(self, MODEL_WRAPPED_BY_NNCF_ATTR_NAME, value)
-
-    def get_clean_shallow_copy(self) -> 'NNCFNetwork':
+    def get_clean_shallow_copy(self) -> "NNCFNetwork":
         # WARNING: Will reset pre- and post-ops of the underlying model. Use save_nncf_module_additions
         # and load_nncf_module_additions to preserve these, or temporary_clean_view().
-        from nncf.torch.utils import save_module_state, load_module_state #pylint: disable=cyclic-import
-        saved_state = save_module_state(self)
-        model_copy = NNCFNetwork(self.get_nncf_wrapped_model(), self.input_infos,
-                                 self._user_dummy_forward_fn, self._wrap_inputs_fn,
-                                 self.scopes_without_shape_matching, self.ignored_scopes, self.target_scopes,
-                                 reset=True)
-        load_module_state(model_copy, saved_state)
-        return model_copy
+        from nncf.torch.utils import load_module_state  # pylint: disable=cyclic-import
+        from nncf.torch.utils import save_module_state
+
+        saved_state = save_module_state(self._model_ref)
+        new_interface = NNCFNetworkInterface(
+            self._model_ref,
+            self._input_infos,
+            self._user_dummy_forward_fn,
+            self._wrap_inputs_fn,
+            self._scopes_without_shape_matching,
+            self._ignored_scopes,
+            self._target_scopes,
+            wrap_outputs_fn=self._wrap_outputs_fn,
+        )
+        self._model_ref._nncf = new_interface  # pylint:disable=protected-access
+        self._model_ref.nncf.reset_nncf_modules()
+        load_module_state(self._model_ref, saved_state)
+        return self._model_ref
 
-    def get_modules_in_nncf_modules_by_type(self, types) -> Dict[Scope, nn.Module]:
+    def get_modules_in_nncf_modules_by_type(self, class_names: List[str]) -> Dict[Scope, nn.Module]:
         nncf_modules = self.get_nncf_modules()
         retval = {}
-        for nncf_module_scope, nncf_module in nncf_modules.items():
+        for nncf_module, nncf_module_scope in nncf_modules.items():
             nncf_module_scope.pop()
-            for relative_scope, target_module in get_all_modules_by_type(nncf_module, types).items():
+            for relative_scope, target_module in get_all_modules_by_type(nncf_module, class_names).items():
                 retval[nncf_module_scope + relative_scope] = target_module
         return retval
 
     def insert_at_point(self, point: PTInsertionPoint, fn_list: List[Callable]):
         if point.insertion_type == PTInsertionType.OPERATOR_PRE_HOOK:
             self._compressed_context.register_pre_hooks(fn_list, point.op_address, point.input_port_id)
         elif point.insertion_type == PTInsertionType.OPERATOR_POST_HOOK:
             self._compressed_context.register_post_hooks(fn_list, point.op_address)
-        elif point.insertion_type in [PTInsertionType.NNCF_MODULE_PRE_OP,
-                                      PTInsertionType.NNCF_MODULE_POST_OP]:
+        elif point.insertion_type in [PTInsertionType.NNCF_MODULE_PRE_OP, PTInsertionType.NNCF_MODULE_POST_OP]:
             nncf_module = self.get_module_by_scope(point.module_scope)
             if not isinstance(nncf_module, _NNCFModuleMixin):
                 raise RuntimeError(
-                    f'Failed to insert pre/post op for not registered custom module {point.module_scope}. NNCF only '
-                    f'supports native PyTorch modules with respect to trainable parameter (weight) compressed, such '
-                    f'as `torch.nn.Conv2d`. If your model contains a custom, non-PyTorch standard module with trainable'
-                    f' weights that should be compressed, you can register it using the '
-                    f'`@nncf.register_module` decorator. Please refer to `Compression of custom modules` section in '
-                    f'docs/Usage.md for more details.')
+                    f"Failed to insert pre/post op for not registered custom module {point.module_scope}. NNCF only "
+                    f"supports native PyTorch modules with respect to trainable parameter (weight) compressed, such "
+                    f"as `torch.nn.Conv2d`. If your model contains a custom, non-PyTorch standard module with trainable"
+                    f" weights that should be compressed, you can register it using the "
+                    f"`@nncf.register_module` decorator. Please refer to `Compression of custom modules` section in "
+                    f"docs/Usage.md for more details."
+                )
 
             norm_target_scope = self._normalize_variable_recurrent_scope(point.module_scope)
-            norm_nncf_scopes = [self._normalize_variable_recurrent_scope(x) for x in self._nncf_module_scopes]
+            norm_nncf_scopes = []
+            for scope_list_for_module in self.get_nncf_module_scopes():
+                norm_nncf_scopes.extend([self._normalize_variable_recurrent_scope(x) for x in scope_list_for_module])
             assert norm_target_scope in norm_nncf_scopes  # Required for proper Recurrent/VariableRecurrent addressing
             if point.insertion_type == PTInsertionType.NNCF_MODULE_PRE_OP:
                 for fn in fn_list:
                     nncf_module.register_pre_forward_operation(fn)
             elif point.insertion_type == PTInsertionType.NNCF_MODULE_POST_OP:
                 for fn in fn_list:
                     nncf_module.register_post_forward_operation(fn)
         else:
             raise RuntimeError("Unsupported insertion type: {}".format(point.insertion_type))
 
-    def __getattr__(self, name):
-        class NotFound:
-            pass
-
-        def get_nncf_network_attr(self, name):
-            if name in self.__dict__:
-                return self.__dict__[name]
-            return NotFound
-
-        def get_nncf_module_attr(self, name):
-            if hasattr(self.__dict__['_modules'][MODEL_WRAPPED_BY_NNCF_ATTR_NAME], name):
-                attr = getattr(self.__dict__['_modules'][MODEL_WRAPPED_BY_NNCF_ATTR_NAME], name)
-                if hasattr(attr, '__self__'):  # If it is a bound function
-                    from functools import partial
-                    attr = partial(attr.__func__, self)
-                    return attr
-                # If it is not a bound function
-                return attr
-            return NotFound
-
-        def get_nn_module_attr(self, name):
-            return super().__getattr__(name)
-
-        attr = get_nncf_network_attr(self, name)
-        if attr != NotFound:
-            return attr
-        attr = get_nncf_module_attr(self, name)
-        if attr != NotFound:
-            return attr
-        return get_nn_module_attr(self, name)
-
-
     def get_graph(self) -> PTNNCFGraph:
         if self._compressed_context.graph.get_nodes_count() == 0 or self._compressed_graph is None:
             self.rebuild_graph()
         return self._compressed_graph
 
     def get_dynamic_graph(self) -> DynamicGraph:
         return self._compressed_context.graph
@@ -395,105 +443,109 @@
         self._compressed_context.enable_node_additions()
 
     def disable_dynamic_graph_building(self):
         self._compressed_context.disable_node_additions()
 
     def _get_dummy_forward_fn_for_graph_building(self, with_input_tracing, with_output_tracing):
         if self._user_dummy_forward_fn is None:
-            return create_dummy_forward_fn(self.input_infos,
-                                           with_input_tracing=with_input_tracing,
-                                           wrap_inputs_fn=self._wrap_inputs_fn,
-                                           wrap_outputs_fn=self._wrap_outputs_fn,
-                                           with_output_tracing=with_output_tracing)
+            return create_dummy_forward_fn(
+                self._input_infos,
+                with_input_tracing=with_input_tracing,
+                wrap_inputs_fn=self._wrap_inputs_fn,
+                wrap_outputs_fn=self._wrap_outputs_fn,
+                with_output_tracing=with_output_tracing,
+            )
 
         def wrapped_user_dummy_forward_fn(*args, **kwargs):
             self._in_user_dummy_forward = True
             retval = self._user_dummy_forward_fn(*args, **kwargs)
             self._in_user_dummy_forward = False
             return retval
 
         return wrapped_user_dummy_forward_fn
 
-    def _replace_modules_by_nncf_modules(self, device, eval_only_op_scopes: List[Scope] = None,
-                                         reset: bool = False):
-        module, self._nncf_module_scopes = replace_modules_by_nncf_modules(
-            self.get_nncf_wrapped_model(), ignored_scopes=self.ignored_scopes,
-            target_scopes=self.target_scopes, eval_op_scopes=eval_only_op_scopes,
-            reset=reset)
-        self._set_nncf_wrapped_model(module.to(device))
-
-    def get_nncf_module_scopes(self) -> List[Scope]:
-        return self._nncf_module_scopes
-
-    def get_nncf_modules(self) -> Dict[Scope, torch.nn.Module]:
-        nncf_module_names_list = NNCF_MODULES + [x.__name__ for x in NNCF_WRAPPED_USER_MODULES_DICT.values()]
-        return get_all_modules_by_type(self.get_nncf_wrapped_model(), nncf_module_names_list)
+    def _replace_modules_by_nncf_modules(
+        self, model: torch.nn.Module, device: torch.device, eval_op_scopes: List[Scope] = None
+    ):
+        _, self._nncf_replaced_modules = replace_modules_by_nncf_modules(
+            model, ignored_scopes=self._ignored_scopes, target_scopes=self._target_scopes, eval_op_scopes=eval_op_scopes
+        )
+        model.to(device)
+        return model
 
-    def get_weighted_original_graph_nodes(self, nncf_module_names: List[str] = None) -> List[NNCFNode]:
-        retval = []
-        for nncf_module_scope in self._nncf_module_scopes:
-            if nncf_module_names is not None:
-                module_name = nncf_module_scope[-1].calling_module_class_name
-                if module_name not in nncf_module_names:
-                    continue
-            nodes_in_scope = self._original_graph.get_op_nodes_in_scope(nncf_module_scope)
-            for node in nodes_in_scope:
-                if node.metatype in OPERATORS_WITH_WEIGHTS_METATYPES:
-                    retval.append(node)
+    def get_nncf_module_scopes(self) -> List[List[Scope]]:
+        return list(self._nncf_replaced_modules.values())
+
+    def get_nncf_modules(self) -> Dict[torch.nn.Module, Scope]:
+        retval = {}
+        for module, scope_set in self._nncf_replaced_modules.items():
+            canonical_scope = next(iter(scope_set))
+            retval[module] = canonical_scope.copy()
         return retval
 
-    def get_nncf_modules_by_module_names(self, nncf_module_names_list: List[str]) -> Dict["Scope", torch.nn.Module]:
-        return get_all_modules_by_type(self.get_nncf_wrapped_model(), nncf_module_names_list)
+    def get_weighted_original_graph_nodes(self, nncf_module_names: List[str] = None) -> List[NNCFNode]:
+        retval = set()
+        for scope_list in self.get_nncf_module_scopes():
+            for nncf_module_scope in scope_list:
+                if nncf_module_names is not None:
+                    module_name = nncf_module_scope[-1].calling_module_class_name
+                    if module_name not in nncf_module_names:
+                        continue
+                nodes_in_scope = self._original_graph.get_op_nodes_in_scope(nncf_module_scope)
+                for node in nodes_in_scope:
+                    if node.metatype in OPERATORS_WITH_WEIGHTS_METATYPES:
+                        retval.add(node)
+
+        return list(sorted(retval, key=str))
 
     def rebuild_graph(self, *input_args):
         self._compressed_context.reset_graph()
-        dummy_forward_fn = self._get_dummy_forward_fn_for_graph_building(with_input_tracing=False,
-                                                                         with_output_tracing=False)
+        dummy_forward_fn = self._get_dummy_forward_fn_for_graph_building(
+            with_input_tracing=False, with_output_tracing=False
+        )
         builder = GraphBuilder(dummy_forward_fn)
-        self._compressed_graph = builder.build_graph(self, self._compressed_context,
-                                                     input_infos=self.input_infos)
-
-    def post_build_graph_actions(self):
-        # Reset initialization flags (`initialized`) for all quantization modules
-        # after dummy `load_state_dict` call.
-        quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]
-        all_quantizations = get_state_dict_names_with_modules(self, quantization_types)
-        for module in all_quantizations.values():
-            module.initialized = False
 
-    def is_scope_in_nncf_module_scope(self, scope: Scope):
-        # TODO: optimize
-        norm_nncf_scopes = [self._normalize_variable_recurrent_scope(x) for x in self._nncf_module_scopes]
+        with training_mode_switcher(self._model_ref, is_training=False):
+            self._compressed_graph = builder.build_graph(
+                self._model_ref, self._compressed_context, input_infos=self._input_infos
+            )
+
+    def is_scope_in_nncf_module_scope(self, scope: Scope) -> bool:
+        norm_nncf_scopes = []
+        for scope_list_for_module in self.get_nncf_module_scopes():
+            norm_nncf_scopes.extend([self._normalize_variable_recurrent_scope(x) for x in scope_list_for_module])
         norm_op_scope = self._normalize_variable_recurrent_scope(scope)
         for nncf_scope in norm_nncf_scopes:
             if norm_op_scope in nncf_scope:
                 return True
         return False
 
     def register_compression_module_type(self, compression_module_type: ExtraCompressionModuleType):
         attr_name = self._compression_module_type_to_attr_name(compression_module_type)
         if compression_module_type in self._extra_module_types:
-            raise RuntimeError("Module type {} is already registered".format(compression_module_type))
+            raise RuntimeError(f"Module type {compression_module_type} is already registered")
+
         self.__setattr__(attr_name, nn.ModuleDict())
         self._extra_module_types.append(compression_module_type)
 
-    def add_compression_module(self, module_key: str, module: nn.Module,
-                               compression_module_type: ExtraCompressionModuleType):
+    def add_compression_module(
+        self, module_key: str, module: nn.Module, compression_module_type: ExtraCompressionModuleType
+    ):
         attr_name = self._compression_module_type_to_attr_name(compression_module_type)
         if compression_module_type not in self._extra_module_types:
-            raise RuntimeError("Module type {} was not registered".format(compression_module_type))
+            raise RuntimeError(f"Module type {compression_module_type} was not registered")
         storage = self.__getattr__(attr_name)
         if module_key in storage:
-            raise RuntimeError("Module {} is already registered under {}".format(module_key, attr_name))
+            raise RuntimeError(f"Module {module_key} is already registered under {attr_name}")
         storage[module_key] = module
 
     def get_compression_modules_by_type(self, compression_module_type: ExtraCompressionModuleType) -> nn.ModuleDict:
         attr_name = self._compression_module_type_to_attr_name(compression_module_type)
         if compression_module_type not in self._extra_module_types:
-            raise RuntimeError("Module type {} was not registered".format(compression_module_type))
+            raise RuntimeError(f"Module type {compression_module_type} was not registered")
         return self.__getattr__(attr_name)
 
     @staticmethod
     def _compression_module_type_to_attr_name(compression_module_type: ExtraCompressionModuleType):
         """
         Required for backward compatibility with checkpoints that store function and activation
         quantizers directly under corresponding attributes of NNCFNetwork.
@@ -515,49 +567,51 @@
     def _normalize_variable_recurrent_scope(scope: Scope):
         """
         Two scopes pointing to an NNCF module that only differ in a Recurrent/VariableRecurrent/VariableRecurrentReverse
         scope node actually point to one and the same module.
         """
         ret_scope = scope.copy()
         for scope_element in ret_scope:
-            if scope_element.calling_module_class_name in ["Recurrent", "VariableRecurrent",
-                                                           "VariableRecurrentReverse"]:
+            if scope_element.calling_module_class_name in [
+                "Recurrent",
+                "VariableRecurrent",
+                "VariableRecurrentReverse",
+            ]:
                 scope_element.calling_module_class_name = "NormalizedName_Recurrent"
         return ret_scope
 
-    def do_dummy_forward(self, force_eval=False):
+    def do_dummy_forward(self, force_eval: bool = False):
         """
         Attention: If run with force_eval=False, this may spoil the batchnorm statistics,
         and an eval run of the model will perform much worse than the train run.
         """
         if force_eval:
-            train_mode = self.training
-            self.eval()
+            train_mode = self._model_ref.training
+            self._model_ref.eval()
         with torch.no_grad():
             with self._compressed_context as ctx:
-                ctx.base_module_thread_local_replica = self
-                self._dummy_forward_fn(self)
+                ctx.base_module_thread_local_replica = self._model_ref
+                self._dummy_forward_fn(self._model_ref)
         if force_eval:
             if train_mode:
-                self.train()
+                self._model_ref.train()
 
     def get_insertion_point_graph(self) -> InsertionPointGraph:
         # Set up a pre- and post-hooks on almost every op in PyTorch
         nncf_graph = self.get_original_graph()
         pre_hooks = []  # type: List[PreHookInsertionPoint]
         post_hooks = []  # type: List[PostHookInsertionPoint]
         for node in nncf_graph.get_all_nodes():
             # Pre-hook insertion point nodes
             # Will insert a pre-hook IP for each input edge. The input edge must be marked with
             # a port ID attribute.
             in_edges = nncf_graph.get_input_edges(node)
             for edge in in_edges:
                 port_id = edge.input_port_id
-                pre_hook_ip = PreHookInsertionPoint(target_node_name=node.node_name,
-                                                    input_port_id=port_id)
+                pre_hook_ip = PreHookInsertionPoint(target_node_name=node.node_name, input_port_id=port_id)
                 pre_hooks.append(pre_hook_ip)
 
             if issubclass(node.metatype, PTSplitMetatype):
                 # chunk returns a tuple of tensors, which can only be handled in NNCF
                 # once post-hook ports are enabled. Work around it for now by disallowing post-hook
                 # insertion for chunks
                 # TODO: enable post-hook ports and remove this
@@ -566,166 +620,434 @@
             # Post-hook insertion point nodes
             post_hook_ip = PostHookInsertionPoint(node.node_name)
             post_hooks.append(post_hook_ip)
 
         weighted_nodes = self.get_weighted_original_graph_nodes()
         weighted_node_names = [weighted_node.node_name for weighted_node in weighted_nodes]
 
-        ip_graph = InsertionPointGraph(self._original_graph, weight_modifiable_node_names=weighted_node_names,
-                                       allowed_pre_hook_insertion_points=pre_hooks,
-                                       allowed_post_hook_insertion_points=post_hooks)
+        ip_graph = InsertionPointGraph(
+            self._original_graph,
+            weight_modifiable_node_names=weighted_node_names,
+            allowed_pre_hook_insertion_points=pre_hooks,
+            allowed_post_hook_insertion_points=post_hooks,
+        )
         return ip_graph
 
     def get_module_by_scope(self, scope: Scope) -> Optional[torch.nn.Module]:
-        curr_module = self.get_nncf_wrapped_model()
-        for scope_element in scope[1:]:  # omit first scope element which corresponds to base module
-            if scope_element.calling_field_name is None:
-                # The module used is being created in-place every time and never stored in the model,
-                # happens for nn.Softmax in BERT implementations.
-                return None
-            # pylint: disable=protected-access
-            next_module = curr_module._modules.get(scope_element.calling_field_name)
-            if next_module is None:
-                raise RuntimeError("Could not find a {} module member in {} module of scope {} during node search"
-                                   .format(scope_element.calling_field_name,
-                                           scope_element.calling_module_class_name,
-                                           str(scope)))
-            curr_module = next_module
-        return curr_module
+        curr_module = self._model_ref
+        return get_module_by_scope(curr_module, scope)
 
     def get_containing_module(self, node_name: NNCFNodeName) -> torch.nn.Module:
         if self._compressed_graph is not None:
             try:
                 scope = self._compressed_graph.get_scope_by_node_name(node_name)
             except RuntimeError:
-                nncf_logger.debug(f"Node {node_name} not found in compressed graph when trying to determine "
-                                  f"the containing module, trying the original graph to see if the node was "
-                                  f"present there during graph building")
+                nncf_logger.debug(
+                    f"Node {node_name} not found in compressed graph when trying to determine "
+                    f"the containing module, trying the original graph to see if the node was "
+                    f"present there during graph building"
+                )
                 scope = self._original_graph.get_scope_by_node_name(node_name)
         else:
             scope = self._original_graph.get_scope_by_node_name(node_name)
         return self.get_module_by_scope(scope)
 
-    def get_parameters_count_in_model(self):
-        """
-        Return total amount of model parameters.
-        """
-        count = 0
-        for param in self.parameters():
-            count = count + param.numel()
-        return count
-
     def get_flops_per_module(self) -> Dict[NNCFNodeName, int]:
         """
         Calculates FLOPS count for modules.
         """
-        model = self
+        model = self._model_ref
         flops_count_dict = {}
 
         def get_hook(name):
-            return functools.partial(compute_FLOPs_hook, dict_to_save=flops_count_dict,
-                                     module_node_name=name)
+            return functools.partial(compute_FLOPs_hook, dict_to_save=flops_count_dict, module_node_name=name)
 
         hook_list = []
         for nncf_node in self._original_graph.get_all_nodes():
             node_module = self.get_containing_module(nncf_node.node_name)
             hook_list.append(node_module.register_forward_hook(get_hook(nncf_node.node_name)))
-        model.do_dummy_forward(force_eval=True)
+        model.nncf.do_dummy_forward(force_eval=True)
 
         for h in hook_list:
             h.remove()
         return flops_count_dict
 
     def get_MACs_in_model(self):
         """
-            Calculates MAC units count for model.
+        Calculates MAC units count for model.
         """
-        flops_count_dict = self.get_flops_per_module()
+        flops_count_dict = self.nncf.get_flops_per_module()
         total_MACs_count = sum(v // 2 for v in flops_count_dict.values())
         return total_MACs_count
 
-    def get_input_infos(self) -> List[ModelInputInfo]:
-        return deepcopy(self.input_infos)
-
     def save_nncf_module_additions(self) -> Dict[Scope, Tuple[torch.nn.ModuleDict, torch.nn.ModuleDict]]:
         retval = {}
-        for module_scope, nncf_module in self.get_nncf_modules().items():
+        for nncf_module, module_scope in self.get_nncf_modules().items():
             retval[module_scope] = (deepcopy(nncf_module.pre_ops), deepcopy(nncf_module.post_ops))
         return retval
 
-    def load_nncf_module_additions(self,
-                                   scope_vs_pre_post_ops_dict: Dict[Scope, Tuple[torch.nn.ModuleDict,
-                                                                                   torch.nn.ModuleDict]]):
-        for module_scope, nncf_module in self.get_nncf_modules().items():
+    def load_nncf_module_additions(
+        self, scope_vs_pre_post_ops_dict: Dict[Scope, Tuple[torch.nn.ModuleDict, torch.nn.ModuleDict]]
+    ):
+        for nncf_module, module_scope in self.get_nncf_modules().items():
             nncf_module.pre_ops = scope_vs_pre_post_ops_dict[module_scope][0]
             nncf_module.post_ops = scope_vs_pre_post_ops_dict[module_scope][1]
 
     def temporary_clean_view(self):
         class Mgr:
             def __init__(self, model: NNCFNetwork):
                 self.model = model
-                self.storage_dict = {}
+                self.nncf_module_state_dicts = {}
+                self.nncf_interface_state_dict = None
+                self.nncf_compression_module_types = []
 
             def __enter__(self):
-                self.storage_dict = self.model.save_nncf_module_additions()
-                clean_model = self.model.get_clean_shallow_copy()
+                self.nncf_module_state_dicts = self.model.nncf.save_nncf_module_additions()
+                self.nncf_interface = self.model.nncf
+                clean_model = self.model.nncf.get_clean_shallow_copy()
                 return clean_model
 
             def __exit__(self, exc_type, exc_val, exc_tb):
-                self.model.load_nncf_module_additions(self.storage_dict)
+                self.model._nncf = self.nncf_interface
+                self.model.nncf.load_nncf_module_additions(self.nncf_module_state_dicts)
 
-        return Mgr(self)
+        return Mgr(self._model_ref)
 
-    def _collect_eval_only_op_scopes(self, model: nn.Module, dummy_forward_fn: Callable) -> List[Scope]:
+    def _collect_eval_op_scopes(self, model: nn.Module, dummy_forward_fn: Callable) -> List[Scope]:
         """
-        Returns scopes of the modules which are executed in evaluation mode only.
+        Returns scopes of the operations in the graph which are executed in evaluation mode.
         """
 
         tracer = GraphTracer(dummy_forward_fn)
         result = []
         eval_graph = tracer.trace_graph(model, as_eval=True)
+        root_scope = Scope()
         for dyn_graph_node in eval_graph.get_all_nodes():
-            result.append(dyn_graph_node.op_exec_context.scope_in_model)
+            scope_in_model = dyn_graph_node.op_exec_context.scope_in_model
+            if scope_in_model != root_scope:  # happens for ops such as /nncf_model_input_* and /nncf_model_output_*
+                result.append(scope_in_model)
         return result
 
-    @property
-    def original_model_accuracy(self):
-        return self._original_model_accuracy
-
     def get_node_to_op_address_mapping(self) -> Dict[NNCFNodeName, OperationAddress]:
         # The IDs of corresponding nodes of the original dynamic graph and original NNCF graph
         # must be equal for this to work.
         retval = {}
         for node in self._original_dynamic_graph.get_all_nodes():
             node_id = node.node_id
             op_address = node.op_exec_context.op_address
             nncf_node = self._original_graph.get_node_by_id(node_id)
             retval[nncf_node.node_name] = op_address
         return retval
 
+    def set_compression_controller(self, ctrl: "PTCompressionAlgorithmController"):
+        self.compression_controller = ctrl
+
+
+class NNCFNetworkMeta(type):
+    """
+    Metaclass for the NNCFNetwork mixin. Has magic methods defined so that the original model object could be
+    extended with NNCF-related functionality via a conventional `nncf_network = NNCFNetwork(original_model, ...)`
+    syntax and at the same time retain its original class so that downstream class-based checks for the original
+    model type don't fail.
+    """
+
+    def __call__(
+        cls,
+        original_model: torch.nn.Module,
+        input_infos: List[ModelInputInfo] = None,
+        dummy_forward_fn: Callable = None,
+        wrap_inputs_fn: Callable[..., None] = None,
+        scopes_without_shape_matching: List[str] = None,
+        ignored_scopes: List[str] = None,
+        target_scopes: List[str] = None,
+        wrap_outputs_fn: Callable[..., None] = None,
+    ) -> "NNCFNetwork":
+        """
+        This function plays the role of a "constructor" call in the `nncf_network = NNCFNetwork(original_model, ...)`
+        syntax. *_scopes arguments are to be passed as string representation of either
+        `nncf.common.graph.graph.NNCFNodeName` or `nncf.torch.dynamic_graph.scope.Scope` objects.
+        :param original_model: The original model object to be extended with NNCF functionality.
+        :param input_infos: A list of descriptors of each tensor input to the model. Will be used to properly generate
+        dummy inputs during internal forward calls of the original model for purposes of control flow graph building.
+        :param dummy_forward_fn: A function to be called instead of the model's original forward function during
+        control flow graph building.
+        :param wrap_inputs_fn: A user-defined function that will be called with the model's forward arguments at each
+        call of the NNCFNetwork object and within which the `nncf.torch.dynamic_graph.io_handling.nncf_model_input`
+        function is expected to be called upon each tensor among the arguments that is to be treated as an input tensor
+        to the model, thus overriding `input_infos`.
+        :param scopes_without_shape_matching: A list of scopes in the model in which the activation tensor shapes will
+        not be considered for purposes of scope matching - this helps handle RNN-like cases.
+        :param ignored_scopes: A list of scopes in the model for which NNCF handling should not be applied. Functions as
+        a "denylist". If left unspecified, nothing will be ignored.
+        :param target_scopes: A list of scopes in the model for which NNCF handling should be applied. Functions as
+        an "allowlist". If left unspecified, everything will be targeted.
+        :param wrap_outputs_fn: Same as `wrap_inputs_fn`, but for marking model outputs with
+        `nncf.torch.dynamic_graph.io_handling.nncf_model_output` calls.
+        :return: The same object as passed in `original_model`, but with internal modules extended/replaced for
+        purposes of further NNCF compression, and its class dynamically extended with the `NNCFNetwork` as a base class.
+        The object will pass both isinstance(retval, original_model.__class__) and isinstance(retval, NNCFNetwork)
+        checks.
+        """
+        original_class = original_model.__class__
+        original_model._nncf = NNCFNetworkInterface(
+            original_model,
+            input_infos,
+            dummy_forward_fn,
+            wrap_inputs_fn,
+            scopes_without_shape_matching,
+            ignored_scopes,
+            target_scopes,
+            wrap_outputs_fn,
+        )  # pylint:disable=protected-access
+        # The new class will also have an adjusted metaclass to avoid a "metaclass conflict" upon
+        # class creation
+        original_metaclass = type(original_model.__class__)
+        class_creation_kwds = {}
+        if original_metaclass is not type:
+            new_metaclass = types.new_class(
+                name=original_metaclass.__name__, bases=(NNCFNetworkMeta, original_metaclass)
+            )
+            class_creation_kwds["metaclass"] = new_metaclass
+        new_class = types.new_class(
+            name=original_model.__class__.__name__,
+            bases=(NNCFNetwork, original_model.__class__),
+            kwds=class_creation_kwds,
+        )
+        # Make the signature of the forward on the resulting object same as for
+        # the original forward.
+        fn = NNCFNetwork.forward
+        new_forward = types.FunctionType(fn.__code__, fn.__globals__, fn.__name__, fn.__defaults__, fn.__closure__)
+        new_forward.__dict__.update(fn.__dict__)
+        new_forward.__signature__ = inspect.signature(original_class.forward)
+        if is_debug():
+            new_forward = debuggable_forward(new_forward)
+        new_class.forward = new_forward
+
+        # Make resulting class keep __module__ attributes of the original class,
+        # otherwise these will point to NNCF
+        new_class.__module__ = original_class.__module__
+        original_model.__class__ = new_class
+
+        if isinstance(original_model, torch.nn.Sequential):
+            # If the top-level module is Sequential, then the addition of the ._nncf module will result in
+            # the NNCFInterface module being iterated over during the torch.nn.Sequential call, with an attempt to call
+            # its forward method, which it effectively doesn't have. Employing a special iterator allows to hide the
+            # NNCFInterface object during iteration.
+            def nncf_safe_iter(self: torch.nn.Sequential):
+                return NNCFSkippingIter(iter(self._modules.values()))  # pylint:disable=protected-access
+
+            original_model.__class__.__iter__ = nncf_safe_iter
+        return original_model
+
+    def __hash__(cls):
+        """
+        Makes the dynamically created class object for the processed original model object return the same value when
+        hashed as the original class. This allows to gracefully handle the situation when the downstream training
+        pipeline checks that the model's class is registered in some registry and determines a training approach
+        based on that.
+        """
+        # expected from a compressed model object to have a NNCFNetwork as 0-th base
+        # and original class as 1-st
+        if len(cls.__bases__) == 2:
+            original_class = cls.__bases__[1]
+            return hash(original_class)
+        return id(NNCFNetwork)  # conforms to a default hashing behaviour in Python for cls objects
+
+    def __eq__(cls, other):
+        """
+        Makes the dynamically created class object for the processed original model object compare equal with the
+        original class object. This allows to gracefully handle the situation when the downstream training
+        pipeline checks that the model's class is registered in some registry and determines a training approach
+        based on that.
+        """
+        if len(cls.__bases__) == 2:
+            original_class = cls.__bases__[1]
+            return original_class == other
+        return other is NNCFNetwork
+
+
+class NNCFNetwork(torch.nn.Module, metaclass=NNCFNetworkMeta):
+    """
+    A mixin-like class to dynamically extend the original model object's class with.
+    """
+
+    def __init__(self, *args, **kwargs):
+        """
+        In normal situations, the __init__ of the NNCFNetwork will never be called. The constructor-like syntax is
+        achieved by a __call__ method defined in the metaclass `NNCFNetworkMeta`.
+        """
+        super().__init__()
+        raise RuntimeError("Direct instantiation of NNCFNetwork objects using __init__ is prohibited.")
+
+    def __call__(self, *args, **kwargs):
+        """
+        Ensures that functor-like calls of the processed model object will directly trigger the NNCF-specific
+        forward call.
+        """
+        return ORIGINAL_CALL(self, *args, **kwargs)
+
+    def forward(self, *args, **kwargs):
+        """
+        Wraps the original forward call, doing additional actions before and after the call to facilitate model
+        graph tracing and calling compression-related hooks.
+        """
+        # pylint:disable=protected-access
+        with self.nncf._compressed_context as ctx:  # type: TracingContext
+            ctx.base_module_thread_local_replica = self
+            args, kwargs = replicate_same_tensors((args, kwargs))
+            if not self.nncf._in_user_dummy_forward:
+                # If a user supplies own dummy forward, he is responsible for
+                # correctly wrapping inputs inside it as well.
+                args, kwargs = self.nncf._strip_traced_tensors(args, kwargs)
+                args, kwargs = self.nncf._wrap_inputs_fn(args, kwargs)
+
+            # For purposes of scope tracking, need the original forward call to occur as if it were
+            # a module call of the correponding object.
+            if self.nncf._bound_original_forward is None:
+                retval = wrap_module_call(self.nncf._original_unbound_forward)(self, *args, **kwargs)
+            else:
+
+                def _unbound_like_original_forward(_self, *args, **kwargs):
+                    return self.nncf._bound_original_forward(*args, **kwargs)
+
+                retval = wrap_module_call(_unbound_like_original_forward)(self, *args, **kwargs)
+
+            retval = replicate_same_tensors(retval)
+            if not self.nncf._in_user_dummy_forward:
+                retval = self.nncf._wrap_outputs_fn(retval)
+
+        if self.nncf._kd_loss_handler is not None and self.training:
+            self.nncf._kd_loss_handler(retval, *args, **kwargs)
+        return retval
+
+    @property
+    def nncf(self) -> NNCFNetworkInterface:
+        """
+        Accessor for all NNCF-specific methods and attributes of the compressed model object.
+        """
+        # self._nncf is being set in the creation function defined in the NNCFNetworkMeta metaclass
+        return self._nncf
+
+    def __getattr__(self, key):
+        """
+        Only defined for purposes of deprecation warnings. This method should be removed after v2.5.0.
+        """
+        try:
+            return super().__getattr__(key)
+        except AttributeError as e:
+            if hasattr(self._nncf, key):
+                warning_deprecated(
+                    "Old style of accessing NNCF-specific attributes and methods on NNCFNetwork "
+                    "objects is deprecated. "
+                    "Access the NNCF-specific attrs through the NNCFInterface, which is "
+                    "set up as an `nncf` attribute on the compressed model object.\n"
+                    "For instance, instead of `compressed_model.get_graph()` "
+                    "you should now write `compressed_model.nncf.get_graph()`.\n"
+                    "The old style will be removed after NNCF v2.5.0"
+                )
+                return getattr(self._nncf, key)
+            raise e
+
+    def __setattr__(self, key, value):
+        # If setting `forward`, set it on the original model.
+        if key == "forward":
+            nncf_logger.warning(
+                "You are setting `forward` on an NNCF-processed model object.\n"
+                "NNCF relies on custom-wrapping the `forward` call in order to function properly.\n"
+                "Arbitrary adjustments to the forward function on an NNCFNetwork object have undefined "
+                "behaviour.\n"
+                "If you need to replace the underlying forward function of the original model so that "
+                "NNCF should be using that instead of the original forward function that NNCF saved "
+                "during the compressed model creation, you can do this by calling:\n"
+                "model.nncf.set_original_unbound_forward(fn)\n"
+                "if `fn` has an unbound 0-th `self` argument, or\n"
+                "with model.nncf.temporary_bound_original_forward(fn): ...\n"
+                "if `fn` already had 0-th `self` argument bound or never had it in the first place."
+            )
+        super().__setattr__(key, value)
+
+    def get_nncf_wrapped_model(self) -> "NNCFNetwork":
+        warning_deprecated(
+            "Calls to NNCFNetwork.get_nncf_wrapped_model() are deprecated and will be removed "
+            "in NNCF v2.6.0.\n"
+            "Starting from NNCF v2.5.0, the compressed model object already inherits the original "
+            "class of the uncompressed model and the forward signature, so the call to "
+            ".get_nncf_wrapped_model() may be simply omitted."
+        )
+        return self
+
+class NNCFSkippingIter:
+    """
+    An iterator over the regular torch.nn.Module iterator that will skip NNCFInterface objects if they come up.
+    """
+
+    def __init__(self, iter_to_wrap: Iterator[Module]):
+        self._iter_to_wrap = iter_to_wrap
+
+    def __next__(self):
+        item = next(self._iter_to_wrap)
+        if isinstance(item, NNCFNetworkInterface):
+            item = next(self._iter_to_wrap)
+        return item
+
+
+class LoadStateListener:
+    """
+    Resets the initialization flags (`initialized`) for all quantization modules on `load_state_dict` call.
+    These flags are used to update not loaded params (from checkpoint or model's state)
+    on initialization stage of algorithm.
+    Flags reset is required on each call of `load_state_dict`, because internal method (`build_graph`)
+    restores model state by calling this method.
+    """
+
+    def __init__(self, model: "NNCFNetwork", all_quantizations: Dict[str, torch.nn.Module]):
+        # pylint: disable=protected-access
+        self.hook = model._register_load_state_dict_pre_hook(
+            functools.partial(self.hook_fn, quantize_modules=list(all_quantizations.values()))
+        )
+
+    def hook_fn(
+        self,
+        state_dict,
+        prefix,
+        local_metadata,
+        strict,
+        missing_keys,
+        unexpected_keys,
+        error_msgs,
+        quantize_modules: List[torch.nn.Module],
+    ):
+        for module in quantize_modules:
+            module.initialized = False
+
+    def close(self):
+        self.hook.remove()
+
 
 class PTModelTransformer(ModelTransformer):
     def __init__(self, model: NNCFNetwork):
         super().__init__(model)
-        self._node_to_op_address_mapping = model.get_node_to_op_address_mapping()
+        self._node_to_op_address_mapping = model.nncf.get_node_to_op_address_mapping()
 
     def transform(self, transformation_layout: PTTransformationLayout) -> NNCFNetwork:
         fns_grouped_by_points = {}  # type: Dict[PTInsertionPoint, List[Tuple[Callable, TransformationPriority]]]
         for transformation_command in transformation_layout.transformations:  # type: PTInsertionCommand
             target_point = transformation_command.target_point  # type: PTTargetPoint
             target_node_name = target_point.target_node_name
-            pt_ip = PTInsertionPoint(target_type=target_point.target_type,
-                                     op_address=self._node_to_op_address_mapping[target_node_name],
-                                     input_port_id=target_point.input_port_id)
+            pt_ip = PTInsertionPoint(
+                target_type=target_point.target_type,
+                op_address=self._node_to_op_address_mapping[target_node_name],
+                input_port_id=target_point.input_port_id,
+            )
             fn = transformation_command.fn
             if target_point.type is TargetType.OPERATION_WITH_WEIGHTS:
                 fn = UpdateWeight(fn)
             tup = (fn, transformation_command.priority)
             if pt_ip not in fns_grouped_by_points:
                 fns_grouped_by_points[pt_ip] = [tup]
             else:
                 fns_grouped_by_points[pt_ip].append(tup)
 
         for pt_ip, fn_list_with_priority in fns_grouped_by_points.items():
             fn_list_with_priority = sorted(fn_list_with_priority, key=lambda x: x[1])
-            self._model.insert_at_point(pt_ip, [x[0] for x in fn_list_with_priority])
+            self._model.nncf.insert_at_point(pt_ip, [x[0] for x in fn_list_with_priority])
         return self._model
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/base_algo.py` & `nncf-2.5.0/nncf/torch/pruning/base_algo.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,173 +1,180 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import List, Dict
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Dict, List
 
 import torch
-
-from nncf.config.schemata.defaults import PRUNE_BATCH_NORMS
-from nncf.config.schemata.defaults import PRUNE_DOWNSAMPLE_CONVS
-from nncf.config.schemata.defaults import PRUNE_FIRST_CONV
-from nncf.torch.pruning.tensor_processor import PTNNCFPruningTensorProcessor
 from texttable import Texttable
 from torch import nn
 
 from nncf import NNCFConfig
 from nncf.common.graph.transformations.commands import TargetType
+from nncf.common.logging import nncf_logger
 from nncf.common.pruning.clusterization import Cluster
 from nncf.common.pruning.clusterization import Clusterization
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
 from nncf.common.pruning.utils import is_prunable_depthwise_conv
-from nncf.common.logging import nncf_logger
 from nncf.config.extractors import extract_algo_specific_config
+from nncf.config.schemata.defaults import PRUNE_BATCH_NORMS
+from nncf.config.schemata.defaults import PRUNE_DOWNSAMPLE_CONVS
+from nncf.config.schemata.defaults import PRUNE_FIRST_CONV
 from nncf.torch.algo_selector import ZeroCompressionLoss
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.graph.transformations.commands import TransformationPriority
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.module_operations import UpdateWeightAndBias
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.pruning.operations import PT_PRUNING_OPERATOR_METATYPES
 from nncf.torch.pruning.structs import PrunedModuleInfo
+from nncf.torch.pruning.tensor_processor import PTNNCFPruningTensorProcessor
 from nncf.torch.pruning.utils import init_output_masks_in_graph
 from nncf.torch.utils import get_model_device
 
 
 class BasePruningAlgoBuilder(PTCompressionAlgorithmBuilder):
     def __init__(self, config, should_init: bool = True):
         super().__init__(config, should_init)
-        params = self._algo_config.get('params', {})
+        params = self._algo_config.get("params", {})
         self._set_default_params_for_ranking_type(params)
         self._params = params
 
-        self.prune_first = params.get('prune_first_conv', PRUNE_FIRST_CONV)
-        self.prune_batch_norms = params.get('prune_batch_norms', PRUNE_BATCH_NORMS)
-        self.prune_downsample_convs = params.get('prune_downsample_convs', PRUNE_DOWNSAMPLE_CONVS)
+        self.prune_first = params.get("prune_first_conv", PRUNE_FIRST_CONV)
+        self.prune_batch_norms = params.get("prune_batch_norms", PRUNE_BATCH_NORMS)
+        self.prune_downsample_convs = params.get("prune_downsample_convs", PRUNE_DOWNSAMPLE_CONVS)
 
         self._prunable_types = self.get_op_types_of_pruned_modules()
 
-        from nncf.common.pruning.node_selector import PruningNodeSelector #pylint: disable=cyclic-import
-        self.pruning_node_selector = PruningNodeSelector(PT_PRUNING_OPERATOR_METATYPES,
-                                                         self._prunable_types,
-                                                         self.get_types_of_grouping_ops(),
-                                                         self.ignored_scopes,
-                                                         self.target_scopes,
-                                                         self.prune_first,
-                                                         self.prune_downsample_convs)
+        from nncf.common.pruning.node_selector import PruningNodeSelector  # pylint: disable=cyclic-import
+
+        self.pruning_node_selector = PruningNodeSelector(
+            PT_PRUNING_OPERATOR_METATYPES,
+            self._prunable_types,
+            self.get_types_of_grouping_ops(),
+            self.ignored_scopes,
+            self.target_scopes,
+            self.prune_first,
+            self.prune_downsample_convs,
+        )
 
         self.pruned_module_groups_info = []
         self._pruned_norms_operators = []
 
     @staticmethod
     def _set_default_params_for_ranking_type(params: Dict) -> None:
         """
         Setting default parameter values of pruning algorithm depends on the ranking type:
         for learned_ranking `all_weights` must be True (in case of False was set by the user, an Exception will be
         raised), `prune_first_conv`, `prune_downsample_convs` are recommended to be True (this
         params will be set to True by default (and remain unchanged if the user sets some value).
         :param params: dict with parameters of the algorithm from config
         """
-        learned_ranking = 'interlayer_ranking_type' in params and params['interlayer_ranking_type'] == 'learned_ranking'
+        learned_ranking = "interlayer_ranking_type" in params and params["interlayer_ranking_type"] == "learned_ranking"
         if not learned_ranking:
             return
-        nncf_logger.info('LeGR pruning sets `prune_first_conv`, `prune_downsample_convs`, '
-                         '`all_weights` to True by default. Adjusting this is not recommended.')
-        params.setdefault('prune_first_conv', True)
-        params.setdefault('prune_downsample_convs', True)
-        if params.get('all_weights') is False:
-            raise Exception('For LeGR pruning the `all_weights` config parameter be set to `true`.'
-                            'Adjust the config accordingly if you want to proceed.')
-        params.setdefault('all_weights', True)
+        nncf_logger.info(
+            "LeGR pruning sets `prune_first_conv`, `prune_downsample_convs`, "
+            "`all_weights` to True by default. Adjusting this is not recommended."
+        )
+        params.setdefault("prune_first_conv", True)
+        params.setdefault("prune_downsample_convs", True)
+        if params.get("all_weights") is False:
+            raise Exception(
+                "For LeGR pruning the `all_weights` config parameter be set to `true`."
+                "Adjust the config accordingly if you want to proceed."
+            )
+        params.setdefault("all_weights", True)
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         layout = PTTransformationLayout()
         commands = self._prune_weights(target_model)
         for command in commands:
             layout.register(command)
         return layout
 
     def _prune_weights(self, target_model: NNCFNetwork):
-        target_model_graph = target_model.get_original_graph()
+        target_model_graph = target_model.nncf.get_original_graph()
         groups_of_nodes_to_prune = self.pruning_node_selector.create_pruning_groups(target_model_graph)
 
         device = get_model_device(target_model)
         insertion_commands = []
         self.pruned_module_groups_info = Clusterization[PrunedModuleInfo](lambda x: x.node_name)
 
         for i, group in enumerate(groups_of_nodes_to_prune.get_all_clusters()):
             group_minfos = []
             for node in group.elements:
                 node_name = node.node_name
-                module = target_model.get_containing_module(node_name)
+                module = target_model.nncf.get_containing_module(node_name)
                 module_scope = target_model_graph.get_scope_by_node_name(node_name)
                 # Check that we need to prune weights in this op
                 assert self._is_pruned_module(module)
 
                 nncf_logger.debug(f"Will prune the weights for the operation: {node_name}")
                 pruning_block = self.create_weight_pruning_operation(module, node_name)
                 # Hook for weights and bias
                 hook = UpdateWeightAndBias(pruning_block).to(device)
                 insertion_commands.append(
                     PTInsertionCommand(
-                        PTTargetPoint(TargetType.PRE_LAYER_OPERATION,
-                                      target_node_name=node_name),
+                        PTTargetPoint(TargetType.PRE_LAYER_OPERATION, target_node_name=node_name),
                         hook,
-                        TransformationPriority.PRUNING_PRIORITY
+                        TransformationPriority.PRUNING_PRIORITY,
+                    )
+                )
+                group_minfos.append(
+                    PrunedModuleInfo(
+                        node_name=node_name,
+                        module_scope=module_scope,
+                        module=module,
+                        operand=pruning_block,
+                        node_id=node.node_id,
+                        is_depthwise=is_prunable_depthwise_conv(node),
                     )
                 )
-                group_minfos.append(PrunedModuleInfo(node_name=node_name,
-                                                     module_scope=module_scope,
-                                                     module=module,
-                                                     operand=pruning_block,
-                                                     node_id=node.node_id,
-                                                     is_depthwise=is_prunable_depthwise_conv(node)))
 
             cluster = Cluster[PrunedModuleInfo](i, group_minfos, [n.node_id for n in group.elements])
             self.pruned_module_groups_info.add_cluster(cluster)
 
         # Propagate masks to find norm layers to prune
         init_output_masks_in_graph(target_model_graph, self.pruned_module_groups_info.get_all_nodes())
-        MaskPropagationAlgorithm(target_model_graph, PT_PRUNING_OPERATOR_METATYPES,
-                                 PTNNCFPruningTensorProcessor).mask_propagation()
+        MaskPropagationAlgorithm(
+            target_model_graph, PT_PRUNING_OPERATOR_METATYPES, PTNNCFPruningTensorProcessor
+        ).mask_propagation()
 
         # Adding binary masks also for Batch/Group/Layer Norms to allow applying masks after propagation
-        types_to_apply_mask = ['group_norm', 'layer_norm']
+        types_to_apply_mask = ["group_norm", "layer_norm"]
         if self.prune_batch_norms:
-            types_to_apply_mask.append('batch_norm')
+            types_to_apply_mask.append("batch_norm")
 
         all_norm_layers = target_model_graph.get_nodes_by_types(types_to_apply_mask)
         for node in all_norm_layers:
-            if node.data['output_mask'] is None:
+            if node.data["output_mask"] is None:
                 # Skip elements that will not be pruned
                 continue
 
             node_name = node.node_name
-            module = target_model.get_containing_module(node_name)
+            module = target_model.nncf.get_containing_module(node_name)
 
             pruning_block = self.create_weight_pruning_operation(module, node_name)
             # Hook for weights and bias
             hook = UpdateWeightAndBias(pruning_block).to(device)
             insertion_commands.append(
                 PTInsertionCommand(
-                    PTTargetPoint(TargetType.PRE_LAYER_OPERATION,
-                                  target_node_name=node_name),
+                    PTTargetPoint(TargetType.PRE_LAYER_OPERATION, target_node_name=node_name),
                     hook,
-                    TransformationPriority.PRUNING_PRIORITY
+                    TransformationPriority.PRUNING_PRIORITY,
                 )
             )
             self._pruned_norms_operators.append((node, pruning_block, module))
         return insertion_commands
 
     def create_weight_pruning_operation(self, module, node_name):
         raise NotImplementedError
@@ -188,28 +195,31 @@
         raise NotImplementedError
 
     def initialize(self, model: NNCFNetwork) -> None:
         pass
 
 
 class BasePruningAlgoController(PTCompressionAlgorithmController):
-    def __init__(self, target_model: NNCFNetwork,
-                 prunable_types: List[str],
-                 pruned_module_groups_info: Clusterization[PrunedModuleInfo],
-                 config: NNCFConfig):
+    def __init__(
+        self,
+        target_model: NNCFNetwork,
+        prunable_types: List[str],
+        pruned_module_groups_info: Clusterization[PrunedModuleInfo],
+        config: NNCFConfig,
+    ):
         super().__init__(target_model)
         self._loss = ZeroCompressionLoss(get_model_device(target_model))
         self._prunable_types = prunable_types
         self.config = config
-        self.pruning_config = extract_algo_specific_config(config, 'filter_pruning')
-        params = self.pruning_config.get('params', {})
+        self.pruning_config = extract_algo_specific_config(config, "filter_pruning")
+        params = self.pruning_config.get("params", {})
         self.pruned_module_groups_info = pruned_module_groups_info
-        self.prune_batch_norms = params.get('prune_batch_norms', PRUNE_BATCH_NORMS)
-        self.prune_first = params.get('prune_first_conv', PRUNE_FIRST_CONV)
-        self.prune_downsample_convs = params.get('prune_downsample_convs', PRUNE_DOWNSAMPLE_CONVS)
+        self.prune_batch_norms = params.get("prune_batch_norms", PRUNE_BATCH_NORMS)
+        self.prune_first = params.get("prune_first_conv", PRUNE_FIRST_CONV)
+        self.prune_downsample_convs = params.get("prune_downsample_convs", PRUNE_DOWNSAMPLE_CONVS)
         self.prune_flops = False
         self.check_pruning_level(params)
         self._hooks = []
 
     def freeze(self):
         raise NotImplementedError
 
@@ -219,52 +229,32 @@
     def step(self, next_step):
         pass
 
     def check_pruning_level(self, params):
         """
         Check that set only one of pruning target params
         """
-        pruning_target = params.get('pruning_target', None)
-        pruning_flops_target = params.get('pruning_flops_target', None)
+        pruning_target = params.get("pruning_target", None)
+        pruning_flops_target = params.get("pruning_flops_target", None)
         if pruning_target and pruning_flops_target:
-            raise ValueError('Only one parameter from \'pruning_target\' and \'pruning_flops_target\' can be set.')
+            raise ValueError("Only one parameter from 'pruning_target' and 'pruning_flops_target' can be set.")
         if pruning_flops_target:
             self.prune_flops = True
 
     def _clean_hooks(self):
         for h in self._hooks:
             h.remove()
         self._hooks = []
 
     def get_mask(self, minfo: PrunedModuleInfo) -> torch.Tensor:
         """
         Returns pruning mask for minfo.module.
         """
         raise NotImplementedError
 
-    @staticmethod
-    def pruning_level_for_weight(minfo: PrunedModuleInfo):
-        """
-        Calculates sparsity level for all weight nodes.
-        """
-        weight = minfo.module.weight
-        pruning_level = 1 - weight.nonzero().size(0) / weight.view(-1).size(0)
-        return pruning_level
-
-    @staticmethod
-    def pruning_level_for_filters(minfo: PrunedModuleInfo):
-        """
-        Calculates sparsity level for weight filter-wise.
-        """
-        dim = minfo.module.target_weight_dim_for_compression
-        weight = minfo.module.weight.transpose(0, dim).contiguous()
-        filters_sum = weight.view(weight.size(0), -1).sum(axis=1)
-        pruning_level = 1 - len(filters_sum.nonzero()) / filters_sum.size(0)
-        return pruning_level
-
     def pruning_level_for_mask(self, minfo: PrunedModuleInfo):
         mask = self.get_mask(minfo)
         pruning_level = 1 - mask.nonzero().size(0) / max(mask.view(-1).size(0), 1)
         return pruning_level
 
     def mask_shape(self, minfo: PrunedModuleInfo):
         mask = self.get_mask(minfo)
@@ -279,12 +269,12 @@
         header = ["Name", "Weight's shape", "Bias shape", "Layer PR"]
         data = [header]
         for minfo in self.pruned_module_groups_info.get_all_nodes():
             drow = {h: 0 for h in header}
             drow["Name"] = str(minfo.module_scope)
             drow["Weight's shape"] = list(minfo.module.weight.size())
             drow["Bias shape"] = list(minfo.module.bias.size()) if minfo.module.bias is not None else []
-            drow["Layer PR"] = self.pruning_level_for_filters(minfo)
+            drow["Layer PR"] = self.pruning_level_for_mask(minfo)
             row = [drow[h] for h in header]
             data.append(row)
         table.add_rows(data)
         return table
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/export_utils.py` & `nncf-2.5.0/nncf/torch/pruning/export_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,31 +1,29 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 
 def get_input_masks(nx_node, nx_graph):
     """
     Return input masks for all inputs of nx_node.
     """
-    input_edges = list(nx_graph.in_edges(nx_node['key']))
-    input_masks = [nx_graph.nodes[input_node]['output_mask'] for input_node, _ in input_edges]
+    input_edges = list(nx_graph.in_edges(nx_node["key"]))
+    input_masks = [nx_graph.nodes[input_node]["output_mask"] for input_node, _ in input_edges]
     return input_masks
 
 
 def identity_mask_propagation(nx_node, nx_graph):
     """
     Propagates input mask through nx_node.
     """
     input_masks = get_input_masks(nx_node, nx_graph)
     assert len(input_masks) == 1
-    nx_node['input_masks'] = input_masks
-    nx_node['output_mask'] = input_masks[0]
+    nx_node["input_masks"] = input_masks
+    nx_node["output_mask"] = input_masks[0]
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/filter_pruning/algo.py` & `nncf-2.5.0/nncf/torch/pruning/filter_pruning/algo.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,213 +1,224 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import json
+from math import isclose
 from pathlib import Path
 from typing import Dict, List, Tuple, Union
 
 import numpy as np
 import torch
-from math import isclose
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionStage
 from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
-from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
+from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
+from nncf.common.logging import nncf_logger
 from nncf.common.pruning.clusterization import Clusterization
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
 from nncf.common.pruning.schedulers import PRUNING_SCHEDULERS
 from nncf.common.pruning.schedulers import PruningScheduler
+from nncf.common.pruning.shape_pruning_processor import ShapePruningProcessor
 from nncf.common.pruning.statistics import FilterPruningStatistics
-from nncf.common.pruning.statistics import PrunedModelTheoreticalBorderline
 from nncf.common.pruning.statistics import PrunedLayerSummary
 from nncf.common.pruning.statistics import PrunedModelStatistics
-from nncf.common.pruning.shape_pruning_processor import ShapePruningProcessor
-from nncf.common.pruning.weights_flops_calculator import WeightsFlopsCalculator
-from nncf.common.pruning.utils import get_rounded_pruned_element_number
+from nncf.common.pruning.statistics import PrunedModelTheoreticalBorderline
 from nncf.common.pruning.utils import get_prunable_layers_in_out_channels
+from nncf.common.pruning.utils import get_rounded_pruned_element_number
+from nncf.common.pruning.weights_flops_calculator import WeightsFlopsCalculator
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
+from nncf.common.utils.backend import copy_model
 from nncf.common.utils.debug import is_debug
-from nncf.common.logging import nncf_logger
 from nncf.common.utils.os import safe_open
 from nncf.config.extractors import extract_bn_adaptation_init_params
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
+from nncf.torch.graph.operator_metatypes import PTDepthwiseConv1dSubtype
+from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
+from nncf.torch.graph.operator_metatypes import PTDepthwiseConv3dSubtype
 from nncf.torch.graph.operator_metatypes import PTModuleConv1dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConv2dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConv3dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConvTranspose1dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConvTranspose2dMetatype
 from nncf.torch.graph.operator_metatypes import PTModuleConvTranspose3dMetatype
-from nncf.torch.graph.operator_metatypes import PTDepthwiseConv1dSubtype
-from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
-from nncf.torch.graph.operator_metatypes import PTDepthwiseConv3dSubtype
 from nncf.torch.graph.operator_metatypes import PTModuleLinearMetatype
 from nncf.torch.layers import NNCF_PRUNING_MODULES_DICT
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.pruning.base_algo import BasePruningAlgoBuilder
 from nncf.torch.pruning.base_algo import BasePruningAlgoController
-from nncf.torch.pruning.operations import PTElementwisePruningOp
-from nncf.torch.pruning.operations import PT_PRUNING_OPERATOR_METATYPES
-from nncf.torch.pruning.tensor_processor import PTNNCFPruningTensorProcessor
 from nncf.torch.pruning.filter_pruning.functions import FILTER_IMPORTANCE_FUNCTIONS
 from nncf.torch.pruning.filter_pruning.functions import calculate_binary_mask
 from nncf.torch.pruning.filter_pruning.functions import tensor_l2_normalizer
 from nncf.torch.pruning.filter_pruning.global_ranking.legr import LeGR
 from nncf.torch.pruning.filter_pruning.layers import FilterPruningMask
+from nncf.torch.pruning.operations import PT_PRUNING_OPERATOR_METATYPES
+from nncf.torch.pruning.operations import ModelPruner
+from nncf.torch.pruning.operations import PrunType
+from nncf.torch.pruning.operations import PTElementwisePruningOp
 from nncf.torch.pruning.structs import PrunedModuleInfo
+from nncf.torch.pruning.tensor_processor import PTNNCFPruningTensorProcessor
 from nncf.torch.pruning.utils import collect_output_shapes
 from nncf.torch.pruning.utils import init_output_masks_in_graph
-from nncf.torch.structures import LeGRInitArgs, DistributedCallbacksArgs
+from nncf.torch.structures import DistributedCallbacksArgs
+from nncf.torch.structures import LeGRInitArgs
 from nncf.torch.utils import get_filters_num
 
-
 GENERAL_CONV_LAYER_METATYPES = [
     PTModuleConv1dMetatype,
     PTDepthwiseConv1dSubtype,
     PTModuleConv2dMetatype,
     PTDepthwiseConv2dSubtype,
     PTModuleConv3dMetatype,
     PTDepthwiseConv3dSubtype,
     PTModuleConvTranspose1dMetatype,
     PTModuleConvTranspose2dMetatype,
     PTModuleConvTranspose3dMetatype,
 ]
-LINEAR_LAYER_METATYPES = [
-    PTModuleLinearMetatype
-]
+LINEAR_LAYER_METATYPES = [PTModuleLinearMetatype]
 
 
-@PT_COMPRESSION_ALGORITHMS.register('filter_pruning')
+@PT_COMPRESSION_ALGORITHMS.register("filter_pruning")
 class FilterPruningBuilder(BasePruningAlgoBuilder):
     def create_weight_pruning_operation(self, module, node_name):
-        return FilterPruningMask(module.weight.size(module.target_weight_dim_for_compression),
-                                 node_name, module.target_weight_dim_for_compression)
+        return FilterPruningMask(
+            module.weight.size(module.target_weight_dim_for_compression),
+            node_name,
+            module.target_weight_dim_for_compression,
+        )
 
     def _build_controller(self, model: NNCFNetwork) -> PTCompressionAlgorithmController:
-        return FilterPruningController(model,
-                                       self._prunable_types,
-                                       self.pruned_module_groups_info,
-                                       self._pruned_norms_operators,
-                                       self.config)
+        return FilterPruningController(
+            model, self._prunable_types, self.pruned_module_groups_info, self._pruned_norms_operators, self.config
+        )
 
     def _is_pruned_module(self, module) -> bool:
         # Currently prune only Convolutions
         return isinstance(module, tuple(NNCF_PRUNING_MODULES_DICT.keys()))
 
     def get_op_types_of_pruned_modules(self) -> List[str]:
         types = [v.op_func_name for v in NNCF_PRUNING_MODULES_DICT]
         return types
 
     def get_types_of_grouping_ops(self) -> List[str]:
         return PTElementwisePruningOp.get_all_op_aliases()
 
 
-@ADAPTIVE_COMPRESSION_CONTROLLERS.register('pt_filter_pruning')
+@api()
+@ADAPTIVE_COMPRESSION_CONTROLLERS.register("pt_filter_pruning")
 class FilterPruningController(BasePruningAlgoController):
-    def __init__(self, target_model: NNCFNetwork,
-                 prunable_types: List[str],
-                 pruned_module_groups: Clusterization[PrunedModuleInfo],
-                 pruned_norms_operators: List[Tuple[NNCFNode, FilterPruningMask, torch.nn.Module]],
-                 config: NNCFConfig):
-        #pylint:disable=too-many-statements
+    def __init__(
+        self,
+        target_model: NNCFNetwork,
+        prunable_types: List[str],
+        pruned_module_groups: Clusterization[PrunedModuleInfo],
+        pruned_norms_operators: List[Tuple[NNCFNode, FilterPruningMask, torch.nn.Module]],
+        config: NNCFConfig,
+    ):
+        # pylint:disable=too-many-statements
         super().__init__(target_model, prunable_types, pruned_module_groups, config)
-        params = self.pruning_config.get('params', {})
+        params = self.pruning_config.get("params", {})
         self._pruned_norms_operators = pruned_norms_operators
         self.frozen = False
         self._pruning_level = 0
-        self.pruning_init = self.pruning_config.get('pruning_init', 0)
+        self.pruning_init = self.pruning_config.get("pruning_init", 0)
         self.pruning_quota = 0.9
         self.normalize_weights = True
 
-        self._graph = self._model.get_original_graph()
-        self._weights_flops_calc = WeightsFlopsCalculator(conv_op_metatypes=GENERAL_CONV_LAYER_METATYPES,
-                                                          linear_op_metatypes=LINEAR_LAYER_METATYPES)
-
-        self._shape_pruning_proc = ShapePruningProcessor(pruning_operations_metatype=PT_PRUNING_OPERATOR_METATYPES,
-                                                          prunable_types=prunable_types)
+        self._graph = self._model.nncf.get_original_graph()
+        self._weights_flops_calc = WeightsFlopsCalculator(
+            conv_op_metatypes=GENERAL_CONV_LAYER_METATYPES, linear_op_metatypes=LINEAR_LAYER_METATYPES
+        )
+
+        self._shape_pruning_proc = ShapePruningProcessor(
+            pruning_operations_metatype=PT_PRUNING_OPERATOR_METATYPES, prunable_types=prunable_types
+        )
         self.pruning_quotas = {}
         self.nodes_flops = {}  # type: Dict[NNCFNodeName, int]
         self.nodes_params_num = {}  # type: Dict[NNCFNodeName, int]
         self._next_nodes = {}  # type: Dict[int, List[NNCFNodeName]]
-        self._output_shapes = {} # type: Dict[NNCFNodeName, int]
+        self._output_shapes = {}  # type: Dict[NNCFNodeName, int]
         _, modules_out_channels = get_prunable_layers_in_out_channels(self._graph)
         self._init_pruned_layers_params(modules_out_channels)
         self.flops_count_init()
 
         self.full_flops = sum(self.nodes_flops.values())
         self.current_flops = self.full_flops
         self.full_params_num = sum(self.nodes_params_num.values())
         self.current_params_num = self.full_params_num
         self.full_filters_num = self._weights_flops_calc.count_filters_num(self._graph, modules_out_channels)
         self.current_filters_num = self.full_filters_num
         self._pruned_layers_num = len(self.pruned_module_groups_info.get_all_nodes())
-        self._prunable_layers_num = len(self._model.get_graph().get_nodes_by_types(self._prunable_types))
-        self._min_possible_flops, self._min_possible_params =\
-            self._calculate_flops_and_weights_in_uniformly_pruned_model(1.)
+        self._prunable_layers_num = len(self._model.nncf.get_graph().get_nodes_by_types(self._prunable_types))
+        (
+            self._min_possible_flops,
+            self._min_possible_params,
+        ) = self._calculate_flops_and_weights_in_uniformly_pruned_model(1.0)
 
         self.weights_normalizer = tensor_l2_normalizer  # for all weights in common case
-        self.filter_importance = FILTER_IMPORTANCE_FUNCTIONS.get(params.get('filter_importance', 'L2'))
-        self.ranking_type = params.get('interlayer_ranking_type', 'unweighted_ranking')
+        self.filter_importance = FILTER_IMPORTANCE_FUNCTIONS.get(params.get("filter_importance", "L2"))
+        self.ranking_type = params.get("interlayer_ranking_type", "unweighted_ranking")
         self.all_weights = params.get("all_weights", False)
-        scheduler_cls = PRUNING_SCHEDULERS.get(params.get('schedule', 'exponential'))
+        scheduler_cls = PRUNING_SCHEDULERS.get(params.get("schedule", "exponential"))
         self._scheduler = scheduler_cls(self, params)
 
-        if self.ranking_type == 'learned_ranking':
+        if self.ranking_type == "learned_ranking":
             # In case of learned_ranking ranking type weights shouldn't be normalized
             self.normalize_weights = False
-            if params.get('load_ranking_coeffs_path'):
-                coeffs_path = params.get('load_ranking_coeffs_path')
-                nncf_logger.info(f'Loading ranking coefficients from file {coeffs_path}')
+            if params.get("load_ranking_coeffs_path"):
+                coeffs_path = params.get("load_ranking_coeffs_path")
+                nncf_logger.info(f"Loading ranking coefficients from file {coeffs_path}")
                 try:
-                    with safe_open(Path(coeffs_path), 'r', encoding='utf8') as coeffs_file:
+                    with safe_open(Path(coeffs_path), "r", encoding="utf8") as coeffs_file:
                         loaded_coeffs = json.load(coeffs_file)
                 except (ValueError, FileNotFoundError) as err:
-                    raise Exception('Can\'t load json with ranking coefficients. Please, check format of json file '
-                                    'and path to the file.') from err
+                    raise Exception(
+                        "Can't load json with ranking coefficients. Please, check format of json file "
+                        "and path to the file."
+                    ) from err
                 ranking_coeffs = {key: tuple(loaded_coeffs[key]) for key in loaded_coeffs}
-                nncf_logger.debug(f'Loaded ranking coefficients = {ranking_coeffs}')
+                nncf_logger.debug(f"Loaded ranking coefficients = {ranking_coeffs}")
                 self.ranking_coeffs = ranking_coeffs
             else:
                 # Ranking can't be trained without registered init struct LeGRInitArgs
                 if not config.has_extra_struct(LeGRInitArgs):
-                    raise Exception('Please, register LeGRInitArgs via register_default_init_args function.')
+                    raise Exception("Please, register LeGRInitArgs via register_default_init_args function.")
                 # Wrapping model for parallelization
                 distributed_wrapping_init_args = config.get_extra_struct(DistributedCallbacksArgs)
                 target_model = distributed_wrapping_init_args.wrap_model(target_model)
                 legr_init_args = config.get_extra_struct(LeGRInitArgs)
                 legr_params = params.get("legr_params", {})
-                if 'max_pruning' not in legr_params:
-                    legr_params['max_pruning'] = self._scheduler.target_level
+                if "max_pruning" not in legr_params:
+                    legr_params["max_pruning"] = self._scheduler.target_level
                 self.legr = LeGR(self, target_model, legr_init_args, **legr_params)
                 self.ranking_coeffs = self.legr.train_global_ranking()
-                nncf_logger.debug(f'Trained ranking coefficients = {self.ranking_coeffs}')
+                nncf_logger.debug(f"Trained ranking coefficients = {self.ranking_coeffs}")
                 # Unwrapping parallelized model
                 _ = distributed_wrapping_init_args.unwrap_model(target_model)
         else:
             self.ranking_coeffs = {node.node_name: (1, 0) for node in self.pruned_module_groups_info.get_all_nodes()}
 
         # Saving ranking coefficients to the specified file
-        if params.get('save_ranking_coeffs_path'):
+        if params.get("save_ranking_coeffs_path"):
             nncf_logger.info(f'Saving ranking coefficients to the file {params.get("save_ranking_coeffs_path")}')
-            with safe_open(Path(params.get('save_ranking_coeffs_path')), 'w', encoding='utf8') as f:
+            with safe_open(Path(params.get("save_ranking_coeffs_path")), "w", encoding="utf8") as f:
                 json.dump(self.ranking_coeffs, f)
 
         self.set_pruning_level(self.pruning_init)
         self._bn_adaptation = None
 
     @property
     def loss(self) -> CompressionLoss:
@@ -224,42 +235,52 @@
     @staticmethod
     def set_mask(minfo: PrunedModuleInfo, mask: torch.Tensor) -> None:
         minfo.operand.binary_filter_pruning_mask = mask
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         if not quickly_collected_only and is_debug():
             stats = PrunedModelTheoreticalBorderline(
-                self._pruned_layers_num, self._prunable_layers_num, self._min_possible_flops,
-                self._min_possible_params, self.full_flops, self.full_params_num)
+                self._pruned_layers_num,
+                self._prunable_layers_num,
+                self._min_possible_flops,
+                self._min_possible_params,
+                self.full_flops,
+                self.full_params_num,
+            )
 
             nncf_logger.debug(stats.to_str())
 
         pruned_layers_summary = {}
         for minfo in self.pruned_module_groups_info.get_all_nodes():
             layer_name = str(minfo.module_scope)
             if layer_name not in pruned_layers_summary:
-                pruned_layers_summary[layer_name] = \
-                    PrunedLayerSummary(layer_name,
-                                       list(minfo.module.weight.size()),
-                                       list(self.mask_shape(minfo)),
-                                       self.pruning_level_for_mask(minfo))
+                pruned_layers_summary[layer_name] = PrunedLayerSummary(
+                    layer_name,
+                    list(minfo.module.weight.size()),
+                    list(self.mask_shape(minfo)),
+                    self.pruning_level_for_mask(minfo),
+                )
 
         self._update_benchmark_statistics()
-        model_statistics = PrunedModelStatistics(self.full_flops, self.current_flops,
-                                                 self.full_params_num, self.current_params_num,
-                                                 self.full_filters_num, self.current_filters_num,
-                                                 list(pruned_layers_summary.values()))
-
-        stats = FilterPruningStatistics(model_statistics,
-                                        self.scheduler.current_pruning_level,
-                                        self.scheduler.target_level,
-                                        self.prune_flops)
+        model_statistics = PrunedModelStatistics(
+            self.full_flops,
+            self.current_flops,
+            self.full_params_num,
+            self.current_params_num,
+            self.full_filters_num,
+            self.current_filters_num,
+            list(pruned_layers_summary.values()),
+        )
+
+        stats = FilterPruningStatistics(
+            model_statistics, self.scheduler.current_pruning_level, self.scheduler.target_level, self.prune_flops
+        )
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('filter_pruning', stats)
+        nncf_stats.register("filter_pruning", stats)
         return nncf_stats
 
     @property
     def pruning_level(self) -> float:
         """Global pruning level in the model"""
         return self._pruning_level
 
@@ -271,42 +292,47 @@
         self._output_shapes = collect_output_shapes(self._graph)
 
         # 2. Init next_nodes for every pruning cluster
         self._next_nodes = self._shape_pruning_proc.get_next_nodes(self._graph, self.pruned_module_groups_info)
 
         # 3. Init pruning quotas
         for cluster in self.pruned_module_groups_info.get_all_clusters():
-            self.pruning_quotas[cluster.id] = np.floor(full_out_channels[cluster.elements[0].node_name] \
-                                                       * self.pruning_quota)
+            self.pruning_quotas[cluster.id] = np.floor(
+                full_out_channels[cluster.elements[0].node_name] * self.pruning_quota
+            )
 
     def flops_count_init(self) -> None:
-        self.nodes_flops, self.nodes_params_num = \
-            self._weights_flops_calc.count_flops_and_weights_per_node(self._graph, self._output_shapes)
+        self.nodes_flops, self.nodes_params_num = self._weights_flops_calc.count_flops_and_weights_per_node(
+            self._graph, self._output_shapes
+        )
 
     def _calculate_flops_and_weights_in_uniformly_pruned_model(self, pruning_level: float) -> Tuple[int, int]:
         """
         Prune all prunable modules in model by pruning_level level and returns number of weights and
         flops of the pruned model.
 
         :param pruning_level: proportion of zero filters in all modules
         :return: flops number in pruned model
         """
-        tmp_in_channels, tmp_out_channels = \
-            self._shape_pruning_proc.calculate_in_out_channels_in_uniformly_pruned_model(
-                graph=self._graph,
-                pruning_groups=self.pruned_module_groups_info,
-                pruning_groups_next_nodes=self._next_nodes,
-                pruning_level=pruning_level)
-
+        (
+            tmp_in_channels,
+            tmp_out_channels,
+        ) = self._shape_pruning_proc.calculate_in_out_channels_in_uniformly_pruned_model(
+            graph=self._graph,
+            pruning_groups=self.pruned_module_groups_info,
+            pruning_groups_next_nodes=self._next_nodes,
+            pruning_level=pruning_level,
+        )
 
         return self._weights_flops_calc.count_flops_and_weights(
-                                             graph=self._graph,
-                                             output_shapes=self._output_shapes,
-                                             input_channels=tmp_in_channels,
-                                             output_channels=tmp_out_channels)
+            graph=self._graph,
+            output_shapes=self._output_shapes,
+            input_channels=tmp_in_channels,
+            output_channels=tmp_out_channels,
+        )
 
     def _find_uniform_pruning_level_for_target_flops(self, target_flops_pruning_level: float) -> float:
         """
         Searching for the minimal uniform layer-wise weight pruning level (proportion of zero filters in a layer)
          needed to achieve the target pruning level in flops.
 
         :param target_flops_pruning_level: target proportion of flops that should be pruned in the model
@@ -323,19 +349,22 @@
             else:
                 left = middle
         flops, params_num = self._calculate_flops_and_weights_in_uniformly_pruned_model(right)
         if flops <= target_flops:
             self.current_flops = flops
             self.current_params_num = params_num
             return right
-        raise RuntimeError("Can't prune the model to get the required "
-                           "pruning level in flops = {}".format(target_flops_pruning_level))
-
-    def set_pruning_level(self, pruning_level: Union[float, Dict[int, float]],
-                          run_batchnorm_adaptation: bool = False) -> None:
+        raise RuntimeError(
+            "Can't prune the model to get the required "
+            "pruning level in flops = {}".format(target_flops_pruning_level)
+        )
+
+    def set_pruning_level(
+        self, pruning_level: Union[float, Dict[int, float]], run_batchnorm_adaptation: bool = False
+    ) -> None:
         """
         Set the global or groupwise pruning level in the model.
         If pruning_level is a float, the correspoding global pruning level is set in the model,
         either in terms of the percentage of filters pruned or as the percentage of flops
         removed, the latter being true in case the "prune_flops" flag of the controller is
         set to True.
         If pruning_level is a dict, the keys should correspond to layer group id's and the
@@ -345,27 +374,26 @@
         passed_pruning_level = pruning_level
 
         if not self.frozen:
             nncf_logger.info("Computing filter importance scores and binary masks...")
             with torch.no_grad():
                 if self.all_weights:
                     if groupwise_pruning_levels_set:
-                        raise RuntimeError('Cannot set group-wise pruning levels with all_weights=True')
+                        raise RuntimeError("Cannot set group-wise pruning levels with all_weights=True")
                     # Non-uniform (global) importance-score-based pruning according
                     # to the global pruning level
                     if self.prune_flops:
                         self._set_binary_masks_for_pruned_modules_globally_by_flops_target(pruning_level)
                     else:
                         self._set_binary_masks_for_pruned_modules_globally(pruning_level)
                 else:
                     if groupwise_pruning_levels_set:
                         group_ids = [group.id for group in self.pruned_module_groups_info.get_all_clusters()]
                         if set(pruning_level.keys()) != set(group_ids):
-                            raise RuntimeError('Groupwise pruning level dict keys do not correspond to '
-                                               'layer group ids')
+                            raise RuntimeError("Groupwise pruning level dict keys do not correspond to layer group ids")
                     else:
                         # Pruning uniformly with the same pruning level across layers
                         if self.prune_flops:
                             # Looking for layerwise pruning level needed for the required flops pruning level
                             pruning_level = self._find_uniform_pruning_level_for_target_flops(pruning_level)
                     self._set_binary_masks_for_pruned_modules_groupwise(pruning_level)
 
@@ -395,42 +423,42 @@
         pruning levels in the model
         """
         groupwise_pruning_level_dict = {}
         for group in self.pruned_module_groups_info.get_all_clusters():
             groupwise_pruning_level_dict[group.id] = self.pruning_level_for_mask(group.elements[0])
         return groupwise_pruning_level_dict
 
-    def _set_binary_masks_for_pruned_modules_groupwise(self,
-                                                       pruning_level: Union[float, Dict[int, float]]) -> None:
+    def _set_binary_masks_for_pruned_modules_groupwise(self, pruning_level: Union[float, Dict[int, float]]) -> None:
         """
         Set the binary mask values according to groupwise pruning level.
         If pruning_level is a float, set the pruning level uniformly across groups.
         If pruning_level is a dict, set specific pruning levels corresponding to each group.
         """
         nncf_logger.debug("Updating binary masks for pruned modules.")
         groupwise_pruning_levels_set = isinstance(pruning_level, dict)
 
         for group in self.pruned_module_groups_info.get_all_clusters():
-            group_pruning_level = pruning_level[group.id] if groupwise_pruning_levels_set \
-                else pruning_level
+            group_pruning_level = pruning_level[group.id] if groupwise_pruning_levels_set else pruning_level
 
             filters_num = torch.tensor([get_filters_num(minfo.module) for minfo in group.elements])
             assert torch.all(filters_num == filters_num[0])
             device = group.elements[0].module.weight.device
 
             cumulative_filters_importance = torch.zeros(filters_num[0]).to(device)
             # 1. Calculate cumulative importance for all filters in group
             for minfo in group.elements:
-                filters_importance = self.filter_importance(minfo.module.weight,
-                                                            minfo.module.target_weight_dim_for_compression)
+                filters_importance = self.filter_importance(
+                    minfo.module.weight, minfo.module.target_weight_dim_for_compression
+                )
                 cumulative_filters_importance += filters_importance
 
             # 2. Calculate threshold
-            num_of_sparse_elems = get_rounded_pruned_element_number(cumulative_filters_importance.size(0),
-                                                                    group_pruning_level)
+            num_of_sparse_elems = get_rounded_pruned_element_number(
+                cumulative_filters_importance.size(0), group_pruning_level
+            )
             threshold = sorted(cumulative_filters_importance)[min(num_of_sparse_elems, filters_num[0] - 1)]
             mask = calculate_binary_mask(cumulative_filters_importance, threshold)
 
             # 3. Set binary masks for filter
             for minfo in group.elements:
                 pruning_module = minfo.operand
                 pruning_module.binary_filter_pruning_mask = mask
@@ -453,16 +481,17 @@
             assert torch.all(filters_num == filters_num[0])
             device = group.elements[0].module.weight.device
 
             cumulative_filters_importance = torch.zeros(filters_num[0]).to(device)
             # Calculate cumulative importance for all filters in this group
             for minfo in group.elements:
                 normalized_weight = self.weights_normalizer(minfo.module.weight)
-                filters_importance = self.filter_importance(normalized_weight,
-                                                            minfo.module.target_weight_dim_for_compression)
+                filters_importance = self.filter_importance(
+                    normalized_weight, minfo.module.target_weight_dim_for_compression
+                )
                 cumulative_filters_importance += filters_importance
 
             filter_importances.append(cumulative_filters_importance)
 
         # 2. Calculate one threshold for all weights
         importances = torch.cat(filter_importances)
         threshold = sorted(importances)[int(pruning_level * importances.size(0))]
@@ -473,30 +502,28 @@
             for minfo in group.elements:
                 pruning_module = minfo.operand
                 pruning_module.binary_filter_pruning_mask = mask
 
         # Calculate actual flops and weights number with new masks
         self._update_benchmark_statistics()
 
-    def _set_binary_masks_for_pruned_modules_globally_by_flops_target(self,
-                                                                      target_flops_pruning_level: float) -> None:
+    def _set_binary_masks_for_pruned_modules_globally_by_flops_target(self, target_flops_pruning_level: float) -> None:
         """
         Sorting all prunable filters in the network by importance and pruning the amount of the
         least important filters sufficient to achieve the target pruning level by flops.
         Filters are pruned one-by-one and the corresponding flops value is checked.
 
         :param target_flops_pruning_level: target proportion of flops removed from the model
         :return:
         """
         target_flops = self.full_flops * (1 - target_flops_pruning_level)
 
         # 1. Initialize masks
         for minfo in self.pruned_module_groups_info.get_all_nodes():
-            new_mask = torch.ones(get_filters_num(minfo.module)).to(
-                minfo.module.weight.device)
+            new_mask = torch.ones(get_filters_num(minfo.module)).to(minfo.module.weight.device)
             self.set_mask(minfo, new_mask)
 
         # 2. Calculate filter importances for all prunable groups
         filter_importances = []
         cluster_indexes = []
         filter_indexes = []
 
@@ -507,18 +534,19 @@
 
             cumulative_filters_importance = torch.zeros(filters_num[0]).to(device)
             # Calculate cumulative importance for all filters in this group
             for minfo in cluster.elements:
                 weight = minfo.module.weight
                 if self.normalize_weights:
                     weight = self.weights_normalizer(weight)
-                filters_importance = self.filter_importance(weight,
-                                                            minfo.module.target_weight_dim_for_compression)
-                scaled_importance = self.ranking_coeffs[minfo.node_name][0] * filters_importance + \
-                                    self.ranking_coeffs[minfo.node_name][1]
+                filters_importance = self.filter_importance(weight, minfo.module.target_weight_dim_for_compression)
+                scaled_importance = (
+                    self.ranking_coeffs[minfo.node_name][0] * filters_importance
+                    + self.ranking_coeffs[minfo.node_name][1]
+                )
                 cumulative_filters_importance += scaled_importance
 
             filter_importances.append(cumulative_filters_importance)
             cluster_indexes.append(cluster.id * torch.ones_like(cumulative_filters_importance))
             filter_indexes.append(torch.arange(len(cumulative_filters_importance)))
 
         importances = torch.cat(filter_importances)
@@ -540,65 +568,71 @@
                 tmp_pruning_quotas[cluster_idx] -= 1
             else:
                 cur_num += 1
                 continue
 
             cluster = self.pruned_module_groups_info.get_cluster_by_id(cluster_idx)
             self._shape_pruning_proc.prune_cluster_shapes(
-                cluster=cluster, pruned_elems=1,
+                cluster=cluster,
+                pruned_elems=1,
                 pruning_groups_next_nodes=self._next_nodes,
-                input_channels=tmp_in_channels, output_channels=tmp_out_channels)
+                input_channels=tmp_in_channels,
+                output_channels=tmp_out_channels,
+            )
 
             for node in cluster.elements:
                 node.operand.binary_filter_pruning_mask[filter_idx] = 0
 
             flops, params_num = self._weights_flops_calc.count_flops_and_weights(
                 graph=self._graph,
                 output_shapes=self._output_shapes,
                 input_channels=tmp_in_channels,
-                output_channels=tmp_out_channels)
+                output_channels=tmp_out_channels,
+            )
             if flops < target_flops:
                 self.current_flops = flops
                 self.current_params_num = params_num
                 return
             cur_num += 1
         raise RuntimeError("Can't prune model to asked flops pruning level")
 
     def _propagate_masks(self):
         nncf_logger.debug("Propagating pruning masks")
         # 1. Propagate masks for all modules
-        graph = self.model.get_original_graph()
+        graph = self.model.nncf.get_original_graph()
 
         init_output_masks_in_graph(graph, self.pruned_module_groups_info.get_all_nodes())
         MaskPropagationAlgorithm(graph, PT_PRUNING_OPERATOR_METATYPES, PTNNCFPruningTensorProcessor).mask_propagation()
 
         # 2. Set the masks for Batch/Group Norms
         pruned_node_modules = []
         for node, pruning_block, node_module in self._pruned_norms_operators:
             if node_module not in pruned_node_modules:
                 # Setting masks for BN nodes
-                pruning_block.binary_filter_pruning_mask = node.data['output_mask'].tensor
+                pruning_block.binary_filter_pruning_mask = node.data["output_mask"].tensor
                 pruned_node_modules.append(node_module)
 
     def prepare_for_export(self):
         """
         Applies pruning masks to layer weights before exporting the model to ONNX.
         """
         self._propagate_masks()
 
         pruned_layers_stats = self.get_stats_for_pruned_modules()
-        nncf_logger.info(f'Pruned layers statistics: \n{pruned_layers_stats.draw()}')
+        nncf_logger.info(f"Pruned layers statistics: \n{pruned_layers_stats.draw()}")
 
     def compression_stage(self) -> CompressionStage:
         target_pruning_level = self.scheduler.target_level
         actual_pruning_level = self._pruning_level
         if actual_pruning_level == 0:
             return CompressionStage.UNCOMPRESSED
-        if isclose(actual_pruning_level, target_pruning_level, abs_tol=1e-5) or \
-                actual_pruning_level > target_pruning_level:
+        if (
+            isclose(actual_pruning_level, target_pruning_level, abs_tol=1e-5)
+            or actual_pruning_level > target_pruning_level
+        ):
             return CompressionStage.FULLY_COMPRESSED
         return CompressionStage.PARTIALLY_COMPRESSED
 
     @property
     def compression_rate(self):
         if self.prune_flops:
             return 1 - self.current_flops / self.full_flops
@@ -629,25 +663,36 @@
         return num_of_sparse_elements_by_node
 
     def _update_benchmark_statistics(self):
         tmp_in_channels, tmp_out_channels = self._shape_pruning_proc.calculate_in_out_channels_by_masks(
             graph=self._graph,
             pruning_groups=self.pruned_module_groups_info,
             pruning_groups_next_nodes=self._next_nodes,
-            num_of_sparse_elements_by_node=self._calculate_num_of_sparse_elements_by_node())
+            num_of_sparse_elements_by_node=self._calculate_num_of_sparse_elements_by_node(),
+        )
 
         self.current_filters_num = self._weights_flops_calc.count_filters_num(
-            graph=self._graph,
-            output_channels=tmp_out_channels)
+            graph=self._graph, output_channels=tmp_out_channels
+        )
 
-        self.current_flops, self.current_params_num = \
-            self._weights_flops_calc.count_flops_and_weights(
-                                          graph=self._graph,
-                                          output_shapes=self._output_shapes,
-                                          input_channels=tmp_in_channels,
-                                          output_channels=tmp_out_channels)
+        self.current_flops, self.current_params_num = self._weights_flops_calc.count_flops_and_weights(
+            graph=self._graph,
+            output_shapes=self._output_shapes,
+            input_channels=tmp_in_channels,
+            output_channels=tmp_out_channels,
+        )
 
     def _run_batchnorm_adaptation(self):
         if self._bn_adaptation is None:
-            self._bn_adaptation = BatchnormAdaptationAlgorithm(**extract_bn_adaptation_init_params(self.config,
-                                                                                                   'filter_pruning'))
+            self._bn_adaptation = BatchnormAdaptationAlgorithm(
+                **extract_bn_adaptation_init_params(self.config, "filter_pruning")
+            )
         self._bn_adaptation.run(self.model)
+
+    def strip_model(self, model: NNCFNetwork, do_copy: bool = False) -> NNCFNetwork:
+        if do_copy:
+            model = copy_model(model)
+
+        graph = model.nncf.get_original_graph()
+        ModelPruner(model, graph, PT_PRUNING_OPERATOR_METATYPES, PrunType.FILL_ZEROS).prune_model()
+
+        return model
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/filter_pruning/functions.py` & `nncf-2.5.0/nncf/torch/pruning/filter_pruning/functions.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import torch
 
 
 def l1_filter_norm(weight_tensor, dim=0):
     """
     Calculates L1 for weight_tensor for the selected dimension.
     """
@@ -45,15 +43,15 @@
     filters_count = weight_tensor.size(0)
     weight_vec = weight_tensor.view(filters_count, -1)
     similarity_matrix = torch.cdist(weight_vec[None, :], weight_vec[None, :], p=2.0)
     return similarity_matrix.squeeze().sum(axis=0).to(weight_tensor.device)
 
 
 FILTER_IMPORTANCE_FUNCTIONS = {
-    'L2': l2_filter_norm,
-    'L1': l1_filter_norm,
-    'geometric_median': geometric_median_filter_norm
+    "L2": l2_filter_norm,
+    "L1": l1_filter_norm,
+    "geometric_median": geometric_median_filter_norm,
 }
 
 
 def calculate_binary_mask(importance, threshold):
     return (importance >= threshold).float()
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/filter_pruning/global_ranking/evolutionary_optimization.py` & `nncf-2.5.0/nncf/torch/pruning/filter_pruning/global_ranking/evolutionary_optimization.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,26 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import queue
-from typing import List, Callable, Optional, Tuple, Dict
+from copy import copy
+from copy import deepcopy
+from functools import partial
+from typing import Callable, Dict, List, Optional, Tuple
 
 import numpy as np
 import torch
 from torch import nn
-from copy import deepcopy, copy
-from functools import partial
 from torch import optim
 
 from nncf.config.config import NNCFConfig
 from nncf.torch.utils import get_filters_num
 
 
 class EvolutionOptimizer:
@@ -45,19 +44,19 @@
         :param initial_filter_norms: Initial filter norms needed to get std and var of filter norms in each leyer.
         :param hparams: hyperparams of the Optimizer, can contain population_size, num_generations, num_samples,
         mutate_percent, sigma_scale
         :param random_seed: random seed, thet should be set during action generation for reproducibility
         """
         self.random_seed = random_seed
         # Optimizer hyper-params
-        self.population_size = hparams.get('population_size', 64)
-        self.num_generations = hparams.get('num_generations', 400)
-        self.num_samples = hparams.get('num_samples', 16)
-        self.mutate_percent = hparams.get('mutate_percent', 0.1)
-        self.scale_sigma = hparams.get('sigma_scale', 1)
+        self.population_size = hparams.get("population_size", 64)
+        self.num_generations = hparams.get("num_generations", 400)
+        self.num_samples = hparams.get("num_samples", 16)
+        self.mutate_percent = hparams.get("mutate_percent", 0.1)
+        self.scale_sigma = hparams.get("sigma_scale", 1)
         self.max_reward = -np.inf
         self.mean_rewards = []
 
         self.indexes_queue = queue.Queue(self.population_size)
         self.oldest_index = None
         self.population_rewards = np.zeros(self.population_size)
         self.population = [None for i in range(self.population_size)]
@@ -65,15 +64,15 @@
         self.best_action = None
         self.last_action = None
         self.num_layers = len(initial_filter_norms)
         self.layer_keys = np.array(list(initial_filter_norms.keys()))
         self.initial_norms_stats = {}
         for key in self.layer_keys:
             layer_norms = initial_filter_norms[key].cpu().detach().numpy()
-            self.initial_norms_stats[key] = {'mean': np.mean(layer_norms), 'std': np.std(layer_norms)}
+            self.initial_norms_stats[key] = {"mean": np.mean(layer_norms), "std": np.std(layer_norms)}
 
         self.cur_state = None
         self.cur_reward = None
         self.cur_episode = None
         self.cur_info = None
 
     def get_best_action(self):
@@ -107,15 +106,15 @@
         episode_num = self.cur_episode
         action = {}
 
         if episode_num < self.population_size - 1:
             # During first population_size generations, generates random actions
             for key in self.layer_keys:
                 scale = np.exp(np.random.normal(0, self.scale_sigma))
-                shift = np.random.normal(0, self.initial_norms_stats[key]['std'])
+                shift = np.random.normal(0, self.initial_norms_stats[key]["std"])
                 action[key] = (scale, shift)
         elif episode_num == self.population_size - 1:
             # Adding identity action to population
             for key in self.layer_keys:
                 action[key] = (1, 0)
         else:
             step_size = 1 - (float(episode_num) / (self.num_generations * 1.25))  # Rename this
@@ -129,15 +128,15 @@
             # 2. Mutate best action
             mutate_num = int(self.mutate_percent * self.num_layers)
             mutate_idxs = np.random.choice(self.layer_keys, mutate_num)
             for key in self.layer_keys:
                 scale, shift = 1, 0
                 if key in mutate_idxs:
                     scale = np.exp(np.random.normal(0, self.scale_sigma * step_size))
-                    shift = np.random.normal(0, self.initial_norms_stats[key]['std'])
+                    shift = np.random.normal(0, self.initial_norms_stats[key]["std"])
                 action[key] = (scale * best_action[key][0], shift + best_action[key][1])
             self.oldest_index = self.indexes_queue.get()
         return action
 
     def ask(self, episode_num: int) -> Dict:
         """
         Predict and returns action for the last told episode information: state, reward, episode_num and info
@@ -165,18 +164,27 @@
 class LeGREvolutionEnv:
     """
     Environment class for optimizing the accuracy of the pruned model with different ranking coefficients.
     During 'step' environment doing step with received action calculates current reward and useful info and return it
     During 'reset' resetting Pruner and environment params changed during iteration.
     """
 
-    def __init__(self, filter_pruner: 'LeGRPruner', model: nn.Module, train_loader: torch.utils.data.DataLoader,
-                 val_loader: torch.utils.data.DataLoader, train_fn: Callable,
-                 train_optimizer: Optional[torch.optim.Optimizer], val_fn: Callable, config: NNCFConfig,
-                 train_steps: int, pruning_max: float):
+    def __init__(
+        self,
+        filter_pruner: "LeGRPruner",
+        model: nn.Module,
+        train_loader: torch.utils.data.DataLoader,
+        val_loader: torch.utils.data.DataLoader,
+        train_fn: Callable,
+        train_optimizer: Optional[torch.optim.Optimizer],
+        val_fn: Callable,
+        config: NNCFConfig,
+        train_steps: int,
+        pruning_max: float,
+    ):
         """
         :param filter_pruner: LeGRPruner, should have an interface for pruning model and resetting pruner.
         :param model: target model for which ranking coefficients are trained
         :param train_loader: data loader for training the model
         :param val_loader: data loader for validating the model
         :param train_fn: callable for training the model
         :param train_optimizer: optional, optimizer for training the model
@@ -258,22 +266,24 @@
 
 class LeGRPruner:
     """
     Wrapper for pruning controller with a simplified interface, allowing prune model with received ranking coefficients
     and resetting all changes in the model made by the environment.
     """
 
-    def __init__(self, filter_pruner_ctrl: 'FilterPruningController', model: nn.Module):
+    def __init__(self, filter_pruner_ctrl: "FilterPruningController", model: nn.Module):
         self.filter_pruner = filter_pruner_ctrl
         self.scheduler = copy(self.filter_pruner.scheduler)
         self.model = model
         self.model_params_copy = None
         self._save_model_weights()
-        self.init_filter_norms = {node.node_name: self.filter_pruner.filter_importance(node.module.weight)
-                                  for node in self.filter_pruner.pruned_module_groups_info.get_all_nodes()}
+        self.init_filter_norms = {
+            node.node_name: self.filter_pruner.filter_importance(node.module.weight)
+            for node in self.filter_pruner.pruned_module_groups_info.get_all_nodes()
+        }
 
     def loss(self) -> float:
         """
         :return: loss for pruning algorithm
         """
         return self.filter_pruner.loss()
 
@@ -290,16 +300,15 @@
         self.model.load_state_dict(self.model_params_copy)
 
     def _reset_masks(self) -> None:
         """
         Resetting masks for all pruned nodes
         """
         for minfo in self.filter_pruner.pruned_module_groups_info.get_all_nodes():
-            new_mask = torch.ones(get_filters_num(minfo.module)).to(
-                minfo.module.weight.device)
+            new_mask = torch.ones(get_filters_num(minfo.module)).to(minfo.module.weight.device)
             self.filter_pruner.set_mask(minfo, new_mask)
 
     def reset(self) -> None:
         """
         Resetting all changes made in the model (and model masks during environment step) by restoring the original
         model weights, resetting masks.
         """
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/filter_pruning/global_ranking/legr.py` & `nncf-2.5.0/nncf/torch/pruning/filter_pruning/global_ranking/legr.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,45 +1,48 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import time
 
 from torch import nn
 
 from nncf.common.logging import nncf_logger
 from nncf.config.schemata.defaults import PRUNING_LEGR_GENERATIONS
 from nncf.config.schemata.defaults import PRUNING_LEGR_MAX_PRUNING
 from nncf.config.schemata.defaults import PRUNING_LEGR_RANDOM_SEED
 from nncf.config.schemata.defaults import PRUNING_LEGR_TRAIN_STEPS
-from nncf.torch.pruning.filter_pruning.global_ranking.evolutionary_optimization import LeGRPruner, EvolutionOptimizer, \
-    LeGREvolutionEnv
+from nncf.torch.pruning.filter_pruning.global_ranking.evolutionary_optimization import EvolutionOptimizer
+from nncf.torch.pruning.filter_pruning.global_ranking.evolutionary_optimization import LeGREvolutionEnv
+from nncf.torch.pruning.filter_pruning.global_ranking.evolutionary_optimization import LeGRPruner
 from nncf.torch.structures import LeGRInitArgs
 
 
 class LeGR:
     """
     Class for training global ranking coefficients with Evolution optimization agent (but this agent can be easily
     replaced by any other RL agent with a similar interface) and LeGR-optimization environment.
     """
-    def __init__(self, pruning_ctrl: 'FilterPruningController',
-                 target_model: nn.Module,
-                 legr_init_args: LeGRInitArgs,
-                 train_steps: int = PRUNING_LEGR_TRAIN_STEPS,
-                 generations: int = PRUNING_LEGR_GENERATIONS,
-                 max_pruning: float = PRUNING_LEGR_MAX_PRUNING,
-                 random_seed: int = PRUNING_LEGR_RANDOM_SEED):
+
+    def __init__(
+        self,
+        pruning_ctrl: "FilterPruningController",
+        target_model: nn.Module,
+        legr_init_args: LeGRInitArgs,
+        train_steps: int = PRUNING_LEGR_TRAIN_STEPS,
+        generations: int = PRUNING_LEGR_GENERATIONS,
+        max_pruning: float = PRUNING_LEGR_MAX_PRUNING,
+        random_seed: int = PRUNING_LEGR_RANDOM_SEED,
+    ):
         """
         Initializing all necessary structures for optimization- LeGREvolutionEnv environment and EvolutionOptimizer
          agent.
         :param pruning_ctrl: pruning controller, an instance of FilterPruningController class
         :param target_model: model for which layers ranking coefficient will be trained
         :param legr_init_args: initial arguments for LeGR algorithm
         :param train_steps: number of training steps to evaluate accuracy of some ranking coefficients (action of agent)
@@ -49,23 +52,28 @@
         """
         self.num_generations = generations
         self.max_pruning = max_pruning
         self.train_steps = train_steps
 
         self.pruner = LeGRPruner(pruning_ctrl, target_model)
         init_filter_norms = self.pruner.init_filter_norms
-        agent_hparams = {
-            'num_generations': self.num_generations
-        }
+        agent_hparams = {"num_generations": self.num_generations}
         self.agent = EvolutionOptimizer(init_filter_norms, agent_hparams, random_seed)
-        self.env = LeGREvolutionEnv(self.pruner, target_model, legr_init_args.train_loader,
-                                    legr_init_args.val_loader, legr_init_args.train_steps_fn,
-                                    legr_init_args.train_optimizer,
-                                    legr_init_args.val_fn, legr_init_args.config,
-                                    train_steps, max_pruning)
+        self.env = LeGREvolutionEnv(
+            self.pruner,
+            target_model,
+            legr_init_args.train_loader,
+            legr_init_args.val_loader,
+            legr_init_args.train_steps_fn,
+            legr_init_args.train_optimizer,
+            legr_init_args.val_fn,
+            legr_init_args.config,
+            train_steps,
+            max_pruning,
+        )
 
     def train_global_ranking(self):
         """
         Training of ranking coefficients. During every generation:
         1. Environment (LeGREvolutionEnv) send reward and useful info from the previous generation to the
         agent (EvolutionOptimizer)
         2. Agent generates new action (considering this information)
@@ -73,15 +81,15 @@
          action and some useful info
 
          In the end, an optimal action from the agent is returned.
         :return: optimal ranking coefficients (action)
         """
         reward_list = []
 
-        nncf_logger.info('Start training LeGR ranking coefficients...')
+        nncf_logger.info("Start training LeGR ranking coefficients...")
 
         generation_time = 0
         end = time.time()
         for episode in range(self.num_generations):
             state, info = self.env.reset()
 
             # Beginning of the episode
@@ -97,15 +105,16 @@
 
                 state = new_state
                 episode_reward.append(reward)
             generation_time = time.time() - end
             end = time.time()
 
             nncf_logger.info(
-                f'Generation = {episode}, Reward = {episode_reward[0]:.3f}, Time = {generation_time:.3f} \n')
+                f"Generation = {episode}, Reward = {episode_reward[0]:.3f}, Time = {generation_time:.3f} \n"
+            )
             reward_list.append(episode_reward[0])
         self.env.reset()
-        nncf_logger.info('Finished training LeGR ranking coefficients.')
-        nncf_logger.info(f'Evolution algorithm rewards history = {reward_list}')
+        nncf_logger.info("Finished training LeGR ranking coefficients.")
+        nncf_logger.info(f"Evolution algorithm rewards history = {reward_list}")
 
         best_ranking = self.agent.get_best_action()
         return best_ranking
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/filter_pruning/layers.py` & `nncf-2.5.0/nncf/torch/pruning/filter_pruning/layers.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,23 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import numpy as np
 import torch
 from torch import nn
 
-from nncf.torch.layer_utils import COMPRESSION_MODULES
 from nncf.common.graph import NNCFNodeName
+from nncf.torch.layer_utils import COMPRESSION_MODULES
 
 
 @COMPRESSION_MODULES.register()
 class FilterPruningMask(nn.Module):
     """
     A module contains the mask for pruning.
     On forward pass applying the mask to weight and bias of the module.
@@ -45,40 +43,45 @@
         for param_name, param_value in params.items():
             # In case of None weight (or bias) mask shouldn't be applied
             if param_value is None:
                 new_params.append(param_value)
                 continue
 
             # For weights self.mask_applying_dim should be used, for bias dim=0
-            dim = 0 if param_name == 'bias' else self.mask_applying_dim
+            dim = 0 if param_name == "bias" else self.mask_applying_dim
             new_params.append(
-                apply_filter_binary_mask(self.binary_filter_pruning_mask, param_value,
-                                         node_name_for_logging=self.node_name, dim=dim)
+                apply_filter_binary_mask(
+                    self.binary_filter_pruning_mask, param_value, node_name_for_logging=self.node_name, dim=dim
+                )
             )
         return new_params
 
 
 def broadcast_filter_mask(filter_mask, shape, dim=0):
     broadcasted_shape = np.ones(len(shape), dtype=np.int64)
     broadcasted_shape[dim] = filter_mask.size(0)
     broadcasted_filter_mask = torch.reshape(filter_mask, tuple(broadcasted_shape))
     return broadcasted_filter_mask
 
 
-def apply_filter_binary_mask(filter_mask: torch.Tensor,
-                             module_parameter: torch.nn.Parameter,
-                             node_name_for_logging: NNCFNodeName = '',
-                             dim: int = 0):
+def apply_filter_binary_mask(
+    filter_mask: torch.Tensor,
+    module_parameter: torch.nn.Parameter,
+    node_name_for_logging: NNCFNodeName = "",
+    dim: int = 0,
+):
     """
     Applying binary filter mask to parameter of the module - usually to weight/bias of convolution or linear layer.
     Mask is applied to a given dimension without overriding parameter's values.
     :param filter_mask: binary mask (should have the same shape as conv weight on the given dimension)
     :param module_parameter: a tensor representing a module parameter (e.g. weight or bias of convolution)
     :param node_name_for_logging: name of the module to which the mask is applied
     :param dim: a dimension to apply the mask (0 by default)
     :return: result with applied mask
     """
     if filter_mask.size(0) != module_parameter.size(dim):
-        raise RuntimeError("Shape of mask = {} for module {} isn't broadcastable to weight shape={}."
-                           " ".format(filter_mask.shape, node_name_for_logging, module_parameter.shape))
+        raise RuntimeError(
+            "Shape of mask = {} for module {} isn't broadcastable to weight shape={}."
+            " ".format(filter_mask.shape, node_name_for_logging, module_parameter.shape)
+        )
     broadcasted_filter_mask = broadcast_filter_mask(filter_mask, module_parameter.shape, dim)
     return module_parameter.mul(broadcasted_filter_mask)
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/operations.py` & `nncf-2.5.0/nncf/torch/pruning/operations.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,491 +1,700 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from enum import Enum
+from enum import auto
 
 import torch
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
 from nncf.common.graph.operator_metatypes import UnknownMetatype
+from nncf.common.logging import nncf_logger
 from nncf.common.pruning.mask_propagation import MaskPropagationAlgorithm
-from nncf.common.pruning.utils import get_input_masks
-from nncf.torch.graph.operator_metatypes import (
-    PTAddMetatype,
-    PTAvgPool2dMetatype,
-    PTBatchNormMetatype,
-    PTCatMetatype,
-    PTConv1dMetatype,
-    PTConv2dMetatype,
-    PTConv3dMetatype,
-    PTConvTranspose1dMetatype,
-    PTConvTranspose2dMetatype,
-    PTConvTranspose3dMetatype,
-    PTDivMetatype,
-    PTDropoutMetatype,
-    PTELUMetatype,
-    PTRELU6Metatype,
-    PTGELUMetatype,
-    PTGroupNormMetatype,
-    PTLayerNormMetatype,
-    PTHardTanhMetatype,
-    PTHardSwishMetatype,
-    PTHardSigmoidMetatype,
-    PTInputNoopMetatype,
-    PTInterpolateMetatype,
-    PTLinearMetatype,
-    PTMatMulMetatype,
-    PTMaxMetatype,
-    PTMaxPool2dMetatype,
-    PTMeanMetatype,
-    PTMinMetatype,
-    PTMulMetatype,
-    PTNoopMetatype,
-    PTOutputNoopMetatype,
-    PTPowerMetatype,
-    PTPRELUMetatype,
-    PTLeakyRELUMetatype,
-    PTRELUMetatype,
-    PTSigmoidMetatype,
-    PTSILUMetatype,
-    PTSoftmaxMetatype,
-    PTSplitMetatype,
-    PTSubMetatype,
-    PTSumMetatype,
-    PTTanhMetatype,
-    PTReshapeMetatype
-)
-from nncf.common.pruning.operations import (
-    InputPruningOp,
-    OutputPruningOp,
-    IdentityMaskForwardPruningOp,
-    ConvolutionPruningOp,
-    TransposeConvolutionPruningOp,
-    BatchNormPruningOp,
-    LinearPruningOp,
-    GroupNormPruningOp,
-    LayerNormPruningOp,
-    ConcatPruningOp,
-    ElementwisePruningOp,
-    ReshapePruningOp,
-    StopMaskForwardPruningOp,
-    SplitPruningOp
-)
-
+from nncf.common.pruning.operations import BatchNormPruningOp
+from nncf.common.pruning.operations import ConcatPruningOp
+from nncf.common.pruning.operations import ConvolutionPruningOp
+from nncf.common.pruning.operations import ElementwisePruningOp
+from nncf.common.pruning.operations import GroupNormPruningOp
+from nncf.common.pruning.operations import IdentityMaskForwardPruningOp
+from nncf.common.pruning.operations import InputPruningOp
+from nncf.common.pruning.operations import LayerNormPruningOp
+from nncf.common.pruning.operations import LinearPruningOp
+from nncf.common.pruning.operations import OutputPruningOp
+from nncf.common.pruning.operations import PadPruningOp
+from nncf.common.pruning.operations import ReshapePruningOp
+from nncf.common.pruning.operations import SplitPruningOp
+from nncf.common.pruning.operations import StopMaskForwardPruningOp
+from nncf.common.pruning.operations import TransposeConvolutionPruningOp
 from nncf.common.pruning.utils import PruningOperationsMetatypeRegistry
+from nncf.common.pruning.utils import get_input_masks
 from nncf.common.pruning.utils import is_prunable_depthwise_conv
-from nncf.common.logging import nncf_logger
+from nncf.torch.graph.operator_metatypes import PTAddMetatype
+from nncf.torch.graph.operator_metatypes import PTAvgPool2dMetatype
+from nncf.torch.graph.operator_metatypes import PTAvgPool3dMetatype
+from nncf.torch.graph.operator_metatypes import PTBatchNormMetatype
+from nncf.torch.graph.operator_metatypes import PTCatMetatype
+from nncf.torch.graph.operator_metatypes import PTConv1dMetatype
+from nncf.torch.graph.operator_metatypes import PTConv2dMetatype
+from nncf.torch.graph.operator_metatypes import PTConv3dMetatype
+from nncf.torch.graph.operator_metatypes import PTConvTranspose1dMetatype
+from nncf.torch.graph.operator_metatypes import PTConvTranspose2dMetatype
+from nncf.torch.graph.operator_metatypes import PTConvTranspose3dMetatype
+from nncf.torch.graph.operator_metatypes import PTDivMetatype
+from nncf.torch.graph.operator_metatypes import PTDropoutMetatype
+from nncf.torch.graph.operator_metatypes import PTELUMetatype
+from nncf.torch.graph.operator_metatypes import PTGELUMetatype
+from nncf.torch.graph.operator_metatypes import PTGroupNormMetatype
+from nncf.torch.graph.operator_metatypes import PTHardSigmoidMetatype
+from nncf.torch.graph.operator_metatypes import PTHardSwishMetatype
+from nncf.torch.graph.operator_metatypes import PTHardTanhMetatype
+from nncf.torch.graph.operator_metatypes import PTInputNoopMetatype
+from nncf.torch.graph.operator_metatypes import PTInterpolateMetatype
+from nncf.torch.graph.operator_metatypes import PTLayerNormMetatype
+from nncf.torch.graph.operator_metatypes import PTLeakyRELUMetatype
+from nncf.torch.graph.operator_metatypes import PTLinearMetatype
+from nncf.torch.graph.operator_metatypes import PTMatMulMetatype
+from nncf.torch.graph.operator_metatypes import PTMaxMetatype
+from nncf.torch.graph.operator_metatypes import PTMaxPool2dMetatype
+from nncf.torch.graph.operator_metatypes import PTMaxPool3dMetatype
+from nncf.torch.graph.operator_metatypes import PTMeanMetatype
+from nncf.torch.graph.operator_metatypes import PTMinMetatype
+from nncf.torch.graph.operator_metatypes import PTMulMetatype
+from nncf.torch.graph.operator_metatypes import PTNoopMetatype
+from nncf.torch.graph.operator_metatypes import PTOutputNoopMetatype
+from nncf.torch.graph.operator_metatypes import PTPadMetatype
+from nncf.torch.graph.operator_metatypes import PTPowerMetatype
+from nncf.torch.graph.operator_metatypes import PTPRELUMetatype
+from nncf.torch.graph.operator_metatypes import PTRELU6Metatype
+from nncf.torch.graph.operator_metatypes import PTRELUMetatype
+from nncf.torch.graph.operator_metatypes import PTReshapeMetatype
+from nncf.torch.graph.operator_metatypes import PTSigmoidMetatype
+from nncf.torch.graph.operator_metatypes import PTSILUMetatype
+from nncf.torch.graph.operator_metatypes import PTSoftmaxMetatype
+from nncf.torch.graph.operator_metatypes import PTSplitMetatype
+from nncf.torch.graph.operator_metatypes import PTSubMetatype
+from nncf.torch.graph.operator_metatypes import PTSumMetatype
+from nncf.torch.graph.operator_metatypes import PTTanhMetatype
 from nncf.torch.layers import NNCF_WRAPPED_USER_MODULES_DICT
 from nncf.torch.nncf_network import NNCFNetwork
+from nncf.torch.pruning.filter_pruning.layers import FilterPruningMask
+from nncf.torch.pruning.filter_pruning.layers import apply_filter_binary_mask
 from nncf.torch.pruning.tensor_processor import PTNNCFPruningTensorProcessor
+from nncf.torch.tensor import PTNNCFTensor
 
 PT_PRUNING_OPERATOR_METATYPES = PruningOperationsMetatypeRegistry("operator_metatypes")
 
 
+class PrunType(Enum):
+    CUT_WEIGHTS = auto()
+    FILL_ZEROS = auto()
+
+
 class PTPruner:
     @classmethod
-    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph) -> None:
+    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
         """
         Prune node by input_masks (if masks is not none and operation support it).
 
         :param model: NNCF network.
         :param node: Node from NNCF graph that will be prune.
         :param graph: Graph of model.
+        :param prun_type: Type of pruning.
         """
 
     @classmethod
     def input_reorder(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
         """
         Reorder input channels of node by input_masks (if masks is not none and operation support it).
         It's needed to make an equivalent network after sorting filters by importance in the previous layer.
 
         :param model: NNCF network.
         :param node: Node from NNCF graph that will reorder input channels.
         :param graph: Graph of model.
         """
 
     @classmethod
-    def output_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph) -> None:
+    def output_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
         """
         Prune node by output_mask (if mask is not none and operation support it).
 
         :param model: NNCF network.
         :param node: Node from NNCF graph that will be prune.
         :param graph: Graph of model.
+        :param prun_type: Type of pruning.
         """
 
     @classmethod
     def output_reorder(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
         """
         Reorder output channels of node by output_mask (if masks is not none and operation support it).
         It's needed for performing pruning of filters by simple crop of the last important elements.
         :param model: NNCF network.
         :param node: Node from NNCF graph that will reorder output channels.
         :param graph: Graph of model.
         """
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('model_input')
+@PT_PRUNING_OPERATOR_METATYPES.register("model_input")
 class PTInputPruningOp(InputPruningOp, PTPruner):
     subtypes = [PTInputNoopMetatype]
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('model_output')
+@PT_PRUNING_OPERATOR_METATYPES.register("model_output")
 class PTOutputPruningOp(OutputPruningOp, PTPruner):
     subtypes = [PTOutputNoopMetatype]
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('identity_mask_propagation')
+@PT_PRUNING_OPERATOR_METATYPES.register("identity_mask_propagation")
 class PTIdentityMaskForwardPruningOp(IdentityMaskForwardPruningOp, PTPruner):
-    subtypes = [PTHardTanhMetatype, PTTanhMetatype, PTRELUMetatype, PTRELU6Metatype, PTLeakyRELUMetatype,
-                PTPRELUMetatype, PTELUMetatype, PTGELUMetatype, PTSigmoidMetatype, PTSoftmaxMetatype,
-                PTAvgPool2dMetatype, PTMaxPool2dMetatype, PTDropoutMetatype, PTSILUMetatype, PTPowerMetatype,
-                PTHardSwishMetatype, PTHardSigmoidMetatype, PTNoopMetatype, PTInterpolateMetatype]
-    additional_types = ['h_sigmoid', 'h_swish', 'RELU']
+    subtypes = [
+        PTHardTanhMetatype,
+        PTTanhMetatype,
+        PTRELUMetatype,
+        PTRELU6Metatype,
+        PTLeakyRELUMetatype,
+        PTPRELUMetatype,
+        PTELUMetatype,
+        PTGELUMetatype,
+        PTSigmoidMetatype,
+        PTSoftmaxMetatype,
+        PTAvgPool2dMetatype,
+        PTMaxPool2dMetatype,
+        PTAvgPool3dMetatype,
+        PTMaxPool3dMetatype,
+        PTDropoutMetatype,
+        PTSILUMetatype,
+        PTPowerMetatype,
+        PTHardSwishMetatype,
+        PTHardSigmoidMetatype,
+        PTNoopMetatype,
+        PTInterpolateMetatype,
+    ]
+    additional_types = ["h_sigmoid", "h_swish", "RELU"]
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('convolution')
+@PT_PRUNING_OPERATOR_METATYPES.register("convolution")
 class PTConvolutionPruningOp(ConvolutionPruningOp, PTPruner):
     subtypes = [PTConv1dMetatype, PTConv2dMetatype, PTConv3dMetatype]
 
     @classmethod
-    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
-        input_mask = node.data['input_masks'][0]
+    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        input_mask = get_input_masks(node, graph)[0]
         if input_mask is None:
             return
-        bool_mask = torch.tensor(input_mask, dtype=torch.bool)
-        new_num_channels = int(torch.sum(input_mask))
+
+        if isinstance(input_mask, PTNNCFTensor):
+            input_mask = input_mask.tensor
 
         is_depthwise = is_prunable_depthwise_conv(node)
-        node_module = model.get_containing_module(node.node_name)
-        old_num_channels = int(node_module.weight.size(1))
+        node_module = model.nncf.get_containing_module(node.node_name)
 
-        if is_depthwise:
-            # In depthwise case prune output channels by input mask, here only fix for new number of input channels
-            node_module.groups = new_num_channels
-            node_module.in_channels = new_num_channels
+        if prun_type == PrunType.CUT_WEIGHTS:
+            bool_mask = torch.tensor(input_mask, dtype=torch.bool)
+            new_num_channels = int(torch.sum(input_mask))
             old_num_channels = int(node_module.weight.size(0))
+            if is_depthwise:
+                # In depthwise case prune output channels by input mask, here only fix for new number of input channels
+                node_module.groups = new_num_channels
+                node_module.in_channels = new_num_channels
+            else:
+                out_channels = node_module.weight.size(0)
+                broadcasted_mask = bool_mask.repeat(out_channels).view(out_channels, bool_mask.size(0))
+                new_weight_shape = list(node_module.weight.shape)
+                new_weight_shape[1] = new_num_channels
+
+                node_module.in_channels = new_num_channels
+                node_module.weight = torch.nn.Parameter(node_module.weight[broadcasted_mask].view(new_weight_shape))
+                nncf_logger.debug(
+                    f'Pruned Convolution {node.data["key"]} by input mask. '
+                    f"Old input filters number: {old_num_channels}, new filters number: {new_num_channels}."
+                )
         else:
-            out_channels = node_module.weight.size(0)
-            broadcasted_mask = bool_mask.repeat(out_channels).view(out_channels, bool_mask.size(0))
-            new_weight_shape = list(node_module.weight.shape)
-            new_weight_shape[1] = new_num_channels
-
-            node_module.in_channels = new_num_channels
-            node_module.weight = torch.nn.Parameter(node_module.weight[broadcasted_mask].view(new_weight_shape))
-
-        nncf_logger.debug(
-            f'Pruned Convolution {node.data["key"]} by input mask. '
-            f'Old input filters number: {old_num_channels}, new filters number: {new_num_channels}.')
+            if not is_depthwise:
+                node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.weight, dim=1))
 
     @classmethod
-    def output_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph) -> None:
-        mask = node.data['output_mask']
+    def output_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        mask = node.data["output_mask"]
         if mask is None:
             return
 
-        bool_mask = torch.tensor(mask, dtype=torch.bool)
+        if isinstance(mask, PTNNCFTensor):
+            mask = mask.tensor
 
-        node_module = model.get_containing_module(node.node_name)
-        old_num_clannels = int(node_module.weight.size(0))
+        node_module = model.nncf.get_containing_module(node.node_name)
+        if prun_type == PrunType.CUT_WEIGHTS:
+            old_num_channels = int(node_module.weight.size(0))
+            bool_mask = torch.tensor(mask, dtype=torch.bool)
 
-        node_module.out_channels = int(torch.sum(mask))
-        node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
+            node_module.out_channels = int(torch.sum(mask))
+            node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
 
-        if node_module.bias is not None:
-            node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
+            if node_module.bias is not None:
+                node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
 
-        nncf_logger.debug(
-            f'Pruned Convolution {node.data["key"]} by pruning mask. '
-            f'Old output filters number: {old_num_clannels}, new filters number: {node_module.out_channels}.')
+            nncf_logger.debug(
+                f'Pruned Convolution {node.data["key"]} by pruning mask. '
+                f"Old output filters number: {old_num_channels}, new filters number: {node_module.out_channels}."
+            )
+        else:
+            node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(mask, node_module.weight))
+            if node_module.bias is not None:
+                node_module.bias = torch.nn.Parameter(apply_filter_binary_mask(mask, node_module.bias))
 
     @classmethod
     def input_reorder(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
         if is_prunable_depthwise_conv(node):
             return
         input_masks = get_input_masks(node, graph)
         reorder_indexes = input_masks[0]
         if reorder_indexes is None:
             return
         reorder_indexes = reorder_indexes.tensor
-        conv = model.get_containing_module(node.node_name)
+        conv = model.nncf.get_containing_module(node.node_name)
         conv.weight.data = torch.index_select(conv.weight.data, 1, reorder_indexes)
         nncf_logger.debug(
-            f'Reordered input channels (first 10 reorder indexes {reorder_indexes[:10]}) '
-            f'of Convolution: {node.data["key"]} ')
+            f"Reordered input channels (first 10 reorder indexes {reorder_indexes[:10]}) "
+            f'of Convolution: {node.data["key"]} '
+        )
 
     @classmethod
     def output_reorder(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
-        reorder_indexes = node.data['output_mask']
+        reorder_indexes = node.data["output_mask"]
         if reorder_indexes is None:
             return
-        conv = model.get_containing_module(node.node_name)
+        conv = model.nncf.get_containing_module(node.node_name)
         reorder_indexes = reorder_indexes.tensor
         conv.weight.data = torch.index_select(conv.weight.data, 0, reorder_indexes)
         if conv.bias is not None:
             conv.bias.data = torch.index_select(conv.bias.data, 0, reorder_indexes)
         nncf_logger.debug(
-            f'Reordered output channels (first 10 reorder indexes {reorder_indexes[:10]}) '
-            f'of Convolution: {node.data["key"]} ')
+            f"Reordered output channels (first 10 reorder indexes {reorder_indexes[:10]}) "
+            f'of Convolution: {node.data["key"]} '
+        )
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('transpose_convolution')
+@PT_PRUNING_OPERATOR_METATYPES.register("transpose_convolution")
 class PTTransposeConvolutionPruningOp(TransposeConvolutionPruningOp, PTPruner):
     subtypes = [PTConvTranspose1dMetatype, PTConvTranspose2dMetatype, PTConvTranspose3dMetatype]
 
     @classmethod
-    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph) -> None:
-        input_mask = node.data['input_masks'][0]
+    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        input_mask = get_input_masks(node, graph)[0]
         if input_mask is None:
             return
-        bool_mask = torch.tensor(input_mask, dtype=torch.bool)
 
-        node_module = model.get_containing_module(node.node_name)
-        old_num_channels = int(node_module.weight.size(0))
+        if isinstance(input_mask, PTNNCFTensor):
+            input_mask = input_mask.tensor
 
-        node_module.in_channels = int(torch.sum(bool_mask))
-        node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
+        node_module = model.nncf.get_containing_module(node.node_name)
+        if prun_type == PrunType.CUT_WEIGHTS:
+            bool_mask = torch.tensor(input_mask, dtype=torch.bool)
+            old_num_channels = int(node_module.weight.size(0))
 
-        nncf_logger.debug(
-            f'Pruned ConvTranspose {node.data["key"]} by input mask. '
-            f'Old input filters number: {old_num_channels}, new filters number: {node_module.in_channels}.')
+            node_module.in_channels = int(torch.sum(bool_mask))
+            node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
+
+            nncf_logger.debug(
+                f'Pruned ConvTranspose {node.data["key"]} by input mask. '
+                f"Old input filters number: {old_num_channels}, new filters number: {node_module.in_channels}."
+            )
+        else:
+            node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.weight))
 
     @classmethod
-    def output_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph) -> None:
-        output_mask = node.data['output_mask']
+    def output_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        output_mask = node.data["output_mask"]
         if output_mask is None:
             return
 
-        bool_mask = torch.tensor(output_mask, dtype=torch.bool)
-        new_num_channels = int(torch.sum(bool_mask))
+        if isinstance(output_mask, PTNNCFTensor):
+            output_mask = output_mask.tensor
 
-        node_module = model.get_containing_module(node.node_name)
-        old_num_channels = int(node_module.weight.size(1))
+        node_module = model.nncf.get_containing_module(node.node_name)
 
-        in_channels = node_module.weight.size(0)
-        broadcasted_mask = bool_mask.repeat(in_channels).view(in_channels, bool_mask.size(0))
-        new_weight_shape = list(node_module.weight.shape)
-        new_weight_shape[1] = new_num_channels
+        if prun_type == PrunType.CUT_WEIGHTS:
+            bool_mask = torch.tensor(output_mask, dtype=torch.bool)
+            new_num_channels = int(torch.sum(bool_mask))
 
-        node_module.out_channels = new_num_channels
-        node_module.weight = torch.nn.Parameter(node_module.weight[broadcasted_mask].view(new_weight_shape))
+            old_num_channels = int(node_module.weight.size(1))
 
-        if node_module.bias is not None:
-            node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
+            in_channels = node_module.weight.size(0)
+            broadcasted_mask = bool_mask.repeat(in_channels).view(in_channels, bool_mask.size(0))
+            new_weight_shape = list(node_module.weight.shape)
+            new_weight_shape[1] = new_num_channels
 
-        nncf_logger.debug(
-            f'Pruned ConvTranspose {node.data["key"]} by pruning mask. '
-            f'Old output filters number: {old_num_channels}, new filters number: {node_module.out_channels}.')
+            node_module.out_channels = new_num_channels
+            node_module.weight = torch.nn.Parameter(node_module.weight[broadcasted_mask].view(new_weight_shape))
+
+            if node_module.bias is not None:
+                node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
+
+            nncf_logger.debug(
+                f'Pruned ConvTranspose {node.data["key"]} by pruning mask. '
+                f"Old output filters number: {old_num_channels}, new filters number: {node_module.out_channels}."
+            )
+        else:
+            node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(output_mask, node_module.weight, dim=1))
+            if node_module.bias is not None:
+                node_module.bias = torch.nn.Parameter(apply_filter_binary_mask(output_mask, node_module.bias))
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('linear')
+@PT_PRUNING_OPERATOR_METATYPES.register("linear")
 class PTLinearPruningOp(LinearPruningOp, PTPruner):
     subtypes = [PTLinearMetatype, PTMatMulMetatype]
 
     @classmethod
+    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        input_mask = get_input_masks(node, graph)[0]
+        if input_mask is None:
+            return
+
+        if isinstance(input_mask, PTNNCFTensor):
+            input_mask = input_mask.tensor
+
+        node_module = model.nncf.get_containing_module(node.node_name)
+
+        if prun_type == PrunType.CUT_WEIGHTS:
+            bool_mask = torch.tensor(input_mask, dtype=torch.bool)
+            in_features = node_module.in_features
+            out_features = node_module.out_features
+            new_in_features = sum(bool_mask)
+            node_module.in_features = new_in_features
+            broadcasted_mask = bool_mask.repeat(out_features).view(out_features, bool_mask.size(0))
+            new_weight_shape = list(node_module.weight.shape)
+            new_weight_shape[1] = new_in_features
+
+            node_module.weight = torch.nn.Parameter(node_module.weight[broadcasted_mask].view(new_weight_shape))
+            nncf_logger.debug(
+                f'Pruned Linear {node.data["key"]} by pruning mask. '
+                f"Old input filters number: {in_features}, new filters number: {node_module.in_features}."
+            )
+        else:
+            node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.weight, dim=1))
+
+    @classmethod
     def input_reorder(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
         input_masks = get_input_masks(node, graph)
         reorder_indexes = input_masks[0]
         if reorder_indexes is None:
             return
         reorder_indexes = reorder_indexes.tensor
-        fc = model.get_containing_module(node.node_name)
+        fc = model.nncf.get_containing_module(node.node_name)
         fc.weight.data = torch.index_select(fc.weight.data, 1, reorder_indexes)
         nncf_logger.debug(
-            'Reordered input channels (first 10 reorder indexes {}) of Linear: {} '.format(reorder_indexes[:10],
-                                                                                           node.data['key']))
+            f"Reordered input channels (first 10 reorder indexes {reorder_indexes[:10]}) of Linear: {node.data['key']}"
+        )
+
+    @classmethod
+    def output_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        output_mask = node.data["output_mask"]
+        if output_mask is None:
+            return
+
+        if isinstance(output_mask, PTNNCFTensor):
+            output_mask = output_mask.tensor
+
+        node_module = model.nncf.get_containing_module(node.node_name)
+
+        if prun_type == PrunType.CUT_WEIGHTS:
+            bool_mask = torch.tensor(output_mask, dtype=torch.bool)
+            out_features = node_module.out_features
+            new_out_features = sum(bool_mask)
+            node_module.out_features = new_out_features
+            node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
+            nncf_logger.debug(
+                f'Pruned Linear {node.data["key"]} by pruning mask. '
+                f"Old output filters number: {out_features}, new filters number: {node_module.out_features}."
+            )
+            if node_module.bias is not None:
+                node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
+        else:
+            node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(output_mask, node_module.weight))
+            if node_module.bias is not None:
+                node_module.bias = torch.nn.Parameter(apply_filter_binary_mask(output_mask, node_module.bias))
 
     @classmethod
     def output_reorder(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
-        reorder_indexes = node.data['output_mask']
+        reorder_indexes = node.data["output_mask"]
         if reorder_indexes is None:
             return
-        fc = model.get_containing_module(node.node_name)
+        fc = model.nncf.get_containing_module(node.node_name)
         reorder_indexes = reorder_indexes.tensor
         fc.weight.data = torch.index_select(fc.weight.data, 0, reorder_indexes)
         if fc.bias is not None:
             fc.bias.data = torch.index_select(fc.bias.data, 0, reorder_indexes)
         nncf_logger.debug(
-            'Reordered output channels (first 10 reorder indexes {}) of Linear: {} '.format(reorder_indexes[:10],
-                                                                                            node.data['key']))
+            f"Reordered output channels (first 10 reorder indexes {reorder_indexes[:10]}) of Linear: {node.data['key']}"
+        )
+
 
-@PT_PRUNING_OPERATOR_METATYPES.register('batch_norm')
+@PT_PRUNING_OPERATOR_METATYPES.register("batch_norm")
 class PTBatchNormPruningOp(BatchNormPruningOp, PTPruner):
     subtypes = [PTBatchNormMetatype]
 
     @classmethod
-    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph) -> None:
-        input_mask = node.data['input_masks'][0]
+    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        input_mask = get_input_masks(node, graph)[0]
         if input_mask is None:
             return
 
-        node_module = model.get_containing_module(node.node_name)
+        if isinstance(input_mask, PTNNCFTensor):
+            input_mask = input_mask.tensor
 
-        bool_mask = torch.tensor(input_mask, dtype=torch.bool)
-        old_num_clannels = int(node_module.weight.size(0))
-        new_num_channels = int(torch.sum(input_mask))
-
-        node_module.num_features = new_num_channels
-        node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
-        node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
-        node_module.running_mean = torch.nn.Parameter(node_module.running_mean[bool_mask], requires_grad=False)
-        node_module.running_var = torch.nn.Parameter(node_module.running_var[bool_mask], requires_grad=False)
+        node_module = model.nncf.get_containing_module(node.node_name)
 
-        nncf_logger.debug(
-            f'Pruned BatchNorm {node.data["key"]} by input mask. '
-            f'Old num features: {old_num_clannels}, new num features: {new_num_channels}.')
+        if prun_type == PrunType.CUT_WEIGHTS:
+            bool_mask = torch.tensor(input_mask, dtype=torch.bool)
+
+            old_num_channels = int(node_module.weight.size(0))
+            new_num_channels = int(torch.sum(input_mask))
+
+            node_module.num_features = new_num_channels
+            node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
+            node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
+            node_module.running_mean = torch.nn.Parameter(node_module.running_mean[bool_mask], requires_grad=False)
+            node_module.running_var = torch.nn.Parameter(node_module.running_var[bool_mask], requires_grad=False)
+
+            nncf_logger.debug(
+                f'Pruned BatchNorm {node.data["key"]} by input mask. '
+                f"Old num features: {old_num_channels}, new num features: {new_num_channels}."
+            )
+        else:
+            node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.weight))
+            node_module.bias = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.bias))
+            node_module.running_mean = torch.nn.Parameter(
+                apply_filter_binary_mask(input_mask, node_module.running_mean), requires_grad=False
+            )
+            node_module.running_var = torch.nn.Parameter(
+                apply_filter_binary_mask(input_mask, node_module.running_var), requires_grad=False
+            )
 
     @classmethod
     def input_reorder(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
         input_masks = get_input_masks(node, graph)
         reorder_indexes = input_masks[0]
         if reorder_indexes is None:
             return
 
         reorder_indexes = reorder_indexes.tensor
         reorder_indexes = reorder_indexes.int()
-        bn = model.get_containing_module(node.node_name)
+        bn = model.nncf.get_containing_module(node.node_name)
 
         bn.weight.data = torch.index_select(bn.weight.data, 0, reorder_indexes)
         bn.bias.data = torch.index_select(bn.bias.data, 0, reorder_indexes)
         bn.running_mean.data = torch.index_select(bn.running_mean.data, 0, reorder_indexes)
         bn.running_var.data = torch.index_select(bn.running_var.data, 0, reorder_indexes)
 
         nncf_logger.debug(
-            f'Reordered channels (first 10 reorder indexes {reorder_indexes[:10]}) of BatchNorm: {node.data["key"]} ')
+            f'Reordered channels (first 10 reorder indexes {reorder_indexes[:10]}) of BatchNorm: {node.data["key"]} '
+        )
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('group_norm')
+@PT_PRUNING_OPERATOR_METATYPES.register("group_norm")
 class PTGroupNormPruningOp(GroupNormPruningOp, PTPruner):
     subtypes = [PTGroupNormMetatype]
 
     @classmethod
-    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph) -> None:
-        input_mask = node.data['input_masks'][0]
+    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        input_mask = get_input_masks(node, graph)[0]
         if input_mask is None:
             return
 
-        node_module = model.get_containing_module(node.node_name)
+        if isinstance(input_mask, PTNNCFTensor):
+            input_mask = input_mask.tensor
+
+        node_module = model.nncf.get_containing_module(node.node_name)
 
-        bool_mask = torch.tensor(input_mask, dtype=torch.bool)
-        old_num_clannels = int(node_module.weight.size(0))
-        new_num_channels = int(torch.sum(input_mask))
+        if prun_type == PrunType.CUT_WEIGHTS:
+            bool_mask = torch.tensor(input_mask, dtype=torch.bool)
+            old_num_channels = int(node_module.weight.size(0))
+            new_num_channels = int(torch.sum(input_mask))
 
-        node_module.num_channels = new_num_channels
-        node_module.num_groups = new_num_channels
+            node_module.num_channels = new_num_channels
+            node_module.num_groups = new_num_channels
 
-        node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
-        node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
+            node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
+            node_module.bias = torch.nn.Parameter(node_module.bias[bool_mask])
 
-        nncf_logger.debug(
-            f'Pruned GroupNorm {node.data["key"]} by input mask. '
-            f'Old num features: {old_num_clannels}, new num features: {new_num_channels}.')
+            nncf_logger.debug(
+                f"Pruned GroupNorm {node.data['key']} by input mask. "
+                f"Old num features: {old_num_channels}, new num features: {new_num_channels}."
+            )
+        else:
+            node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.weight))
+            node_module.bias = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.bias))
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('layer_norm')
+@PT_PRUNING_OPERATOR_METATYPES.register("layer_norm")
 class PTLayerNormPruningOp(LayerNormPruningOp, PTPruner):
     subtypes = [PTLayerNormMetatype]
 
     @classmethod
     def input_reorder(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph):
         input_masks = get_input_masks(node, graph)
         reorder_indexes = input_masks[0]
         if reorder_indexes is None:
             return
 
         reorder_indexes = reorder_indexes.tensor
-        ln = model.get_containing_module(node.node_name)
+        ln = model.nncf.get_containing_module(node.node_name)
         ln.weight.data = torch.index_select(ln.weight.data, 0, reorder_indexes)
         ln.bias.data = torch.index_select(ln.bias.data, 0, reorder_indexes)
 
         nncf_logger.debug(
-            'Reordered channels (first 10 reorder indexes {}) of LayerNorm: {} '.format(reorder_indexes[:10],
-                                                                                        node.data['key']))
+            "Reordered channels (first 10 reorder indexes {}) of LayerNorm: {} ".format(
+                reorder_indexes[:10], node.data["key"]
+            )
+        )
 
+    @classmethod
+    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        input_mask = get_input_masks(node, graph)[0]
+        if input_mask is None:
+            return
+
+        if isinstance(input_mask, PTNNCFTensor):
+            input_mask = input_mask.tensor
+
+        node_module = model.nncf.get_containing_module(node.node_name)
+
+        if prun_type == PrunType.CUT_WEIGHTS:
+            raise RuntimeError("LayerNorm does not support pruning by cutting channels")
 
-@PT_PRUNING_OPERATOR_METATYPES.register('elementwise')
+        node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.weight))
+        node_module.bias = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.bias))
+
+
+@PT_PRUNING_OPERATOR_METATYPES.register("elementwise")
 class PTElementwisePruningOp(ElementwisePruningOp, PTPruner):
     subtypes = [PTAddMetatype, PTSubMetatype, PTDivMetatype, PTMulMetatype]
 
     @classmethod
-    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph) -> None:
-        input_mask = node.data['input_masks'][0]
+    def input_prune(cls, model: NNCFNetwork, node: NNCFNode, graph: NNCFGraph, prun_type: PrunType) -> None:
+        input_mask = get_input_masks(node, graph)[0]
         if input_mask is None:
             return
 
-        bool_mask = torch.tensor(input_mask, dtype=torch.bool)
-        node_module = model.get_containing_module(node.node_name)
+        if isinstance(input_mask, PTNNCFTensor):
+            input_mask = input_mask.tensor
 
-        if isinstance(node_module, tuple(NNCF_WRAPPED_USER_MODULES_DICT)):
-            assert node_module.target_weight_dim_for_compression == 0, \
-                "Implemented only for target_weight_dim_for_compression == 0"
-            old_num_clannels = int(node_module.weight.size(0))
-            new_num_channels = int(torch.sum(input_mask))
-            node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
-            node_module.n_channels = new_num_channels
+        node_module = model.nncf.get_containing_module(node.node_name)
 
-            nncf_logger.debig(
-                f'Pruned Elementwise {node.data["key"]} by input mask. '
-                f'Old num features: {old_num_clannels}, new num features: {new_num_channels}.')
+        if isinstance(node_module, tuple(NNCF_WRAPPED_USER_MODULES_DICT)):
+            assert (
+                node_module.target_weight_dim_for_compression == 0
+            ), "Implemented only for target_weight_dim_for_compression == 0"
+            if prun_type == PrunType.CUT_WEIGHTS:
+                bool_mask = torch.tensor(input_mask, dtype=torch.bool)
+                old_num_channels = int(node_module.weight.size(0))
+                new_num_channels = int(torch.sum(input_mask))
+                node_module.weight = torch.nn.Parameter(node_module.weight[bool_mask])
+                node_module.n_channels = new_num_channels
+
+                nncf_logger.debug(
+                    f'Pruned Elementwise {node.data["key"]} by input mask. '
+                    f"Old num features: {old_num_channels}, new num features: {new_num_channels}."
+                )
+            else:
+                node_module.weight = torch.nn.Parameter(apply_filter_binary_mask(input_mask, node_module.weight))
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('stop_propagation_ops')
+@PT_PRUNING_OPERATOR_METATYPES.register("stop_propagation_ops")
 class PTStopMaskForwardPruningOp(StopMaskForwardPruningOp, PTPruner):
-    subtypes = [PTMeanMetatype, PTMaxMetatype, PTMinMetatype, PTSumMetatype,
-                UnknownMetatype]
+    subtypes = [PTMeanMetatype, PTMaxMetatype, PTMinMetatype, PTSumMetatype, UnknownMetatype]
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('reshape')
+@PT_PRUNING_OPERATOR_METATYPES.register("reshape")
 class PTReshape(ReshapePruningOp, PTPruner):
     subtypes = [PTReshapeMetatype]
 
 
-@PT_PRUNING_OPERATOR_METATYPES.register('concat')
+@PT_PRUNING_OPERATOR_METATYPES.register("concat")
 class PTConcatPruningOp(ConcatPruningOp, PTPruner):
     subtypes = [PTCatMetatype]
 
-@PT_PRUNING_OPERATOR_METATYPES.register('chunk')
+
+@PT_PRUNING_OPERATOR_METATYPES.register("chunk")
 class PTSplitPruningOp(SplitPruningOp, PTPruner):
     subtypes = [PTSplitMetatype]
 
+
+@PT_PRUNING_OPERATOR_METATYPES.register("pad")
+class PTPadPruningOp(PadPruningOp, PTPruner):
+    subtypes = [PTPadMetatype]
+
+
 class ModelPruner(MaskPropagationAlgorithm):
-    def __init__(self, model: NNCFNetwork, graph: NNCFGraph,
-                 pruning_operator_metatypes: PruningOperationsMetatypeRegistry):
+    def __init__(
+        self,
+        model: NNCFNetwork,
+        graph: NNCFGraph,
+        pruning_operator_metatypes: PruningOperationsMetatypeRegistry,
+        prun_type: PrunType = PrunType.FILL_ZEROS,
+    ):
         super().__init__(graph, pruning_operator_metatypes, PTNNCFPruningTensorProcessor)
         self._model = model
+        self._prun_type = prun_type
 
     def apply_mask(self):
         """
         Applying propagated masks for all nodes in topological order:
         1. running input_prune method for this node
         2. running output_prune method for this node
         """
         pruned_node_modules = []
         with torch.no_grad():
             for node in self._graph.topological_sort():
                 node_cls = self.get_meta_operation_by_type_name(node.node_type)
-                node_module = self._model.get_containing_module(node.node_name)
+                node_module = self._model.nncf.get_containing_module(node.node_name)
                 if node_module not in pruned_node_modules:
-                    node_cls.input_prune(self._model, node, self._graph)
-                    node_cls.output_prune(self._model, node, self._graph)
+                    node_cls.input_prune(self._model, node, self._graph, self._prun_type)
+                    node_cls.output_prune(self._model, node, self._graph, self._prun_type)
                     pruned_node_modules.append(node_module)
-            nncf_logger.info('Finished applying pruning masks.')
+            nncf_logger.info("Finished applying pruning masks.")
+
+    def remove_filter_pruning_operations(self) -> None:
+        """
+        Remove all filter pruning operation in the model.
+
+        :param model: Target model.
+        """
+        for node in self._model.nncf.get_original_graph().get_all_nodes():
+            if node.node_type in ["nncf_model_input", "nncf_model_output"]:
+                continue
+
+            nncf_module = self._model.nncf.get_containing_module(node.node_name)
+
+            if hasattr(nncf_module, "pre_ops"):
+                for key in list(nncf_module.pre_ops.keys()):
+                    op = nncf_module.get_pre_op(key)
+                    if isinstance(op.op, FilterPruningMask):
+                        nncf_module.remove_pre_forward_operation(key)
+
+            if hasattr(nncf_module, "post_ops"):
+                for key in list(nncf_module.post_ops.keys()):
+                    op = nncf_module.post_ops(key)
+                    if isinstance(op.op, FilterPruningMask):
+                        nncf_module.remove_post_forward_operation(key)
 
     def prune_model(self):
         """
         Model pruner work in two stages:
         1. Mask propagation: propagate pruning masks through the graph.
-        2. Applying calculated masks
+        2. Applying calculated masks.
+        3. Remove filter pruning operations.
         """
-        nncf_logger.info('Start pruning model')
+        nncf_logger.info("Start pruning model")
         self.mask_propagation()
         self.apply_mask()
-        nncf_logger.info('Finished pruning model')
+        self.remove_filter_pruning_operations()
+        nncf_logger.info("Finished pruning model")
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/structs.py` & `nncf-2.5.0/nncf/torch/pruning/structs.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,34 +1,35 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import Callable
 
 from torch import nn
 
 from nncf.common.graph import NNCFNodeName
 from nncf.common.pruning.structs import PrunedLayerInfoBase
 from nncf.torch.dynamic_graph.scope import Scope
 
 
 class PrunedModuleInfo(PrunedLayerInfoBase):
-    def __init__(self, node_name: NNCFNodeName,
-                 module_scope: Scope,
-                 module: nn.Module,
-                 operand: Callable,
-                 node_id: int,
-                 is_depthwise: bool):
+    def __init__(
+        self,
+        node_name: NNCFNodeName,
+        module_scope: Scope,
+        module: nn.Module,
+        operand: Callable,
+        node_id: int,
+        is_depthwise: bool,
+    ):
         super().__init__(node_name, node_id, is_depthwise)
         self.module_scope = module_scope
         self.module = module
         self.operand = operand
         self.key = self.node_name
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/tensor_processor.py` & `nncf-2.5.0/nncf/torch/pruning/tensor_processor.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List, Union
 
 import torch
 
 from nncf.common.pruning.tensor_processor import NNCFPruningBaseTensorProcessor
 from nncf.common.tensor import NNCFTensor
```

### Comparing `nncf-2.4.0/nncf/torch/pruning/utils.py` & `nncf-2.5.0/nncf/torch/pruning/utils.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,77 +1,72 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Dict
-from typing import Optional
-from typing import List
-from typing import Tuple
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Dict, List, Optional, Tuple
 
 import torch
 
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.layer_attributes import LinearLayerAttributes
+from nncf.common.logging import nncf_logger
 from nncf.torch.graph.graph import NNCFNode
 from nncf.torch.layers import NNCF_GENERAL_CONV_MODULES_DICT
 from nncf.torch.layers import NNCF_LINEAR_MODULES_DICT
-from nncf.torch.tensor import PTNNCFTensor
 from nncf.torch.nncf_network import NNCFNetwork
-from nncf.common.logging import nncf_logger
+from nncf.torch.tensor import PTNNCFTensor
 
 
 def get_bn_node_for_conv(graph: NNCFGraph, conv_node: NNCFNode) -> Optional[NNCFNode]:
     successors = graph.get_next_nodes(conv_node)
     for succ in successors:
-        if succ.node_type == 'batch_norm':
+        if succ.node_type == "batch_norm":
             return succ
     return None
 
 
 def get_bn_for_conv_node_by_name(target_model: NNCFNetwork, conv_node_name: NNCFNodeName) -> Optional[torch.nn.Module]:
     """
     Returns a batch norm module in target_model that corresponds immediately following a given
     convolution node in the model's NNCFGraph representation.
     :param target_model: NNCFNetwork to work with
     :param module_scope:
     :return: batch norm module
     """
-    graph = target_model.get_original_graph()
+    graph = target_model.nncf.get_original_graph()
     conv_node = graph.get_node_by_name(conv_node_name)
     bn_node = get_bn_node_for_conv(graph, conv_node)
     if bn_node is None:
         return None
-    bn_module = target_model.get_containing_module(bn_node.node_name)
+    bn_module = target_model.nncf.get_containing_module(bn_node.node_name)
     return bn_module
 
 
 def init_output_masks_in_graph(graph: NNCFGraph, nodes: List):
     """
     Initialize masks in graph for mask propagation algorithm
 
     :param graph: NNCFNetwork
     :param nodes: list with pruned nodes
     """
     for node in graph.get_all_nodes():
-        node.data.pop('output_mask', None)
+        node.data.pop("output_mask", None)
 
     for minfo in nodes:
         mask = minfo.operand.binary_filter_pruning_mask
         nncf_node = graph.get_node_by_id(minfo.nncf_node_id)
-        nncf_node.data['output_mask'] = PTNNCFTensor(mask)
+        nncf_node.data["output_mask"] = PTNNCFTensor(mask)
 
 
 def _calculate_output_shape(graph: NNCFGraph, node: NNCFNode) -> Tuple[int, ...]:
     """
     Calculates output shape of convolution layer by input edge.
 
     :param graph: the model graph
@@ -88,30 +83,30 @@
             if attrs.transpose:
                 shape[i] = (shape[i] - 1) * attrs.stride[i] - 2 * attrs.padding_values[i] + attrs.kernel_size[i]
             else:
                 shape[i] = (shape[i] + 2 * attrs.padding_values[i] - attrs.kernel_size[i]) // attrs.stride[i] + 1
     elif isinstance(attrs, LinearLayerAttributes):
         shape = shape[:-1] + [attrs.out_features]
     else:
-        raise RuntimeError(f'Unexpected node type {node.node_type} is fed to _calculate_output_shape')
+        raise RuntimeError(f"Unexpected node type {node.node_type} is fed to _calculate_output_shape")
     return tuple(shape)
 
 
 def collect_output_shapes(graph: NNCFGraph) -> Dict[NNCFNodeName, List[int]]:
     """
     Collects output dimension shapes for convolutions and fully connected layers
     from the connected edges in the NNCFGraph.
 
     :param graph: NNCFGraph.
     :return: Dictionary of output dimension shapes. E.g {node_name: (height, width)}
     """
     modules_out_shapes = {}
     output_shape_collecting_info = [
-       (NNCF_GENERAL_CONV_MODULES_DICT, slice(2, None)),
-       (NNCF_LINEAR_MODULES_DICT, slice(None)),
+        (NNCF_GENERAL_CONV_MODULES_DICT, slice(2, None)),
+        (NNCF_LINEAR_MODULES_DICT, slice(None)),
     ]
     for nncf_module_type, shape_slice in output_shape_collecting_info:
         for node in graph.get_nodes_by_types([v.op_func_name for v in nncf_module_type]):
             output_edges = graph.get_output_edges(node)
             if output_edges:
                 out_edge = output_edges[0]
                 out_shape = out_edge.tensor_shape[shape_slice]
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/__init__.py` & `nncf-2.5.0/nncf/tensorflow/pruning/filter_pruning/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,13 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Backend-specific implementations of filter-wise pruning.
 """
-# Required for correct QUANTIZATION_MODULES registry functioning
-from . import layers
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/adjust_padding.py` & `nncf-2.5.0/nncf/torch/quantization/adjust_padding.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,31 +1,29 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from collections import namedtuple
 from typing import NamedTuple
 
 import networkx as nx
 import torch
 
 from nncf.common.graph import NNCFNodeName
+from nncf.common.quantization.structs import QuantizationMode
 from nncf.torch.layers import NNCFConv2d
 from nncf.torch.module_operations import UpdatePaddingValue
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.layers import BaseQuantizer
-from nncf.torch.quantization.layers import QuantizationMode
 from nncf.torch.quantization.layers import QuantizerConfig
 from nncf.torch.quantization.layers import SymmetricQuantizer
 
 
 class AdjustPaddingArgs(NamedTuple):
     weight_bitwidth: int
     activation_quantizer: BaseQuantizer
@@ -41,45 +39,49 @@
     to transform u4 to i4 in the VPU plugin by shifting the input by half of the quantization range to left. Padding
     value should be shifted as well. And to make it zero after the shift (non-zero padding values are not
     supported), the model should be trained with padding value equal to the half of the quantization range.
     """
 
     def __init__(self, activation_quantizer: SymmetricQuantizer):
         if not isinstance(activation_quantizer, SymmetricQuantizer):
-            raise RuntimeError('Padding adjustment is not supported for not symmetric quantization')
+            raise RuntimeError("Padding adjustment is not supported for not symmetric quantization")
         self._activation_quantizer = activation_quantizer
         self._is_enabled = True
 
     def __call__(self, previous_padding_value) -> torch.Tensor:
         if self._is_enabled:
             scale = self._activation_quantizer.scale
             eps = self._activation_quantizer.eps
             safe_scale = abs(scale) + eps
             return safe_scale / 2
         return previous_padding_value
 
     @staticmethod
     def is_config_applicable(qconfig: QuantizerConfig):
-        return not qconfig.per_channel and qconfig.num_bits == 4 and \
-               not qconfig.signedness_to_force and qconfig.mode == QuantizationMode.SYMMETRIC
+        return (
+            not qconfig.per_channel
+            and qconfig.num_bits == 4
+            and not qconfig.signedness_to_force
+            and qconfig.mode == QuantizationMode.SYMMETRIC
+        )
 
 
 def add_adjust_padding_nodes(bitwidth_graph: nx.DiGraph, model: NNCFNetwork) -> nx.DiGraph():
     # pylint:disable=protected-access
 
-    NewNodeArgs = namedtuple('NewNodeArgs', ('node_key', 'attr', 'parent_node_key'))
-    nncf_graph = model.get_graph()
+    NewNodeArgs = namedtuple("NewNodeArgs", ("node_key", "attr", "parent_node_key"))
+    nncf_graph = model.nncf.get_graph()
     args = []
     for node_key in bitwidth_graph.nodes:
         node = nncf_graph.get_node_by_key(node_key)
-        module = model.get_containing_module(node.node_name)
+        module = model.nncf.get_containing_module(node.node_name)
         if isinstance(module, NNCFConv2d):
             adjust_padding_ops = filter(lambda x: isinstance(x, UpdatePaddingValue), module.pre_ops.values())
             for _ in adjust_padding_ops:
-                new_node_key = f'{node_key}_apad'
-                attr = dict(type='', label='adjust_padding_value', style='filled', color='yellow')
+                new_node_key = f"{node_key}_apad"
+                attr = dict(type="", label="adjust_padding_value", style="filled", color="yellow")
                 args.append(NewNodeArgs(new_node_key, attr, node_key))
 
     for arg in args:
         bitwidth_graph.add_node(arg.node_key, **arg.attr)
         bitwidth_graph.add_edge(arg.node_key, arg.parent_node_key)
     return bitwidth_graph
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/algo.py` & `nncf-2.5.0/nncf/torch/quantization/algo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,55 +1,52 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# pylint:disable=too-many-lines
 """
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Contains builder and controller class definitions for the quantization algorithm.
 """
+
 import shutil
-# pylint:disable=too-many-lines
 from collections import Counter
 from collections import OrderedDict
 from copy import deepcopy
 from enum import IntEnum
 from pathlib import Path
 from string import Template
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Set
-from typing import Tuple
-from typing import Union
+from typing import Any, Dict, List, Optional, Set, Tuple
 
 import networkx as nx
 import numpy as np
 import torch
 from torch import nn
 
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.graph import NNCFGraph
 from nncf.common.graph import NNCFNode
-from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.definitions import MODEL_INPUT_OP_NAME
 from nncf.common.graph.layer_attributes import ConvolutionLayerAttributes
 from nncf.common.graph.layer_attributes import WeightedLayerAttributes
+from nncf.common.graph.patterns.manager import PatternsManager
+from nncf.common.graph.patterns.manager import TargetDevice
 from nncf.common.graph.transformations.commands import TargetPoint
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.graph.utils import get_first_nodes_of_type
-from nncf.common.hardware.config import get_hw_config_type
 from nncf.common.hardware.config import HWConfig
 from nncf.common.hardware.config import HWConfigType
+from nncf.common.hardware.config import get_hw_config_type
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
 from nncf.common.insertion_point_graph import InsertionPointGraph
 from nncf.common.insertion_point_graph import InsertionPointGraphNodeType
 from nncf.common.logging import nncf_logger
 from nncf.common.logging.logger import DuplicateFilter
 from nncf.common.quantization.config_assignment import assign_qconfig_lists_to_modules
 from nncf.common.quantization.quantizer_setup import DEFAULT_QUANTIZER_CONFIG
@@ -61,16 +58,18 @@
 from nncf.common.quantization.structs import QuantizableWeightedLayerNode
 from nncf.common.quantization.structs import QuantizationConstraints
 from nncf.common.quantization.structs import QuantizationPreset
 from nncf.common.quantization.structs import QuantizerGroup
 from nncf.common.quantization.structs import QuantizerId
 from nncf.common.quantization.structs import WeightQuantizerId
 from nncf.common.schedulers import BaseCompressionScheduler
-from nncf.common.scopes import matches_any
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
+from nncf.common.utils.backend import BackendType
+from nncf.common.utils.backend import copy_model
 from nncf.common.utils.debug import is_debug
 from nncf.common.utils.dot_file_rw import write_dot_graph
 from nncf.common.utils.os import safe_open
 from nncf.config import NNCFConfig
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.extractors import extract_bn_adaptation_init_params
 from nncf.config.extractors import extract_range_init_params
@@ -92,15 +91,14 @@
 from nncf.torch.graph.operator_metatypes import PTDepthwiseConv2dSubtype
 from nncf.torch.graph.operator_metatypes import PTModuleConv2dMetatype
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.graph.transformations.commands import TransformationPriority
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.hardware.config import PTHWConfig
-from nncf.torch.hardware.fused_patterns import PT_HW_FUSED_PATTERNS
 from nncf.torch.initialization import SimpleDataLoaderRunner
 from nncf.torch.module_operations import UpdatePaddingValue
 from nncf.torch.nncf_network import EXTERNAL_QUANTIZERS_STORAGE_NAME
 from nncf.torch.nncf_network import ExtraCompressionModuleType
 from nncf.torch.nncf_network import LoadStateListener
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.adjust_padding import AdjustPaddingArgs
@@ -129,30 +127,34 @@
 from nncf.torch.quantization.precision_init.adjacent_quantizers import GroupsOfAdjacentQuantizers
 from nncf.torch.quantization.precision_init.autoq_init import AutoQPrecisionInitParams
 from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitializer
 from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitParams
 from nncf.torch.quantization.precision_init.hawq_init import HAWQPrecisionInitParams
 from nncf.torch.quantization.precision_init.manual_init import ManualPrecisionInitParams
 from nncf.torch.quantization.schedulers import QUANTIZATION_SCHEDULERS
+from nncf.torch.quantization.strip import remove_disabled_quantizers
+from nncf.torch.quantization.strip import replace_quantizer_to_torch_native_module
 from nncf.torch.quantization.structs import NonWeightQuantizerInfo
 from nncf.torch.quantization.structs import WeightQuantizerInfo
 from nncf.torch.quantization.translator import PTTargetPointTranslator
 from nncf.torch.structures import AutoQPrecisionInitArgs
 from nncf.torch.structures import QuantizationPrecisionInitArgs
 from nncf.torch.tensor_statistics.algo import TensorStatisticsCollectionBuilder
 from nncf.torch.tensor_statistics.collectors import ReductionShape
 from nncf.torch.tensor_statistics.statistics import MinMaxTensorStatistic
 from nncf.torch.tensor_statistics.statistics import TensorStatistic
 from nncf.torch.tensor_statistics.statistics import pt_convert_stat_to_min_max_tensor_stat
 from nncf.torch.utils import get_model_device
 from nncf.torch.utils import get_model_dtype
 from nncf.torch.utils import get_state_dict_names_with_modules
 from nncf.torch.utils import is_main_process
+from nncf.torch.utils import training_mode_switcher
+
+QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME = "version"
 
-QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME = 'version'
 
 class QuantizerBuilderStateVersion(IntEnum):
     # In Quantization builder state SingleConfigQuantizerSetup is being saved as quantizer setup.
     v1 = 1
     # In Quantization builder state PTQuantizerSetup is being saved as quantizer setup.
     v2 = 2
 
@@ -160,296 +162,285 @@
     def from_compression_state(compression_state):
         if QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME in compression_state:
             return compression_state.get(QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME)
         return QuantizerBuilderStateVersion.v1
 
 
 class QuantizerSetupGeneratorBase:
-    def __init__(self, quant_config: Dict,
-                 target_model: NNCFNetwork,
-                 precision_init_type: str = None,
-                 precision_init_params: BasePrecisionInitParams = None,
-                 range_init_params: PTRangeInitParams = None,
-                 hw_config: HWConfig = None):
+    def __init__(
+        self,
+        quant_config: Dict,
+        target_model: NNCFNetwork,
+        precision_init_type: str = None,
+        precision_init_params: BasePrecisionInitParams = None,
+        range_init_params: PTRangeInitParams = None,
+        hw_config: HWConfig = None,
+    ):
         self._target_model = target_model
         self._quantization_config = quant_config
         self.hw_config = hw_config
         self._target_device = None if hw_config is None else hw_config.target_device
-        self._quantize_inputs = self._quantization_config.get('quantize_inputs', QUANTIZE_INPUTS)
-        self._quantize_outputs = self._quantization_config.get('quantize_outputs', QUANTIZE_OUTPUTS)
+        self._quantize_inputs = self._quantization_config.get("quantize_inputs", QUANTIZE_INPUTS)
+        self._quantize_outputs = self._quantization_config.get("quantize_outputs", QUANTIZE_OUTPUTS)
 
-        self.ignored_scopes = self._quantization_config.get('ignored_scopes')
-        self.target_scopes = self._quantization_config.get('target_scopes')
+        self.ignored_scopes = self._quantization_config.get("ignored_scopes")
+        self.target_scopes = self._quantization_config.get("target_scopes")
 
         self.global_quantizer_constraints = {}  # type: Dict[QuantizerGroup, QuantizationConstraints]
         self._ignored_scopes_per_group = {}  # type: Dict[QuantizerGroup, List[str]]
         self._target_scopes_per_group = {}  # type: Dict[QuantizerGroup, List[str]]
 
         for quantizer_group in QuantizerGroup:
             self._parse_group_params(self._quantization_config, quantizer_group)
 
         self._precision_init_type = precision_init_type
         self._precision_init_params = precision_init_params
         self._range_init_params = range_init_params
-        self._num_potential_quantized_weights = len(self._target_model.get_nncf_modules())
+        self._num_potential_quantized_weights = len(self._target_model.nncf.get_nncf_modules())
 
     def generate_setup(self) -> SingleConfigQuantizerSetup:
         raise NotImplementedError
 
     def get_build_time_metric_infos(self):
         raise NotImplementedError
 
     def _parse_group_params(self, quant_config: Dict, quantizer_group: QuantizerGroup):
         group_name = quantizer_group.value
         params_dict = {}
         params_dict_from_config = quant_config.get(group_name, {})
-        preset = quant_config.get('preset')
-        if self._target_device in ['ANY', 'CPU', 'GPU'] or \
-                (self._target_device is None and preset is not None):
-            preset = QuantizationPreset(quant_config.get('preset', QUANTIZATION_PRESET))
+        preset = quant_config.get("preset")
+        if self._target_device in ["ANY", "CPU", "GPU"] or (self._target_device is None and preset is not None):
+            preset = QuantizationPreset(quant_config.get("preset", QUANTIZATION_PRESET))
             params_dict = preset.get_params_configured_by_preset(quantizer_group)
             overridden_params = params_dict.keys() & params_dict_from_config.keys()
             if overridden_params:
-                nncf_logger.info(f'Preset quantizer parameters {overridden_params} explicitly overridden by config.')
+                nncf_logger.info(f"Preset quantizer parameters {overridden_params} explicitly overridden by config.")
         params_dict.update(params_dict_from_config)
         self.global_quantizer_constraints[quantizer_group] = QuantizationConstraints.from_config_dict(params_dict)
-        self._ignored_scopes_per_group[quantizer_group] = params_dict_from_config.get('ignored_scopes')
-        self._target_scopes_per_group[quantizer_group] = params_dict_from_config.get('target_scopes')
+        self._ignored_scopes_per_group[quantizer_group] = params_dict_from_config.get("ignored_scopes", [])
+        if self.ignored_scopes is not None:
+            self._ignored_scopes_per_group[quantizer_group] += self.ignored_scopes
+        target_scopes = params_dict_from_config.get("target_scopes")
+        if target_scopes is None and self.target_scopes is not None:
+            self._target_scopes_per_group[quantizer_group] = self.target_scopes
+        else:
+            self._target_scopes_per_group[quantizer_group] = target_scopes
 
     def _get_default_qconfig(self, constraints: QuantizationConstraints = None):
         qconfig = deepcopy(DEFAULT_QUANTIZER_CONFIG)
         if constraints is not None:
             qconfig = constraints.apply_constraints_to(qconfig)
         return qconfig
 
-    def _should_consider_scope_for_group(self, node_name: NNCFNodeName, group: QuantizerGroup) -> bool:
-        if self.target_scopes is not None or self._target_scopes_per_group[group] is not None:
-            if matches_any(node_name, self.target_scopes):
-                return True
-            if matches_any(node_name, self._target_scopes_per_group[group]):
-                return True
-
-            return False
-
-        if matches_any(node_name, self.ignored_scopes):
-            return False
-        if matches_any(node_name, self._ignored_scopes_per_group[group]):
-            return False
-
-        return True
-
     def _filter_by_ignored_algo(self, nodes: List[NNCFNode]) -> List[NNCFNode]:
         retval = []
         for node in nodes:
-            if 'quantization' in node.ignored_algorithms:
-                continue
-            retval.append(node)
-        return retval
-
-    def _filter_by_weight_ignored_target_scopes(self, weighted_nodes: List[NNCFNode]) -> List[NNCFNode]:
-        retval = []
-        for node in weighted_nodes:
-            if not self._should_consider_scope_for_group(node.node_name, QuantizerGroup.WEIGHTS):
-                nncf_logger.info(f"Ignored adding weight quantizer for: {node.node_name}")
+            if "quantization" in node.ignored_algorithms:
                 continue
             retval.append(node)
         return retval
 
-    def _assign_qconfig_lists_to_modules(self, weighted_nodes: List[NNCFNode]) -> \
-        Dict[NNCFNode, List[QuantizerConfig]]:
+    def _assign_qconfig_lists_to_modules(self, weighted_nodes: List[NNCFNode]) -> Dict[NNCFNode, List[QuantizerConfig]]:
         raise NotImplementedError
 
     def get_quantizable_module_nodes(self) -> List[QuantizableWeightedLayerNode]:
-        weighted_nodes = self._target_model.get_original_graph().get_nodes_by_metatypes(QUANTIZATION_LAYER_METATYPES)
+        weighted_nodes = self._target_model.nncf.get_original_graph().get_nodes_by_metatypes(
+            QUANTIZATION_LAYER_METATYPES
+        )
         quantized_modules_with_potential_qconfig = []
 
         weighted_nodes = self._filter_by_ignored_algo(weighted_nodes)
-        weighted_nodes = self._filter_by_weight_ignored_target_scopes(weighted_nodes)
         weighted_node_vs_qconfig_list = self._assign_qconfig_lists_to_modules(weighted_nodes)
 
         for node, qconfig_list in weighted_node_vs_qconfig_list.items():
             if qconfig_list is not None:
                 qconfig_list_copy = deepcopy(qconfig_list)
-                quantized_modules_with_potential_qconfig.append(QuantizableWeightedLayerNode(node,
-                                                                                             qconfig_list_copy))
+                quantized_modules_with_potential_qconfig.append(QuantizableWeightedLayerNode(node, qconfig_list_copy))
         return quantized_modules_with_potential_qconfig
 
 
 class IQuantizerSetupDisambiguator:
     def select_final_quantizer_setup(self, multi_config_setup: MultiConfigQuantizerSetup) -> SingleConfigQuantizerSetup:
         raise NotImplementedError
 
 
 class DefaultQuantizerSetupDisambiguator(IQuantizerSetupDisambiguator):
-    def __init__(self, target_model: NNCFNetwork,
-                 precision_init_type: str = None,
-                 precision_init_params: BasePrecisionInitParams = None,
-                 range_init_params: PTRangeInitParams = None,
-                 override_bit_options_with_precision_init: bool = False,
-                 hw_config: HWConfig = None):
+    def __init__(
+        self,
+        target_model: NNCFNetwork,
+        precision_init_type: str = None,
+        precision_init_params: BasePrecisionInitParams = None,
+        range_init_params: PTRangeInitParams = None,
+        override_bit_options_with_precision_init: bool = False,
+        hw_config: HWConfig = None,
+    ):
         self._precision_init_type = precision_init_type
         self._precision_init_params = precision_init_params
         self._range_init_params = range_init_params
         self._target_model = target_model
         self._override_bit_options_with_precision_init = override_bit_options_with_precision_init
         self.hw_config = hw_config
 
     @staticmethod
     def select_first_qconfig_with_bitwidth_variants_for_each_point(
-        multi_config_setup: MultiConfigQuantizerSetup) -> MultiConfigQuantizerSetup:
+        multi_config_setup: MultiConfigQuantizerSetup,
+    ) -> MultiConfigQuantizerSetup:
         new_setup = deepcopy(multi_config_setup)
         for qp_id, qp in multi_config_setup.quantization_points.items():
             main_qconfig = qp.possible_qconfigs[0]
             constrained_qconfig_list = [main_qconfig]
             if len(qp.possible_qconfigs) > 1:
                 constrained_qconfig_list += list(filter(main_qconfig.is_a_bitwidth_variant, qp.possible_qconfigs[1:]))
             new_setup.quantization_points[qp_id].possible_qconfigs = constrained_qconfig_list
         return new_setup
 
     def select_final_quantizer_setup(self, multi_config_setup: MultiConfigQuantizerSetup) -> SingleConfigQuantizerSetup:
         if self._precision_init_type is not None:
-            with self._target_model.temporary_clean_view() as intermediate_model:
-                stats = QuantizationBuilder.get_statistics_for_quantizer_setup(intermediate_model,
-                                                                               multi_config_setup,
-                                                                               self._range_init_params)
-                bitwidth_varying_only_multi_setup = \
-                    self.select_first_qconfig_with_bitwidth_variants_for_each_point(multi_config_setup)
+            with self._target_model.nncf.temporary_clean_view() as intermediate_model:
+                stats = QuantizationBuilder.get_statistics_for_quantizer_setup(
+                    intermediate_model, multi_config_setup, self._range_init_params
+                )
+                bitwidth_varying_only_multi_setup = self.select_first_qconfig_with_bitwidth_variants_for_each_point(
+                    multi_config_setup
+                )
 
                 init_setup = bitwidth_varying_only_multi_setup.select_first_qconfig_for_each_point()
-                intermediate_builder = ExperimentalQuantizationBuilder(bitwidth_varying_only_multi_setup,
-                                                                       init_setup,
-                                                                       stats,
-                                                                       hw_config=self.hw_config)
+                intermediate_builder = ExperimentalQuantizationBuilder(
+                    bitwidth_varying_only_multi_setup, init_setup, stats, hw_config=self.hw_config
+                )
                 intermediate_builder.apply_to(intermediate_model)
                 intermediate_ctrl = intermediate_builder.build_controller(intermediate_model)
 
                 # intermediate_ctrl.init_range()
                 hw_constraints = HardwareQuantizationConstraints()
                 if not self._override_bit_options_with_precision_init:
                     for qp_id, qp in multi_config_setup.quantization_points.items():
                         quantizer_module_id = intermediate_ctrl.setup_to_module_id_translation_dict[qp_id]
                         hw_constraints.add(quantizer_module_id, qp.possible_qconfigs)
-                final_quantizer_setup = intermediate_ctrl.init_precision(self._precision_init_type,
-                                                                         self._precision_init_params,
-                                                                         hw_constraints)
+                final_quantizer_setup = intermediate_ctrl.init_precision(
+                    self._precision_init_type, self._precision_init_params, hw_constraints
+                )
         else:
             final_quantizer_setup = multi_config_setup.select_first_qconfig_for_each_point()
         return final_quantizer_setup
 
 
 class PropagationBasedQuantizerSetupGenerator(QuantizerSetupGeneratorBase):
-    def __init__(self, quant_config: Dict, target_model: NNCFNetwork,
-                 hw_config: HWConfig = None,
-                 precision_init_type: str = None,
-                 precision_init_params: BasePrecisionInitParams = None,
-                 range_init_params: PTRangeInitParams = None,
-                 debug_interface: 'QuantizationDebugInterface' = None):
-        super().__init__(quant_config, target_model, precision_init_type,
-                         precision_init_params, range_init_params,
-                         hw_config)
-
-        self._pattern_fusing_graph = PT_HW_FUSED_PATTERNS.get_full_pattern_graph()
+    def __init__(
+        self,
+        quant_config: Dict,
+        target_model: NNCFNetwork,
+        hw_config: HWConfig = None,
+        device: TargetDevice = None,
+        precision_init_type: str = None,
+        precision_init_params: BasePrecisionInitParams = None,
+        range_init_params: PTRangeInitParams = None,
+        debug_interface: "QuantizationDebugInterface" = None,
+    ):
+        super().__init__(
+            quant_config, target_model, precision_init_type, precision_init_params, range_init_params, hw_config
+        )
 
+        self._pattern_fusing_graph = PatternsManager.get_full_hw_pattern_graph(backend=BackendType.TORCH, device=device)
 
         self._hw_precision_constraints = HardwareQuantizationConstraints()
         self._debug_interface = debug_interface
         self._num_potential_quantized_activations = 0
 
         act_config = quant_config.get(QuantizerGroup.ACTIVATIONS.value, {})
-        self._unified_scale_ops = act_config.get('unified_scale_ops')
+        self._unified_scale_ops = act_config.get("unified_scale_ops")
 
     def generate_setup(self) -> SingleConfigQuantizerSetup:
         quantizable_module_nodes = self.get_quantizable_module_nodes()
 
-        insertion_point_graph = self._target_model.get_insertion_point_graph()
+        insertion_point_graph = self._target_model.nncf.get_insertion_point_graph()
         if self._debug_interface:
             self._debug_interface.visualize_insertion_point_graph(insertion_point_graph)
-        from nncf.common.quantization.quantizer_propagation.solver import \
-            QuantizerPropagationSolver  # pylint: disable=cyclic-import
-
-        def str_or_list_to_list(list_or_str: Union[List[str], str]) -> List:
-            if list_or_str is None:
-                return []
-            return [list_or_str] if isinstance(list_or_str, str) else list_or_str
-
-        ignored_scopes_for_solver = str_or_list_to_list(self.ignored_scopes) + \
-                                    str_or_list_to_list(self._ignored_scopes_per_group[QuantizerGroup.ACTIVATIONS])
-        prop_graph_solver = QuantizerPropagationSolver(ignored_scopes=ignored_scopes_for_solver,
-                                                       target_scopes=self.target_scopes,
-                                                       hw_config=self.hw_config,
-                                                       default_trait_to_metatype_map=DEFAULT_PT_QUANT_TRAIT_TO_OP_DICT,
-                                                       default_qconfig_list=[self._get_default_qconfig(
-                                                           constraints=self.global_quantizer_constraints[
-                                                               QuantizerGroup.ACTIVATIONS])],
-                                                       quantizable_layer_nodes=quantizable_module_nodes,
-                                                       scope_overrides=self._quantization_config.get("scope_overrides",
-                                                                                                     {}),
-                                                       global_constraints=self.global_quantizer_constraints,
-                                                       additional_unified_scale_op_scopes=self._unified_scale_ops,
-                                                       quantize_outputs=self._quantize_outputs)
+        from nncf.common.quantization.quantizer_propagation.solver import (
+            QuantizerPropagationSolver,  # pylint: disable=cyclic-import
+        )
+
+        prop_graph_solver = QuantizerPropagationSolver(
+            activation_ignored_scopes=self._ignored_scopes_per_group[QuantizerGroup.ACTIVATIONS],
+            weight_ignored_scopes=self._ignored_scopes_per_group[QuantizerGroup.WEIGHTS],
+            activation_target_scopes=self._target_scopes_per_group[QuantizerGroup.ACTIVATIONS],
+            weight_target_scopes=self._target_scopes_per_group[QuantizerGroup.WEIGHTS],
+            hw_config=self.hw_config,
+            default_trait_to_metatype_map=DEFAULT_PT_QUANT_TRAIT_TO_OP_DICT,
+            default_qconfig_list=[
+                self._get_default_qconfig(constraints=self.global_quantizer_constraints[QuantizerGroup.ACTIVATIONS])
+            ],
+            quantizable_layer_nodes=quantizable_module_nodes,
+            scope_overrides=self._quantization_config.get("scope_overrides", {}),
+            global_constraints=self.global_quantizer_constraints,
+            additional_unified_scale_op_scopes=self._unified_scale_ops,
+            quantize_outputs=self._quantize_outputs,
+        )
 
         merged_ip_graph = insertion_point_graph.get_ip_graph_with_merged_hw_optimized_operations(
-            self._pattern_fusing_graph)
+            self._pattern_fusing_graph
+        )
         quantization_proposal = prop_graph_solver.run_on_ip_graph(merged_ip_graph)
         self._num_potential_quantized_activations = prop_graph_solver.get_num_potential_quantized_activations()
 
         quantizer_setup = deepcopy(quantization_proposal.quantizer_setup)
         quantization_proposal.quantizer_setup = quantizer_setup
 
         disambiguator = DefaultQuantizerSetupDisambiguator(
             self._target_model,
             self._precision_init_type,
             self._precision_init_params,
             self._range_init_params,
             override_bit_options_with_precision_init=self.hw_config is None,
-            hw_config=self.hw_config)
+            hw_config=self.hw_config,
+        )
 
         single_config_quantizer_setup = disambiguator.select_final_quantizer_setup(
-            quantization_proposal.quantizer_setup)
+            quantization_proposal.quantizer_setup
+        )
 
-        finalized_proposal = quantization_proposal.finalize(single_config_quantizer_setup,
-                                                            strict=self.hw_config is not None)
+        finalized_proposal = quantization_proposal.finalize(
+            single_config_quantizer_setup, strict=self.hw_config is not None
+        )
         finalized_quantizer_setup = prop_graph_solver.get_final_quantizer_setup(finalized_proposal)
         finalized_quantizer_setup = self._handle_quantize_inputs_option(finalized_quantizer_setup)
         return finalized_quantizer_setup
 
-    def _assign_qconfig_lists_to_modules(self, weighted_nodes: List[NNCFNode]) -> Dict[NNCFNode,
-                                                                                           List[QuantizerConfig]]:
+    def _assign_qconfig_lists_to_modules(self, weighted_nodes: List[NNCFNode]) -> Dict[NNCFNode, List[QuantizerConfig]]:
         global_constraints = self.global_quantizer_constraints[QuantizerGroup.WEIGHTS]
         scope_overrides_dict = self._quantization_config.get("scope_overrides", {})
-        return assign_qconfig_lists_to_modules(weighted_nodes,
-                                               self._get_default_qconfig(),
-                                               global_constraints,
-                                               scope_overrides_dict,
-                                               self.hw_config)
+        return assign_qconfig_lists_to_modules(
+            weighted_nodes, self._get_default_qconfig(), global_constraints, scope_overrides_dict, self.hw_config
+        )
 
     def _handle_quantize_inputs_option(self, quantizer_setup: SingleConfigQuantizerSetup) -> SingleConfigQuantizerSetup:
-        nncf_graph = self._target_model.get_original_graph()
+        nncf_graph = self._target_model.nncf.get_original_graph()
         qp_ids_to_discard = []
         for qp_id, qp in quantizer_setup.quantization_points.items():
             if qp.is_activation_quantization_point():
                 insertion_point = qp.insertion_point
                 target_node = nncf_graph.get_node_by_name(insertion_point.target_node_name)
                 if not self._quantize_inputs and target_node.node_type == MODEL_INPUT_OP_NAME:
                     qp_ids_to_discard.append(qp_id)
         for qp_id in qp_ids_to_discard:
             quantizer_setup.discard(qp_id, keep_shared_input_qps=True)
         return quantizer_setup
 
     def get_build_time_metric_infos(self):
-        return QuantizationShareBuildTimeInfo(self._num_potential_quantized_activations,
-                                              self._num_potential_quantized_weights)
+        return QuantizationShareBuildTimeInfo(
+            self._num_potential_quantized_activations, self._num_potential_quantized_weights
+        )
 
 
 class QBuilderStateNames:
-    BUILD_TIME_METRIC_INFOS = 'build_time_metric_infos'
-    QUANTIZER_SETUP = 'quantizer_setup'
+    BUILD_TIME_METRIC_INFOS = "build_time_metric_infos"
+    QUANTIZER_SETUP = "quantizer_setup"
 
 
-@PT_COMPRESSION_ALGORITHMS.register('quantization')
+@PT_COMPRESSION_ALGORITHMS.register("quantization")
 class QuantizationBuilder(PTCompressionAlgorithmBuilder):
     _state_names = QBuilderStateNames
 
     def __init__(self, config, should_init: bool = True):
         super().__init__(config, should_init)
         self._debug_interface = QuantizationDebugInterface() if is_debug() else None
         self._weight_quantizers = OrderedDict()  # Quantizers applied via UpdateWeights
@@ -465,56 +456,59 @@
         self._pt_quantizer_setup = None  # type: Optional[PTQuantizerSetup]
         self._minmax_values_for_range_init = {}  # type: Optional[Dict[QuantizationPointId, MinMaxTensorStatistic]]
 
         # can be False to disable setting of adjust padding operations on precision init, because it may add unnecessary
         # noise on model evaluation (e.g. in AutoQ)
         self._should_setup_adjust_pad_ops = True
         hw_config_type = None
-        target_device = self.config.get('target_device', 'ANY')
-        hw_config_type = get_hw_config_type(target_device)
+        self._target_device = self.config.get("target_device", "ANY")
+        hw_config_type = get_hw_config_type(self._target_device)
         if hw_config_type is not None:
             hw_config_path = PTHWConfig.get_path_to_hw_config(hw_config_type)
             self.hw_config = PTHWConfig.from_json(hw_config_path)
 
         algo_config = self._get_algo_specific_config_section()
-        if target_device == 'VPU' and 'preset' in algo_config:
+        if self._target_device == "VPU" and "preset" in algo_config:
             raise RuntimeError("The VPU target device does not support presets.")
 
         self._range_init_params = None
         self._precision_init_type = None
         self._precision_init_params = None
         if self.should_init:
             self._parse_init_params()
 
         self._use_logarithm_scale_per_group = {}  # type: Dict[QuantizerGroup, bool]
 
         for quantizer_group in QuantizerGroup:
             group_name = quantizer_group.value
             params_dict = self._algo_config.get(group_name, {})
-            self._use_logarithm_scale_per_group[quantizer_group] = params_dict.get('logarithm_scale',
-                                                                                   QUANTIZATION_LOGARITHM_SCALE)
+            self._use_logarithm_scale_per_group[quantizer_group] = params_dict.get(
+                "logarithm_scale", QUANTIZATION_LOGARITHM_SCALE
+            )
 
-        self._overflow_fix = self._algo_config.get('overflow_fix', QUANTIZATION_OVERFLOW_FIX)
-        self._device_for_callable_obj_creation = 'cpu'
+        self._overflow_fix = self._algo_config.get("overflow_fix", QUANTIZATION_OVERFLOW_FIX)
+        self._device_for_callable_obj_creation = "cpu"
 
     def _load_state_without_name(self, state_without_name: Dict[str, Any]):
         """
         Initializes object from the state.
 
         :param state_without_name: Output of `get_state()` method.
         """
         quantizer_setup_state = state_without_name[self._state_names.QUANTIZER_SETUP]
         version = state_without_name.get(QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME, QuantizerBuilderStateVersion.v1)
         if version == QuantizerBuilderStateVersion.v1:
-            self._legacy_single_config_quantizer_setup_from_comp_state =\
-                SingleConfigQuantizerSetup.from_state(quantizer_setup_state)
+            self._legacy_single_config_quantizer_setup_from_comp_state = SingleConfigQuantizerSetup.from_state(
+                quantizer_setup_state
+            )
         else:
             self._pt_quantizer_setup = PTQuantizerSetup.from_state(quantizer_setup_state)
         self._build_time_metric_infos = QuantizationShareBuildTimeInfo.from_state(
-            state_without_name[self._state_names.BUILD_TIME_METRIC_INFOS])
+            state_without_name[self._state_names.BUILD_TIME_METRIC_INFOS]
+        )
 
     def _get_state_without_name(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
@@ -524,76 +518,79 @@
             build_time_metric_infos_state = self._build_time_metric_infos.get_state()
         quantizer_setup_state = {}
         if self._pt_quantizer_setup:
             quantizer_setup_state = self._pt_quantizer_setup.get_state()
         return {
             self._state_names.QUANTIZER_SETUP: quantizer_setup_state,
             self._state_names.BUILD_TIME_METRIC_INFOS: build_time_metric_infos_state,
-            QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME: max(QuantizerBuilderStateVersion).value
+            QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME: max(QuantizerBuilderStateVersion).value,
         }
 
     def _parse_init_params(self):
         self._range_init_params = self._parse_range_init_params()
         self._precision_init_type, self._precision_init_params = self._parse_precision_init_params(
-            self._algo_config.get('initializer', {}))
+            self._algo_config.get("initializer", {})
+        )
 
     def _parse_range_init_params(self) -> Optional[PTRangeInitParams]:
         range_init_params = extract_range_init_params(self.config)
         return PTRangeInitParams(**range_init_params) if range_init_params is not None else None
 
     def _parse_precision_init_params(self, initializer_config: Dict) -> Tuple[str, BasePrecisionInitParams]:
-        init_precision_config = initializer_config.get('precision', None)
+        init_precision_config = initializer_config.get("precision", None)
         if not init_precision_config:
             return None, None
-        precision_init_type = init_precision_config.get('type', 'manual')
+        precision_init_type = init_precision_config.get("type", "manual")
         if precision_init_type not in PRECISION_INIT_TYPES_VS_DESCRIPTION:
             raise RuntimeError(f"Unrecognized precision init type: {precision_init_type}")
-        if precision_init_type == 'hawq':
+        if precision_init_type == "hawq":
             try:
                 precision_init_args = self.config.get_extra_struct(QuantizationPrecisionInitArgs)
             except KeyError as e:
                 raise ValueError(
-                    'Specified non-manual precision initialization in the NNCF config, '
-                    'but the initializing data loader and loss criterion are not provided as an extra struct. '
-                    'Refer to `NNCFConfig.register_extra_structs` and the `QuantizationPrecisionInitArgs` '
-                    'class') from e
-            precision_init_params = HAWQPrecisionInitParams.from_config(
-                init_precision_config,
-                precision_init_args
-            )
+                    "Specified non-manual precision initialization in the NNCF config, "
+                    "but the initializing data loader and loss criterion are not provided as an extra struct. "
+                    "Refer to `NNCFConfig.register_extra_structs` and the `QuantizationPrecisionInitArgs` "
+                    "class"
+                ) from e
+            precision_init_params = HAWQPrecisionInitParams.from_config(init_precision_config, precision_init_args)
         elif precision_init_type == "autoq":
             if self.hw_config is not None and self.hw_config.target_device != HWConfigType.VPU.value:
-                raise ValueError("Unsupported device ({}). Automatic Precision Initialization only supports for "
-                                 "target_device NONE or VPU".format(self.hw_config.target_device))
+                raise ValueError(
+                    "Unsupported device ({}). Automatic Precision Initialization only supports for "
+                    "target_device NONE or VPU".format(self.hw_config.target_device)
+                )
             try:
                 precision_init_args = self.config.get_extra_struct(AutoQPrecisionInitArgs)
             except KeyError as e:
-                raise ValueError('Specified Automated precision initialization in the NNCF config, '
-                                 'but the initializing data loader and loss criterion are not provided as an extra '
-                                 'struct. Refer to `NNCFConfig.register_extra_structs` and the '
-                                 '`AutoQPrecisionInitArgs` class') from e
+                raise ValueError(
+                    "Specified Automated precision initialization in the NNCF config, "
+                    "but the initializing data loader and loss criterion are not provided as an extra "
+                    "struct. Refer to `NNCFConfig.register_extra_structs` and the "
+                    "`AutoQPrecisionInitArgs` class"
+                ) from e
 
             hw_config_type = None
             if self.hw_config is not None:
                 hw_config_type = HWConfigType(self.hw_config.target_device)
-            precision_init_params = AutoQPrecisionInitParams.from_config(init_precision_config,
-                                                                         precision_init_args,
-                                                                         hw_config_type)
+            precision_init_params = AutoQPrecisionInitParams.from_config(
+                init_precision_config, precision_init_args, hw_config_type
+            )
         elif precision_init_type == "manual":
             precision_init_params = ManualPrecisionInitParams.from_config(init_precision_config)
         else:
             raise ValueError(f"Unhandled precision init type: {precision_init_type}")
         return precision_init_type, precision_init_params
 
-    def _get_minmax_values_for_quantizer_locations(self,
-                                                   quantizer_setup: SingleConfigQuantizerSetup,
-                                                   tensor_statistics: Dict[PTTargetPoint, Dict[ReductionShape,
-                                                                                               TensorStatistic]],
-                                                   target_model_graph: PTNNCFGraph) -> \
-            Dict[QuantizationPointId, MinMaxTensorStatistic]:
+    def _get_minmax_values_for_quantizer_locations(
+        self,
+        quantizer_setup: SingleConfigQuantizerSetup,
+        tensor_statistics: Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]],
+        target_model_graph: PTNNCFGraph,
+    ) -> Dict[QuantizationPointId, MinMaxTensorStatistic]:
         retval = {}
         for qp_id, qp in quantizer_setup.quantization_points.items():
             qip = qp.insertion_point
             tp = PTTargetPointTranslator.translate(qip)
             if tp not in tensor_statistics:
                 nncf_logger.debug(f"TP {tp} not found in tensor statistics")
                 retval[qp_id] = None
@@ -603,220 +600,231 @@
                     layer_attrs = target_node.layer_attributes
                     assert isinstance(layer_attrs, WeightedLayerAttributes)
                     input_shape = layer_attrs.get_weight_shape()
                     channel_idx = layer_attrs.get_target_dim_for_compression()
                 else:
                     input_shape = target_model_graph.get_input_shape_for_insertion_point(qp.insertion_point)
                     channel_idx = 1  # channel dim for activations
-                scale_shape = tuple(get_scale_shape(input_shape,
-                                                    qp.is_weight_quantization_point(),
-                                                    qp.qconfig.per_channel,
-                                                    channel_idx))
+                scale_shape = tuple(
+                    get_scale_shape(input_shape, qp.is_weight_quantization_point(), qp.qconfig.per_channel, channel_idx)
+                )
 
                 if scale_shape not in tensor_statistics[tp]:
                     nncf_logger.debug(f"Did not collect tensor statistics at {tp} for shape {scale_shape}")
                     retval[qp_id] = None
                 else:
                     minmax_stat = pt_convert_stat_to_min_max_tensor_stat(tensor_statistics[tp][scale_shape])
                     retval[qp_id] = minmax_stat
         return retval
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         # TODO (vshampor): a simpler solution would be to always create callables on CPU and
         # to move these to model-specific device upon actual application, but would this impact
         # the time required to create a compressed model?
         self._device_for_callable_obj_creation = get_model_device(target_model)
-        target_model.register_compression_module_type(ExtraCompressionModuleType.EXTERNAL_QUANTIZER)
+        target_model.nncf.register_compression_module_type(ExtraCompressionModuleType.EXTERNAL_QUANTIZER)
         if self._pt_quantizer_setup is None:
             self._pt_quantizer_setup = self._get_quantizer_setup(target_model)
 
         dup_filter = DuplicateFilter()  # so that the overflow fix warning is only logged once
         nncf_logger.addFilter(dup_filter)
-        insertion_commands, setup_to_module_id_translation_dict = \
-            self._build_insertion_commands_list_for_quantizer_setup(self._pt_quantizer_setup,
-                                                                    target_model,
-                                                                    self._minmax_values_for_range_init)
+        (
+            insertion_commands,
+            setup_to_module_id_translation_dict,
+        ) = self._build_insertion_commands_list_for_quantizer_setup(
+            self._pt_quantizer_setup, target_model, self._minmax_values_for_range_init
+        )
         nncf_logger.removeFilter(dup_filter)
 
         transformation_layout = PTTransformationLayout()
         for command in insertion_commands:
             transformation_layout.register(command)
 
         self._setup_to_module_id_translation_dict = setup_to_module_id_translation_dict
         all_quantizations = {}
         all_quantizations.update({k: v.quantizer_module_ref for k, v in self._weight_quantizers.items()})
         all_quantizations.update({k: v.quantizer_module_ref for k, v in self._non_weight_quantizers.items()})
-        self._groups_of_adjacent_quantizers.parse_from_quantizer_setup(all_quantizations,
-                                                                       self._pt_quantizer_setup,
-                                                                       setup_to_module_id_translation_dict)
+        self._groups_of_adjacent_quantizers.parse_from_quantizer_setup(
+            all_quantizations, self._pt_quantizer_setup, setup_to_module_id_translation_dict
+        )
 
         # NOTE: Order of activations must be the same to correctly broadcast parameters (e.g. scales) in distributed
         # mode (see call of `_dist_broadcast_coalesced` in torch/nn/parallel/distributed.py for more details)
         # pylint: disable=protected-access
-        target_model.sort_compression_modules(ExtraCompressionModuleType.EXTERNAL_QUANTIZER)
+        target_model.nncf.sort_compression_modules(ExtraCompressionModuleType.EXTERNAL_QUANTIZER)
 
         if self._debug_interface is not None:
-            target_model.debug_interface.add_interface(self._debug_interface)
+            target_model.nncf.debug_interface.add_interface(self._debug_interface)
 
         quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]
         all_quantizations = get_state_dict_names_with_modules(target_model, quantization_types)
-        target_model._load_listener = LoadStateListener(target_model, all_quantizations)
+        target_model.nncf._load_listener = LoadStateListener(target_model, all_quantizations)
 
         return transformation_layout
 
     @staticmethod
-    def get_statistics_for_quantizer_setup(target_model: NNCFNetwork,
-                                           quantizer_setup: QuantizerSetupBase,
-                                           range_init_params: PTRangeInitParams) \
-        -> Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]]:
+    def get_statistics_for_quantizer_setup(
+        target_model: NNCFNetwork, quantizer_setup: QuantizerSetupBase, range_init_params: PTRangeInitParams
+    ) -> Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]]:
         if range_init_params is None:
             return {}
-        observation_points_vs_collectors_dict = StatCollectorGenerator. \
-            generate_collectors_for_range_init_statistics_collection(target_model.get_original_graph(),
-                                                                     quantizer_setup,
-                                                                     range_init_params)
-
-        with target_model.temporary_clean_view() as intermediate_model:
-            stat_builder = TensorStatisticsCollectionBuilder(NNCFConfig(),
-                                                             observation_points_vs_collectors_dict)
+        observation_points_vs_collectors_dict = (
+            StatCollectorGenerator.generate_collectors_for_range_init_statistics_collection(
+                target_model.nncf.get_original_graph(), quantizer_setup, range_init_params
+            )
+        )
+
+        with target_model.nncf.temporary_clean_view() as intermediate_model:
+            stat_builder = TensorStatisticsCollectionBuilder(NNCFConfig(), observation_points_vs_collectors_dict)
             stat_builder.apply_to(intermediate_model)
             stat_ctrl = stat_builder.build_controller(intermediate_model)
             runner = SimpleDataLoaderRunner(intermediate_model, range_init_params.device)
-            runner.progressbar_description = 'Collecting tensor statistics'
-            runner.run(range_init_params.init_range_data_loader,
-                       range_init_params.get_max_num_init_steps())
+            runner.progressbar_description = "Collecting tensor statistics"
+            with training_mode_switcher(intermediate_model, is_training=False):
+                # Run statistics collection in eval mode, otherwise it may fail because graph was built in eval mode
+                runner.run(range_init_params.init_range_data_loader, range_init_params.get_max_num_init_steps())
 
         retval = {}
         for ip, rs_vs_collector in stat_ctrl.ip_vs_collector_dict.items():
             retval[ip] = {rs: collector.get_statistics() for rs, collector in rs_vs_collector.items()}
         return retval
 
-    def _get_statistics_for_final_range_init(self, target_model: NNCFNetwork,
-                                             quantizer_setup: QuantizerSetupBase,
-                                             range_init_params: PTRangeInitParams) \
-            -> Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]]:
-
+    def _get_statistics_for_final_range_init(
+        self, target_model: NNCFNetwork, quantizer_setup: QuantizerSetupBase, range_init_params: PTRangeInitParams
+    ) -> Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]]:
         return self.get_statistics_for_quantizer_setup(target_model, quantizer_setup, range_init_params)
 
     def _get_single_config_quantizer_setup(self, target_model) -> SingleConfigQuantizerSetup:
-        setup_generator = PropagationBasedQuantizerSetupGenerator(self._algo_config,
-                                                                  target_model,
-                                                                  self.hw_config,
-                                                                  self._precision_init_type,
-                                                                  self._precision_init_params,
-                                                                  self._range_init_params,
-                                                                  self._debug_interface)
+        setup_generator = PropagationBasedQuantizerSetupGenerator(
+            self._algo_config,
+            target_model,
+            self.hw_config,
+            self._target_device,
+            self._precision_init_type,
+            self._precision_init_params,
+            self._range_init_params,
+            self._debug_interface,
+        )
         single_config_quantizer_setup = setup_generator.generate_setup()
         self._build_time_metric_infos = setup_generator.get_build_time_metric_infos()
         return single_config_quantizer_setup
 
     # pylint: disable=too-many-branches
     def _get_quantizer_setup(self, target_model: NNCFNetwork) -> PTQuantizerSetup:
         if self._legacy_single_config_quantizer_setup_from_comp_state is None:
             single_config_quantizer_setup = self._get_single_config_quantizer_setup(target_model)
         else:
             single_config_quantizer_setup = self._legacy_single_config_quantizer_setup_from_comp_state
 
-        target_model_graph = target_model.get_original_graph()
+        target_model_graph = target_model.nncf.get_original_graph()
 
         if is_main_process() and self.should_init:
-            stats_for_range_init = self._get_statistics_for_final_range_init(target_model,
-                                                                             single_config_quantizer_setup,
-                                                                             self._range_init_params)
+            stats_for_range_init = self._get_statistics_for_final_range_init(
+                target_model, single_config_quantizer_setup, self._range_init_params
+            )
             self._minmax_values_for_range_init = self._get_minmax_values_for_quantizer_locations(
-                single_config_quantizer_setup,
-                stats_for_range_init,
-                target_model_graph)
+                single_config_quantizer_setup, stats_for_range_init, target_model_graph
+            )
 
-            self._check_and_log_missing_stats_for_setup(single_config_quantizer_setup,
-                                                        self._minmax_values_for_range_init)
+            self._check_and_log_missing_stats_for_setup(
+                single_config_quantizer_setup, self._minmax_values_for_range_init
+            )
 
         bitwidth_per_scope = BasePrecisionInitializer.get_bitwidth_per_scope(single_config_quantizer_setup)
         str_bw = [str(element) for element in bitwidth_per_scope]
-        nncf_logger.debug('\n'.join(['\n\"bitwidth_per_scope\": [', ',\n'.join(str_bw), ']']))
+        nncf_logger.debug("\n".join(['\n"bitwidth_per_scope": [', ",\n".join(str_bw), "]"]))
 
-        setup = PTQuantizerSetup(single_config_quantizer_setup.unified_scale_groups,
-                                single_config_quantizer_setup.shared_input_operation_set_groups)
+        setup = PTQuantizerSetup(
+            single_config_quantizer_setup.unified_scale_groups,
+            single_config_quantizer_setup.shared_input_operation_set_groups,
+        )
 
         for qp_id, qp in single_config_quantizer_setup.quantization_points.items():
             qconfig = qp.qconfig
             insertion_point = qp.insertion_point  # QuantizationInsertionPointBase
 
-            if qp.is_weight_quantization_point():
-                use_logarithm_scale = self._use_logarithm_scale_per_group[QuantizerGroup.WEIGHTS]
-                narrow_range = True
-            else:
-                use_logarithm_scale = self._use_logarithm_scale_per_group[QuantizerGroup.ACTIVATIONS]
-                narrow_range = False
-
             compression_lr_multiplier = self._get_compression_lr_multiplier()
 
             half_range = False
             if self.hw_config and qp.is_weight_quantization_point():
                 target_node = target_model_graph.get_node_by_name(insertion_point.target_node_name)
-                if self.hw_config.target_device in ['CPU', 'ANY'] and qconfig.num_bits == 8:
-                    if self._overflow_fix == 'enable':
+                if self.hw_config.target_device in ["CPU", "ANY"] and qconfig.num_bits == 8:
+                    if self._overflow_fix == "enable":
                         half_range = True
-                        quantizers_with_overflow_fix_str = 'all weight quantizers'
-                    elif self._overflow_fix == 'first_layer_only':
-                        if target_node in get_first_nodes_of_type(target_model_graph, ['conv2d']):
+                        quantizers_with_overflow_fix_str = "all weight quantizers"
+                    elif self._overflow_fix == "first_layer_only":
+                        if target_node in get_first_nodes_of_type(target_model_graph, ["conv2d", "conv3d"]):
                             half_range = True
-                            quantizers_with_overflow_fix_str = 'first convolution weight quantizers'
-                    elif self._overflow_fix != 'disable':
+                            quantizers_with_overflow_fix_str = "first convolution weight quantizers"
+                    elif self._overflow_fix != "disable":
                         raise RuntimeError(f"Unknown overflow fix type: {self._overflow_fix}")
                     if half_range:
-                        nncf_logger.debug(f'Overflow issue fix will be applied to {quantizers_with_overflow_fix_str}')
+                        nncf_logger.debug(f"Overflow issue fix will be applied to {quantizers_with_overflow_fix_str}")
+
+            if qp.is_weight_quantization_point():
+                use_logarithm_scale = self._use_logarithm_scale_per_group[QuantizerGroup.WEIGHTS]
+                narrow_range = qconfig.num_bits == 8 and not half_range
+            else:
+                use_logarithm_scale = self._use_logarithm_scale_per_group[QuantizerGroup.ACTIVATIONS]
+                narrow_range = False
 
             if qp.is_weight_quantization_point():
                 target_node = target_model_graph.get_node_by_name(insertion_point.target_node_name)
                 layer_attributes = target_node.layer_attributes
                 assert isinstance(layer_attributes, WeightedLayerAttributes)
-                scale_shape = get_scale_shape(layer_attributes.get_weight_shape(),
-                                              is_weights=True,
-                                              per_channel=qconfig.per_channel,
-                                              channel_idx=layer_attributes.get_target_dim_for_compression())
+                scale_shape = get_scale_shape(
+                    layer_attributes.get_weight_shape(),
+                    is_weights=True,
+                    per_channel=qconfig.per_channel,
+                    channel_idx=layer_attributes.get_target_dim_for_compression(),
+                )
             else:
                 input_shape = target_model_graph.get_input_shape_for_insertion_point(insertion_point)
-                scale_shape = get_scale_shape(list(input_shape),
-                                                    is_weights=False, per_channel=qconfig.per_channel)
+                scale_shape = get_scale_shape(list(input_shape), is_weights=False, per_channel=qconfig.per_channel)
 
-            qspec = PTQuantizerSpec.from_config(qconfig,
-                                                narrow_range=narrow_range,
-                                                scale_shape=tuple(scale_shape),
-                                                logarithm_scale=use_logarithm_scale,
-                                                half_range=half_range,
-                                                is_quantized_on_export=qp.is_weight_quantization_point(),
-                                                compression_lr_multiplier=compression_lr_multiplier)
-            pt_qp = PTQuantizationPoint(qspec, PTTargetPointTranslator.translate(insertion_point),
-                                        qp.directly_quantized_operator_node_names)
+            qspec = PTQuantizerSpec.from_config(
+                qconfig,
+                narrow_range=narrow_range,
+                scale_shape=tuple(scale_shape),
+                logarithm_scale=use_logarithm_scale,
+                half_range=half_range,
+                is_quantized_on_export=qp.is_weight_quantization_point(),
+                compression_lr_multiplier=compression_lr_multiplier,
+            )
+            pt_qp = PTQuantizationPoint(
+                qspec, PTTargetPointTranslator.translate(insertion_point), qp.directly_quantized_operator_node_names
+            )
             setup.add_quantization_point(qp_id, pt_qp)
 
         return setup
 
     def _build_controller(self, model: NNCFNetwork) -> PTCompressionAlgorithmController:
-        return QuantizationController(model,
-                                      self.config,
-                                      self._debug_interface,
-                                      self._weight_quantizers,
-                                      self._non_weight_quantizers,
-                                      self._groups_of_adjacent_quantizers,
-                                      self._quantizers_input_shapes,
-                                      build_time_metric_info=self._build_time_metric_infos,
-                                      build_time_range_init_params=self._range_init_params)
+        return QuantizationController(
+            model,
+            self.config,
+            self._debug_interface,
+            self._weight_quantizers,
+            self._non_weight_quantizers,
+            self._groups_of_adjacent_quantizers,
+            self._quantizers_input_shapes,
+            build_time_metric_info=self._build_time_metric_infos,
+            build_time_range_init_params=self._range_init_params,
+        )
 
     def __create_quantize_module(self, quantizer_spec: PTQuantizerSpec):
         quantizer_cls = QUANTIZATION_MODULES.get(quantizer_spec.mode)
         return quantizer_cls(quantizer_spec)
 
     @staticmethod
     def _get_adjust_padding_args(
-            target_model_graph: NNCFGraph,
-            quantization_point: PTQuantizationPoint,
-            activation_quantizer: BaseQuantizer,
-            quantization_points: List[PTQuantizationPoint]) -> List[AdjustPaddingArgs]:
+        target_model_graph: NNCFGraph,
+        quantization_point: PTQuantizationPoint,
+        activation_quantizer: BaseQuantizer,
+        quantization_points: List[PTQuantizationPoint],
+    ) -> List[AdjustPaddingArgs]:
         result = []
         for op_node_name in quantization_point.directly_quantized_operator_node_names:
             weight_bitwidth = None
             for qp in quantization_points:
                 is_weight = qp.is_weight_quantization_point()
                 if is_weight and (qp.target_point.target_node_name == op_node_name):
                     weight_bitwidth = qp.qspec.num_bits
@@ -841,163 +849,188 @@
         return result
 
     def _add_adjust_padding_ops(self, adjust_padding_args: List[AdjustPaddingArgs]):
         commands = []
         for args in adjust_padding_args:
             ap = CalculatePaddingAdjustment(args.activation_quantizer)
             op = UpdatePaddingValue(ap).to(self._device_for_callable_obj_creation)
-            insertion_point = PTTargetPoint(target_type=TargetType.PRE_LAYER_OPERATION,
-                                            target_node_name=args.module_op_node_name)
-            nncf_logger.debug(f'Padding will be adjusted for {args.module_op_node_name}')
+            insertion_point = PTTargetPoint(
+                target_type=TargetType.PRE_LAYER_OPERATION, target_node_name=args.module_op_node_name
+            )
+            nncf_logger.debug(f"Padding will be adjusted for {args.module_op_node_name}")
             commands.append(PTInsertionCommand(insertion_point, op, TransformationPriority.DEFAULT_PRIORITY))
         return commands
 
     class ExternalQuantizerCallHook:
         """
         Cannot simply register the quantizer module as a callable hook, since we need to call
         a thread-local version of the quantizer module during base module execution.
         """
 
-        def __init__(self, context: TracingContext, quantizer_storage_key: str,
-                     debug_interface: 'QuantizationDebugInterface' = None):
+        def __init__(
+            self,
+            context: TracingContext,
+            quantizer_storage_key: str,
+            debug_interface: "QuantizationDebugInterface" = None,
+        ):
             self.compressed_context = context
             self.quantizer_storage_key = quantizer_storage_key
             self.debug_interface = debug_interface
 
         def __call__(self, *args, **kwargs):
             if self.debug_interface is not None:
                 self.debug_interface.register_activation_quantize_call(str(self.quantizer_storage_key))
             replica = self.compressed_context.base_module_thread_local_replica
-            storage = getattr(replica, EXTERNAL_QUANTIZERS_STORAGE_NAME)
+            storage = getattr(replica.nncf, EXTERNAL_QUANTIZERS_STORAGE_NAME)
             return storage[self.quantizer_storage_key](*args, **kwargs)
 
     @staticmethod
-    def _check_and_log_missing_stats_for_setup(quantizer_setup: SingleConfigQuantizerSetup,
-                                               minmax_values_for_range_init: Dict[QuantizationPointId,
-                                                                                  MinMaxTensorStatistic]):
+    def _check_and_log_missing_stats_for_setup(
+        quantizer_setup: SingleConfigQuantizerSetup,
+        minmax_values_for_range_init: Dict[QuantizationPointId, MinMaxTensorStatistic],
+    ):
         tps_with_uncollected_stats = set()
         for qp_id in quantizer_setup.quantization_points.keys():
             if qp_id not in minmax_values_for_range_init:
                 tps_with_uncollected_stats.add(quantizer_setup.quantization_points[qp_id].insertion_point)
         if tps_with_uncollected_stats:
             nncf_logger.error("Tensor statistics for the following locations were not collected:")
             for tp in tps_with_uncollected_stats:
                 nncf_logger.error(f"\t{tp}")
-            nncf_logger.error("The corresponding quantizer range will not be initialized! If the model has "
-                              "data-dependent control flow branches, make sure that your initializing data loader is "
-                              "producing data that allows the model cover to all of these branches. If this is not the "
-                              "case, consider adding the corresponding nodes to `ignored_scopes`.")
-
-    def _build_insertion_commands_list_for_quantizer_setup(self,
-                                                           quantizer_setup: PTQuantizerSetup,
-                                                           target_model: NNCFNetwork,
-                                                           minmax_values_for_range_init: Dict[
-                                                               QuantizationPointId, MinMaxTensorStatistic]) -> \
-            Tuple[List[PTInsertionCommand], Dict[QuantizationPointId, QuantizerId]]:
+            nncf_logger.error(
+                "The corresponding quantizer range will not be initialized! If the model has "
+                "data-dependent control flow branches, make sure that your initializing data loader is "
+                "producing data that allows the model cover to all of these branches. If this is not the "
+                "case, consider adding the corresponding nodes to `ignored_scopes`."
+            )
+
+    def _build_insertion_commands_list_for_quantizer_setup(
+        self,
+        quantizer_setup: PTQuantizerSetup,
+        target_model: NNCFNetwork,
+        minmax_values_for_range_init: Dict[QuantizationPointId, MinMaxTensorStatistic],
+    ) -> Tuple[List[PTInsertionCommand], Dict[QuantizationPointId, QuantizerId]]:
         insertion_commands = []
         qp_id_vs_quant_module_id_dict = {}  # type: Dict[QuantizationPointId, QuantizerId]
-        target_model_graph = target_model.get_original_graph()
+        target_model_graph = target_model.nncf.get_original_graph()
         non_unified_scales_quantization_point_ids = set(quantizer_setup.quantization_points.keys())
         already_weight_quantized_shared_layers = {}  # type: Dict[str, QuantizerId]
 
         for unified_scales_group in quantizer_setup.unified_scale_groups.values():
             for us_qp_id in unified_scales_group:
                 non_unified_scales_quantization_point_ids.discard(us_qp_id)
 
-            filtered_unified_scales_group, shared_weight_quantized_layers_in_group = \
-                self._remove_shared_layer_weight_quantization_point_duplicates(unified_scales_group,
-                                                                               quantizer_setup,
-                                                                               target_model_graph)
+            (
+                filtered_unified_scales_group,
+                shared_weight_quantized_layers_in_group,
+            ) = self._remove_shared_layer_weight_quantization_point_duplicates(
+                unified_scales_group, quantizer_setup, target_model_graph
+            )
 
             quant_module_id, commands = self._build_commands_for_single_unified_scale_group(
-                target_model,
-                quantizer_setup,
-                filtered_unified_scales_group,
-                minmax_values_for_range_init)
+                target_model, quantizer_setup, filtered_unified_scales_group, minmax_values_for_range_init
+            )
 
             for layer_name in shared_weight_quantized_layers_in_group:
                 if layer_name in already_weight_quantized_shared_layers:
-                    raise RuntimeError("Attempted to assign a unified-scale quantizer to a shared layer node that has "
-                                       "already had its weights quantized by another unified-scale quantizer!")
+                    raise RuntimeError(
+                        "Attempted to assign a unified-scale quantizer to a shared layer node that has "
+                        "already had its weights quantized by another unified-scale quantizer!"
+                    )
                 already_weight_quantized_shared_layers[layer_name] = quant_module_id
 
             for us_qp_id in unified_scales_group:
                 qp_id_vs_quant_module_id_dict[us_qp_id] = quant_module_id
             insertion_commands += commands
 
         for qp_id in non_unified_scales_quantization_point_ids:
             qp = quantizer_setup.quantization_points[qp_id]
             nncf_node = target_model_graph.get_node_by_name(qp.target_point.target_node_name)
             if qp.is_weight_quantization_point() and nncf_node.is_shared():
                 layer_name = nncf_node.layer_name
                 if layer_name in already_weight_quantized_shared_layers:
                     nncf_logger.debug(
                         f"Filtering a regular weight quantization point {qp_id} - "
-                        f"already quantized as a shared layer {nncf_node.layer_name}")
+                        f"already quantized as a shared layer {nncf_node.layer_name}"
+                    )
                     qp_id_vs_quant_module_id_dict[qp_id] = already_weight_quantized_shared_layers[layer_name]
                     continue
 
             qspec = quantizer_setup.quantization_points[qp_id].qspec
             tp = quantizer_setup.quantization_points[qp_id].target_point
 
             range_init_minmax_values = None
             if minmax_values_for_range_init:
                 minmax_stat = minmax_values_for_range_init[qp_id] if qp_id in minmax_values_for_range_init else None
                 if minmax_stat is not None:
                     range_init_minmax_values = (minmax_stat.min_values, minmax_stat.max_values)
 
-            quantizer_module_id, commands = self._quantize_at_points_by_single_module(target_model,
-                                                                                      [tp, ],
-                                                                                      qspec,
-                                                                                      range_init_minmax_values)
+            quantizer_module_id, commands = self._quantize_at_points_by_single_module(
+                target_model,
+                [
+                    tp,
+                ],
+                qspec,
+                range_init_minmax_values,
+            )
 
-            if qp.is_weight_quantization_point() and nncf_node.is_shared() and \
-                    nncf_node.layer_name not in already_weight_quantized_shared_layers:
+            if (
+                qp.is_weight_quantization_point()
+                and nncf_node.is_shared()
+                and nncf_node.layer_name not in already_weight_quantized_shared_layers
+            ):
                 already_weight_quantized_shared_layers[nncf_node.layer_name] = quantizer_module_id
 
             qp_id_vs_quant_module_id_dict[qp_id] = quantizer_module_id
             insertion_commands += commands
 
-        adjust_padding_args = self._collect_adjust_padding_args(non_unified_scales_quantization_point_ids,
-                                                                qp_id_vs_quant_module_id_dict, quantizer_setup,
-                                                                target_model_graph)
+        adjust_padding_args = self._collect_adjust_padding_args(
+            non_unified_scales_quantization_point_ids,
+            qp_id_vs_quant_module_id_dict,
+            quantizer_setup,
+            target_model_graph,
+        )
 
         commands = self._add_adjust_padding_ops(adjust_padding_args)
         if commands:
             insertion_commands += commands
 
         return insertion_commands, qp_id_vs_quant_module_id_dict
 
     def _remove_shared_layer_weight_quantization_point_duplicates(
-            self,
-            unified_scales_group: Set[QuantizationPointId],
-            quantizer_setup: PTQuantizerSetup,
-            target_model_graph: NNCFGraph) -> Tuple[Set[QuantizationPointId], Set[str]]:
+        self,
+        unified_scales_group: Set[QuantizationPointId],
+        quantizer_setup: PTQuantizerSetup,
+        target_model_graph: NNCFGraph,
+    ) -> Tuple[Set[QuantizationPointId], Set[str]]:
         observed_shared_layer_names = set()
         retval = set()
         for us_qp_id in unified_scales_group:
             qp = quantizer_setup.quantization_points[us_qp_id]
             if qp.is_weight_quantization_point():
                 nncf_node = target_model_graph.get_node_by_name(qp.target_point.target_node_name)
                 if nncf_node.is_shared():
                     if nncf_node.layer_name not in observed_shared_layer_names:
                         observed_shared_layer_names.add(nncf_node.layer_name)
                     else:
-                        nncf_logger.debug(f"Filtering a unified-scale weight quantization point {us_qp_id} "
-                                          f"- already quantized as a shared layer {nncf_node.layer_name}")
+                        nncf_logger.debug(
+                            f"Filtering a unified-scale weight quantization point {us_qp_id} "
+                            f"- already quantized as a shared layer {nncf_node.layer_name}"
+                        )
                         continue
             retval.add(us_qp_id)
         return retval, observed_shared_layer_names
 
-
-    def _collect_adjust_padding_args(self,
-                                     non_unified_scales_quantization_point_ids: Set[QuantizationPointId],
-                                     qp_id_vs_quant_module_id_dict: Dict[QuantizationPointId, QuantizerId],
-                                     quantizer_setup: PTQuantizerSetup,
-                                     target_model_graph: NNCFGraph) -> List[AdjustPaddingArgs]:
+    def _collect_adjust_padding_args(
+        self,
+        non_unified_scales_quantization_point_ids: Set[QuantizationPointId],
+        qp_id_vs_quant_module_id_dict: Dict[QuantizationPointId, QuantizerId],
+        quantizer_setup: PTQuantizerSetup,
+        target_model_graph: NNCFGraph,
+    ) -> List[AdjustPaddingArgs]:
         def weight_qp_filter_fn(qp_id_):
             qp_ = quantizer_setup.quantization_points[qp_id_]
             return qp_.is_weight_quantization_point()
 
         weight_qps = list(filter(weight_qp_filter_fn, non_unified_scales_quantization_point_ids))
         adjust_padding_args = []
         adjust_padding_operation_set = set()
@@ -1018,51 +1051,61 @@
                     qp_ = quantizer_setup.quantization_points[qp_id_]
                     node_matched = target_node.node_name in qp_.directly_quantized_operator_node_names
                     return qp_.is_activation_quantization_point() and node_matched
 
                 for qp_id in filter(is_qp_quantizing_same_op_as_wqp, shared_input_group):
                     quantizer_module_id = qp_id_vs_quant_module_id_dict[qp_id]
                     activation_quantizer = self._non_weight_quantizers[quantizer_module_id].quantizer_module_ref
-                    args = self._get_adjust_padding_args(target_model_graph,
-                                                         wqp, activation_quantizer,
-                                                         list(quantizer_setup.quantization_points.values()))
+                    args = self._get_adjust_padding_args(
+                        target_model_graph,
+                        wqp,
+                        activation_quantizer,
+                        list(quantizer_setup.quantization_points.values()),
+                    )
                     if args:
                         adjust_padding_args.extend(args)
         return adjust_padding_args
 
-    def _build_commands_for_single_unified_scale_group(self,
-                                                       target_model: NNCFNetwork,
-                                                       quantizer_setup: PTQuantizerSetup,
-                                                       unified_scales_group: Set[QuantizationPointId],
-                                                       minmax_values_for_range_init: Dict[QuantizationPointId,
-                                                                                          MinMaxTensorStatistic]) -> \
-            Tuple[QuantizerId, List[PTInsertionCommand]]:
+    def _build_commands_for_single_unified_scale_group(
+        self,
+        target_model: NNCFNetwork,
+        quantizer_setup: PTQuantizerSetup,
+        unified_scales_group: Set[QuantizationPointId],
+        minmax_values_for_range_init: Dict[QuantizationPointId, MinMaxTensorStatistic],
+    ) -> Tuple[QuantizerId, List[PTInsertionCommand]]:
         qp_ids_list_for_current_group = list(unified_scales_group)
 
         # The primary insertion point (to be associated with the actual quantizer module, not just hooks to it)
         # will be determined based on the string representation of said insertion point, to avoid random selection.
         # Weight insertion points are given priority.
-        weight_qp_ids = [qp_id for qp_id in qp_ids_list_for_current_group
-            if quantizer_setup.quantization_points[qp_id].is_weight_quantization_point()]
-        act_qp_ids = [qp_id for qp_id in qp_ids_list_for_current_group
-            if quantizer_setup.quantization_points[qp_id].is_activation_quantization_point()]
+        weight_qp_ids = [
+            qp_id
+            for qp_id in qp_ids_list_for_current_group
+            if quantizer_setup.quantization_points[qp_id].is_weight_quantization_point()
+        ]
+        act_qp_ids = [
+            qp_id
+            for qp_id in qp_ids_list_for_current_group
+            if quantizer_setup.quantization_points[qp_id].is_activation_quantization_point()
+        ]
+
         def ip_str_repr_key_lambda(x):
             return str(quantizer_setup.quantization_points[x].target_point.target_node_name)
+
         sorted_wqp_ids = sorted(weight_qp_ids, key=ip_str_repr_key_lambda)
         sorted_aqp_ids = sorted(act_qp_ids, key=ip_str_repr_key_lambda)
         sorted_qp_ids = sorted_wqp_ids + sorted_aqp_ids
 
         primary_qp_id = sorted_qp_ids[0]
         linked_qp_ids = sorted_qp_ids[1:]
         qspec = quantizer_setup.quantization_points[primary_qp_id].qspec
         linked_qspecs = [quantizer_setup.quantization_points[qp_id].qspec for qp_id in linked_qp_ids]
         for linked_qspec in linked_qspecs:
             if not qspec == linked_qspec:
-                raise RuntimeError("The qspecs for unified scale quantization points should"
-                                   "be identical!")
+                raise RuntimeError("The qspecs for unified scale quantization points should be identical!")
 
         range_init_minmax_values = None
         if minmax_values_for_range_init:
             # Hopefully this will suffice.
             # TODO: gather unified statistic by linking stat collectors_and_modules_to_init instead
             min_values = None
             max_values = None
@@ -1080,30 +1123,31 @@
                     max_values = minmax_stat.max_values
                 else:
                     max_values = torch.max(max_values, minmax_stat.max_values)
             if min_values is not None and max_values is not None:
                 range_init_minmax_values = min_values, max_values
 
         target_points = [quantizer_setup.quantization_points[qp_id].target_point for qp_id in sorted_qp_ids]
-        quantizer_module_id, commands = self._quantize_at_points_by_single_module(target_model,
-                                                                                  target_points,
-                                                                                  qspec,
-                                                                                  range_init_minmax_values)
+        quantizer_module_id, commands = self._quantize_at_points_by_single_module(
+            target_model, target_points, qspec, range_init_minmax_values
+        )
         return quantizer_module_id, commands
 
     def _select_final_qconfig(self, quantizer_config_list: List[QuantizerConfig]) -> QuantizerConfig:
         # Quantizer config list entries should arrive in the same order as they are listed
         # in the HW config, where they are sorted by descending order of priority
         return quantizer_config_list[0]
 
-    def _quantize_at_points_by_single_module(self, target_model: NNCFNetwork,
-                                             insertion_points: List[PTTargetPoint],
-                                             qspec: PTQuantizerSpec,
-                                             range_init_minmax_values: Tuple[torch.Tensor, torch.Tensor] = None) -> \
-            Tuple[QuantizerId, List[PTInsertionCommand]]:
+    def _quantize_at_points_by_single_module(
+        self,
+        target_model: NNCFNetwork,
+        insertion_points: List[PTTargetPoint],
+        qspec: PTQuantizerSpec,
+        range_init_minmax_values: Tuple[torch.Tensor, torch.Tensor] = None,
+    ) -> Tuple[QuantizerId, List[PTInsertionCommand]]:
         """
         Will generate insertion commands for quantization at possibly multiple points
         in the network using one and the same trainable quantizer module. The trainable
         quantizer module will be saved either inside the weightable module which weights
         it quantizes (for single-point weight quantization), or in a NNCFNetwork wrapper
         module (i.e. in a storage external to the original module).
         :param: target_model - the model to be quantized.
@@ -1112,18 +1156,18 @@
         :param: qconfig - the QuantizerConfig for the resulting quantizer module
         :param: range_init_minmax_values - a pair of minimum and maximum values of input statistics
         for initializing the quantizer's trainable parameters
         :return: A tuple with the identifier of the new quantizer module and a list of
         insertion commands registering this module for quantization at spots described by
         insertion_points.
         """
-        #pylint:disable=too-many-branches
-        #pylint:disable=too-many-statements
+        # pylint:disable=too-many-branches
+        # pylint:disable=too-many-statements
 
-        target_model_graph = target_model.get_original_graph()
+        target_model_graph = target_model.nncf.get_original_graph()
         if not insertion_points:
             raise RuntimeError("No insertion points to put quantizers into!")
 
         def is_weights(ip: PTTargetPoint) -> bool:
             return ip.target_type is TargetType.OPERATION_WITH_WEIGHTS
 
         primary_ip = insertion_points[0]
@@ -1133,126 +1177,127 @@
             # Need to cast to the model's current dtype since the statistics could have been gathered in an
             # AMP autocast model (and therefore be FP16 since AMP autocast switches precision of activations
             # at forward pass time)
             own_type = get_model_dtype(target_model)
             min_values = range_init_minmax_values[0].type(own_type)
             max_values = range_init_minmax_values[1].type(own_type)
 
-            quantizer.apply_minmax_init(min_values=min_values,
-                                        max_values=max_values,
-                                        log_module_name=str(primary_ip))
+            quantizer.apply_minmax_init(min_values=min_values, max_values=max_values, log_module_name=str(primary_ip))
 
         qids = []  # type: List[QuantizerId]
         for ip in insertion_points:
             if is_weights(ip):
                 qids.append(WeightQuantizerId(ip.target_node_name))
             else:
                 qids.append(NonWeightQuantizerId(ip.target_node_name, ip.input_port_id))
 
         serialized_insertions_list = [str(x) for x in qids]
         external_quantizer_storage_key = ";".join(serialized_insertions_list)
         if len(insertion_points) > 1:
-            linked_quantizers_str = '\n'.join(serialized_insertions_list)
+            linked_quantizers_str = "\n".join(serialized_insertions_list)
             nncf_logger.info(f"Scales will be unified for quantizer group:\n{linked_quantizers_str}\n")
 
         if is_weights(primary_ip):
             primary_qid = WeightQuantizerId(primary_ip.target_node_name)
-            self._weight_quantizers[primary_qid] = WeightQuantizerInfo(quantizer,
-                                                                       target_model.get_containing_module(
-                                                                           primary_ip.target_node_name
-                                                                       ),
-                                                                       insertion_points)
+            self._weight_quantizers[primary_qid] = WeightQuantizerInfo(
+                quantizer, target_model.nncf.get_containing_module(primary_ip.target_node_name), insertion_points
+            )
             module_node = target_model_graph.get_node_by_name(primary_ip.target_node_name)
             layer_attributes = module_node.layer_attributes
             input_shape = layer_attributes.get_weight_shape()
             self._quantizers_input_shapes[primary_qid] = tuple(input_shape)
         else:
             primary_qid = NonWeightQuantizerId(primary_ip.target_node_name, primary_ip.input_port_id)
-            self._non_weight_quantizers[primary_qid] = \
-                NonWeightQuantizerInfo(quantizer, insertion_points)
+            self._non_weight_quantizers[primary_qid] = NonWeightQuantizerInfo(quantizer, insertion_points)
             input_shape = target_model_graph.get_input_shape_for_insertion_point(insertion_points[0])
             self._quantizers_input_shapes[primary_qid] = input_shape
 
         if not (is_weights(primary_ip) and len(insertion_points) == 1):
-            assert external_quantizer_storage_key not in target_model.get_compression_modules_by_type(
-                ExtraCompressionModuleType.EXTERNAL_QUANTIZER)
+            assert external_quantizer_storage_key not in target_model.nncf.get_compression_modules_by_type(
+                ExtraCompressionModuleType.EXTERNAL_QUANTIZER
+            )
 
-            target_model.add_compression_module(external_quantizer_storage_key, quantizer,
-                                                ExtraCompressionModuleType.EXTERNAL_QUANTIZER)
+            target_model.nncf.add_compression_module(
+                external_quantizer_storage_key, quantizer, ExtraCompressionModuleType.EXTERNAL_QUANTIZER
+            )
 
         insertion_commands = []
         for curr_insertion_point in insertion_points:
             if curr_insertion_point in self._processed_insertion_points:
-                raise RuntimeError(
-                    "Insertion point {} already quantized!".format(str(curr_insertion_point))
-                )
+                raise RuntimeError("Insertion point {} already quantized!".format(str(curr_insertion_point)))
             self._processed_insertion_points.add(curr_insertion_point)
 
             if is_weights(curr_insertion_point):
                 if len(insertion_points) == 1:
                     # For backward compatibility, if only one weight is quantized by a single quantizer,
                     # insert UpdateWeight ops with a genuine quantizer module
                     callable_obj = quantizer
                 else:
                     # Otherwise use external quantizer module storage since the quantization points will have to
                     # share the single module and this would be impossible for multiple weight quantizer sharing if
                     # the corresponding UpdateWeights operations contained real modules (these would simply get copied
                     # by PyTorch internals)
-                    callable_obj = self.ExternalQuantizerCallHook(target_model.get_tracing_context(),
-                                                                  external_quantizer_storage_key,
-                                                                  self._debug_interface)
+                    callable_obj = self.ExternalQuantizerCallHook(
+                        target_model.nncf.get_tracing_context(), external_quantizer_storage_key, self._debug_interface
+                    )
             else:
                 # Hooks will be identical for each affected op_address in the linked scenario
                 # - will call one and the same quantizer
-                callable_obj = self.ExternalQuantizerCallHook(target_model.get_tracing_context(),
-                                                              external_quantizer_storage_key,
-                                                              self._debug_interface)
-
-            nncf_logger.debug(f"Performing "
-                              f"{'signed' if quantizer.signed else 'unsigned'} "
-                              f"{'logarithm_scale' if quantizer.is_using_log_scale_storage else ''} "
-                              f"{'weight' if is_weights(curr_insertion_point) else 'activation'} "
-                              f"quantization for: {str(curr_insertion_point)}")
-
-            insertion_commands.append(PTInsertionCommand(curr_insertion_point,
-                                                         callable_obj,
-                                                         TransformationPriority.QUANTIZATION_PRIORITY))
+                callable_obj = self.ExternalQuantizerCallHook(
+                    target_model.nncf.get_tracing_context(), external_quantizer_storage_key, self._debug_interface
+                )
+
+            nncf_logger.debug(
+                f"Performing "
+                f"{'signed' if quantizer.signed else 'unsigned'} "
+                f"{'logarithm_scale' if quantizer.is_using_log_scale_storage else ''} "
+                f"{'weight' if is_weights(curr_insertion_point) else 'activation'} "
+                f"quantization for: {str(curr_insertion_point)}"
+            )
+
+            insertion_commands.append(
+                PTInsertionCommand(curr_insertion_point, callable_obj, TransformationPriority.QUANTIZATION_PRIORITY)
+            )
         return primary_qid, insertion_commands
 
     def _are_frozen_layers_allowed(self) -> Tuple[bool, str]:
-        message_template = Template('Frozen layers are$denial allowed for $algo_prefix quantization')
+        message_template = Template("Frozen layers are$denial allowed for $algo_prefix quantization")
         bits = set()
         bits.update({wq.quantizer_module_ref.num_bits for wq in self._weight_quantizers.values()})
         bits.update({nwq.quantizer_module_ref.num_bits for nwq in self._non_weight_quantizers.values()})
 
         if self._precision_init_params or len(bits) > 1:
-            return False, message_template.substitute(denial=' not', algo_prefix='mixed precision')
+            return False, message_template.substitute(denial=" not", algo_prefix="mixed precision")
 
         if len(bits) == 1:
             bitwidth = bits.pop()
-            algo_prefix = f'INT{bitwidth}'
+            algo_prefix = f"INT{bitwidth}"
             if bitwidth == 8:
-                return True, message_template.substitute(denial='', algo_prefix=algo_prefix)
-            return False, message_template.substitute(denial=' not', algo_prefix=algo_prefix)
-        return True, message_template.substitute(denial='', algo_prefix='empty')
+                return True, message_template.substitute(denial="", algo_prefix=algo_prefix)
+            return False, message_template.substitute(denial=" not", algo_prefix=algo_prefix)
+        return True, message_template.substitute(denial="", algo_prefix="empty")
 
     def _get_compression_lr_multiplier(self) -> Optional[float]:
-        return self.config.get_redefinable_global_param_value_for_algo('compression_lr_multiplier',
-                                                                       self.name)
+        return self.config.get_redefinable_global_param_value_for_algo("compression_lr_multiplier", self.name)
 
     def initialize(self, model: NNCFNetwork) -> None:
         if is_main_process() and self.should_init:
             bn_adapt_params = self._parse_bn_adapt_params()
             if bn_adapt_params is not None:
                 bn_adaptation = BatchnormAdaptationAlgorithm(
-                    **extract_bn_adaptation_init_params(self.config, 'quantization'))
+                    **extract_bn_adaptation_init_params(self.config, "quantization")
+                )
                 bn_adaptation.run(model)
 
 
 class QuantizationControllerBase(PTCompressionAlgorithmController):
+    """
+    Base controller class for the quantization controllers in PT.
+    """
+
     def enable_activation_quantization(self):
         raise NotImplementedError
 
     def enable_weight_quantization(self):
         raise NotImplementedError
 
     def disable_activation_quantization(self):
@@ -1261,24 +1306,32 @@
     def disable_weight_quantization(self):
         raise NotImplementedError
 
     def init_range(self):
         raise NotImplementedError
 
 
+@api()
 class QuantizationController(QuantizationControllerBase):
-    def __init__(self, target_model: NNCFNetwork,
-                 config: NNCFConfig,
-                 debug_interface: 'QuantizationDebugInterface',
-                 weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
-                 non_weight_quantizers: Dict[NonWeightQuantizerId, NonWeightQuantizerInfo],
-                 groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
-                 quantizers_input_shapes: Dict[QuantizerId, Tuple[int]],
-                 build_time_metric_info: QuantizationShareBuildTimeInfo = None,
-                 build_time_range_init_params: PTRangeInitParams = None):
+    """
+    Controller for the quantization algorithm in PT.
+    """
+
+    def __init__(
+        self,
+        target_model: NNCFNetwork,
+        config: NNCFConfig,
+        debug_interface: "QuantizationDebugInterface",
+        weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
+        non_weight_quantizers: Dict[NonWeightQuantizerId, NonWeightQuantizerInfo],
+        groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
+        quantizers_input_shapes: Dict[QuantizerId, Tuple[int]],
+        build_time_metric_info: QuantizationShareBuildTimeInfo = None,
+        build_time_range_init_params: PTRangeInitParams = None,
+    ):
         super().__init__(target_model)
         self._loss = ZeroCompressionLoss(get_model_device(target_model))
         self._scheduler = BaseCompressionScheduler()
         self.debug_interface = debug_interface
         self.config = config
         algo_config = self._get_algo_config()
         self._build_time_range_init_params = build_time_range_init_params
@@ -1289,26 +1342,28 @@
         self.all_quantizations.update({k: v.quantizer_module_ref for k, v in self.weight_quantizers.items()})
         self.all_quantizations.update({k: v.quantizer_module_ref for k, v in self.non_weight_quantizers.items()})
         self._quantizers_input_shapes = quantizers_input_shapes
         self._distributed = False
         self._groups_of_adjacent_quantizers = groups_of_adjacent_quantizers
         self._bn_adaptation = None
         self._build_time_metric_info = build_time_metric_info
+        self._target_device = self.config.get("target_device", "ANY")
 
-        should_export_to_onnx_qdq = algo_config.get('export_to_onnx_standard_ops',
-                                                    QUANTIZATION_EXPORT_TO_ONNX_STANDARD_OPS)
+        should_export_to_onnx_qdq = algo_config.get(
+            "export_to_onnx_standard_ops", QUANTIZATION_EXPORT_TO_ONNX_STANDARD_OPS
+        )
         if should_export_to_onnx_qdq:
             export_mode = QuantizerExportMode.ONNX_QUANTIZE_DEQUANTIZE_PAIRS
         else:
             export_mode = QuantizerExportMode.FAKE_QUANTIZE
 
         for quantizer in self.all_quantizations.values():  # type: BaseQuantizer
             quantizer.set_export_mode(export_mode)
 
-        params = algo_config.get('params', None)
+        params = algo_config.get("params", None)
         self.is_staged_scheduler = bool(params)
 
         # Staged scheduler must be created after initialized to prevent extra logic with disabled quantizations
         if self.is_staged_scheduler:
             scheduler_cls = QUANTIZATION_SCHEDULERS.get("staged")
             self._scheduler = scheduler_cls(self, params)
 
@@ -1323,70 +1378,81 @@
     @property
     def groups_of_adjacent_quantizers(self) -> GroupsOfAdjacentQuantizers:
         return self._groups_of_adjacent_quantizers
 
     def prepare_for_export(self):
         for quantizer_id, quantizer in self.all_quantizations.items():
             if not quantizer.is_enabled_quantization():
-                nncf_logger.debug(f'Disabled quantization on export to ONNX: {quantizer_id}')
+                nncf_logger.debug(f"Disabled quantization on export to ONNX: {quantizer_id}")
 
     def distributed(self):
         self._distributed = True
         self._broadcast_initialized_params_for_each_quantizer()
 
     def _get_algo_config(self) -> Dict:
-        return extract_algo_specific_config(self.config, 'quantization')
+        return extract_algo_specific_config(self.config, "quantization")
 
     def _broadcast_initialized_params_for_each_quantizer(self):
         # NOTE: Order of quantization modules must be the same on GPUs to correctly broadcast num_bits
         sorted_quantizers = OrderedDict(sorted(self.all_quantizations.items(), key=lambda x: str(x[0])))
         for quantizer in sorted_quantizers.values():  # type: BaseQuantizer
             quantizer.broadcast_initialized_params()
 
     def _do_runtime_range_init(self, range_init_params: PTRangeInitParams):
         modules_to_init = OrderedDict()
         for wq_id, wq_info in self.weight_quantizers.items():
             group = QuantizerGroup.WEIGHTS
             init_config = range_init_params.get_init_config_for_scope_and_group(wq_id, group)
             is_weights = True
-            modules_to_init[str(wq_id)] = (wq_info.quantizer_module_ref, init_config,
-                                           is_weights, self._quantizers_input_shapes[wq_id])
+            modules_to_init[str(wq_id)] = (
+                wq_info.quantizer_module_ref,
+                init_config,
+                is_weights,
+                self._quantizers_input_shapes[wq_id],
+            )
 
         for aq_id, aq_info in self.non_weight_quantizers.items():
             group = QuantizerGroup.ACTIVATIONS
             init_config = range_init_params.get_init_config_for_scope_and_group(aq_id, group)
             is_weights = False
-            modules_to_init[str(aq_id)] = (aq_info.quantizer_module_ref, init_config,
-                                           is_weights, self._quantizers_input_shapes[aq_id])
+            modules_to_init[str(aq_id)] = (
+                aq_info.quantizer_module_ref,
+                init_config,
+                is_weights,
+                self._quantizers_input_shapes[aq_id],
+            )
 
         # NOTE: Order of modules must be the same to correctly broadcast parameters (e.g. input_low
         # and input_range)
         modules_to_init = OrderedDict(sorted(modules_to_init.items()))
         self.modules_to_range_init = modules_to_init
         runner = DataLoaderRangeInitializeRunner(self._model, modules_to_init, range_init_params.device)
 
         quantizers = [module for module, config, is_weights, input_shape in modules_to_init.values()]
         quantizers_switcher = QuantizersSwitcher(quantizers)
         # bypass quantization to collect statistics from floating point model
         quantizers_switcher.disable_quantizers()
-        runner.run(range_init_params.init_range_data_loader,
-                   range_init_params.get_max_num_init_steps())
+        with training_mode_switcher(self._model, is_training=False):
+            # Statistics should be collected in eval mode because the model in train mode may behave differently
+            runner.run(range_init_params.init_range_data_loader, range_init_params.get_max_num_init_steps())
         quantizers_switcher.enable_quantizers()
 
-        self._model.rebuild_graph()
+        self._model.nncf.rebuild_graph()
 
     def compression_stage(self) -> CompressionStage:
         if self.is_staged_scheduler:
             return self.scheduler.compression_stage()
         return CompressionStage.FULLY_COMPRESSED
 
-    def init_precision(self,
-                       precision_init_type: str,
-                       precision_init_params: BasePrecisionInitParams,
-                       precision_constraints: HardwareQuantizationConstraints) -> SingleConfigQuantizerSetup:
+    def init_precision(
+        self,
+        precision_init_type: str,
+        precision_init_params: BasePrecisionInitParams,
+        precision_constraints: HardwareQuantizationConstraints,
+    ) -> SingleConfigQuantizerSetup:
         """
         Precision initialization happens based on an measure of layer sensitivity to perturbations. The measure is
         calculated by average Hessian trace estimation for each layer using Hutchinson algorithm.
         """
         init_impl = PrecisionInitializerFactory.create(precision_init_type)
         initializer = init_impl(self, precision_init_params, precision_constraints)
         nncf_logger.info("Initializing quantizer precisions...")
@@ -1397,18 +1463,20 @@
         Tracks input statistics for quantizers in the model and sets ranges of the quantizers to correspond to
         minimum and maximum input tensor levels observed.
         :param range_init_params: specifies parameters for this range initialization call; if None, the parameters
         that were used during compressed model creation will be used.
         """
         if range_init_params is None:
             if self._build_time_range_init_params is None:
-                nncf_logger.error("Requested a quantization controller to do range initialization without "
-                                  "`range_init_params` function parameter supplied, but the build time range "
-                                  "initialization was not supplied with params as well. "
-                                  "Range initialization will not be done.")
+                nncf_logger.error(
+                    "Requested a quantization controller to do range initialization without "
+                    "`range_init_params` function parameter supplied, but the build time range "
+                    "initialization was not supplied with params as well. "
+                    "Range initialization will not be done."
+                )
                 return
             range_init_params = self._build_time_range_init_params
 
         self._do_runtime_range_init(range_init_params)
 
         if self._distributed:
             self._broadcast_initialized_params_for_each_quantizer()
@@ -1427,81 +1495,94 @@
 
     def disable_weight_quantization(self):
         for m in self.weight_quantizers.values():
             m.quantizer_module_ref.disable_quantization()
 
     def statistics(self, quickly_collected_only=False) -> NNCFStatistics:
         if not quickly_collected_only and is_debug():
-            stats = MemoryConsumptionStatisticsCollector(self.model,
-                                                         self.weight_quantizers,
-                                                         self.non_weight_quantizers).collect()
+            stats = MemoryConsumptionStatisticsCollector(
+                self.model, self.weight_quantizers, self.non_weight_quantizers
+            ).collect()
             nncf_logger.debug(stats.to_str())
 
-            stats = ShareEdgesQuantizedDataPathStatisticsCollector(self.model, self).collect()
+            stats = ShareEdgesQuantizedDataPathStatisticsCollector(self.model, self, self._target_device).collect()
             nncf_logger.debug(stats.to_str())
 
-        collector = PTQuantizationStatisticsCollector(self.weight_quantizers,
-                                                      self.non_weight_quantizers,
-                                                      self._build_time_metric_info)
+        collector = PTQuantizationStatisticsCollector(
+            self.weight_quantizers, self.non_weight_quantizers, self._build_time_metric_info
+        )
         stats = collector.collect()
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('quantization', stats)
+        nncf_stats.register("quantization", stats)
         return nncf_stats
 
+    def strip_model(self, model: NNCFNetwork, do_copy: bool = False) -> NNCFNetwork:
+        if do_copy:
+            model = copy_model(model)
+        model = replace_quantizer_to_torch_native_module(model)
+        model = remove_disabled_quantizers(model)
+        return model
+
 
 class QuantizationDebugInterface(DebugInterface):
-    QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME = 'quantized_modules'
-    ACTIVATION_QUANTIZERS_TRACKER_NAME = 'activation_quantizers'
+    QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME = "quantized_modules"
+    ACTIVATION_QUANTIZERS_TRACKER_NAME = "activation_quantizers"
 
     def __init__(self):
         self.call_trackers = {
             self.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME: CallCountTracker(
-                QuantizationDebugInterface.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME),
+                QuantizationDebugInterface.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME
+            ),
             self.ACTIVATION_QUANTIZERS_TRACKER_NAME: CallCountTracker(
-                QuantizationDebugInterface.ACTIVATION_QUANTIZERS_TRACKER_NAME),
+                QuantizationDebugInterface.ACTIVATION_QUANTIZERS_TRACKER_NAME
+            ),
         }
         self.graph_size = 0
 
         from nncf.common.utils.debug import DEBUG_LOG_DIR  # pylint: disable=cyclic-import
+
         self.dump_dir = Path(DEBUG_LOG_DIR) / Path("debug_dumps")
         self.dump_dir.mkdir(parents=True, exist_ok=True)
         self.scale_dump_dir = self.dump_dir / Path("scale")
         if self.scale_dump_dir.exists():
             shutil.rmtree(str(self.scale_dump_dir))
         self.scale_dump_dir.mkdir(parents=True, exist_ok=True)
         self.forward_call_count = 0
         self._strict_forward = False
 
     def init_actual(self, owner_model: NNCFNetwork):
         quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]
-        quantizers_in_nncf_modules = owner_model.get_modules_in_nncf_modules_by_type(quantization_types)
-        nncf_module_quantizations_id_list = [str(scope) for scope in
-                                             quantizers_in_nncf_modules.keys()]  # type: List[str]
-
-        activation_quantizer_id_list = owner_model.get_compression_modules_by_type(
-            ExtraCompressionModuleType.EXTERNAL_QUANTIZER).keys()  # type: List[str]
+        quantizers_in_nncf_modules = owner_model.nncf.get_modules_in_nncf_modules_by_type(quantization_types)
+        nncf_module_quantizations_id_list = [
+            str(scope) for scope in quantizers_in_nncf_modules.keys()
+        ]  # type: List[str]
+
+        activation_quantizer_id_list = owner_model.nncf.get_compression_modules_by_type(
+            ExtraCompressionModuleType.EXTERNAL_QUANTIZER
+        ).keys()  # type: List[str]
         self.call_trackers[self.QUANTIZERS_IN_NNCF_MODULES_TRACKER_NAME].init_with_key_list(
-            nncf_module_quantizations_id_list)
-        self.call_trackers[self.ACTIVATION_QUANTIZERS_TRACKER_NAME].init_with_key_list(
-            activation_quantizer_id_list)
+            nncf_module_quantizations_id_list
+        )
+        self.call_trackers[self.ACTIVATION_QUANTIZERS_TRACKER_NAME].init_with_key_list(activation_quantizer_id_list)
         self._strict_forward = True
 
-    def pre_forward_actions(self, module: 'NNCFNetwork'):
+    def pre_forward_actions(self, module: "NNCFNetwork"):
         self.reset_counters()
 
-    def post_forward_actions(self, module: 'NNCFNetwork'):
+    def post_forward_actions(self, module: "NNCFNetwork"):
         self.register_forward_call()
         # pylint:disable=protected-access
-        ctx = module.get_tracing_context()
+        ctx = module.nncf.get_tracing_context()
         self.set_graph_size(ctx.graph.get_nodes_count())
 
         quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]
-        nncf_module_quantizations = module.get_modules_in_nncf_modules_by_type(
-            quantization_types)  # type: Dict['Scope', nn.Module]
+        nncf_module_quantizations = module.nncf.get_modules_in_nncf_modules_by_type(
+            quantization_types
+        )  # type: Dict['Scope', nn.Module]
 
         for qm_scope, qm_module in nncf_module_quantizations.items():
             # Important - this will not work for DataParallel since it copies the
             # entire parent module for each thread and the `call_count` attributes
             # are incremented for thread local copies of `qm_module`, which are not
             # the same as the primary copies of `qm_module` iterated over at this point
             self.register_quantizer_module_call(str(qm_scope), qm_module.call_count)
@@ -1517,15 +1598,16 @@
                 if tracker.get_never_called_keys():
                     # This will always trigger for DataParallel - disregard or disable debug mode
                     # for DataParallel runs
                     raise RuntimeError(f"{tracker.name} has never called modules: {tracker.get_never_called_keys()}!")
 
     def dump_scale(self, quantizer_scale_params: Dict[str, torch.Tensor], quantizer_name: str):
         import re
-        quantizer_normalized_name = re.sub(r'[^\w\-_\. ]', '_', quantizer_name)
+
+        quantizer_normalized_name = re.sub(r"[^\w\-_\. ]", "_", quantizer_name)
         for scale_param_name, scale_param in quantizer_scale_params.items():
             fname = "{}_{}.txt".format(quantizer_normalized_name, scale_param_name)
             with safe_open(self.scale_dump_dir / fname, "ab") as file:
                 np.savetxt(file, scale_param.cpu().numpy().flatten())
 
     def reset_counters(self):
         for tracker in self.call_trackers.values():
@@ -1550,97 +1632,103 @@
             overcalled = tracker.get_overcalled_keys_with_call_counts()
             if overcalled:
                 msg += f" {len(overcalled)} entries called more than once;"
             nncf_logger.debug(msg)
 
     def set_graph_size(self, new_size):
         if new_size != self.graph_size:
-            nncf_logger.debug('\n')
+            nncf_logger.debug("\n")
             nncf_logger.debug(
-                f"Warning - graph size has changed from {self.graph_size} to {new_size} since last forward")
+                f"Warning - graph size has changed from {self.graph_size} to {new_size} since last forward"
+            )
         self.graph_size = new_size
 
     def register_forward_call(self):
         self.forward_call_count += 1
 
     def visualize_insertion_point_graph(self, insertion_point_graph: InsertionPointGraph):
         out_graph = nx.MultiDiGraph()
         for node_key, node in insertion_point_graph.nodes.items():
-            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] in [InsertionPointGraphNodeType.PRE_HOOK,
-                                                                 InsertionPointGraphNodeType.POST_HOOK]:
-                target_point_data = node[
-                    InsertionPointGraph.INSERTION_POINT_NODE_ATTR]  # type: TargetPoint
+            if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] in [
+                InsertionPointGraphNodeType.PRE_HOOK,
+                InsertionPointGraphNodeType.POST_HOOK,
+            ]:
+                target_point_data = node[InsertionPointGraph.INSERTION_POINT_NODE_ATTR]  # type: TargetPoint
                 label = "TP: {}".format(str(target_point_data))
                 out_graph.add_node(node_key, label=label, color="red")
             elif node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:
                 out_graph.add_node(node_key)
             else:
                 raise RuntimeError("Invalid InsertionPointGraph node!")
         for u, v in insertion_point_graph.edges:
             out_graph.add_edge(u, v)
 
         for node_key, node in insertion_point_graph.nodes.items():
             if node[InsertionPointGraph.NODE_TYPE_NODE_ATTR] == InsertionPointGraphNodeType.OPERATOR:
                 for ip_node_key in node[InsertionPointGraph.ASSOCIATED_IP_NODE_KEYS_NODE_ATTR]:
-                    out_graph.add_edge(node_key, ip_node_key, style="dashed", headport='e', tailport='e')
+                    out_graph.add_edge(node_key, ip_node_key, style="dashed", headport="e", tailport="e")
 
         write_dot_graph(out_graph, self.dump_dir / Path("insertion_point_graph.dot"))
 
 
 class ExperimentalQuantizationBuilder(QuantizationBuilder):
-    def __init__(self, quantizer_setup: MultiConfigQuantizerSetup,
-                 initial_quantizer_setup: SingleConfigQuantizerSetup,
-                 tensor_stats_for_all_setup_variations: Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]],
-                 hw_config: HWConfig = None):
+    def __init__(
+        self,
+        quantizer_setup: MultiConfigQuantizerSetup,
+        initial_quantizer_setup: SingleConfigQuantizerSetup,
+        tensor_stats_for_all_setup_variations: Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]],
+        hw_config: HWConfig = None,
+    ):
         should_init = bool(tensor_stats_for_all_setup_variations)
         super().__init__(NNCFConfig(), should_init=should_init)
         self._initial_quantizer_setup = initial_quantizer_setup
         self._quantizer_setup = quantizer_setup
         self._tensor_stats = tensor_stats_for_all_setup_variations
         self._should_setup_adjust_pad_ops = False
         self.hw_config = hw_config
 
     def _handle_frozen_layers(self, target_model: NNCFNetwork):
         pass
 
     def _get_single_config_quantizer_setup(self, target_model) -> SingleConfigQuantizerSetup:
         return self._initial_quantizer_setup
 
-    def _get_statistics_for_final_range_init(self,
-                                             target_model: NNCFNetwork,
-                                             quantizer_setup: QuantizerSetupBase,
-                                             range_init_params: PTRangeInitParams) -> Dict[
-        PTTargetPoint, Dict[ReductionShape, TensorStatistic]]:
+    def _get_statistics_for_final_range_init(
+        self, target_model: NNCFNetwork, quantizer_setup: QuantizerSetupBase, range_init_params: PTRangeInitParams
+    ) -> Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]]:
         return self._tensor_stats
 
-    def _build_controller(self, model: NNCFNetwork) -> 'ExperimentalQuantizationController':
+    def _build_controller(self, model: NNCFNetwork) -> "ExperimentalQuantizationController":
         groups_of_adjacent_quantizers = GroupsOfAdjacentQuantizers()
         all_quantizations = {}  # type: Dict[QuantizerId, BaseQuantizer]
         all_quantizations.update({k: v.quantizer_module_ref for k, v in self._weight_quantizers.items()})
         all_quantizations.update({k: v.quantizer_module_ref for k, v in self._non_weight_quantizers.items()})
 
-        groups_of_adjacent_quantizers.parse_from_quantizer_setup(all_quantizations,
-                                                                 self._pt_quantizer_setup,
-                                                                 self._setup_to_module_id_translation_dict)
-
-        build_time_metric_infos = QuantizationShareBuildTimeInfo(len(self._non_weight_quantizers),
-                                                                 len(self._weight_quantizers))
-
-        return ExperimentalQuantizationController(model,
-                                                  self._weight_quantizers,
-                                                  self._non_weight_quantizers,
-                                                  groups_of_adjacent_quantizers,
-                                                  self._quantizers_input_shapes,
-                                                  self._quantizer_setup,
-                                                  self._initial_quantizer_setup,
-                                                  self._setup_to_module_id_translation_dict,
-                                                  self._tensor_stats,
-                                                  build_time_metric_infos,
-                                                  self._should_setup_adjust_pad_ops,
-                                                  self.hw_config)
+        groups_of_adjacent_quantizers.parse_from_quantizer_setup(
+            all_quantizations, self._pt_quantizer_setup, self._setup_to_module_id_translation_dict
+        )
+
+        build_time_metric_infos = QuantizationShareBuildTimeInfo(
+            len(self._non_weight_quantizers), len(self._weight_quantizers)
+        )
+
+        return ExperimentalQuantizationController(
+            model,
+            self._weight_quantizers,
+            self._non_weight_quantizers,
+            groups_of_adjacent_quantizers,
+            self._quantizers_input_shapes,
+            self._quantizer_setup,
+            self._initial_quantizer_setup,
+            self._setup_to_module_id_translation_dict,
+            self._tensor_stats,
+            build_time_metric_infos,
+            self._should_setup_adjust_pad_ops,
+            self.hw_config,
+        )
 
     def initialize(self, model: NNCFNetwork) -> None:
         pass
 
     def _get_algo_specific_config_section(self) -> Dict:
         return {}
 
@@ -1648,34 +1736,39 @@
         return None
 
     def _get_compression_lr_multiplier(self) -> Optional[float]:
         return None
 
 
 class ExperimentalQuantizationController(QuantizationController):
-    def __init__(self, target_model: NNCFNetwork,
-                 weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
-                 non_weight_quantizers: Dict[NonWeightQuantizerId, NonWeightQuantizerInfo],
-                 groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
-                 quantizers_input_shapes: Dict[QuantizerId, Tuple[int]],
-                 quantizer_setup: MultiConfigQuantizerSetup,
-                 initial_quantizer_setup: SingleConfigQuantizerSetup,
-                 setup_to_module_id_translation_dict: Dict[QuantizationPointId, QuantizerId],
-                 tensor_stats: Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]],
-                 build_time_metric_info: QuantizationShareBuildTimeInfo,
-                 should_setup_adjust_pad_ops=False,
-                 hw_config: HWConfig = None):
-        super().__init__(target_model,
-                         NNCFConfig(),
-                         debug_interface=None,
-                         weight_quantizers=weight_quantizers,
-                         non_weight_quantizers=non_weight_quantizers,
-                         groups_of_adjacent_quantizers=groups_of_adjacent_quantizers,
-                         quantizers_input_shapes=quantizers_input_shapes,
-                         build_time_metric_info=build_time_metric_info)
+    def __init__(
+        self,
+        target_model: NNCFNetwork,
+        weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
+        non_weight_quantizers: Dict[NonWeightQuantizerId, NonWeightQuantizerInfo],
+        groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
+        quantizers_input_shapes: Dict[QuantizerId, Tuple[int]],
+        quantizer_setup: MultiConfigQuantizerSetup,
+        initial_quantizer_setup: SingleConfigQuantizerSetup,
+        setup_to_module_id_translation_dict: Dict[QuantizationPointId, QuantizerId],
+        tensor_stats: Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]],
+        build_time_metric_info: QuantizationShareBuildTimeInfo,
+        should_setup_adjust_pad_ops=False,
+        hw_config: HWConfig = None,
+    ):
+        super().__init__(
+            target_model,
+            NNCFConfig(),
+            debug_interface=None,
+            weight_quantizers=weight_quantizers,
+            non_weight_quantizers=non_weight_quantizers,
+            groups_of_adjacent_quantizers=groups_of_adjacent_quantizers,
+            quantizers_input_shapes=quantizers_input_shapes,
+            build_time_metric_info=build_time_metric_info,
+        )
         self._target_model_ref = target_model
         self._should_setup_adjust_pad_ops = should_setup_adjust_pad_ops
         self._quantizer_setup = quantizer_setup
         self._initial_quantizer_setup = initial_quantizer_setup
         self._tensor_stats = tensor_stats
         self.setup_to_module_id_translation_dict = setup_to_module_id_translation_dict
         self.module_id_to_qp_id_translation_dict = {}  # type: Dict[QuantizerId, Set[QuantizationPointId]]
@@ -1708,38 +1801,46 @@
         if Counter(current_setup.quantization_points.keys()) != Counter(quantizer_setup.quantization_points.keys()):
             raise ValueError("The new setup is inconsistent with the original parameter space!")
         for qp_id, qp in quantizer_setup.quantization_points.items():
             current_qconfig = current_setup.quantization_points[qp_id].qconfig
             new_qconfig = quantizer_setup.quantization_points[qp_id].qconfig
             new_padding_adjust_applicable = CalculatePaddingAdjustment.is_config_applicable(new_qconfig)
             current_padding_adjust_applicable = CalculatePaddingAdjustment.is_config_applicable(current_qconfig)
-            need_padding_regeneration = \
-                self._should_setup_adjust_pad_ops and \
-                qp.is_activation_quantization_point() and \
-                new_padding_adjust_applicable != current_padding_adjust_applicable
-            if current_qconfig.per_channel != new_qconfig.per_channel or \
-                    (new_qconfig.signedness_to_force is not None and
-                     current_qconfig.signedness_to_force != new_qconfig.signedness_to_force) or \
-                    current_qconfig.mode != new_qconfig.mode or \
-                    need_padding_regeneration:
+            need_padding_regeneration = (
+                self._should_setup_adjust_pad_ops
+                and qp.is_activation_quantization_point()
+                and new_padding_adjust_applicable != current_padding_adjust_applicable
+            )
+            if (
+                current_qconfig.per_channel != new_qconfig.per_channel
+                or (
+                    new_qconfig.signedness_to_force is not None
+                    and current_qconfig.signedness_to_force != new_qconfig.signedness_to_force
+                )
+                or current_qconfig.mode != new_qconfig.mode
+                or need_padding_regeneration
+            ):
                 return True
         return False
 
-    def apply_new_quantizer_setup(self, quantizer_setup: SingleConfigQuantizerSetup) -> Tuple[
-            'ExperimentalQuantizationController', NNCFNetwork]:
+    def apply_new_quantizer_setup(
+        self, quantizer_setup: SingleConfigQuantizerSetup
+    ) -> Tuple["ExperimentalQuantizationController", NNCFNetwork]:
         if not self.is_new_setup_requires_regeneration(quantizer_setup):
             for qp_id, qp in quantizer_setup.quantization_points.items():
                 quant_module_id = self.setup_to_module_id_translation_dict[qp_id]
                 quant_module = self.all_quantizations[quant_module_id]
                 quant_module.num_bits = qp.qconfig.num_bits
             return self, self._target_model_ref
-        new_model = self._target_model_ref.get_clean_shallow_copy()
-        new_builder = ExperimentalQuantizationBuilder(self._quantizer_setup,
-                                                      initial_quantizer_setup=quantizer_setup,
-                                                      tensor_stats_for_all_setup_variations=self._tensor_stats,
-                                                      hw_config=self.hw_config)
+        new_model = self._target_model_ref.nncf.get_clean_shallow_copy()
+        new_builder = ExperimentalQuantizationBuilder(
+            self._quantizer_setup,
+            initial_quantizer_setup=quantizer_setup,
+            tensor_stats_for_all_setup_variations=self._tensor_stats,
+            hw_config=self.hw_config,
+        )
         new_builder.apply_to(new_model)
         new_ctrl = new_builder.build_controller(new_model)
         return new_ctrl, new_model
 
     def _get_algo_config(self) -> Dict:
         return {}
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/default_quantization.py` & `nncf-2.5.0/nncf/torch/quantization/default_quantization.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Dict, List
 
-from nncf.common.quantization.quantizer_propagation.structs import QuantizationTrait
 from nncf.common.graph.operator_metatypes import UnknownMetatype
+from nncf.common.quantization.quantizer_propagation.structs import QuantizationTrait
 from nncf.torch.graph import operator_metatypes
-from nncf.torch.graph.operator_metatypes import PTOperatorMetatype
 from nncf.torch.graph.operator_metatypes import OPERATORS_WITH_WEIGHTS_METATYPES
+from nncf.torch.graph.operator_metatypes import PTOperatorMetatype
 
 # If there are no some metatypes it means that they are considered as QuantizationTrait.QuantizationAgnostic
 
 DEFAULT_PT_QUANT_TRAIT_TO_OP_DICT = {
     QuantizationTrait.INPUTS_QUANTIZABLE: [
         operator_metatypes.PTConv2dMetatype,
         operator_metatypes.PTModuleConv2dMetatype,
@@ -51,28 +49,33 @@
         operator_metatypes.PTMatMulMetatype,
         operator_metatypes.PTMeanMetatype,
         operator_metatypes.PTRoundMetatype,
         operator_metatypes.PTPixelShuffleMetatype,
         operator_metatypes.PTBatchNormMetatype,
         operator_metatypes.PTModuleBatchNormMetatype,
         operator_metatypes.PTAvgPool2dMetatype,
-        operator_metatypes.PTAvgPool3dMetatype
+        operator_metatypes.PTAvgPool3dMetatype,
     ],
     QuantizationTrait.NON_QUANTIZABLE: [
         operator_metatypes.PTSigmoidMetatype,
-        operator_metatypes.PTExpMetatype,
         operator_metatypes.PTSoftmaxMetatype,
-        UnknownMetatype
-    ],
-    QuantizationTrait.CONCAT: [
-        operator_metatypes.PTCatMetatype
+        operator_metatypes.PTRELUMetatype,
+        operator_metatypes.PTDeformConv2dMetatype,
+        operator_metatypes.PTModuleDeformConv2dMetatype,
+        UnknownMetatype,
+        # Ticket: 108478
+        operator_metatypes.PTAbsMetatype,
+        operator_metatypes.PTExpMetatype,
+        operator_metatypes.PTLogMetatype,
+        operator_metatypes.PTSqrtMetatype,
     ],
+    QuantizationTrait.CONCAT: [operator_metatypes.PTCatMetatype],
     QuantizationTrait.OUTPUT_QUANTIZATION_AS_WEIGHTS: [
         operator_metatypes.PTEmbeddingMetatype,
         operator_metatypes.PTModuleEmbeddingMetatype,
         operator_metatypes.PTEmbeddingBagMetatype,
-        operator_metatypes.PTModuleEmbeddingBagMetatype
-    ]
+        operator_metatypes.PTModuleEmbeddingBagMetatype,
+    ],
 }  # type: Dict[QuantizationTrait, List[PTOperatorMetatype]]
 
 
-QUANTIZATION_LAYER_METATYPES = OPERATORS_WITH_WEIGHTS_METATYPES # type: List[PTOperatorMetatype]
+QUANTIZATION_LAYER_METATYPES = OPERATORS_WITH_WEIGHTS_METATYPES  # type: List[PTOperatorMetatype]
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/extensions.py` & `nncf-2.5.0/nncf/torch/quantization/extensions.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,100 +1,108 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import os.path
 import subprocess
 
 import torch
 
 from nncf import nncf_logger
-from nncf.torch.extensions import CudaNotAvailableStub, ExtensionsType, ExtensionLoader, EXTENSIONS
 from nncf.definitions import NNCF_PACKAGE_ROOT_DIR
+from nncf.torch.extensions import EXTENSIONS
+from nncf.torch.extensions import CudaNotAvailableStub
+from nncf.torch.extensions import ExtensionLoader
 from nncf.torch.extensions import ExtensionNamespace
+from nncf.torch.extensions import ExtensionsType
 from nncf.torch.quantization.reference import ReferenceQuantizedFunctions
 
 BASE_EXT_DIR = os.path.join(NNCF_PACKAGE_ROOT_DIR, "torch/extensions/src/quantization")
 
 EXT_INCLUDE_DIRS = [
     os.path.join(NNCF_PACKAGE_ROOT_DIR, "torch/extensions/include"),
 ]
 
 CPU_EXT_SRC_LIST = [
     os.path.join(BASE_EXT_DIR, "cpu/functions_cpu.cpp"),
-    os.path.join(NNCF_PACKAGE_ROOT_DIR, "torch/extensions/src/common/cpu/tensor_funcs.cpp")
+    os.path.join(NNCF_PACKAGE_ROOT_DIR, "torch/extensions/src/common/cpu/tensor_funcs.cpp"),
 ]
 
 CUDA_EXT_SRC_LIST = [
     os.path.join(BASE_EXT_DIR, "cuda/functions_cuda.cpp"),
-    os.path.join(BASE_EXT_DIR, "cuda/functions_cuda_impl.cu")
+    os.path.join(BASE_EXT_DIR, "cuda/functions_cuda_impl.cu"),
 ]
 
 
 @EXTENSIONS.register()
 class QuantizedFunctionsCPULoader(ExtensionLoader):
     @classmethod
     def extension_type(cls):
         return ExtensionsType.CPU
 
     @classmethod
     def name(cls) -> str:
-        return 'quantized_functions_cpu'
+        return "quantized_functions_cpu"
 
     @classmethod
     def load(cls):
         try:
-            retval = torch.utils.cpp_extension.load(cls.name(),
-                          CPU_EXT_SRC_LIST,
-                          extra_include_paths=EXT_INCLUDE_DIRS,
-                          build_directory=cls.get_build_dir(),
-                          verbose=False)
+            retval = torch.utils.cpp_extension.load(
+                cls.name(),
+                CPU_EXT_SRC_LIST,
+                extra_include_paths=EXT_INCLUDE_DIRS,
+                build_directory=cls.get_build_dir(),
+                verbose=False,
+            )
         except Exception as e:  # pylint:disable=broad-except
-            nncf_logger.warning(f"Could not compile CPU quantization extensions. "
-                                f"Falling back on torch native operations - "
-                                f"CPU quantization fine-tuning may be slower than expected.\n"
-                                f"Reason: {str(e)}")
+            nncf_logger.warning(
+                f"Could not compile CPU quantization extensions. "
+                f"Falling back on torch native operations - "
+                f"CPU quantization fine-tuning may be slower than expected.\n"
+                f"Reason: {str(e)}"
+            )
             retval = ReferenceQuantizedFunctions
         return retval
 
 
 @EXTENSIONS.register()
 class QuantizedFunctionsCUDALoader(ExtensionLoader):
     @classmethod
     def extension_type(cls):
         return ExtensionsType.CUDA
 
     @classmethod
     def load(cls):
         try:
-            return torch.utils.cpp_extension.load(cls.name(),
-                        CUDA_EXT_SRC_LIST,
-                        extra_include_paths=EXT_INCLUDE_DIRS,
-                        build_directory=cls.get_build_dir(),
-                        verbose=False)
+            return torch.utils.cpp_extension.load(
+                cls.name(),
+                CUDA_EXT_SRC_LIST,
+                extra_include_paths=EXT_INCLUDE_DIRS,
+                build_directory=cls.get_build_dir(),
+                verbose=False,
+            )
         except (subprocess.CalledProcessError, OSError, RuntimeError) as e:
             assert torch.cuda.is_available()
-            raise RuntimeError("CUDA is available for PyTorch, but NNCF could not compile "
-                               "GPU quantization extensions. Make sure that you have installed CUDA development "
-                               "tools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for "
-                               "guidance) and that 'nvcc' is available on your system's PATH variable.\n") from e
-
+            raise RuntimeError(
+                "CUDA is available for PyTorch, but NNCF could not compile "
+                "GPU quantization extensions. Make sure that you have installed CUDA development "
+                "tools (see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html for "
+                "guidance) and that 'nvcc' is available on your system's PATH variable.\n"
+            ) from e
 
     @classmethod
     def name(cls) -> str:
-        return 'quantized_functions_cuda'
+        return "quantized_functions_cuda"
 
 
 QuantizedFunctionsCPU = ExtensionNamespace(QuantizedFunctionsCPULoader())
 
 if torch.cuda.is_available():
     QuantizedFunctionsCUDA = ExtensionNamespace(QuantizedFunctionsCUDALoader())
 else:
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/hessian_trace.py` & `nncf-2.5.0/nncf/torch/quantization/hessian_trace.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,68 +1,71 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from functools import partial
-from typing import List, Union, Any, Callable
+from typing import Any, Callable, List, Union
 
 import torch
-
-from nncf.torch.utils import get_model_device
-from nncf.torch.utils import is_tensor
-from nncf.torch.nested_objects_traversal import objwalk
 from torch import Tensor
 from torch import nn
 from torch.nn import Parameter
 from torch.nn.modules.loss import _Loss
 from torch.utils.data import DataLoader
 
-from nncf.torch.initialization import wrap_dataloader_for_init, PTInitializingDataLoader
 from nncf.common.logging import nncf_logger
+from nncf.torch.initialization import PTInitializingDataLoader
+from nncf.torch.initialization import wrap_dataloader_for_init
+from nncf.torch.nested_objects_traversal import objwalk
+from nncf.torch.utils import get_model_device
+from nncf.torch.utils import is_tensor
 
 
 class ParameterHandler:
     def __init__(self, parameters: List[Parameter], device: str):
         self._device = device
         self._parameters = parameters
 
     @property
     def parameters(self) -> List[Parameter]:
         return self._parameters
 
     def get_gradients(self) -> List[Union[Tensor, float]]:
         gradients = []
         for parameter in self.parameters:
-            gradients.append(0. if parameter.grad is None else parameter.grad + 0.)
+            gradients.append(0.0 if parameter.grad is None else parameter.grad + 0.0)
         return gradients
 
     def sample_rademacher_like_params(self) -> List[Tensor]:
         def sample(parameter):
             r = torch.randint_like(parameter, high=2, device=self._device)
             return r.masked_fill_(r == 0, -1)
 
         return [sample(p) for p in self.parameters]
 
     def sample_normal_like_params(self) -> List[Tensor]:
         return [torch.randn(p.size(), device=self._device) for p in self.parameters]
 
 
 class GradientsCalculator:
-
-    def __init__(self, model: nn.Module, criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor], criterion: _Loss,
-                 data_loader: PTInitializingDataLoader, num_data_iter: int,
-                 paramerter_handler: ParameterHandler):
+    def __init__(
+        self,
+        model: nn.Module,
+        criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor],
+        criterion: _Loss,
+        data_loader: PTInitializingDataLoader,
+        num_data_iter: int,
+        paramerter_handler: ParameterHandler,
+    ):
         self._model = model
         self._criterion_fn = criterion_fn
         self._criterion = criterion
         self._data_loader = data_loader
         self._num_data_iter = num_data_iter
         self._parameter_handler = paramerter_handler
         self.num_iter = 0
@@ -96,65 +99,69 @@
 
 
 class HessianTraceEstimator:
     """
     Performs estimation of Hessian Trace based on Hutchinson algorithm.
     """
 
-    def __init__(self, model: nn.Module, criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor], criterion: _Loss,
-                 device: str, data_loader: DataLoader,
-                 num_data_points: int):
+    def __init__(
+        self,
+        model: nn.Module,
+        criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor],
+        criterion: _Loss,
+        device: str,
+        data_loader: DataLoader,
+        num_data_points: int,
+    ):
         self._model = model
         parameters = [p for p in model.parameters() if p.requires_grad]
         self._parameter_handler = ParameterHandler(parameters, device)
         self._batch_size = data_loader.batch_size
         data_loader = wrap_dataloader_for_init(data_loader)
         self._num_data_iter = num_data_points // self._batch_size if num_data_points >= self._batch_size else 1
-        self._gradients_calculator = GradientsCalculator(self._model, criterion_fn, criterion, data_loader,
-                                                         self._num_data_iter,
-                                                         self._parameter_handler)
+        self._gradients_calculator = GradientsCalculator(
+            self._model, criterion_fn, criterion, data_loader, self._num_data_iter, self._parameter_handler
+        )
         self._diff_eps = 1e-6
 
     def get_average_traces(self, max_iter=500, tolerance=1e-5) -> Tensor:
         """
         Estimates average hessian trace for each parameter
         :param max_iter: maximum number of iterations for Hutchinson algorithm
         :param tolerance: - minimum relative tolerance for stopping the algorithm.
         It's calculated  between mean average trace from previous iteration and current one.
         :return: Tensor with average hessian trace per parameter
         """
-        avg_total_trace = 0.
+        avg_total_trace = 0.0
         avg_traces_per_iter = []  # type: List[Tensor]
         mean_avg_traces_per_param = None
 
         for i in range(max_iter):
             avg_traces_per_iter.append(self._calc_avg_traces_per_param())
 
             mean_avg_traces_per_param = self._get_mean(avg_traces_per_iter)
             mean_avg_total_trace = torch.sum(mean_avg_traces_per_param)
 
             diff_avg = abs(mean_avg_total_trace - avg_total_trace) / (avg_total_trace + self._diff_eps)
             if diff_avg < tolerance:
                 return mean_avg_traces_per_param
             avg_total_trace = mean_avg_total_trace
-            nncf_logger.debug(f'{i}# difference_avg={diff_avg} avg_trace={avg_total_trace}')
+            nncf_logger.debug(f"{i}# difference_avg={diff_avg} avg_trace={avg_total_trace}")
 
         return mean_avg_traces_per_param
 
     def _calc_avg_traces_per_param(self) -> Tensor:
         v = self._parameter_handler.sample_rademacher_like_params()
         vhp = self._parameter_handler.sample_normal_like_params()
         num_all_data = self._num_data_iter * self._batch_size
         for gradients in self._gradients_calculator:
-            vhp_curr = torch.autograd.grad(gradients,
-                                           self._parameter_handler.parameters,
-                                           grad_outputs=v,
-                                           only_inputs=True,
-                                           retain_graph=False)
-            vhp = [a + b * float(self._batch_size) + 0. for a, b in zip(vhp, vhp_curr)]
+            vhp_curr = torch.autograd.grad(
+                gradients, self._parameter_handler.parameters, grad_outputs=v, only_inputs=True, retain_graph=False
+            )
+            vhp = [a + b * float(self._batch_size) + 0.0 for a, b in zip(vhp, vhp_curr)]
         vhp = [a / float(num_all_data) for a in vhp]
         avg_traces_per_param = torch.stack([torch.sum(a * b) / a.size().numel() for (a, b) in zip(vhp, v)])
         return avg_traces_per_param
 
     @staticmethod
     def _get_mean(data: List[Tensor]) -> Tensor:
         return torch.mean(torch.stack(data), dim=0)
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/init_precision.py` & `nncf-2.5.0/nncf/torch/quantization/init_precision.py`

 * *Files identical despite different names*

### Comparing `nncf-2.4.0/nncf/torch/quantization/init_range.py` & `nncf-2.5.0/nncf/torch/quantization/init_range.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,104 +1,103 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from collections import OrderedDict
 from copy import deepcopy
-from typing import Callable
-from typing import Dict
-from typing import List
-from typing import Tuple
+from typing import Callable, Dict, List, Tuple
 
 import numpy as np
 
 from nncf.common.graph.layer_attributes import WeightedLayerAttributes
+from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.initialization.range import RangeInitConfig
 from nncf.common.quantization.initialization.range import RangeInitParams
-from nncf.common.quantization.initialization.range import RangeInitCollectorParams
 from nncf.common.quantization.quantizer_setup import QuantizationPointBase
 from nncf.common.quantization.quantizer_setup import QuantizerSetupBase
-from nncf.config.schemata.algo.quantization import RANGE_INIT_TYPES_VS_DESCRIPTIONS
-from nncf.torch.quantization.layers import get_scale_shape
 from nncf.common.quantization.structs import NonWeightQuantizerId
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.common.quantization.structs import QuantizerGroup
 from nncf.common.quantization.structs import QuantizerId
 from nncf.common.quantization.structs import WeightQuantizerId
 from nncf.common.scopes import should_consider_scope
-from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.common.tensor_statistics.collectors import ReductionShape
+from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
+from nncf.config.schemata.algo.quantization import RANGE_INIT_TYPES_VS_DESCRIPTIONS
 from nncf.torch.graph.graph import PTNNCFGraph
 from nncf.torch.initialization import DataLoaderBaseRunner
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.layers import BaseQuantizer
-from nncf.torch.quantization.translator import PTTargetPointTranslator
 from nncf.torch.quantization.layers import SymmetricQuantizer
+from nncf.torch.quantization.layers import get_scale_shape
+from nncf.torch.quantization.translator import PTTargetPointTranslator
 from nncf.torch.tensor_statistics.algo import TensorStatisticObservationPoint
-from nncf.torch.tensor_statistics.collectors import PTMinMaxStatisticCollector
-from nncf.torch.tensor_statistics.collectors import PTMedianMADStatisticCollector
-from nncf.torch.tensor_statistics.collectors import PTPercentileStatisticCollector
+from nncf.torch.tensor_statistics.collectors import PTMeanMinMaxStatisticCollector
 from nncf.torch.tensor_statistics.collectors import PTMeanPercentileStatisticCollector
+from nncf.torch.tensor_statistics.collectors import PTMedianMADStatisticCollector
+from nncf.torch.tensor_statistics.collectors import PTMinMaxStatisticCollector
 from nncf.torch.tensor_statistics.collectors import PTMixedMinMaxStatisticCollector
-from nncf.torch.tensor_statistics.collectors import PTMeanMinMaxStatisticCollector
+from nncf.torch.tensor_statistics.collectors import PTPercentileStatisticCollector
 from nncf.torch.tensor_statistics.statistics import pt_convert_stat_to_min_max_tensor_stat
 
 
 class PTRangeInitParams(RangeInitParams):
-
     def get_max_num_init_steps(self) -> int:
         steps = []
         if self.global_init_config is not None:
             steps.append(self.global_init_config.num_init_samples)
         for pl_config in self.per_layer_range_init_configs:
             steps.append(pl_config.num_init_samples)
         batch_size = self.init_range_data_loader.batch_size
         return int(np.ceil(max(steps) / batch_size))
 
     def get_init_config_for_quantization_point(self, qp: QuantizationPointBase) -> RangeInitConfig:
         if qp.is_weight_quantization_point():
             qid = WeightQuantizerId(qp.insertion_point.target_node_name)
             group = QuantizerGroup.WEIGHTS
         else:
-            qid = NonWeightQuantizerId(qp.insertion_point.target_node_name,
-                                       qp.insertion_point.input_port_id)
+            qid = NonWeightQuantizerId(qp.insertion_point.target_node_name, qp.insertion_point.input_port_id)
             group = QuantizerGroup.ACTIVATIONS
         return self.get_init_config_for_scope_and_group(qid, group)
 
     def get_init_config_for_scope_and_group(self, qid: QuantizerId, group: QuantizerGroup) -> RangeInitConfig:
         matches = []  # type: List[RangeInitConfig]
         for pl_config in self.per_layer_range_init_configs:
             if should_consider_scope(qid, pl_config.ignored_scopes, pl_config.target_scopes):
                 if group == pl_config.target_group or pl_config.target_group is None:
-                    matches.append(RangeInitConfig(pl_config.init_type, pl_config.num_init_samples,
-                                                   pl_config.init_type_specific_params))
+                    matches.append(
+                        RangeInitConfig(
+                            pl_config.init_type, pl_config.num_init_samples, pl_config.init_type_specific_params
+                        )
+                    )
         if len(matches) > 1:
-            raise ValueError("Location {} matches more than one per-layer initialization parameter "
-                             "definition!".format(str(qid)))
+            raise ValueError(
+                "Location {} matches more than one per-layer initialization parameter definition!".format(str(qid))
+            )
         if len(matches) == 1:
             return matches[0]
         if not matches and self.global_init_config is not None:
             return deepcopy(self.global_init_config)
 
-        raise ValueError("Location {} does not match any per-layer initialization parameter "
-                         "definition!".format(str(qid)))
+        raise ValueError(
+            "Location {} does not match any per-layer initialization parameter definition!".format(str(qid))
+        )
 
 
 class PTRangeInitCollectorParams(RangeInitCollectorParams):
-    def __init__(self, is_weights: bool, mode: QuantizationMode, per_channel: bool, input_shape: tuple,
-                 channel_idx: int):
+    def __init__(
+        self, is_weights: bool, mode: QuantizationMode, per_channel: bool, input_shape: tuple, channel_idx: int
+    ):
         """
 
         :param input_shape: Shape of the input tensor.
         :param channel_idx: Channel dimension.
         """
         super().__init__(is_weights, mode, per_channel)
         self._input_shape = input_shape
@@ -118,141 +117,146 @@
             reduction_shape.remove(val)
         if self.use_per_sample_stats(per_sample_stats):
             reduction_shape = reduction_shape[1:]  # Assumes batch is the first dimension
         return tuple(reduction_shape)
 
 
 class StatCollectorGenerator:
-
     @staticmethod
-    def generate_collectors_for_range_init_statistics_collection(target_model_graph: PTNNCFGraph,
-                                                                 quantizer_setup: QuantizerSetupBase,
-                                                                 range_init_params: PTRangeInitParams) -> \
-            Dict[TensorStatisticObservationPoint, Dict[ReductionShape, TensorStatisticCollectorBase]]:
+    def generate_collectors_for_range_init_statistics_collection(
+        target_model_graph: PTNNCFGraph, quantizer_setup: QuantizerSetupBase, range_init_params: PTRangeInitParams
+    ) -> Dict[TensorStatisticObservationPoint, Dict[ReductionShape, TensorStatisticCollectorBase]]:
         retval = {}
         for qp in quantizer_setup.quantization_points.values():
             init_config = range_init_params.get_init_config_for_quantization_point(qp)
             is_weights = qp.is_weight_quantization_point()
-            num_batches = int(np.ceil(
-                init_config.num_init_samples / range_init_params.init_range_data_loader.batch_size))
+            num_batches = int(
+                np.ceil(init_config.num_init_samples / range_init_params.init_range_data_loader.batch_size)
+            )
             if is_weights:
                 # No need to store extra statistics in memory since weights won't change during range init
                 num_batches = 1
 
             tp = PTTargetPointTranslator.translate(qp.insertion_point)
             scale_shapes_vs_params = StatCollectorGenerator.get_all_scale_shapes_with_params(qp, target_model_graph)
 
-            obs_p = TensorStatisticObservationPoint(
-                tp,
-                reduction_shapes=set(scale_shapes_vs_params.keys()))
+            obs_p = TensorStatisticObservationPoint(tp, reduction_shapes=set(scale_shapes_vs_params.keys()))
 
             retval[obs_p] = {}
             for scale_shape in obs_p.reduction_shapes:
                 collector_params = scale_shapes_vs_params[scale_shape]
                 collector = StatCollectorGenerator.generate_stat_collector_for_range_init_config(
-                    init_config,
-                    scale_shape,
-                    collector_params,
-                    num_samples_to_collect_override=num_batches)
+                    init_config, scale_shape, collector_params, num_samples_to_collect_override=num_batches
+                )
                 retval[obs_p][scale_shape] = collector
 
         return retval
 
     @staticmethod
     def generate_stat_collector_for_range_init_config(
-            init_config: RangeInitConfig,
-            reduction_shape: ReductionShape = None,
-            collector_params = None,
-            num_samples_to_collect_override: int = None) -> TensorStatisticCollectorBase:
+        init_config: RangeInitConfig,
+        reduction_shape: ReductionShape = None,
+        collector_params=None,
+        num_samples_to_collect_override: int = None,
+    ) -> TensorStatisticCollectorBase:
         num_samples = init_config.num_init_samples
         if num_samples_to_collect_override is not None:
             num_samples = num_samples_to_collect_override
         if init_config.init_type not in RANGE_INIT_TYPES_VS_DESCRIPTIONS:
             raise RuntimeError("Unknown range init type: {}".format(init_config.init_type))
         if init_config.init_type == "min_max":
             reduction_shape_converted = collector_params.convert_reduction_shape(per_sample_stats=False)
-            return PTMinMaxStatisticCollector(collector_params.use_abs_max, reduction_shape_converted,
-                                              reduction_shape, num_samples)
+            return PTMinMaxStatisticCollector(
+                collector_params.use_abs_max, reduction_shape_converted, reduction_shape, num_samples
+            )
         if init_config.init_type == "mixed_min_max":
             reduction_shape_converted = collector_params.convert_reduction_shape(per_sample_stats=True)
-            return PTMixedMinMaxStatisticCollector(collector_params.use_per_sample_stats(per_sample_stats=True),
-                                                   collector_params.use_abs_max,
-                                                   collector_params.use_means_of_mins,
-                                                   collector_params.use_means_of_maxs,
-                                                   reduction_shape_converted,
-                                                   reduction_shape, num_samples)
+            return PTMixedMinMaxStatisticCollector(
+                collector_params.use_per_sample_stats(per_sample_stats=True),
+                collector_params.use_abs_max,
+                collector_params.use_means_of_mins,
+                collector_params.use_means_of_maxs,
+                reduction_shape_converted,
+                reduction_shape,
+                num_samples,
+            )
         if init_config.init_type == "mean_min_max":
             reduction_shape_converted = collector_params.convert_reduction_shape(per_sample_stats=False)
-            return PTMeanMinMaxStatisticCollector(collector_params.use_per_sample_stats(per_sample_stats=False),
-                                                  collector_params.use_abs_max, reduction_shape_converted,
-                                                  reduction_shape, num_samples)
+            return PTMeanMinMaxStatisticCollector(
+                collector_params.use_per_sample_stats(per_sample_stats=False),
+                collector_params.use_abs_max,
+                reduction_shape_converted,
+                reduction_shape,
+                num_samples,
+            )
         if init_config.init_type == "threesigma":
             return PTMedianMADStatisticCollector(reduction_shape, num_samples)
         if init_config.init_type == "percentile":
             min_percentile = init_config.init_type_specific_params.get("min_percentile", 0.1)
             max_percentile = init_config.init_type_specific_params.get("max_percentile", 99.9)
             return PTPercentileStatisticCollector([min_percentile, max_percentile], reduction_shape, num_samples)
         if init_config.init_type == "mean_percentile":
             min_percentile = init_config.init_type_specific_params.get("min_percentile", 0.1)
             max_percentile = init_config.init_type_specific_params.get("max_percentile", 99.9)
             return PTMeanPercentileStatisticCollector([min_percentile, max_percentile], reduction_shape, num_samples)
         raise ValueError("Range init type not handled!")
 
     @classmethod
-    def get_all_scale_shapes_with_params(cls, qp: QuantizationPointBase,
-                                         target_nncf_graph: PTNNCFGraph) -> Dict[ReductionShape,
-                                                                                 PTRangeInitCollectorParams]:
+    def get_all_scale_shapes_with_params(
+        cls, qp: QuantizationPointBase, target_nncf_graph: PTNNCFGraph
+    ) -> Dict[ReductionShape, PTRangeInitCollectorParams]:
         qconfigs = qp.get_all_configs_list()
         if qp.is_weight_quantization_point():
             module_node = target_nncf_graph.get_node_by_name(qp.insertion_point.target_node_name)
             layer_attributes = module_node.layer_attributes
             assert isinstance(layer_attributes, WeightedLayerAttributes)
             input_shape = layer_attributes.get_weight_shape()
             channel_idx = layer_attributes.get_target_dim_for_compression()
         else:
             input_shape = target_nncf_graph.get_input_shape_for_insertion_point(qp.insertion_point)
             channel_idx = 1  # channel dim for activations
 
         retval = {}
         for qconfig in qconfigs:
             is_weights = qp.is_weight_quantization_point()
-            scale_shape = tuple(get_scale_shape(input_shape,
-                                          is_weights=is_weights,
-                                          per_channel=qconfig.per_channel,
-                                          channel_idx=channel_idx))
+            scale_shape = tuple(
+                get_scale_shape(
+                    input_shape, is_weights=is_weights, per_channel=qconfig.per_channel, channel_idx=channel_idx
+                )
+            )
 
             if scale_shape not in retval:
-                retval[scale_shape] = PTRangeInitCollectorParams(is_weights,
-                                                                 qconfig.mode,
-                                                                 qconfig.per_channel,
-                                                                 input_shape,
-                                                                 channel_idx)
+                retval[scale_shape] = PTRangeInitCollectorParams(
+                    is_weights, qconfig.mode, qconfig.per_channel, input_shape, channel_idx
+                )
         return retval
 
 
 class DataLoaderRangeInitializeRunner(DataLoaderBaseRunner):
     def __init__(
-            self,
-            model: NNCFNetwork,
-            modules_to_init_vs_init_configs: Dict[str, Tuple[BaseQuantizer, RangeInitConfig, bool, Tuple[int]]],
-            init_device: str,
-            batch_size: int = None
+        self,
+        model: NNCFNetwork,
+        modules_to_init_vs_init_configs: Dict[str, Tuple[BaseQuantizer, RangeInitConfig, bool, Tuple[int]]],
+        init_device: str,
+        batch_size: int = None,
     ):
         super().__init__(model, init_device)
         self.modules_to_init = modules_to_init_vs_init_configs
-        self.progressbar_description = 'Range parameters initialization'
+        self.progressbar_description = "Range parameters initialization"
 
-        #pylint:disable=line-too-long
-        self.collectors_and_modules_to_init = OrderedDict()  # type: Dict[str, Tuple[TensorStatisticCollectorBase, BaseQuantizer]]
+        self.collectors_and_modules_to_init = (
+            OrderedDict()
+        )  # type: Dict[str, Tuple[TensorStatisticCollectorBase, BaseQuantizer]]
         self.hook_handles = []
         self.batch_size = batch_size
 
     def _get_fwd_hook(self, collector: TensorStatisticCollectorBase) -> Callable:
         def fwd_hook(module, input_, output):
             collector.register_input(input_[0])
+
         return fwd_hook
 
     def _prepare_initialization(self):
         for name, data in self.modules_to_init.items():
             quantizer_module, init_config, is_weights, input_shape = data
             num_samples_override = None
             if self.batch_size is not None:
@@ -261,45 +265,39 @@
 
             if isinstance(quantizer_module, SymmetricQuantizer):
                 mode = QuantizationMode.SYMMETRIC
             else:
                 mode = QuantizationMode.ASYMMETRIC
 
             shape = quantizer_module.scale_shape
-            if shape == (1,): # Per-tensor
+            if shape == (1,):  # Per-tensor
                 channel_idx = None
             elif len(shape) > 1 and all(item == 1 for item in shape):
                 channel_idx = 0  # (1, 1, 1, 1) - doest not matter which dim is channel_idx
             else:
                 if not is_weights:
                     channel_idx = 1  # channel dim for activations
                 else:
                     channel_idx = [i for i, val in enumerate(shape) if val != 1][0]
 
-            collector_params = PTRangeInitCollectorParams(is_weights,
-                                                          mode,
-                                                          quantizer_module.per_channel,
-                                                          input_shape,
-                                                          channel_idx)
+            collector_params = PTRangeInitCollectorParams(
+                is_weights, mode, quantizer_module.per_channel, input_shape, channel_idx
+            )
 
             collector = StatCollectorGenerator.generate_stat_collector_for_range_init_config(
-                init_config,
-                tuple(quantizer_module.scale_shape),
-                collector_params,
-                num_samples_override
+                init_config, tuple(quantizer_module.scale_shape), collector_params, num_samples_override
             )
 
             self.collectors_and_modules_to_init[name] = collector, quantizer_module
 
-            self.hook_handles.append(
-                quantizer_module.register_forward_hook(self._get_fwd_hook(collector))
-            )
+            self.hook_handles.append(quantizer_module.register_forward_hook(self._get_fwd_hook(collector)))
 
     def _apply_initializers(self):
         for handle in self.hook_handles:
             handle.remove()
         for scope_str, collector_and_module in self.collectors_and_modules_to_init.items():
             collector, quantizer_module = collector_and_module
             target_stat = collector.get_statistics()
             minmax_stats = pt_convert_stat_to_min_max_tensor_stat(target_stat)
-            quantizer_module.apply_minmax_init(minmax_stats.min_values, minmax_stats.max_values,
-                                               log_module_name=scope_str)
+            quantizer_module.apply_minmax_init(
+                minmax_stats.min_values, minmax_stats.max_values, log_module_name=scope_str
+            )
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/layers.py` & `nncf-2.5.0/nncf/torch/quantization/layers.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,38 +1,39 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# pylint:disable=too-many-lines
+
+from abc import ABC
+from abc import abstractmethod
 from enum import Enum
 from functools import partial
-from typing import Any
-from typing import Dict
-from typing import List
-from typing import Optional
-from typing import Tuple
+from typing import Any, Dict, List, Optional, Tuple
 
 import numpy as np
 import torch
+from pkg_resources import parse_version
 from torch import distributed
 from torch import nn
 
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
 from nncf.common.quantization.quantizer_setup import QuantizationPointId
 from nncf.common.quantization.quantizer_setup import QuantizerSetupBase
 from nncf.common.quantization.quantizers import calculate_asymmetric_level_ranges
 from nncf.common.quantization.quantizers import calculate_symmetric_level_ranges
+from nncf.common.quantization.quantizers import get_num_levels
 from nncf.common.quantization.structs import QuantizationMode
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import QuantizerSpec
 from nncf.common.utils.debug import is_debug
 from nncf.common.utils.registry import Registry
 from nncf.torch.checkpoint_loading import OPTIONAL_PARAMETERS_REGISTRY
 from nncf.torch.dynamic_graph.context import no_nncf_trace
@@ -48,180 +49,196 @@
 from nncf.torch.quantization.quantize_functions import get_scale_zp_from_input_low_input_high
 from nncf.torch.quantization.quantize_functions import symmetric_quantize
 from nncf.torch.utils import get_flat_tensor_contents_string
 from nncf.torch.utils import get_model_device
 from nncf.torch.utils import is_tracing_state
 from nncf.torch.utils import no_jit_trace
 
-QUANTIZATION_MODULES = Registry('quantization_modules')
-INITIALIZABLE_MODULES = Registry('initializable_modules')
+QUANTIZATION_MODULES = Registry("quantization_modules")
+INITIALIZABLE_MODULES = Registry("initializable_modules")
 
 
 class QuantizerExportMode(Enum):
     FAKE_QUANTIZE = "fake_quantize"
     ONNX_QUANTIZE_DEQUANTIZE_PAIRS = "quantize_dequantize"
 
 
 class PTQSpecStateNames:
-    NUM_BITS = 'num_bits'
-    MODE = 'mode'
-    SIGNED_TO_FORCE = 'signedness_to_force'
-    NARROW_RANGE = 'narrow_range'
-    HALF_RANGE = 'half_range'
-    SCALE_SHAPE = 'scale_shape'
-    LOGARITHM_SCALE = 'logarithm_scale'
-    IS_QUANTIZED_ON_EXPORT = 'is_quantized_on_export'
-    COMPRESSION_LR_MULTIPLIER = 'compression_lr_multiplier'
+    NUM_BITS = "num_bits"
+    MODE = "mode"
+    SIGNED_TO_FORCE = "signedness_to_force"
+    NARROW_RANGE = "narrow_range"
+    HALF_RANGE = "half_range"
+    SCALE_SHAPE = "scale_shape"
+    LOGARITHM_SCALE = "logarithm_scale"
+    IS_QUANTIZED_ON_EXPORT = "is_quantized_on_export"
+    COMPRESSION_LR_MULTIPLIER = "compression_lr_multiplier"
 
 
 class PTQuantizerSpec(QuantizerSpec):
     _state_names = PTQSpecStateNames
 
-    def __init__(self, num_bits: int,
-                 mode: QuantizationMode,
-                 signedness_to_force: Optional[bool],
-                 narrow_range: bool,
-                 half_range: bool,
-                 scale_shape: Tuple[int, ...],
-                 logarithm_scale: bool,
-                 is_quantized_on_export: bool = False,
-                 compression_lr_multiplier: float = None):
+    def __init__(
+        self,
+        num_bits: int,
+        mode: QuantizationMode,
+        signedness_to_force: Optional[bool],
+        narrow_range: bool,
+        half_range: bool,
+        scale_shape: Tuple[int, ...],
+        logarithm_scale: bool,
+        is_quantized_on_export: bool = False,
+        compression_lr_multiplier: float = None,
+    ):
         """
         :param scale_shape: Shape of quantizer scale parameters
         :param logarithm_scale: Whether to use log of scale as optimized parameter instead of scale itself.
         :param compression_lr_multiplier: Used to increase/decrease gradients for quantization parameters.
         :param is_quantized_on_export: Export to onnx weights quantized or non quantized. Should not be True for
             activation quantizers.
         """
         super().__init__(num_bits, mode, signedness_to_force, narrow_range, half_range)
-        self.per_channel = scale_shape != (1, )
+        self.per_channel = scale_shape != (1,)
         self.scale_shape = scale_shape
         self.logarithm_scale = logarithm_scale
         self.compression_lr_multiplier = compression_lr_multiplier
         self.is_quantized_on_export = is_quantized_on_export
 
     @classmethod
-    def from_config(cls, qconfig: QuantizerConfig, narrow_range: bool,
-                    half_range: bool, scale_shape: Tuple[int],
-                    logarithm_scale: bool, is_quantized_on_export: bool,
-                    compression_lr_multiplier: float) -> 'PTQuantizerSpec':
-        return cls(qconfig.num_bits,
-                   qconfig.mode,
-                   qconfig.signedness_to_force,
-                   narrow_range,
-                   half_range,
-                   scale_shape,
-                   logarithm_scale,
-                   is_quantized_on_export,
-                   compression_lr_multiplier)
+    def from_config(
+        cls,
+        qconfig: QuantizerConfig,
+        narrow_range: bool,
+        half_range: bool,
+        scale_shape: Tuple[int],
+        logarithm_scale: bool,
+        is_quantized_on_export: bool,
+        compression_lr_multiplier: float,
+    ) -> "PTQuantizerSpec":
+        return cls(
+            qconfig.num_bits,
+            qconfig.mode,
+            qconfig.signedness_to_force,
+            narrow_range,
+            half_range,
+            scale_shape,
+            logarithm_scale,
+            is_quantized_on_export,
+            compression_lr_multiplier,
+        )
 
     def __eq__(self, other):
         return self.__dict__ == other.__dict__
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'PTQuantizationPoint':
+    def from_state(cls, state: Dict[str, Any]) -> "PTQuantizationPoint":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         kwargs = {
-            cls._state_names.NUM_BITS: state['num_bits'],
-            cls._state_names.MODE: state['mode'],
-            cls._state_names.SIGNED_TO_FORCE: state['signedness_to_force'],
-            cls._state_names.NARROW_RANGE: state['narrow_range'],
-            cls._state_names.HALF_RANGE: state['half_range'],
-            cls._state_names.SCALE_SHAPE: state['scale_shape'],
-            cls._state_names.LOGARITHM_SCALE: state['logarithm_scale'],
-            cls._state_names.IS_QUANTIZED_ON_EXPORT: state['is_quantized_on_export'],
-            cls._state_names.COMPRESSION_LR_MULTIPLIER: state['compression_lr_multiplier']
+            cls._state_names.NUM_BITS: state["num_bits"],
+            cls._state_names.MODE: state["mode"],
+            cls._state_names.SIGNED_TO_FORCE: state["signedness_to_force"],
+            cls._state_names.NARROW_RANGE: state["narrow_range"],
+            cls._state_names.HALF_RANGE: state["half_range"],
+            cls._state_names.SCALE_SHAPE: state["scale_shape"],
+            cls._state_names.LOGARITHM_SCALE: state["logarithm_scale"],
+            cls._state_names.IS_QUANTIZED_ON_EXPORT: state["is_quantized_on_export"],
+            cls._state_names.COMPRESSION_LR_MULTIPLIER: state["compression_lr_multiplier"],
         }
         return cls(**kwargs)
 
     def get_state(self):
-        return {self._state_names.NUM_BITS: self.num_bits,
-                self._state_names.MODE: self.mode,
-                self._state_names.SIGNED_TO_FORCE: self.signedness_to_force,
-                self._state_names.NARROW_RANGE: self.narrow_range,
-                self._state_names.HALF_RANGE: self.half_range,
-                self._state_names.SCALE_SHAPE: self.scale_shape,
-                self._state_names.LOGARITHM_SCALE: self.logarithm_scale,
-                self._state_names.IS_QUANTIZED_ON_EXPORT: self.is_quantized_on_export,
-                self._state_names.COMPRESSION_LR_MULTIPLIER: self.compression_lr_multiplier}
+        return {
+            self._state_names.NUM_BITS: self.num_bits,
+            self._state_names.MODE: self.mode,
+            self._state_names.SIGNED_TO_FORCE: self.signedness_to_force,
+            self._state_names.NARROW_RANGE: self.narrow_range,
+            self._state_names.HALF_RANGE: self.half_range,
+            self._state_names.SCALE_SHAPE: self.scale_shape,
+            self._state_names.LOGARITHM_SCALE: self.logarithm_scale,
+            self._state_names.IS_QUANTIZED_ON_EXPORT: self.is_quantized_on_export,
+            self._state_names.COMPRESSION_LR_MULTIPLIER: self.compression_lr_multiplier,
+        }
 
 
 class PTQPointStateNames:
-    QSPEC = 'qspec'
-    TARGET_POINT = 'target_point'
-    NAMES_OF_QUANTIZED_OPS = 'directly_quantized_operator_node_names'
+    QSPEC = "qspec"
+    TARGET_POINT = "target_point"
+    NAMES_OF_QUANTIZED_OPS = "directly_quantized_operator_node_names"
 
 
 class PTQuantizationPoint:
     _state_names = PTQPointStateNames
 
-    def __init__(self, qspec: PTQuantizerSpec, target_point: PTTargetPoint,
-                 directly_quantized_operator_node_names: List[NNCFNodeName]):
+    def __init__(
+        self,
+        qspec: PTQuantizerSpec,
+        target_point: PTTargetPoint,
+        directly_quantized_operator_node_names: List[NNCFNodeName],
+    ):
         self.qspec = qspec
         self.target_point = target_point
         self.directly_quantized_operator_node_names = directly_quantized_operator_node_names
 
     def is_activation_quantization_point(self) -> bool:
         return not self.is_weight_quantization_point()
 
     def is_weight_quantization_point(self) -> bool:
         return self.target_point.target_type == TargetType.OPERATION_WITH_WEIGHTS
 
     def __str__(self):
-        return str(self.target_point) + ' ' + str(self.qspec)
+        return str(self.target_point) + " " + str(self.qspec)
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
         return {
             self._state_names.TARGET_POINT: self.target_point.get_state(),
             self._state_names.QSPEC: self.qspec.get_state(),
-            self._state_names.NAMES_OF_QUANTIZED_OPS: self.directly_quantized_operator_node_names
+            self._state_names.NAMES_OF_QUANTIZED_OPS: self.directly_quantized_operator_node_names,
         }
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'PTQuantizationPoint':
+    def from_state(cls, state: Dict[str, Any]) -> "PTQuantizationPoint":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         kwargs = {
             cls._state_names.TARGET_POINT: PTTargetPoint.from_state(state[cls._state_names.TARGET_POINT]),
             cls._state_names.QSPEC: PTQuantizerSpec.from_state(state[cls._state_names.QSPEC]),
-            cls._state_names.NAMES_OF_QUANTIZED_OPS: state[cls._state_names.NAMES_OF_QUANTIZED_OPS]
+            cls._state_names.NAMES_OF_QUANTIZED_OPS: state[cls._state_names.NAMES_OF_QUANTIZED_OPS],
         }
         return cls(**kwargs)
 
 
 class PTQSetupStateNames:
-    SHARED_INPUT_OPERATION_SET_GROUPS = 'shared_input_operation_set_groups'
-    UNIFIED_SCALE_GROUPS = 'unified_scale_groups'
-    QUANTIZATION_POINTS = 'quantization_points'
+    SHARED_INPUT_OPERATION_SET_GROUPS = "shared_input_operation_set_groups"
+    UNIFIED_SCALE_GROUPS = "unified_scale_groups"
+    QUANTIZATION_POINTS = "quantization_points"
 
 
 class PTQuantizerSetup(QuantizerSetupBase):
     _state_names = PTQSetupStateNames
 
     def __init__(self, unified_scale_groups, shared_input_operation_set_groups):
         super().__init__()
         self.unified_scale_groups = unified_scale_groups
         self.shared_input_operation_set_groups = shared_input_operation_set_groups
-        self.quantization_points = {}  # type: Dict[QuantizationPointId, PTQuantizationPoint]
 
     @classmethod
-    def from_state(cls, state: Dict) -> 'PTQuantizerSetup':
+    def from_state(cls, state: Dict) -> "PTQuantizerSetup":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
 
         def decode_qp(pair):
@@ -260,64 +277,93 @@
             self._state_names.SHARED_INPUT_OPERATION_SET_GROUPS: shared_input_operation_set_groups_state,
         }
 
     def add_quantization_point(self, qp_id: QuantizationPointId, qp: PTQuantizationPoint):
         self.quantization_points[qp_id] = qp
 
 
-class BaseQuantizer(nn.Module):
+class BaseQuantizer(nn.Module, ABC):
     # pylint:disable=too-many-public-methods
     def __init__(self, qspec: PTQuantizerSpec):
         super().__init__()
         self._narrow_range = qspec.narrow_range
         self._signedness_to_force = qspec.signedness_to_force
         self._is_using_log_scale_storage = qspec.logarithm_scale
         self._half_range = qspec.half_range
         self._is_quantized_on_export = qspec.is_quantized_on_export
-        self._num_bits = CompressionParameter(torch.IntTensor([qspec.num_bits]), requires_grad=False,
-                                              compression_lr_multiplier=qspec.compression_lr_multiplier)
-        OPTIONAL_PARAMETERS_REGISTRY.register('_num_bits')
-        self.level_high = None
-        self.level_low = None
+        self._num_bits = CompressionParameter(
+            torch.IntTensor([qspec.num_bits]),
+            requires_grad=False,
+            compression_lr_multiplier=qspec.compression_lr_multiplier,
+        )
+        OPTIONAL_PARAMETERS_REGISTRY.register("_num_bits")
+
+        # These must be made buffers, since they impact the "forward" behaviour and the model can be used
+        # in DDP scenarios, so these must be properly synchronized across processes.
+        self.register_buffer("_level_low", torch.IntTensor([0]), persistent=False)
+        self.register_buffer("_level_high", torch.IntTensor([0]), persistent=False)
 
-        self.levels = 0
-        ENABLED_VAR_NAME = 'enabled'
+        ENABLED_VAR_NAME = "enabled"
         self.register_buffer(ENABLED_VAR_NAME, torch.IntTensor([1]))
         OPTIONAL_PARAMETERS_REGISTRY.register(ENABLED_VAR_NAME)
         self.initialized = False
         self.call_count = 0
         self._scale_shape = qspec.scale_shape
         self._export_mode = QuantizerExportMode.FAKE_QUANTIZE
 
         class LoadStateListener:
             """
-               Check whether a quantization module are going to be updated by new values from state_dict or checkpoint.
+            Check whether a quantization module are going to be updated by new values from state_dict or checkpoint.
             """
 
             def __init__(self, module):
                 # pylint: disable=protected-access
                 self.hook = module._register_load_state_dict_pre_hook(partial(self.hook_fn, module=module))
 
-            def hook_fn(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs,
-                        module):
+            def hook_fn(
+                self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, module
+            ):
                 for module_key in module.state_dict().keys():
                     candidate = prefix + module_key
                     if candidate in state_dict:
                         module.initialized = True
 
             def close(self):
                 self.hook.remove()
 
         self.load_listener = LoadStateListener(self)
+        self._old_level_range_setting = False
+
+    @property
+    def level_low(self) -> int:
+        return self._level_low.item()
+
+    @level_low.setter
+    def level_low(self, val: int):
+        self._level_low.fill_(val)
 
+    @property
+    def level_high(self) -> int:
+        return self._level_high.item()
+
+    @level_high.setter
+    def level_high(self, val: int):
+        self._level_high.fill_(val)
+
+    @property
+    def levels(self):
+        return get_num_levels(self.level_low, self.level_high)
+
+    @abstractmethod
     def enable_gradients(self):
-        raise NotImplementedError
+        pass
 
+    @abstractmethod
     def disable_gradients(self):
-        raise NotImplementedError
+        pass
 
     def is_enabled_quantization(self):
         with no_jit_trace():
             return self.enabled[0].item() == 1
 
     def enable_quantization(self):
         self.enabled[0] = 1
@@ -329,15 +375,16 @@
 
     def forward(self, x):
         if is_debug():
             self.call_count += 1
         # TODO: refactor to get rid of extra if's and calls on each forward
         if not self.is_enabled_quantization():
             return x
-        self.set_level_ranges()
+        if self._old_level_range_setting:
+            self.set_levels()
         is_exporting = is_tracing_state()
         if is_exporting:
             with no_nncf_trace():
                 x = self.run_export_quantization(x)
 
             # The underlying operator (registered via register_operator) must be executed,
             # otherwise the dynamic graph won't be traced as it was during regular inference.
@@ -346,68 +393,74 @@
             # graph-structure independent trace info (e.g. current op scope and call count),
             # this is important for LSTMs etc. where determining the "first nodes in iteration
             # scopes" depends on whether the input tensors to an operation were traced or not.
             return self.quantize(x, execute_traced_op_as_identity=True)
 
         return self.quantize(x, execute_traced_op_as_identity=False)
 
+    @abstractmethod
     def quantize(self, x, execute_traced_op_as_identity: bool = False):
-        raise NotImplementedError
+        pass
 
     def reset_call_counter(self):
         self.call_count = 0
 
+    @abstractmethod
     def get_trainable_params(self) -> Dict[str, torch.Tensor]:
-        raise NotImplementedError
+        pass
 
-    def apply_minmax_init(self,
-                          min_values: torch.Tensor,
-                          max_values: torch.Tensor,
-                          log_module_name: str = None):
+    def apply_minmax_init(self, min_values: torch.Tensor, max_values: torch.Tensor, log_module_name: str = None):
         """min_values and max_values must have the same shape as specified in self.scale_shape"""
         if self.initialized:
             nncf_logger.debug(f"Skipped initializing {log_module_name} - loaded from checkpoint")
             return
 
         if torch.all(torch.isinf(min_values)) or torch.all(torch.isinf(max_values)):
-            raise ValueError(f'Statistics are not collected for {log_module_name}')
+            raise ValueError(f"Statistics are not collected for {log_module_name}")
 
         if torch.any(torch.eq(min_values, np.inf)) or torch.any(torch.eq(max_values, -np.inf)):
-            raise ValueError(f'Some of the values in statistics have infinite value for {log_module_name}')
+            raise ValueError(f"Some of the values in statistics have infinite value for {log_module_name}")
 
         own_device = get_model_device(self)
         min_values = min_values.to(own_device)
         max_values = max_values.to(own_device)
         self._apply_minmax_init(min_values, max_values, log_module_name)
 
-    def _apply_minmax_init(self,
-                           min_values: torch.Tensor,
-                           max_values: torch.Tensor,
-                           log_module_name: str = None):
-        raise NotImplementedError
+    @abstractmethod
+    def _apply_minmax_init(self, min_values: torch.Tensor, max_values: torch.Tensor, log_module_name: str = None):
+        pass
+
+    @abstractmethod
+    def set_levels(self):
+        """Must set the self._level_low and self._level_high buffers according to the current quantizer state
+        and type, and called whenever the state of the quantizer is updated in a way that affects the effective level
+        ranges."""
 
-    def set_level_ranges(self):
-        raise NotImplementedError
+    @property
+    def is_half_range(self):
+        return self._half_range
 
     @property
     def is_using_log_scale_storage(self):
         return self._is_using_log_scale_storage
 
     @property
+    @abstractmethod
     def signed(self):
-        raise NotImplementedError
+        pass
 
     @property
     def num_bits(self):
         with no_jit_trace():
             return self._num_bits.item()
 
     @num_bits.setter
     def num_bits(self, num_bits: int):
         self._num_bits.fill_(num_bits)
+        self.set_levels()
 
     @property
     def narrow_range(self) -> bool:
         return self._narrow_range
 
     @property
     def scale_shape(self) -> Tuple[int, ...]:
@@ -416,38 +469,41 @@
 
     def broadcast_initialized_params(self, src: int = 0):
         distributed.broadcast(self._num_bits, src=src)
 
     def set_export_mode(self, mode: QuantizerExportMode):
         self._export_mode = mode
 
+    @abstractmethod
     def _get_input_low_input_high(self):
-        raise NotImplementedError
+        pass
 
+    @abstractmethod
     def _prepare_export_quantization(self, x: torch.Tensor):
-        raise NotImplementedError
+        pass
 
     def _prepare_fq_export_quantization(self, x: torch.Tensor):
         x, level_high, level_low, input_low, input_high = self._prepare_export_quantization(x)
         with no_jit_trace():
             levels = level_high - level_low + 1
         return x, levels, input_low, input_high
 
     def _prepare_qdq_export_quantization(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:
         x, level_high, level_low, input_low, input_high = self._prepare_export_quantization(x)
         with no_jit_trace():
-            y_scale, y_zero_point = get_scale_zp_from_input_low_input_high(level_low,
-                                                                           level_high,
-                                                                           input_low,
-                                                                           input_high)
+            levels = level_high - level_low + 1
+            assert levels in [255, 256], "Can only export to INT8 256-level ONNX Quantize/Dequantize pairs"
+
+            y_scale, y_zero_point = get_scale_zp_from_input_low_input_high(level_low, level_high, input_low, input_high)
             possible_axes = self._possible_per_channel_dimensions()
             if len(possible_axes) > 1:
                 raise RuntimeError(
                     f"Impossible to determine the per-channel axis for a scale shape {self.scale_shape} - "
-                    f"more than one dimension is >1")
+                    f"more than one dimension is >1"
+                )
             if not possible_axes:
                 # Impossible to determine proper axis for per-channel quantization because we have
                 # scale shape ~ [1, 1, 1, 1], therefore falling back to per-tensor style export
                 axis = 1  # default value by opset, ignored in per-tensor quantization anyway
                 y_scale = y_scale.flatten()[0]
                 y_zero_point = y_zero_point.flatten()[0]
             else:
@@ -458,42 +514,50 @@
     def _possible_per_channel_dimensions(self) -> List[int]:
         return [i for i in range(len(self.scale_shape)) if self.scale_shape[i] > 1]
 
     def run_export_quantization(self, x: torch.Tensor):
         with torch.no_grad():
             if self._export_mode == QuantizerExportMode.FAKE_QUANTIZE:
                 x, levels, input_low, input_high = self._prepare_fq_export_quantization(x)
-                return ExportQuantizeToFakeQuantize.apply(x, levels,
-                                                          input_low,
-                                                          input_high,
-                                                          input_low,
-                                                          input_high)
+                return ExportQuantizeToFakeQuantize.apply(x, levels, input_low, input_high, input_low, input_high)
             if self._export_mode == QuantizerExportMode.ONNX_QUANTIZE_DEQUANTIZE_PAIRS:
                 x, y_scale, y_zero_point, axis = self._prepare_qdq_export_quantization(x)
                 return ExportQuantizeToONNXQuantDequant.apply(x, y_scale, y_zero_point, axis)
-        raise RuntimeError('Unknown export mode')
+        raise RuntimeError("Unknown export mode")
 
     def extra_repr(self):
-        return 'bit={}, ch={}'.format(
-            self.num_bits, self.per_channel)
+        return "bit={}, ch={}".format(self.num_bits, self.per_channel)
 
+    @abstractmethod
     def get_quantizer_config(self) -> QuantizerConfig:
-        raise NotImplementedError
+        pass
 
     @property
     def per_channel(self) -> bool:
         numel = 1
         for el in self.scale_shape:
             numel *= el
-        is_per_tensor = ((numel == 1) and (len(self.scale_shape) == 1))
+        is_per_tensor = (numel == 1) and (len(self.scale_shape) == 1)
         return not is_per_tensor
 
+    @abstractmethod
+    def get_parameters_for_torch_fq(self) -> Tuple[int, int, torch.Tensor, torch.Tensor]:
+        """
+        Get parameters for conversion to native FakeQuantize.
+
+        :return: A Tuple
+            quant_max - Fixed the low quant number.
+            quant_min - Fixed the high quant number.
+            scale - Quantizer scale.
+            zero_point - Quantizer zero point.
+        """
+
 
 class QuantizersSwitcher:
-    """ Enables/disables quantizers with saving and restoring original state """
+    """Enables/disables quantizers with saving and restoring original state"""
 
     def __init__(self, quantizers: List[BaseQuantizer]):
         self.originally_disabled = []  # type: List[BaseQuantizer]
         self.originally_enabled = []  # type: List[BaseQuantizer]
         self._quantizers = quantizers
 
     def disable_quantizers(self):
@@ -510,81 +574,99 @@
                 self.originally_enabled.append(module)
             if module not in self.originally_disabled:
                 module.enable_quantization()
         self.originally_disabled = []
 
 
 class StorageRedirectingLoadStateDictHook:
-    def __init__(self, storage_attribute_in_module: str, name_in_state_dict: str,
-                 use_log_storage_in_module: bool = False):
+    def __init__(
+        self, storage_attribute_in_module: str, name_in_state_dict: str, use_log_storage_in_module: bool = False
+    ):
         self._storage_attribute_in_module = storage_attribute_in_module
         self._name_in_state_dict = name_in_state_dict
         self._use_log_storage_in_module = use_log_storage_in_module
 
-    def __call__(self, state_dict, prefix, local_metadata, strict,
-                 missing_keys, unexpected_keys, error_msgs) -> None:
+    def __call__(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None:
         state_dict_key = prefix + self._name_in_state_dict
         if state_dict_key in state_dict:
             v = state_dict.pop(state_dict_key)
             if self._use_log_storage_in_module:
                 v = v.abs().log().detach()
             state_dict[prefix + self._storage_attribute_in_module] = v
         else:
             missing_keys.append(state_dict_key)
 
 
 class StorageRedirectingStateDictHook:
-    def __init__(self, storage_attribute_in_module: str, name_in_state_dict: str,
-                 use_log_storage_in_module: bool = False):
+    def __init__(
+        self, storage_attribute_in_module: str, name_in_state_dict: str, use_log_storage_in_module: bool = False
+    ):
         self._storage_attribute_in_module = storage_attribute_in_module
         self._name_in_state_dict = name_in_state_dict
         self._use_log_storage_in_module = use_log_storage_in_module
 
     def __call__(self, module, state_dict, prefix, local_metadata) -> None:
         v = state_dict.pop(prefix + self._storage_attribute_in_module)
         if self._use_log_storage_in_module:
             v = v.exp().detach()
         state_dict[prefix + self._name_in_state_dict] = v
 
 
 @COMPRESSION_MODULES.register()
 @QUANTIZATION_MODULES.register(QuantizationMode.SYMMETRIC)
 class SymmetricQuantizer(BaseQuantizer):
-    SCALE_PARAM_NAME = 'scale'
-    _SCALE_PARAM_STORAGE_ATTR = '_scale_param_storage'
+    SCALE_PARAM_NAME = "scale"
+    _SCALE_PARAM_STORAGE_ATTR = "_scale_param_storage"
 
     def __init__(self, qspec: PTQuantizerSpec):
         super().__init__(qspec)
-        self.signed_tensor = CompressionParameter(torch.IntTensor([0]), requires_grad=False,
-                                                  compression_lr_multiplier=qspec.compression_lr_multiplier)
+        self.signed_tensor = CompressionParameter(
+            torch.IntTensor([0]), requires_grad=False, compression_lr_multiplier=qspec.compression_lr_multiplier
+        )
         self.collect_scale_statistics = False
 
-        setattr(self, self._SCALE_PARAM_STORAGE_ATTR,
-                CompressionParameter(torch.ones(self.scale_shape), requires_grad=True,
-                                     compression_lr_multiplier=qspec.compression_lr_multiplier))
+        setattr(
+            self,
+            self._SCALE_PARAM_STORAGE_ATTR,
+            CompressionParameter(
+                torch.ones(self.scale_shape),
+                requires_grad=True,
+                compression_lr_multiplier=qspec.compression_lr_multiplier,
+            ),
+        )
         if self._is_using_log_scale_storage:
             self._scale_param_storage.data.log_()
             self.eps = 0
         else:
             self.eps = 1e-16
         if qspec.signedness_to_force is not None:
-            self.signed = int(qspec.signedness_to_force)
-        self.set_level_ranges()
+            self.signed = bool(qspec.signedness_to_force)
+        self.set_levels()
 
-        self._register_load_state_dict_pre_hook(StorageRedirectingLoadStateDictHook(
-            storage_attribute_in_module=self._SCALE_PARAM_STORAGE_ATTR,
-            name_in_state_dict=self.SCALE_PARAM_NAME,
-            use_log_storage_in_module=self._is_using_log_scale_storage
-        ))
-
-        self._register_state_dict_hook(StorageRedirectingStateDictHook(
-            storage_attribute_in_module=self._SCALE_PARAM_STORAGE_ATTR,
-            name_in_state_dict=self.SCALE_PARAM_NAME,
-            use_log_storage_in_module=self._is_using_log_scale_storage
-        ))
+        self._register_load_state_dict_pre_hook(
+            StorageRedirectingLoadStateDictHook(
+                storage_attribute_in_module=self._SCALE_PARAM_STORAGE_ATTR,
+                name_in_state_dict=self.SCALE_PARAM_NAME,
+                use_log_storage_in_module=self._is_using_log_scale_storage,
+            )
+        )
+
+        self._register_state_dict_hook(
+            StorageRedirectingStateDictHook(
+                storage_attribute_in_module=self._SCALE_PARAM_STORAGE_ATTR,
+                name_in_state_dict=self.SCALE_PARAM_NAME,
+                use_log_storage_in_module=self._is_using_log_scale_storage,
+            )
+        )
+
+        if parse_version(torch.__version__) >= parse_version("1.12"):
+            # Values of level_low, level_high must be recalculated for load new signed parameter.
+            self.register_load_state_dict_post_hook(lambda module, _: module.set_levels())
+        else:
+            self._old_level_range_setting = True
 
     @property
     def scale(self):
         return self._scale_param_storage.exp() if self._is_using_log_scale_storage else self._scale_param_storage
 
     @scale.setter
     def scale(self, v):
@@ -605,128 +687,177 @@
 
     def enable_gradients(self):
         self._scale_param_storage.requires_grad = True
 
     def disable_gradients(self):
         self._scale_param_storage.requires_grad = False
 
-    def set_level_ranges(self):
+    def set_levels(self):
         scaled_num_bits = 1 if self._half_range else 0
-        self.level_low, self.level_high, self.levels = self.calculate_level_ranges(self.num_bits - scaled_num_bits,
-                                                                                   self.signed)
-
-    @staticmethod
-    def calculate_level_ranges(num_bits, signed):
-        return calculate_symmetric_level_ranges(num_bits, signed)
+        self.level_low, self.level_high = calculate_symmetric_level_ranges(
+            self.num_bits - scaled_num_bits, self.signed, self._narrow_range
+        )
 
     @property
     def signed(self):
         with no_jit_trace():
             return self.signed_tensor.item() == 1
 
     @signed.setter
     def signed(self, signed: bool):
-        self.signed_tensor.fill_(signed)
+        self.signed_tensor.fill_(int(signed))
+        self.set_levels()
 
     def quantize(self, x, execute_traced_op_as_identity: bool = False):
-        return symmetric_quantize(x, self.levels, self.level_low, self.level_high, self.scale, self.eps,
-                                  skip=execute_traced_op_as_identity)
+        return symmetric_quantize(
+            x, self.levels, self.level_low, self.level_high, self.scale, self.eps, skip=execute_traced_op_as_identity
+        )
 
     def get_trainable_params(self) -> Dict[str, torch.Tensor]:
         return {self.SCALE_PARAM_NAME: self.scale.detach()}
 
     def _apply_minmax_init(self, min_values, max_values, log_module_name: str = None):
         sign = torch.any(torch.lt(min_values, 0))
         if self._signedness_to_force is not None and sign != self._signedness_to_force:
             nncf_logger.debug(f"Forcing signed to {self._signedness_to_force} for module {log_module_name}")
             sign = self._signedness_to_force
-        self.signed = int(sign)
+        self.signed = sign
 
         abs_max = torch.max(torch.abs(max_values), torch.abs(min_values))
         SCALE_LOWER_THRESHOLD = 0.1
         mask = torch.gt(abs_max, SCALE_LOWER_THRESHOLD)
-        self._scale_param_storage.data = torch.where(mask, abs_max,
-                                                     SCALE_LOWER_THRESHOLD * torch.ones_like(self._scale_param_storage))
+        self._scale_param_storage.data = torch.where(
+            mask, abs_max, SCALE_LOWER_THRESHOLD * torch.ones_like(self._scale_param_storage)
+        )
         if self._is_using_log_scale_storage:
             self._scale_param_storage.data.log_()
 
         nncf_logger.debug(
-            f"Set sign: {self.signed} and scale: {get_flat_tensor_contents_string(self.scale)} for {log_module_name}")
+            f"Set sign: {self.signed} and scale: {get_flat_tensor_contents_string(self.scale)} for {log_module_name}"
+        )
 
     def broadcast_initialized_params(self, src: int = 0):
         super().broadcast_initialized_params(src)
         distributed.broadcast(self._scale_param_storage, src=src)
         distributed.broadcast(self.signed_tensor, src=src)
 
+    def get_input_low_input_high(self):
+        return self._get_input_low_input_high(self.scale, self.level_low, self.level_high, self.eps)
+
     def _get_input_low_input_high(self, scale, level_low, level_high, eps):
         input_range = abs(scale) + eps
         input_low = input_range * level_low / level_high
         input_high = input_range
         return input_low, input_high
 
     def _prepare_export_quantization(self, x: torch.Tensor):
         with no_jit_trace():
-            input_low, input_high = self._get_input_low_input_high(self.scale,
-                                                                   self.level_low,
-                                                                   self.level_high,
-                                                                   self.eps)
+            input_low, input_high = self._get_input_low_input_high(
+                self.scale, self.level_low, self.level_high, self.eps
+            )
             level_low = self.level_low
             level_high = self.level_high
             if self._half_range:
                 x = torch.min(torch.max(x, input_low), input_high)
                 level_low = 2 * self.level_low
                 level_high = 2 * self.level_high + 1
-                input_low, input_high = self._get_input_low_input_high(level_high / self.level_high * self.scale,
-                                                                       level_low,
-                                                                       level_high,
-                                                                       self.eps)
+                input_low, input_high = self._get_input_low_input_high(
+                    level_high / self.level_high * self.scale, level_low, level_high, self.eps
+                )
             if self._is_quantized_on_export:
                 x = self.quantize(x, execute_traced_op_as_identity=False)
         return x, level_high, level_low, input_low, input_high
 
+    def get_parameters_for_torch_fq(self) -> Tuple[int, int, torch.Tensor, torch.Tensor]:
+        """
+        Get parameters for conversion to native FakeQuantize.
+
+        :return: A Tuple
+            quant_max - Fixed the low quant number.
+            quant_min - Fixed the high quant number.
+            scale - Quantizer scale.
+            zero_point - Quantizer zero point.
+        """
+        with torch.no_grad():
+            input_low, input_high = self._get_input_low_input_high(
+                self.scale, self.level_low, self.level_high, self.eps
+            )
+            level_low = self.level_low
+            level_high = self.level_high
+
+            scale, zero_point = get_scale_zp_from_input_low_input_high(level_low, level_high, input_low, input_high)
+
+            if self.narrow_range:
+                if level_low < 0:
+                    level_low -= 1
+                else:
+                    level_high += 1
+
+            if self._half_range:
+                level_low = 2 * level_low
+                level_high = 2 * level_high + 1
+
+            scale = scale.view(-1)
+            zero_point = zero_point.view(-1).to(dtype=torch.int32)
+
+        return level_low, level_high, scale, zero_point
+
     def get_quantizer_config(self) -> QuantizerConfig:
-        return QuantizerConfig(num_bits=self.num_bits,
-                               mode=QuantizationMode.SYMMETRIC,
-                               signedness_to_force=self.signed,
-                               per_channel=self.per_channel)
+        return QuantizerConfig(
+            num_bits=self.num_bits,
+            mode=QuantizationMode.SYMMETRIC,
+            signedness_to_force=self.signed,
+            per_channel=self.per_channel,
+        )
 
 
 @COMPRESSION_MODULES.register()
 @QUANTIZATION_MODULES.register(QuantizationMode.ASYMMETRIC)
 class AsymmetricQuantizer(BaseQuantizer):
-    INPUT_LOW_PARAM_NAME = 'input_low'
-    INPUT_RANGE_PARAM_NAME = 'input_range'
-    _INPUT_RANGE_PARAM_STORAGE_ATTR = '_input_range_param_storage'
+    INPUT_LOW_PARAM_NAME = "input_low"
+    INPUT_RANGE_PARAM_NAME = "input_range"
+    _INPUT_RANGE_PARAM_STORAGE_ATTR = "_input_range_param_storage"
 
     def __init__(self, qspec: PTQuantizerSpec):
         super().__init__(qspec)
-        self.input_low = CompressionParameter(torch.zeros(self.scale_shape), requires_grad=True,
-                                              compression_lr_multiplier=qspec.compression_lr_multiplier)
-        setattr(self, self._INPUT_RANGE_PARAM_STORAGE_ATTR,
-                CompressionParameter(torch.ones(self.scale_shape), requires_grad=True,
-                                     compression_lr_multiplier=qspec.compression_lr_multiplier))
+        self.input_low = CompressionParameter(
+            torch.zeros(self.scale_shape), requires_grad=True, compression_lr_multiplier=qspec.compression_lr_multiplier
+        )
+        setattr(
+            self,
+            self._INPUT_RANGE_PARAM_STORAGE_ATTR,
+            CompressionParameter(
+                torch.ones(self.scale_shape),
+                requires_grad=True,
+                compression_lr_multiplier=qspec.compression_lr_multiplier,
+            ),
+        )
 
         if self._is_using_log_scale_storage:
             self._input_range_param_storage.data.log_()
             self.eps = 0
         else:
             self.eps = 1e-16
-        self.set_level_ranges()
+        self.set_levels()
 
-        self._register_load_state_dict_pre_hook(StorageRedirectingLoadStateDictHook(
-            storage_attribute_in_module=self._INPUT_RANGE_PARAM_STORAGE_ATTR,
-            name_in_state_dict=self.INPUT_RANGE_PARAM_NAME,
-            use_log_storage_in_module=self._is_using_log_scale_storage
-        ))
-
-        self._register_state_dict_hook(StorageRedirectingStateDictHook(
-            storage_attribute_in_module=self._INPUT_RANGE_PARAM_STORAGE_ATTR,
-            name_in_state_dict=self.INPUT_RANGE_PARAM_NAME,
-            use_log_storage_in_module=self._is_using_log_scale_storage
-        ))
+        self._register_load_state_dict_pre_hook(
+            StorageRedirectingLoadStateDictHook(
+                storage_attribute_in_module=self._INPUT_RANGE_PARAM_STORAGE_ATTR,
+                name_in_state_dict=self.INPUT_RANGE_PARAM_NAME,
+                use_log_storage_in_module=self._is_using_log_scale_storage,
+            )
+        )
+
+        self._register_state_dict_hook(
+            StorageRedirectingStateDictHook(
+                storage_attribute_in_module=self._INPUT_RANGE_PARAM_STORAGE_ATTR,
+                name_in_state_dict=self.INPUT_RANGE_PARAM_NAME,
+                use_log_storage_in_module=self._is_using_log_scale_storage,
+            )
+        )
 
     @property
     def input_range(self):
         if self._is_using_log_scale_storage:
             return self._input_range_param_storage.exp()
         return self._input_range_param_storage
 
@@ -755,96 +886,133 @@
         self.input_low.requires_grad = False
         self._input_range_param_storage.requires_grad = False
 
     @property
     def signed(self):
         return True
 
-    def set_level_ranges(self):
+    def set_levels(self):
         scaled_num_bits = 1 if self._half_range else 0
-        self.level_low, self.level_high, self.levels = self.calculate_level_ranges(self.num_bits - scaled_num_bits)
-
-    @staticmethod
-    def calculate_level_ranges(num_bits):
-        return calculate_asymmetric_level_ranges(num_bits)
+        self.level_low, self.level_high = calculate_asymmetric_level_ranges(self.num_bits - scaled_num_bits)
 
     def quantize(self, x, execute_traced_op_as_identity: bool = False):
-        return asymmetric_quantize(x, self.levels, self.level_low, self.level_high, self.input_low, self.input_range,
-                                   self.eps, skip=execute_traced_op_as_identity)
+        return asymmetric_quantize(
+            x,
+            self.levels,
+            self.level_low,
+            self.level_high,
+            self.input_low,
+            self.input_range,
+            self.eps,
+            skip=execute_traced_op_as_identity,
+        )
 
     def get_trainable_params(self) -> Dict[str, torch.Tensor]:
-        return {self.INPUT_LOW_PARAM_NAME: self.input_low.detach(),
-                self.INPUT_RANGE_PARAM_NAME: self.input_range.detach()}
+        return {
+            self.INPUT_LOW_PARAM_NAME: self.input_low.detach(),
+            self.INPUT_RANGE_PARAM_NAME: self.input_range.detach(),
+        }
 
     def _apply_minmax_init(self, min_values, max_values, log_module_name: str = None):
         ranges = max_values - min_values
         max_range = torch.max(max_values - min_values)
         eps = 1e-2
         correction = (clamp(ranges, low=eps * max_range, high=max_range) - ranges) * 0.5
         self._input_range_param_storage.data = (ranges + 2 * correction).data
         if self._is_using_log_scale_storage:
             self._input_range_param_storage.data.log_()
 
         self.input_low.data = (min_values - correction).data
 
         nncf_logger.debug(
             f"Set input_low: {get_flat_tensor_contents_string(self.input_low)} "
-            f"and input_range: {get_flat_tensor_contents_string(self.input_range)} for {log_module_name}")
+            f"and input_range: {get_flat_tensor_contents_string(self.input_range)} for {log_module_name}"
+        )
 
     def broadcast_initialized_params(self, src: int = 0):
         super().broadcast_initialized_params(src)
         distributed.broadcast(self.input_low, src)
         distributed.broadcast(self._input_range_param_storage, src)
 
+    def get_input_low_input_high(self):
+        return self._get_input_low_input_high(self.input_range, self.input_low, self.levels, self.eps)
+
     def _get_input_low_input_high(self, input_range, input_low, levels, eps):
         input_range_safe = abs(input_range) + eps
         input_low, input_range_tuned = TuneRange.apply(input_low, input_range_safe, levels)
         input_high = input_low + input_range_tuned
         return input_low, input_high
 
     def _prepare_export_quantization(self, x: torch.Tensor):
         with no_jit_trace():
-            input_low, input_high = self._get_input_low_input_high(self.input_range,
-                                                                   self.input_low,
-                                                                   self.levels,
-                                                                   self.eps)
+            input_low, input_high = self._get_input_low_input_high(
+                self.input_range, self.input_low, self.levels, self.eps
+            )
             level_low = self.level_low
             level_high = self.level_high
             if self._half_range:
                 x = torch.min(torch.max(x, input_low), input_high)
                 level_low = 2 * level_low
                 level_high = 2 * level_high + 1
-                input_low, input_high = self._get_input_low_input_high(level_high / self.level_high * self.input_range,
-                                                                       self.input_low,
-                                                                       self.levels,
-                                                                       self.eps)
+                input_low, input_high = self._get_input_low_input_high(
+                    level_high / self.level_high * self.input_range, self.input_low, self.levels, self.eps
+                )
             if self._is_quantized_on_export:
                 x = self.quantize(x, execute_traced_op_as_identity=False)
         return x, level_high, level_low, input_low, input_high
 
+    def get_parameters_for_torch_fq(self) -> Tuple[int, int, torch.Tensor, torch.Tensor]:
+        """
+        Get parameters for conversion to native FakeQuantize.
+
+        :return: A Tuple
+            quant_max - Fixed the low quant number.
+            quant_min - Fixed the high quant number.
+            scale - Quantizer scale.
+            zero_point - Quantizer zero point.
+        """
+        with torch.no_grad():
+            input_low, input_high = self._get_input_low_input_high(
+                self.input_range, self.input_low, self.levels, self.eps
+            )
+            level_low = self.level_low
+            level_high = self.level_high
+
+            scale, zero_point = get_scale_zp_from_input_low_input_high(level_low, level_high, input_low, input_high)
+
+            if self._half_range:
+                level_low = 2 * level_low
+                level_high = 2 * level_high + 1
+
+            scale = scale.view(-1)
+            zero_point = zero_point.view(-1).to(dtype=torch.int32)
+
+        return level_low, level_high, scale, zero_point
+
     def get_quantizer_config(self) -> QuantizerConfig:
-        return QuantizerConfig(num_bits=self.num_bits,
-                               mode=QuantizationMode.ASYMMETRIC,
-                               signedness_to_force=self.signed,
-                               per_channel=self.per_channel)
+        return QuantizerConfig(
+            num_bits=self.num_bits,
+            mode=QuantizationMode.ASYMMETRIC,
+            signedness_to_force=self.signed,
+            per_channel=self.per_channel,
+        )
 
 
 def get_per_channel_scale_shape(input_shape, is_weights, channel_idx: int = None):
     scale_shape = [1 for _ in input_shape]
     if channel_idx is None:
         if is_weights:
             channel_idx = 0  # Per weight channel scales
         else:
             channel_idx = 1  # Per activation channel scales
     scale_shape[channel_idx] = input_shape[channel_idx]
     return scale_shape
 
 
-def get_scale_shape(input_shape: List[int], is_weights: bool, per_channel: bool,
-                    channel_idx: int = None) -> List[int]:
+def get_scale_shape(input_shape: List[int], is_weights: bool, per_channel: bool, channel_idx: int = None) -> List[int]:
     """
     Assumes that input_shape is supplied in either [B, C, H, W] or [N_out, N_in, H, W] format,
     or derivatives.
     :param input_shape: The input shape of the tensor; semantic meaning of dimensions should be as described above.
     :param is_weights: Whether the tensor corresponds to weights, in which case the per-channel scaling dimension
         is selected based on the [N_out, N_in, H, W] format
     :param per_channel: If True, will generate a per-channel scale shape, otherwise will generate a scale shape
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/metrics.py` & `nncf-2.5.0/nncf/torch/quantization/metrics.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,86 +1,86 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Dict, List, Tuple, Optional, Any
+from collections import deque
+from copy import deepcopy
 from itertools import chain
+from typing import Any, Dict, List, Optional, Tuple
 
 import networkx as nx
-import torch
 import numpy as np
-from collections import deque
-from copy import deepcopy
+import torch
 
+from nncf.common.collector import StatisticsCollector
 from nncf.common.graph import NNCFGraph
+from nncf.common.graph.graph_matching import find_subgraphs_matching_pattern
+from nncf.common.graph.patterns.manager import PatternsManager
+from nncf.common.graph.patterns.manager import TargetDevice
+from nncf.common.quantization.collectors import QuantizationStatisticsCollector
+from nncf.common.quantization.collectors import QuantizerDescription
+from nncf.common.quantization.quantizer_propagation.structs import QuantizationTrait
+from nncf.common.quantization.structs import NonWeightQuantizerId
+from nncf.common.quantization.structs import WeightQuantizerId
+from nncf.common.utils.backend import BackendType
 from nncf.common.utils.debug import is_debug
-from nncf.torch.hardware.fused_patterns import PT_HW_FUSED_PATTERNS
+from nncf.torch.nncf_module_replacement import is_nncf_module
+from nncf.torch.nncf_network import NNCFNetwork
+from nncf.torch.nncf_network import PTNNCFGraph
 from nncf.torch.quantization.default_quantization import DEFAULT_PT_QUANT_TRAIT_TO_OP_DICT
 from nncf.torch.quantization.layers import BaseQuantizer
 from nncf.torch.quantization.layers import SymmetricQuantizer
-from nncf.torch.nncf_network import NNCFNetwork, PTNNCFGraph
-from nncf.torch.dynamic_graph.transform_graph import is_nncf_module
-from nncf.common.quantization.quantizer_propagation.structs import QuantizationTrait
-from nncf.common.quantization.structs import WeightQuantizerId
-from nncf.common.quantization.structs import NonWeightQuantizerId
-from nncf.torch.quantization.structs import WeightQuantizerInfo
-from nncf.torch.quantization.structs import NonWeightQuantizerInfo
-from nncf.common.collector import StatisticsCollector
 from nncf.torch.quantization.statistics import MemoryConsumptionStatistics
 from nncf.torch.quantization.statistics import QuantizationConfigurationStatistics
-from nncf.common.quantization.collectors import QuantizerDescription
-from nncf.common.quantization.collectors import QuantizationStatisticsCollector
-from nncf.common.graph.graph_matching import find_subgraphs_matching_pattern
+from nncf.torch.quantization.structs import NonWeightQuantizerInfo
+from nncf.torch.quantization.structs import WeightQuantizerInfo
 
 
 class QuantizationShareBuildTimeInfo:
     def __init__(self, aq_potential_num: int, wq_potential_num: int):
         self.aq_potential_num = aq_potential_num
         self.wq_potential_num = wq_potential_num
 
     def get_state(self) -> Dict[str, Any]:
         """
         Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
         represents state of the object.
 
         :return: state of the object
         """
-        return {
-            'aq_potential_num': self.aq_potential_num,
-            'wq_potential_num': self.wq_potential_num
-        }
+        return {"aq_potential_num": self.aq_potential_num, "wq_potential_num": self.wq_potential_num}
 
     @classmethod
-    def from_state(cls, state: Dict[str, Any]) -> 'QuantizationShareBuildTimeInfo':
+    def from_state(cls, state: Dict[str, Any]) -> "QuantizationShareBuildTimeInfo":
         """
         Creates the object from its state.
 
         :param state: Output of `get_state()` method.
         """
         return cls(**state)
 
 
 class PTQuantizationStatisticsCollector(QuantizationStatisticsCollector):
     """
     Implementation of the quantization statistics collector for the PyTorch backend.
     """
 
-    def __init__(self,
-                 weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
-                 non_weight_quantizers: Dict[NonWeightQuantizerId, NonWeightQuantizerInfo],
-                 build_time_info: QuantizationShareBuildTimeInfo):
+    def __init__(
+        self,
+        weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
+        non_weight_quantizers: Dict[NonWeightQuantizerId, NonWeightQuantizerInfo],
+        build_time_info: QuantizationShareBuildTimeInfo,
+    ):
         """
         Initializes a collector of the quantization statistics.
         """
         self._weight_quantizers = {k: v.quantizer_module_ref for k, v in weight_quantizers.items()}
         self._non_weight_quantizers = {k: v.quantizer_module_ref for k, v in non_weight_quantizers.items()}
         self._info = build_time_info
 
@@ -89,29 +89,24 @@
         Collects descriptions of the quantizers.
 
         :return: Descriptions of the quantizers.
         """
         # `True` for weight quantizer, `False` otherwise.
         quantizers = chain(
             map(lambda x: (True, x), self._weight_quantizers.values()),
-            map(lambda x: (False, x), self._non_weight_quantizers.values())
+            map(lambda x: (False, x), self._non_weight_quantizers.values()),
         )
 
         quantizers_descriptions = []
         for is_weight_quantizer, q in quantizers:
             is_symmetric = isinstance(q, SymmetricQuantizer)
 
             quantizers_descriptions.append(
                 QuantizerDescription(
-                    q.num_bits,
-                    q.per_channel,
-                    q.signed,
-                    is_symmetric,
-                    is_weight_quantizer,
-                    q.is_enabled_quantization()
+                    q.num_bits, q.per_channel, q.signed, is_symmetric, is_weight_quantizer, q.is_enabled_quantization()
                 )
             )
 
         return quantizers_descriptions
 
     def _get_potential_quantizers_num(self) -> Tuple[int, int]:
         """
@@ -131,31 +126,33 @@
         - how many times memory consumption for network weights will decrease.
         - how many times memory consumption* for activations tensor will decrease.
 
     * Reflects host memory consumption, assuming only the final low-precision output activation tensors are stored
       in host memory (i.e. assuming intermediate accumulation results are only stored in device memory)
     """
 
-    def __init__(self,
-                 compressed_model: NNCFNetwork,
-                 weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
-                 non_weight_quantizers: Dict[NonWeightQuantizerId, NonWeightQuantizerInfo]):
+    def __init__(
+        self,
+        compressed_model: NNCFNetwork,
+        weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
+        non_weight_quantizers: Dict[NonWeightQuantizerId, NonWeightQuantizerInfo],
+    ):
         """
         Initializes collector of the memory consumption statistics.
         """
         self._compressed_model = compressed_model
         self._weight_quantizers = weight_quantizers
         self._non_weight_quantizers = non_weight_quantizers
 
     def collect(self) -> MemoryConsumptionStatistics:
         stats = MemoryConsumptionStatistics()
 
         fp_num_bits = 32
-        nncf_modules = self._compressed_model.get_nncf_modules()
-        for nncf_module in nncf_modules.values():
+        nncf_modules = self._compressed_model.nncf.get_nncf_modules()
+        for nncf_module in nncf_modules:
             count_el = np.prod(nncf_module.weight.shape)
             stats.fp32_weight_size += count_el * fp_num_bits
             quantizer = self._get_weight_quantizer_for_module(nncf_module)
             if quantizer is not None:
                 num_bits = quantizer.num_bits
                 stats.quantized_weight_size += count_el * num_bits
             else:
@@ -165,44 +162,45 @@
             stats.weight_memory_consumption_decrease = stats.fp32_weight_size / stats.quantized_weight_size
         except ZeroDivisionError:
             stats.weight_memory_consumption_decrease = 0
 
         stats.quantized_weight_size /= 2**23
         stats.fp32_weight_size /= 2**23
 
-        original_graph = deepcopy(self._compressed_model.get_original_graph())
+        original_graph = deepcopy(self._compressed_model.nncf.get_original_graph())
 
         memory_consumption_fp_model = {}
         memory_consumption_compressed_model = {}
         # pylint: disable=protected-access
         original_nx_graph = original_graph._nx_graph
         nx.set_edge_attributes(original_nx_graph, 32, "precision")
 
         for u, v in original_nx_graph.edges:
             shape = original_nx_graph.edges[u, v][NNCFGraph.ACTIVATION_SHAPE_EDGE_ATTR]
             num_bits = self._get_precision_for_activation_tensor(u, v, original_nx_graph)
-            original_nx_graph.edges[u, v]['precision'] = num_bits
+            original_nx_graph.edges[u, v]["precision"] = num_bits
             u_node_name = original_nx_graph.nodes[u][NNCFGraph.NODE_NAME_ATTR]
             memory_consumption_fp_model[u_node_name] = np.prod(shape) * fp_num_bits
             memory_consumption_compressed_model[u_node_name] = np.prod(shape) * num_bits
         try:
             stats.max_fp32_activation_size = max(memory_consumption_fp_model.values()) / 2**23
             stats.max_compressed_activation_size = max(memory_consumption_compressed_model.values()) / 2**23
         except ValueError:
             stats.max_fp32_activation_size = 0
             stats.max_compressed_activation_size = 0
         return stats
 
     def _get_precision_for_activation_tensor(self, u_node: str, v_node: str, original_nx_graph: nx.DiGraph) -> int:
         # pylint: disable=protected-access
         pred_u_nodes = original_nx_graph._pred[u_node]
-        precision_enter_activation_tensor =\
-             max([0] + [original_nx_graph.edges[pred_u_node, u_node]['precision'] for pred_u_node in pred_u_nodes])
+        precision_enter_activation_tensor = max(
+            [0] + [original_nx_graph.edges[pred_u_node, u_node]["precision"] for pred_u_node in pred_u_nodes]
+        )
         u_node_name = original_nx_graph.nodes[u_node][NNCFGraph.NODE_NAME_ATTR]
-        module = self._compressed_model.get_containing_module(u_node_name)
+        module = self._compressed_model.nncf.get_containing_module(u_node_name)
         if is_nncf_module(module):
             quantizer = self._get_weight_quantizer_for_module(module)
             if quantizer is not None:
                 precision = max(quantizer.num_bits, precision_enter_activation_tensor)
             else:
                 precision = 32
             return precision
@@ -224,28 +222,30 @@
 
 class ShareEdgesQuantizedDataPathStatisticsCollector(StatisticsCollector):
     """
     This metric calculates the percentage of quantized edges relative to the total number of edges
     in the original network graph. "Quantized edge" is an edge representing a quantized activation tensor.
     """
 
-    QUANTIZED_EDGES_ATTR = 'quantized'
-    PASSED_EDGES_ATTR = 'passed'
-    NODES_GRAPH_ATTR = 'nodes'
-    IS_MERGED_GRAPH_ATTR = 'is_merged'
+    QUANTIZED_EDGES_ATTR = "quantized"
+    PASSED_EDGES_ATTR = "passed"
+    NODES_GRAPH_ATTR = "nodes"
+    IS_MERGED_GRAPH_ATTR = "is_merged"
 
-    def __init__(self, compressed_model: NNCFNetwork, qctrl: 'QuantizationController'):
+    def __init__(self, compressed_model: NNCFNetwork, qctrl: "QuantizationController", target_device: TargetDevice):
         self._compressed_model = compressed_model
         self._qctrl = qctrl  # type: QuantizationController
         self.stats = QuantizationConfigurationStatistics(0, 0)
+        self._target_device = target_device
 
     def collect(self) -> QuantizationConfigurationStatistics:
         # pylint: disable=too-many-branches
-        merged_original_graph =\
-            self.get_merged_original_graph_with_patterns(self._compressed_model.get_original_graph())
+        merged_original_graph = self.get_merged_original_graph_with_patterns(
+            self._compressed_model.nncf.get_original_graph()
+        )
         self.stats.quantized_edges_in_cfg = 0
         nx.set_edge_attributes(merged_original_graph, False, self.QUANTIZED_EDGES_ATTR)
         nx.set_edge_attributes(merged_original_graph, False, self.PASSED_EDGES_ATTR)
         # pylint: disable=protected-access
         input_nodes = [node for node in merged_original_graph.nodes if len(merged_original_graph._pred[node]) == 0]
         queue = deque()
         for input_node in input_nodes:
@@ -254,15 +254,15 @@
             for next_node_key in next_nodes:
                 edge = merged_original_graph.edges[input_node, next_node_key]
                 edge[self.PASSED_EDGES_ATTR] = True
                 edge[self.QUANTIZED_EDGES_ATTR] = True
                 self.stats.quantized_edges_in_cfg += 1
                 queue.appendleft(next_node_key)
         visited_nodes = {}
-        #pylint: disable=too-many-nested-blocks
+        # pylint: disable=too-many-nested-blocks
         while len(queue) != 0:
             node_key = queue.pop()
             if node_key in visited_nodes:
                 continue
             if self._all_enter_edges_in_node_of_type(merged_original_graph, node_key, self.PASSED_EDGES_ATTR):
                 visited_nodes[node_key] = True
                 node = merged_original_graph.nodes[node_key]
@@ -279,30 +279,30 @@
                         self._marking_edges(merged_original_graph, node_key, queue)
                     else:
                         self._marking_edges(merged_original_graph, node_key, queue, False)
                 else:
                     node_name = str(node[NNCFGraph.NODE_NAME_ATTR])
 
                     matched = False
-                    for aq_key in self._compressed_model.external_quantizers.keys():
+                    for aq_key in self._compressed_model.nncf.external_quantizers.keys():
                         if node_name in aq_key:
                             matched = True
                             break
                     if matched:
                         self._marking_edges(merged_original_graph, node_key, queue)
                     else:
                         is_op_non_change_precision_activation_tensor = True
                         node_metatype = node[NNCFGraph.METATYPE_ATTR]
-                        is_op_non_change_precision_activation_tensor =\
-                            node_metatype not in\
-                                DEFAULT_PT_QUANT_TRAIT_TO_OP_DICT[QuantizationTrait.INPUTS_QUANTIZABLE]
-
-                        status = is_op_non_change_precision_activation_tensor and\
-                            self._all_enter_edges_in_node_of_type(merged_original_graph,
-                                                                  node_key, self.QUANTIZED_EDGES_ATTR)
+                        is_op_non_change_precision_activation_tensor = (
+                            node_metatype not in DEFAULT_PT_QUANT_TRAIT_TO_OP_DICT[QuantizationTrait.INPUTS_QUANTIZABLE]
+                        )
+
+                        status = is_op_non_change_precision_activation_tensor and self._all_enter_edges_in_node_of_type(
+                            merged_original_graph, node_key, self.QUANTIZED_EDGES_ATTR
+                        )
                         self._marking_edges(merged_original_graph, node_key, queue, status)
             else:
                 queue.appendleft(node_key)
         self.num_merged_original_graph_edges = len(merged_original_graph.edges)
         self.stats.total_edges_in_cfg = self.num_merged_original_graph_edges
         return self.stats
 
@@ -325,15 +325,15 @@
             edge[self.QUANTIZED_EDGES_ATTR] = mark
             edge[self.PASSED_EDGES_ATTR] = True
             queue.appendleft(next_node_key)
             if mark:
                 self.stats.quantized_edges_in_cfg += 1
 
     def get_merged_original_graph_with_patterns(self, original_graph: PTNNCFGraph):
-        pattern = PT_HW_FUSED_PATTERNS.get_full_pattern_graph()
+        pattern = PatternsManager.get_full_hw_pattern_graph(backend=BackendType.TORCH, device=self._target_device)
         # pylint: disable=protected-access
         matches = find_subgraphs_matching_pattern(original_graph._nx_graph, pattern)
         merged_graph = deepcopy(original_graph._nx_graph)
         nx.set_node_attributes(merged_graph, False, self.IS_MERGED_GRAPH_ATTR)
         for match in matches:
             if len(match) == 1:
                 continue
@@ -349,22 +349,22 @@
             out_edge_copies_dict = {}
             for out_edge_key in out_edges:
                 out_edge_copies_dict[out_edge_key] = deepcopy(merged_graph.edges[out_edge_key])
 
             merged_node_key = ""
             merged_nodes = []
             for node_key in match:
-                merged_node_key += node_key + '\n'
+                merged_node_key += node_key + "\n"
                 # pylint: disable=protected-access
                 merged_nodes.append(original_graph._nx_graph.nodes[node_key])
                 merged_graph.remove_node(node_key)
             merged_node_attrs = {
                 PTNNCFGraph.KEY_NODE_ATTR: merged_node_key,
                 self.NODES_GRAPH_ATTR: merged_nodes,
-                self.IS_MERGED_GRAPH_ATTR: True
+                self.IS_MERGED_GRAPH_ATTR: True,
             }
             merged_graph.add_node(merged_node_key, **merged_node_attrs)
             for in_edge_key, in_edge_attrs in in_edge_copies_dict.items():
                 merged_graph.add_edge(in_edge_key[0], merged_node_key, **in_edge_attrs)
             for out_edge_key, out_edge_attrs in out_edge_copies_dict.items():
                 merged_graph.add_edge(merged_node_key, out_edge_key[1], **out_edge_attrs)
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_constraints.py` & `nncf-2.5.0/nncf/torch/quantization/precision_constraints.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from copy import deepcopy
 from typing import Dict, List
 
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.quantization.structs import QuantizerId
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/adjacent_quantizers.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/adjacent_quantizers.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,40 +1,36 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Dict
-from typing import List
-from typing import NamedTuple
-from typing import Tuple
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Dict, List, NamedTuple, Tuple
 
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
-from nncf.torch.quantization.layers import BaseQuantizer
-from nncf.common.quantization.structs import QuantizerId
 from nncf.common.quantization.quantizer_setup import QuantizationPointId
 from nncf.common.quantization.quantizer_setup import QuantizerSetupBase
+from nncf.common.quantization.structs import QuantizerId
+from nncf.torch.quantization.layers import BaseQuantizer
 
 
 class AdjacentQuantizers(NamedTuple):
     """
     Combines activation and weight quantizers so that each quantizer is in the same group as the operation that it is
     affecting. Each quantizer that does not affect any node (e.g. if it only affects other quantizers as a topmost
     quantizer in a requantization scenario) will be placed in a separate group.
     :param: activation_quantizers   list of pairs of activation quantizers with their ids
     :param: weight_quantizers   list of pairs of weight quantizers with their ids
     """
+
     activation_quantizers: List[Tuple[QuantizerId, BaseQuantizer]]
     weight_quantizers: List[Tuple[QuantizerId, BaseQuantizer]]
 
 
 class GroupsOfAdjacentQuantizers:
     """
     Contains groups of adjacent quantizers
@@ -47,29 +43,34 @@
         self._quantizer_per_group_id = {}
         self._groups_of_adjacent_quantizers: List[AdjacentQuantizers] = []
 
     def get_group_id_for_quantizer(self, quantizer_id: QuantizerId):
         return self._quantizer_per_group_id.get(quantizer_id, None)
 
     def get_adjacent_quantizers_by_group_id(self, group_id):
-        return self._groups_of_adjacent_quantizers[group_id].weight_quantizers + \
-               self._groups_of_adjacent_quantizers[group_id].activation_quantizers
+        return (
+            self._groups_of_adjacent_quantizers[group_id].weight_quantizers
+            + self._groups_of_adjacent_quantizers[group_id].activation_quantizers
+        )
 
     def __iter__(self):
         return iter(self._groups_of_adjacent_quantizers)
 
     def __bool__(self):
         return bool(self._groups_of_adjacent_quantizers) and bool(self._quantizer_per_group_id)
 
     def __getitem__(self, group_id):
         return self._groups_of_adjacent_quantizers[group_id]
 
-    def parse_from_quantizer_setup(self, all_quantizations: Dict[QuantizerId, BaseQuantizer],
-                                   quantizer_setup: QuantizerSetupBase,
-                                   quantization_point_id_vs_quantizer_id: Dict[QuantizationPointId, QuantizerId]):
+    def parse_from_quantizer_setup(
+        self,
+        all_quantizations: Dict[QuantizerId, BaseQuantizer],
+        quantizer_setup: QuantizerSetupBase,
+        quantization_point_id_vs_quantizer_id: Dict[QuantizationPointId, QuantizerId],
+    ):
         for group_idx, group in quantizer_setup.shared_input_operation_set_groups.items():
             act_quant_tuples = []  # type: List[Tuple[QuantizerId, BaseQuantizer]]
             wt_quant_tuples = []  # type: List[Tuple[QuantizerId, BaseQuantizer]]
 
             quantized_node_per_activation_qp_id = {}  # type: Dict[NNCFNodeName, QuantizationPointId]
             module_scope_per_weight_qp_id = {}  # type: Dict[NNCFNodeName, QuantizationPointId]
 
@@ -86,19 +87,22 @@
                     act_quant_tuples.append(resulting_tuple)
                     quantized_node_names = qp.directly_quantized_operator_node_names
                     quantized_node_per_activation_qp_id.update({node_name: qp_id for node_name in quantized_node_names})
                 self._quantizer_per_group_id[quant_id] = group_idx
 
             for weight_quantized_module_node_name, w_qp_id in module_scope_per_weight_qp_id.items():
                 if weight_quantized_module_node_name not in quantized_node_per_activation_qp_id:
-                    nncf_logger.debug(f'Module {weight_quantized_module_node_name} has quantized weights'
-                                        f' and no quantized inputs!')
+                    nncf_logger.debug(
+                        f"Module {weight_quantized_module_node_name} has quantized weights" f" and no quantized inputs!"
+                    )
                     continue
                 a_qp_id = quantized_node_per_activation_qp_id[weight_quantized_module_node_name]
                 if w_qp_id in self.weight_qp_id_per_activation_qp_id:
-                    nncf_logger.debug(f'Multiple weight quantizers per activation quantizer '
-                                      f'for {weight_quantized_module_node_name}')
+                    nncf_logger.debug(
+                        f"Multiple weight quantizers per activation quantizer "
+                        f"for {weight_quantized_module_node_name}"
+                    )
                     continue
                 self.weight_qp_id_per_activation_qp_id[w_qp_id] = a_qp_id
 
             adj_quants = AdjacentQuantizers(act_quant_tuples, wt_quant_tuples)
             self._groups_of_adjacent_quantizers.append(adj_quants)
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/autoq_init.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/autoq_init.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,61 +1,62 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
+import json
+import math
+import os
+import os.path as osp
+import time
 from collections import OrderedDict
-from typing import Dict, Tuple, List, Optional
+from copy import deepcopy
+from datetime import datetime
+from pathlib import Path
+from typing import Dict, List, Optional, Tuple
 
-import os
+import numpy as np
+import pandas as pd
 
-from nncf.common.utils.debug import is_debug
 from nncf.common.hardware.config import HWConfigType
 from nncf.common.logging import nncf_logger
+from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
+from nncf.common.utils.debug import is_debug
 from nncf.common.utils.os import safe_open
 from nncf.config.schemata.defaults import AUTOQ_EVAL_SUBSET_RATIO
 from nncf.config.schemata.defaults import AUTOQ_ITER_NUMBER
 from nncf.config.schemata.defaults import AUTOQ_WARMUP_ITER_NUMBER
 from nncf.config.schemata.defaults import PRECISION_INIT_BITWIDTHS
 from nncf.torch.quantization.precision_constraints import HardwareQuantizationConstraints
-from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitializer, BasePrecisionInitParams
-from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
+from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitializer
+from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitParams
 from nncf.torch.structures import AutoQPrecisionInitArgs
 
-from pathlib import Path
-import os.path as osp
-import time
-from datetime import datetime
-import json
-import math
-import numpy as np
-import pandas as pd
-from copy import deepcopy
-
 
 class AutoQPrecisionInitParams(BasePrecisionInitParams):
-    def __init__(self, user_init_args: AutoQPrecisionInitArgs,
-                 dump_autoq_data: bool = False,
-                 iter_number: int = 0,
-                 warmup_iter_number: int = None,
-                 compression_ratio: float = None,
-                 eval_subset_ratio: float = None,
-                 ddpg_hparams_dict: Dict = None,
-                 hw_cfg_type: HWConfigType = None,
-                 skip_constraint: bool = False,
-                 finetune: bool = False,
-                 bits: List[int] = None):
+    def __init__(
+        self,
+        user_init_args: AutoQPrecisionInitArgs,
+        dump_autoq_data: bool = False,
+        iter_number: int = 0,
+        warmup_iter_number: int = None,
+        compression_ratio: float = None,
+        eval_subset_ratio: float = None,
+        ddpg_hparams_dict: Dict = None,
+        hw_cfg_type: HWConfigType = None,
+        skip_constraint: bool = False,
+        finetune: bool = False,
+        bits: List[int] = None,
+    ):
         super().__init__(user_init_args)
         self.dump_autoq_data = dump_autoq_data
         self.iter_number = iter_number
         self.compression_ratio = compression_ratio
         self.eval_subset_ratio = eval_subset_ratio
         self.warmup_iter_number = warmup_iter_number
         if ddpg_hparams_dict is None:
@@ -64,151 +65,166 @@
             self.ddpg_hparams_dict = ddpg_hparams_dict
         self.hw_cfg_type = hw_cfg_type
         self.skip_constraint = skip_constraint
         self.finetune = finetune
         self.bits = bits
 
     @classmethod
-    def from_config(cls, autoq_init_config_dict: Dict,
-                    user_init_args: AutoQPrecisionInitArgs,
-                    target_hw_config_type: Optional[HWConfigType]) -> 'AutoQPrecisionInitParams':
+    def from_config(
+        cls,
+        autoq_init_config_dict: Dict,
+        user_init_args: AutoQPrecisionInitArgs,
+        target_hw_config_type: Optional[HWConfigType],
+    ) -> "AutoQPrecisionInitParams":
         dict_copy = autoq_init_config_dict.copy()
-        dump_autoq_data = dict_copy.pop('dump_init_precision_data', False)
-        iter_number = dict_copy.pop('iter_number', AUTOQ_ITER_NUMBER)
-        compression_ratio = dict_copy.pop('compression_ratio', 0.15)
-        eval_subset_ratio = dict_copy.pop('eval_subset_ratio', AUTOQ_EVAL_SUBSET_RATIO)
-        warmup_iter_number = dict_copy.pop('warmup_iter_number', AUTOQ_WARMUP_ITER_NUMBER)
-        skip_constraint = dict_copy.pop('skip_constraint', False)
-        finetune = dict_copy.pop('finetune', False)
-        bits = dict_copy.pop('bits', PRECISION_INIT_BITWIDTHS)
+        dump_autoq_data = dict_copy.pop("dump_init_precision_data", False)
+        iter_number = dict_copy.pop("iter_number", AUTOQ_ITER_NUMBER)
+        compression_ratio = dict_copy.pop("compression_ratio", 0.15)
+        eval_subset_ratio = dict_copy.pop("eval_subset_ratio", AUTOQ_EVAL_SUBSET_RATIO)
+        warmup_iter_number = dict_copy.pop("warmup_iter_number", AUTOQ_WARMUP_ITER_NUMBER)
+        skip_constraint = dict_copy.pop("skip_constraint", False)
+        finetune = dict_copy.pop("finetune", False)
+        bits = dict_copy.pop("bits", PRECISION_INIT_BITWIDTHS)
 
         return cls(
             user_init_args=user_init_args,
             dump_autoq_data=dump_autoq_data,
             iter_number=iter_number,
             warmup_iter_number=warmup_iter_number,
             ddpg_hparams_dict=dict_copy,
             hw_cfg_type=target_hw_config_type,
             compression_ratio=compression_ratio,
             eval_subset_ratio=eval_subset_ratio,
             skip_constraint=skip_constraint,
             finetune=finetune,
-            bits=bits)
+            bits=bits,
+        )
 
 
 class AutoQPrecisionInitializer(BasePrecisionInitializer):
-    def __init__(self,
-                 algo: 'ExperimentalQuantizationController',
-                 params: AutoQPrecisionInitParams,
-                 hw_precision_constraints: HardwareQuantizationConstraints):
+    def __init__(
+        self,
+        algo: "ExperimentalQuantizationController",
+        params: AutoQPrecisionInitParams,
+        hw_precision_constraints: HardwareQuantizationConstraints,
+    ):
         super().__init__(algo, params, hw_precision_constraints)
         self.quantization_controller = algo
         self._params = params
         self._init_args = params.user_init_args
         self._dump_autoq_data = params.dump_autoq_data
         self._iter_number = params.iter_number
         self._warmup_iter_number = params.warmup_iter_number
         self._ddpg_hparams_override = params.ddpg_hparams_dict
         self._hw_cfg_type = params.hw_cfg_type
 
     def apply_init(self) -> SingleConfigQuantizerSetup:
-        from nncf.torch.automl.environment.quantization_env import QuantizationEnv #pylint: disable=cyclic-import
-        from nncf.torch.automl.agent.ddpg.ddpg import DDPG #pylint: disable=cyclic-import
-        from nncf.common.utils.debug import DEBUG_LOG_DIR #pylint: disable=cyclic-import
+        from nncf.common.utils.debug import DEBUG_LOG_DIR  # pylint: disable=cyclic-import
+        from nncf.torch.automl.agent.ddpg.ddpg import DDPG  # pylint: disable=cyclic-import
+        from nncf.torch.automl.environment.quantization_env import QuantizationEnv  # pylint: disable=cyclic-import
 
         if self._dump_autoq_data or is_debug():
-            dump_dir = self._init_args.config.get('log_dir', None)
+            dump_dir = self._init_args.config.get("log_dir", None)
             if dump_dir is None:
                 dump_dir = DEBUG_LOG_DIR
             self.dump_dir = Path(dump_dir) / Path("autoq") / Path("autoq_agent_dump")
             self.dump_dir.mkdir(parents=True, exist_ok=True)
 
-            self.policy_dict = OrderedDict() #key: episode
-            self.best_policy_dict = OrderedDict() #key: episode
+            self.policy_dict = OrderedDict()  # key: episode
+            self.best_policy_dict = OrderedDict()  # key: episode
 
-            self._init_args.config['episodic_nncfcfg'] = str(self.dump_dir / "episodic_nncfcfg")
-            os.makedirs(self._init_args.config['episodic_nncfcfg'], exist_ok=True)
+            self._init_args.config["episodic_nncfcfg"] = str(self.dump_dir / "episodic_nncfcfg")
+            os.makedirs(self._init_args.config["episodic_nncfcfg"], exist_ok=True)
 
             try:
                 from torch.utils.tensorboard import SummaryWriter
+
                 self.tb_writer = SummaryWriter(self.dump_dir)
                 # log compression config to tensorboard
-                self.tb_writer.add_text('AutoQ/run_config',
-                                         json.dumps(self._init_args.config['compression'],
-                                         indent=4, sort_keys=False).replace("\n", "\n\n"), 0)
+                self.tb_writer.add_text(
+                    "AutoQ/run_config",
+                    json.dumps(self._init_args.config["compression"], indent=4, sort_keys=False).replace("\n", "\n\n"),
+                    0,
+                )
             except ModuleNotFoundError:
-                nncf_logger.warning("Tensorboard installation not found! Install tensorboard Python package "
-                                    "in order for AutoQ tensorboard statistics data to be dumped")
+                nncf_logger.warning(
+                    "Tensorboard installation not found! Install tensorboard Python package "
+                    "in order for AutoQ tensorboard statistics data to be dumped"
+                )
 
         start_ts = datetime.now()
 
-        from nncf.torch.automl.environment.quantization_env import QuantizationEnvParams #pylint: disable=cyclic-import
-        env_params = QuantizationEnvParams(compression_ratio=self._params.compression_ratio,
+        from nncf.torch.automl.environment.quantization_env import (
+            QuantizationEnvParams,  # pylint: disable=cyclic-import
+        )
+
+        env_params = QuantizationEnvParams(
+            compression_ratio=self._params.compression_ratio,
             eval_subset_ratio=self._params.eval_subset_ratio,
             skip_constraint=self._params.skip_constraint,
             performant_bw=True,
             finetune=self._params.finetune,
             bits=self._params.bits,
             dump_init_precision_data=self._dump_autoq_data,
-            log_dir=Path(DEBUG_LOG_DIR) / Path("autoq"))
+            log_dir=Path(DEBUG_LOG_DIR) / Path("autoq"),
+        )
 
         # Instantiate Quantization Environment
         env = QuantizationEnv(
             self._model,
             self.quantization_controller,
             self._hw_precision_constraints,
             self._init_args.data_loader,
             self._init_args.eval_fn,
             hw_config_type=self._hw_cfg_type,
-            params=env_params)
+            params=env_params,
+        )
 
         nb_state = len(env.state_list)
         nb_action = 1
 
         # Control buffer length at run manager level
         if "warmup_iter_number" not in self._ddpg_hparams_override:
             self._ddpg_hparams_override["warmup_iter_number"] = self._warmup_iter_number
-        self._ddpg_hparams_override["rmsize"] = \
-            self._warmup_iter_number * (len(env.master_df)+1)
+        self._ddpg_hparams_override["rmsize"] = self._warmup_iter_number * (len(env.master_df) + 1)
 
         # Instantiate Automation Agent
         agent = DDPG(nb_state, nb_action, self._iter_number, hparam_override=self._ddpg_hparams_override)
 
         if self._dump_autoq_data and self.tb_writer is not None:
             # Need to replace '|' in nodestr (QuantizerId/QuantizerPointId)
             # to '+' as it is a special character in markdown
-            temp_df = deepcopy(env.master_df[env.state_list + ['n_op']])
-            temp_df["modified_nodestr"] = list(map(lambda x: x.replace("|","+"), temp_df.index.tolist()))
+            temp_df = deepcopy(env.master_df[env.state_list + ["n_op"]])
+            temp_df["modified_nodestr"] = list(map(lambda x: x.replace("|", "+"), temp_df.index.tolist()))
             temp_df = temp_df.set_index("modified_nodestr").reset_index()
-            self.tb_writer.add_text('AutoQ/state_embedding', temp_df.to_markdown())
+            self.tb_writer.add_text("AutoQ/state_embedding", temp_df.to_markdown())
 
         best_policy, best_reward = self._search(agent, env)
 
         end_ts = datetime.now()
 
         final_qid_vs_qconfig_map = env.select_config_for_actions(best_policy)
 
         final_quantizer_setup = self.quantization_controller.get_quantizer_setup_for_current_state()
         for qp_id, qconf in final_qid_vs_qconfig_map.items():
             final_quantizer_setup.quantization_points[qp_id].qconfig = qconf
 
         str_bw = [str(element) for element in self.get_bitwidth_per_scope(final_quantizer_setup)]
-        nncf_logger.info('\n'.join(['[AutoQ]\n\"bitwidth_per_scope\": [', ',\n'.join(str_bw), ']']))
-        nncf_logger.info(f'[AutoQ] best_reward: {best_reward}')
-        nncf_logger.info(f'[AutoQ] best_policy: {best_policy}')
+        nncf_logger.info("\n".join(['[AutoQ]\n"bitwidth_per_scope": [', ",\n".join(str_bw), "]"]))
+        nncf_logger.info(f"[AutoQ] best_reward: {best_reward}")
+        nncf_logger.info(f"[AutoQ] best_policy: {best_policy}")
         nncf_logger.info("[AutoQ] Search completed.")
         nncf_logger.info("[AutoQ] Elapsed time of AutoQ Precision Initialization (): {}".format(end_ts - start_ts))
         return final_quantizer_setup
 
-
-    def _search(self, agent: 'DDPG', env: 'QuantizationEnv') -> Tuple[pd.Series, float]:
+    def _search(self, agent: "DDPG", env: "QuantizationEnv") -> Tuple[pd.Series, float]:
         # pylint: disable=too-many-branches,too-many-statements
         best_reward = -math.inf
         episode = 0
-        episode_reward = 0.
+        episode_reward = 0.0
         observation = None
         transition_buffer = []  # Transition buffer
 
         while episode < self._iter_number:  # counting based on episode
             episode_start_ts = time.time()
             if observation is None:
                 # reset if it is the start of episode
@@ -228,229 +244,244 @@
 
             # update
             episode_reward += reward
             observation = deepcopy(observation2)
 
             if done:  # end of episode
                 nncf_logger.info(
-                    f'## Episode[{episode}], '
-                    f'reward: {episode_reward:.3f}, '
+                    f"## Episode[{episode}], "
+                    f"reward: {episode_reward:.3f}, "
                     f'acc: {info["accuracy"]:.3f}, '
                     f'model_ratio: {info["model_ratio"]:.3f}, '
                     f'model_size(MB): {info["model_size"] / 8e6:.2f}, '
-                    f'BOP_ratio: {info["bop_ratio"]:.3f}\n')
+                    f'BOP_ratio: {info["bop_ratio"]:.3f}\n'
+                )
 
                 # Replay Buffer Management
-                if agent.memory.nb_entries % (len(env.master_df)+1) > 0:
+                if agent.memory.nb_entries % (len(env.master_df) + 1) > 0:
                     raise ValueError("logical bug in buffer management, uneven episode length")
-                if agent.memory.limit % (len(env.master_df)+1) > 0:
+                if agent.memory.limit % (len(env.master_df) + 1) > 0:
                     raise ValueError("replay buffer size must be divisible by episode step length")
 
                 if agent.memory.nb_entries + len(transition_buffer) >= agent.memory.limit:
-                    step_reward_per_episode = agent.memory.rewards.data[::(len(transition_buffer)+1)]
-                    sorted_index_of_episodes = np.argsort(step_reward_per_episode) # ascending order
+                    step_reward_per_episode = agent.memory.rewards.data[:: (len(transition_buffer) + 1)]
+                    sorted_index_of_episodes = np.argsort(step_reward_per_episode)  # ascending order
 
                     # Retain the top 30% of highest rewarded episodes,
                     # discard by sampling an episode uniformly from the lower 70% episodes
-                    discard_candidates = sorted_index_of_episodes[:int(len(sorted_index_of_episodes)*.7)]
+                    discard_candidates = sorted_index_of_episodes[: int(len(sorted_index_of_episodes) * 0.7)]
                     discard_episode = np.random.choice(discard_candidates)
 
-                    discard_start_index = (discard_episode)*(len(transition_buffer)+1)
-                    discard_end_index = (discard_episode+1)*(len(transition_buffer)+1)
+                    discard_start_index = (discard_episode) * (len(transition_buffer) + 1)
+                    discard_end_index = (discard_episode + 1) * (len(transition_buffer) + 1)
 
                     agent.memory.discard(slice(discard_start_index, discard_end_index))
                 # = EO Replay Buffer Management
 
                 final_reward = transition_buffer[-1][0]
-                r_per_step = final_reward/len(env.master_df)
+                r_per_step = final_reward / len(env.master_df)
 
                 for i, (_, s_t, _, a_t, done) in enumerate(transition_buffer):
                     # Revision of prev_action as it could be modified by constrainer -------
                     if i == 0:
                         prev_action = 0.0
                     else:
-                        prev_action = env.master_df['action'][i-1] / 8 #ducktape scaling
-                    if prev_action != s_t['prev_action']:
-                        s_t['prev_action'] = prev_action
+                        prev_action = env.master_df["action"][i - 1] / 8  # ducktape scaling
+                    if prev_action != s_t["prev_action"]:
+                        s_t["prev_action"] = prev_action
                     # EO ------------------------
                     agent.observe(r_per_step, s_t, a_t, done)
 
-                agent.memory.append(
-                    observation,
-                    agent.select_action(observation, episode=episode),
-                    0., False
-                )
+                agent.memory.append(observation, agent.select_action(observation, episode=episode), 0.0, False)
 
                 # update DDPG networks, note that this loop must not be
                 # in the loop above to avoid non-numeric value in replay buffer
                 for i, (_, s_t, _, a_t, done) in enumerate(transition_buffer):
                     if episode >= agent.warmup_iter_number:
                         for _ in range(agent.n_update):
                             agent.update_policy()
 
                 # reset
                 observation = None
-                episode_reward = 0.
+                episode_reward = 0.0
                 transition_buffer = []
 
                 value_loss = agent.get_value_loss()
                 policy_loss = agent.get_policy_loss()
                 delta = agent.get_delta()
 
                 nncf_stats = env.qctrl.statistics()
-                bit_stats_df = pd.DataFrame.from_dict(
-                    [nncf_stats.quantization.num_wq_per_bitwidth,
-                     nncf_stats.quantization.num_aq_per_bitwidth])\
-                         .fillna(0).astype(int).rename(index={0:'WQ',1:'AQ'}).transpose().sort_index(ascending=False)
-                bit_stats_df.index.name = 'bitwidth'
-                bit_stats_df=bit_stats_df.reset_index()
+                bit_stats_df = (
+                    pd.DataFrame.from_dict(
+                        [nncf_stats.quantization.num_wq_per_bitwidth, nncf_stats.quantization.num_aq_per_bitwidth]
+                    )
+                    .fillna(0)
+                    .astype(int)
+                    .rename(index={0: "WQ", 1: "AQ"})
+                    .transpose()
+                    .sort_index(ascending=False)
+                )
+                bit_stats_df.index.name = "bitwidth"
+                bit_stats_df = bit_stats_df.reset_index()
 
                 if final_reward > best_reward:
                     best_reward = final_reward
-                    best_policy = deepcopy(env.master_df['action'])
-                    info_tuple = (episode, best_reward, info['accuracy'], info['model_ratio'], info['bop_ratio'])
+                    best_policy = deepcopy(env.master_df["action"])
+                    info_tuple = (episode, best_reward, info["accuracy"], info["model_ratio"], info["bop_ratio"])
                     self._dump_best_episode(info_tuple, bit_stats_df, env)
-                    log_str = f'## Episode[{episode}] ' \
-                              f'New best policy: {best_policy.values.tolist()}, ' \
-                              f'reward: {best_reward:.3f}, ' \
-                              f'acc: {info["accuracy"]:.3f}, ' \
-                              f'model_ratio: {info["model_ratio"]:.3f},' \
-                              f' BOP_ratio: {info["bop_ratio"]:.3f}'
+                    log_str = (
+                        f"## Episode[{episode}] "
+                        f"New best policy: {best_policy.values.tolist()}, "
+                        f"reward: {best_reward:.3f}, "
+                        f'acc: {info["accuracy"]:.3f}, '
+                        f'model_ratio: {info["model_ratio"]:.3f},'
+                        f' BOP_ratio: {info["bop_ratio"]:.3f}'
+                    )
                     nncf_logger.info(f"\033[92m {log_str}\033[00m")
 
-                episodic_info_tuple = (episode, final_reward, best_reward,
-                                       info['accuracy'], info['model_ratio'], info['bop_ratio'],
-                                       value_loss, policy_loss, delta)
+                episodic_info_tuple = (
+                    episode,
+                    final_reward,
+                    best_reward,
+                    info["accuracy"],
+                    info["model_ratio"],
+                    info["bop_ratio"],
+                    value_loss,
+                    policy_loss,
+                    delta,
+                )
                 self._dump_episode(episodic_info_tuple, bit_stats_df, env, agent)
 
                 episode_elapsed = time.time() - episode_start_ts
 
                 nncf_logger.info(f'## Episode[{episode}] Policy: \n{env.master_df["action"].to_string()}\n')
-                nncf_logger.info(f'## Episode[{episode}] Elapsed: {episode_elapsed:.3f}\n')
+                nncf_logger.info(f"## Episode[{episode}] Elapsed: {episode_elapsed:.3f}\n")
 
                 episode += 1
 
         return best_policy, best_reward
 
-
-    def _dump_best_episode(self, info_tuple: Tuple, bit_stats_df: pd.DataFrame, env: 'QuantizationEnv'):
+    def _dump_best_episode(self, info_tuple: Tuple, bit_stats_df: pd.DataFrame, env: "QuantizationEnv"):
         if self._dump_autoq_data:
             episode = info_tuple[0]
-            self.best_policy_dict[episode] = env.master_df['action'].astype('int')
+            self.best_policy_dict[episode] = env.master_df["action"].astype("int")
 
-            pd.DataFrame(
-                self.best_policy_dict.values(), index=self.best_policy_dict.keys()).T.sort_index(
-                    axis=1, ascending=False).to_csv(
-                        osp.join(self.dump_dir, "best_policy.csv"), index_label="nodestr")
+            pd.DataFrame(self.best_policy_dict.values(), index=self.best_policy_dict.keys()).T.sort_index(
+                axis=1, ascending=False
+            ).to_csv(osp.join(self.dump_dir, "best_policy.csv"), index_label="nodestr")
 
             best_policy_string = self._generate_tensorboard_logging_string(
-                bit_stats_df, env.master_df, info_tuple, env.skip_constraint)
+                bit_stats_df, env.master_df, info_tuple, env.skip_constraint
+            )
 
             list_of_dump_dict = []
             for i, _ in enumerate(env.groups_of_adjacent_quantizers):
                 list_of_dump_dict.append(env.master_df.loc[env.adjq_groupwise_df_lut_keys[i], ["action"]].to_dict())
-            best_policy_string += "\t\n\t# Precision(s) per Group of Adjacent Quantizers\n\t" \
-                                    + json.dumps(list_of_dump_dict, indent=4).replace("\n","\n\t") + "\n\n"
-
-            self.tb_writer.add_text('AutoQ/best_policy', best_policy_string, episode)
-
-
-    def _dump_episode(self,
-                      episodic_info_tuple: Tuple, bit_stats_df: pd.DataFrame,
-                      env: 'QuantizationEnv', agent: 'DDPG'):
+            best_policy_string += (
+                "\t\n\t# Precision(s) per Group of Adjacent Quantizers\n\t"
+                + json.dumps(list_of_dump_dict, indent=4).replace("\n", "\n\t")
+                + "\n\n"
+            )
+
+            self.tb_writer.add_text("AutoQ/best_policy", best_policy_string, episode)
+
+    def _dump_episode(
+        self, episodic_info_tuple: Tuple, bit_stats_df: pd.DataFrame, env: "QuantizationEnv", agent: "DDPG"
+    ):
         if self._dump_autoq_data:
             episode, final_reward, _, accuracy, model_ratio, bop_ratio, _, _, _ = episodic_info_tuple
 
-            current_bitwidth_per_scope = self.get_bitwidth_per_scope(
-                env.qctrl.get_quantizer_setup_for_current_state())
+            current_bitwidth_per_scope = self.get_bitwidth_per_scope(env.qctrl.get_quantizer_setup_for_current_state())
 
             current_episode_nncfcfg = deepcopy(self._init_args.config)
-            current_episode_nncfcfg['compression']['initializer']['precision'] = \
-                {"bitwidth_per_scope": current_bitwidth_per_scope}
+            current_episode_nncfcfg["compression"]["initializer"]["precision"] = {
+                "bitwidth_per_scope": current_bitwidth_per_scope
+            }
 
             # Save nncf compression cfg
-            episode_cfgfile =  '{0}/{1:03d}_nncfcfg.json'.format(
-                str(self._init_args.config['episodic_nncfcfg']), episode)
+            episode_cfgfile = "{0}/{1:03d}_nncfcfg.json".format(
+                str(self._init_args.config["episodic_nncfcfg"]), episode
+            )
 
             with safe_open(Path(episode_cfgfile), "w") as outfile:
                 json.dump(current_episode_nncfcfg, outfile, indent=4, sort_keys=False)
 
-            self.policy_dict[episode] = env.master_df['action'].astype('int')
-            pd.DataFrame(
-                self.policy_dict.values(), index=self.policy_dict.keys()).T.sort_index(axis=1, ascending=False).to_csv(
-                    osp.join(self.dump_dir, "policy_per_episode.csv"), index_label="nodestr")
+            self.policy_dict[episode] = env.master_df["action"].astype("int")
+            pd.DataFrame(self.policy_dict.values(), index=self.policy_dict.keys()).T.sort_index(
+                axis=1, ascending=False
+            ).to_csv(osp.join(self.dump_dir, "policy_per_episode.csv"), index_label="nodestr")
 
             # log current episode policy and feedback as text
             info_tuple = (episode, final_reward, accuracy, model_ratio, bop_ratio)
             current_strategy_string = self._generate_tensorboard_logging_string(
-                bit_stats_df, env.master_df, info_tuple, env.skip_constraint)
+                bit_stats_df, env.master_df, info_tuple, env.skip_constraint
+            )
 
             if env.performant_bw is True:
                 list_of_dump_dict = []
                 for i, _ in enumerate(env.groups_of_adjacent_quantizers):
                     list_of_dump_dict.append(
-                        env.master_df.loc[
-                            env.adjq_groupwise_df_lut_keys[i], ["action", "action_aligned"]
-                        ].to_dict()
+                        env.master_df.loc[env.adjq_groupwise_df_lut_keys[i], ["action", "action_aligned"]].to_dict()
                     )
-                current_strategy_string += "\t\n\t# Precision(s) per Group of Adjacent Quantizers\n\t" \
-                                            + json.dumps(list_of_dump_dict, indent=4).replace("\n","\n\t") + "\n\n"
+                current_strategy_string += (
+                    "\t\n\t# Precision(s) per Group of Adjacent Quantizers\n\t"
+                    + json.dumps(list_of_dump_dict, indent=4).replace("\n", "\n\t")
+                    + "\n\n"
+                )
 
-            self.tb_writer.add_text('AutoQ/current_policy', current_strategy_string, episode)
+            self.tb_writer.add_text("AutoQ/current_policy", current_strategy_string, episode)
 
             # visualization over episode
             if self.tb_writer is not None:
                 self._add_to_tensorboard(self.tb_writer, episodic_info_tuple)
 
             if episode % int((self._iter_number + 10) / 10) == 0:
                 agent.save_model(self.dump_dir)
 
+    def _add_to_tensorboard(self, tb_writer: "SummaryWriter", log_tuple: Tuple):
+        episode, final_reward, best_reward, accuracy, model_ratio, bop_ratio, value_loss, policy_loss, delta = log_tuple
 
-    def _add_to_tensorboard(self, tb_writer: 'SummaryWriter', log_tuple: Tuple):
-        episode, final_reward, best_reward, \
-            accuracy, model_ratio, bop_ratio, \
-                value_loss, policy_loss, delta = log_tuple
-
-        tb_writer.add_scalar('AutoQ/reward/last', final_reward, episode)
-        tb_writer.add_scalar('AutoQ/reward/best', best_reward, episode)
-        tb_writer.add_scalar('AutoQ/accuracy', accuracy, episode)
-        tb_writer.add_scalar('AutoQ/model_ratio', model_ratio, episode)
-        tb_writer.add_scalar('AutoQ/bop_ratio', bop_ratio, episode)
-        tb_writer.add_scalar('AutoQ/agent/value_loss', value_loss, episode)
-        tb_writer.add_scalar('AutoQ/agent/policy_loss', policy_loss, episode)
-        tb_writer.add_scalar('AutoQ/agent/delta', delta, episode)
-
-
-    def _generate_tensorboard_logging_string(self,
-                                             bit_stats_df: pd.DataFrame, master_df: pd.DataFrame,
-                                             info_tuple: Tuple, skip_constraint=False) -> str:
-        qdf = master_df # For readibility
+        tb_writer.add_scalar("AutoQ/reward/last", final_reward, episode)
+        tb_writer.add_scalar("AutoQ/reward/best", best_reward, episode)
+        tb_writer.add_scalar("AutoQ/accuracy", accuracy, episode)
+        tb_writer.add_scalar("AutoQ/model_ratio", model_ratio, episode)
+        tb_writer.add_scalar("AutoQ/bop_ratio", bop_ratio, episode)
+        tb_writer.add_scalar("AutoQ/agent/value_loss", value_loss, episode)
+        tb_writer.add_scalar("AutoQ/agent/policy_loss", policy_loss, episode)
+        tb_writer.add_scalar("AutoQ/agent/delta", delta, episode)
+
+    def _generate_tensorboard_logging_string(
+        self, bit_stats_df: pd.DataFrame, master_df: pd.DataFrame, info_tuple: Tuple, skip_constraint=False
+    ) -> str:
+        qdf = master_df  # For readibility
         episode, reward, accuracy, model_ratio, bop_ratio = info_tuple
 
         text_string = bit_stats_df.to_markdown() + "\n\n\n"
         text_string += "Episode: {:>4}, Reward: {:.3f}, ".format(episode, reward)
-        text_string += "Accuracy: {:.3f}, Model_Size_Ratio: {:.3f}, BOP_Ratio: {:.3f}\n\n\n"\
-            .format(accuracy, model_ratio, bop_ratio)
+        text_string += "Accuracy: {:.3f}, Model_Size_Ratio: {:.3f}, BOP_Ratio: {:.3f}\n\n\n".format(
+            accuracy, model_ratio, bop_ratio
+        )
 
         for _, row_id in enumerate(qdf.index.tolist()):
-            Qtype = '(WQ)' if qdf.is_wt_quantizer[row_id] else '(AQ)'
-
-            if skip_constraint is False and \
-                qdf.loc[row_id, 'action'] != qdf.loc[row_id, 'unconstrained_action']:
+            Qtype = "(WQ)" if qdf.is_wt_quantizer[row_id] else "(AQ)"
 
+            if skip_constraint is False and qdf.loc[row_id, "action"] != qdf.loc[row_id, "unconstrained_action"]:
                 text_string += "\t{} <= {} | {} {} \n".format(
-                    str(int(qdf.loc[row_id, 'action'])), str(int(qdf.loc[row_id, 'unconstrained_action'])),
-                    Qtype, row_id)
+                    str(int(qdf.loc[row_id, "action"])),
+                    str(int(qdf.loc[row_id, "unconstrained_action"])),
+                    Qtype,
+                    row_id,
+                )
 
             else:
-                text_string += "\t{} | {} {} \n".format(
-                    str(int(qdf.loc[row_id, 'action'])), Qtype, row_id)
+                text_string += "\t{} | {} {} \n".format(str(int(qdf.loc[row_id, "action"])), Qtype, row_id)
 
         return text_string
 
+
 def map_precision(action: float) -> int:
     precision_set = [2, 4, 8]
     precision_set = np.array(sorted(precision_set))
     tuned_point = precision_set + 3
     max_bit = max(precision_set)
 
     i = None
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/base_init.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/base_init.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,62 +8,65 @@
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
 from collections import OrderedDict
-from typing import Dict
-from typing import List
-from typing import Union
-
 from copy import deepcopy
+from typing import Dict, List, Union
 
 from nncf.common.graph import NNCFNodeName
+from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
+from nncf.common.quantization.structs import QuantizerId
+from nncf.common.quantization.structs import WeightQuantizerId
 from nncf.torch.dynamic_graph.scope import Scope
 from nncf.torch.module_operations import UpdateWeight
 from nncf.torch.nncf_network import ExtraCompressionModuleType
 from nncf.torch.nncf_network import NNCFNetwork
-from nncf.torch.quantization.layers import BaseQuantizer
 from nncf.torch.quantization.layers import QUANTIZATION_MODULES
+from nncf.torch.quantization.layers import BaseQuantizer
 from nncf.torch.quantization.precision_constraints import HardwareQuantizationConstraints
-from nncf.common.quantization.structs import QuantizerId
-from nncf.common.quantization.structs import WeightQuantizerId
-from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
 from nncf.torch.quantization.structs import WeightQuantizerInfo
 from nncf.torch.structures import NNCFExtraConfigStruct
 from nncf.torch.utils import get_all_modules_by_type
 
 
 class BasePrecisionInitParams:
-    def __init__(self,
-                 user_init_args: NNCFExtraConfigStruct = None):
+    def __init__(self, user_init_args: NNCFExtraConfigStruct = None):
         self.user_init_args = user_init_args
 
 
 class BasePrecisionInitializer:
-    def __init__(self, algo: 'ExperimentalQuantizationController',
-                 params: BasePrecisionInitParams,
-                 hw_precision_constraints: HardwareQuantizationConstraints = None):
+    def __init__(
+        self,
+        algo: "ExperimentalQuantizationController",
+        params: BasePrecisionInitParams,
+        hw_precision_constraints: HardwareQuantizationConstraints = None,
+    ):
         self._algo = algo
         self._model = self._algo._model  # type: NNCFNetwork
         all_quantizers = algo.all_quantizations
         self._hw_precision_constraints = hw_precision_constraints
         self.original_precisions = {q_id: quantizer.num_bits for q_id, quantizer in all_quantizers.items()}
-        self._quantizers_handler = WeightQuantizersHandler(self._model, self._algo.weight_quantizers,
-                                                           self._hw_precision_constraints)
+        self._quantizers_handler = WeightQuantizersHandler(
+            self._model, self._algo.weight_quantizers, self._hw_precision_constraints
+        )
         quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]
-        self._weight_quantizations_by_execution_order = self._quantizers_handler. \
-            get_weight_quantizers_in_execution_order_per_id()
+        self._weight_quantizations_by_execution_order = (
+            self._quantizers_handler.get_weight_quantizers_in_execution_order_per_id()
+        )
 
         self._all_quantizers_per_scope = get_all_modules_by_type(
-            self._model.get_compression_modules_by_type(ExtraCompressionModuleType.EXTERNAL_QUANTIZER),
-            quantization_types)
+            self._model.nncf.get_compression_modules_by_type(ExtraCompressionModuleType.EXTERNAL_QUANTIZER),
+            quantization_types,
+        )
         self._all_quantizers_per_scope.update(
-            self._quantizers_handler.get_all_weight_quantizers_in_execution_order_per_scope())
+            self._quantizers_handler.get_all_weight_quantizers_in_execution_order_per_scope()
+        )
 
     def apply_init(self) -> SingleConfigQuantizerSetup:
         raise NotImplementedError
 
     @staticmethod
     def get_bitwidth_per_scope(quantizer_setup: SingleConfigQuantizerSetup) -> List[List[Union[int, str]]]:
         scope_vs_bitwidth = {}
@@ -88,33 +91,35 @@
     def get_owning_module_scope_from_wq_scope(wq_scope: Scope) -> Scope:
         retval = deepcopy(wq_scope)
         retval.pop()
         retval.pop()
         retval.pop()
         return retval
 
-    def __init__(self, model: NNCFNetwork,
-                 weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
-                 constraints: HardwareQuantizationConstraints):
+    def __init__(
+        self,
+        model: NNCFNetwork,
+        weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
+        constraints: HardwareQuantizationConstraints,
+    ):
         self._wq_affected_module_node_name_vs_qid_dict = {k.target_node_name: k for k in weight_quantizers.keys()}
         self._quantizer_module_scope_vs_qid_dict = {}  # type: Dict[Scope, WeightQuantizerId]
         self._skipped_quantized_weight_node_names = []
         self._skipped_weight_quantizers = {}  # type: Dict[WeightQuantizerId, BaseQuantizer]
         self._weight_quantizers_in_execution_order_per_scope = OrderedDict()  # type: Dict[Scope, BaseQuantizer]
         self._weight_quantizers_in_execution_order = OrderedDict()  # type: Dict[WeightQuantizerId, BaseQuantizer]
 
         quantization_types = [class_type.__name__ for class_type in QUANTIZATION_MODULES.registry_dict.values()]
-        weight_module_dict = model.get_nncf_wrapped_model()
-        quantizers_in_execution_order_per_scope = get_all_modules_by_type(weight_module_dict,
-                                                                          quantization_types)
+        weight_module_dict = model
+        quantizers_in_execution_order_per_scope = get_all_modules_by_type(weight_module_dict, quantization_types)
 
         for scope, quantizer in quantizers_in_execution_order_per_scope.items():
             if self.is_wq_scope(scope):
                 affected_module_scope = self.get_owning_module_scope_from_wq_scope(scope)
-                affected_module_node = model.get_original_graph().get_op_nodes_in_scope(affected_module_scope)[0]
+                affected_module_node = model.nncf.get_original_graph().get_op_nodes_in_scope(affected_module_scope)[0]
                 if affected_module_node.node_name in self._wq_affected_module_node_name_vs_qid_dict:
                     qid = self._wq_affected_module_node_name_vs_qid_dict[affected_module_node.node_name]
                     if len(constraints.get_all_unique_bitwidths(qid)) != 1:
                         self._weight_quantizers_in_execution_order_per_scope[scope] = quantizer
                         self._weight_quantizers_in_execution_order[qid] = quantizer
                     else:
                         self._skipped_quantized_weight_node_names.append(affected_module_node.node_name)
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/bitwidth_graph.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/bitwidth_graph.py`

 * *Files 9% similar despite different names*

```diff
@@ -12,119 +12,126 @@
 """
 from collections import defaultdict
 from typing import Dict
 
 import networkx as nx
 
 from nncf.common.graph import NNCFGraph
+from nncf.common.logging import nncf_logger
+from nncf.common.quantization.structs import NonWeightQuantizerId
 from nncf.torch.layers import NNCFConv2d
 from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.algo import QuantizationController
 from nncf.torch.quantization.precision_init.adjacent_quantizers import GroupsOfAdjacentQuantizers
-from nncf.common.quantization.structs import NonWeightQuantizerId
 from nncf.torch.quantization.structs import NonWeightQuantizerInfo
 
 
-from nncf.common.logging import nncf_logger
-
-
 class BitwidthGraph:
-    def __init__(self, algo_ctrl: QuantizationController, model: NNCFNetwork,
-                       groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
-                       add_flops=False):
+    def __init__(
+        self,
+        algo_ctrl: QuantizationController,
+        model: NNCFNetwork,
+        groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
+        add_flops=False,
+    ):
         # pylint:disable=too-many-branches
         # pylint:disable=too-many-statements
-        nncf_graph = model.get_graph()
+        nncf_graph = model.nncf.get_graph()
         self._nx_graph = nncf_graph.get_graph_for_structure_analysis()
         if add_flops:
-            flops_per_module = model.get_flops_per_module()
+            flops_per_module = model.nncf.get_flops_per_module()
 
             flops_vs_node_group = defaultdict(set)  # type: Dict[int, Tuple[int, Set[NNCFNode]]]
             for idx, module_node_name_and_flops in enumerate(flops_per_module.items()):
                 module_node_name, flops = module_node_name_and_flops
                 node_set = set(nncf_graph.get_op_nodes_in_scope(nncf_graph.get_scope_by_node_name(module_node_name)))
                 flops_vs_node_group[idx] = (flops, node_set)
 
         grouped_mode = bool(groups_of_adjacent_quantizers)
         for node_key in nncf_graph.get_all_node_keys():
             node = nncf_graph.get_node_by_key(node_key)
-            color = ''
+            color = ""
             operator_name = node.node_type
-            module = model.get_containing_module(node.node_name)
+            module = model.nncf.get_containing_module(node.node_name)
             if isinstance(module, NNCFConv2d):
-                color = 'lightblue'
+                color = "lightblue"
                 if module.groups == module.in_channels and module.in_channels > 1:
-                    operator_name = 'DW_Conv2d'
-                    color = 'purple'
-                kernel_size = 'x'.join(map(str, module.kernel_size))
-                operator_name += f'_k{kernel_size}'
+                    operator_name = "DW_Conv2d"
+                    color = "purple"
+                kernel_size = "x".join(map(str, module.kernel_size))
+                operator_name += f"_k{kernel_size}"
                 padding_values = set(module.padding)
                 padding_enabled = len(padding_values) >= 1 and padding_values.pop()
                 if padding_enabled:
-                    operator_name += '_PAD'
+                    operator_name += "_PAD"
                 if add_flops:
-                    matches = [f_nodes_tpl for idx, f_nodes_tpl in flops_vs_node_group.items()
-                               if node in f_nodes_tpl[1]]
+                    matches = [
+                        f_nodes_tpl for idx, f_nodes_tpl in flops_vs_node_group.items() if node in f_nodes_tpl[1]
+                    ]
                     assert len(matches) == 1
                     flops, affected_nodes = next(iter(matches))
-                    operator_name += f'_FLOPS:{str(flops)}'
+                    operator_name += f"_FLOPS:{str(flops)}"
                     if len(affected_nodes) > 1:
                         node_ids = sorted([n.node_id for n in affected_nodes])
-                        operator_name += "(shared among nodes {})".format(",".join(
-                            [str(node_id) for node_id in node_ids]))
-            operator_name += '_#{}'.format(node.node_id)
+                        operator_name += "(shared among nodes {})".format(
+                            ",".join([str(node_id) for node_id in node_ids])
+                        )
+            operator_name += "_#{}".format(node.node_id)
             target_node_to_draw = self._nx_graph.nodes[node_key]
-            target_node_to_draw['label'] = operator_name
-            target_node_to_draw['style'] = 'filled'
+            target_node_to_draw["label"] = operator_name
+            target_node_to_draw["style"] = "filled"
             if color:
-                target_node_to_draw['color'] = color
+                target_node_to_draw["color"] = color
 
         non_weight_quantizers = algo_ctrl.non_weight_quantizers
-        bitwidth_color_map = {2: 'purple', 4: 'red', 8: 'green', 6: 'orange'}
+        bitwidth_color_map = {2: "purple", 4: "red", 8: "green", 6: "orange"}
         for quantizer_id, quantizer_info in non_weight_quantizers.items():
-            self._paint_activation_quantizer_node(nncf_graph, quantizer_id,
-                                                  quantizer_info, bitwidth_color_map,
-                                                  groups_of_adjacent_quantizers)
+            self._paint_activation_quantizer_node(
+                nncf_graph, quantizer_id, quantizer_info, bitwidth_color_map, groups_of_adjacent_quantizers
+            )
         for wq_id, wq_info in algo_ctrl.weight_quantizers.items():
-            nodes = [nncf_graph.get_node_by_name(tp.target_node_name)
-                     for tp in wq_info.affected_insertions]
+            nodes = [nncf_graph.get_node_by_name(tp.target_node_name) for tp in wq_info.affected_insertions]
             if not nodes:
-                raise AttributeError('Failed to get affected nodes for quantized module node: {}'.format(
-                    wq_id.target_node_name))
+                raise AttributeError(
+                    "Failed to get affected nodes for quantized module node: {}".format(wq_id.target_node_name)
+                )
             preds = [nncf_graph.get_previous_nodes(node) for node in nodes]
             wq_nodes = []
             for pred_list in preds:
                 for pred_node in pred_list:
-                    if 'UpdateWeight' in pred_node.node_name:
+                    if "UpdateWeight" in pred_node.node_name:
                         wq_nodes.append(pred_node)
             assert len(wq_nodes) == 1
 
             node = wq_nodes[0]
             node_id = node.node_id
             key = nncf_graph.get_node_key_by_id(node_id)
             nx_node_to_draw_upon = self._nx_graph.nodes[key]
             quantizer = wq_info.quantizer_module_ref
             bitwidths = quantizer.num_bits
-            nx_node_to_draw_upon['label'] = 'WFQ_[{}]_#{}'.format(quantizer.get_quantizer_config(), str(node_id))
+            nx_node_to_draw_upon["label"] = "WFQ_[{}]_#{}".format(quantizer.get_quantizer_config(), str(node_id))
             if grouped_mode:
-                group_id_str = 'UNDEFINED'
+                group_id_str = "UNDEFINED"
                 group_id = groups_of_adjacent_quantizers.get_group_id_for_quantizer(wq_id)
                 if group_id is None:
-                    nncf_logger.debug(f'No group for weight quantizer for: {wq_id}')
+                    nncf_logger.debug(f"No group for weight quantizer for: {wq_id}")
                 else:
                     group_id_str = str(group_id)
-                nx_node_to_draw_upon['label'] += '_G' + group_id_str
-            nx_node_to_draw_upon['color'] = bitwidth_color_map[bitwidths]
-            nx_node_to_draw_upon['style'] = 'filled'
-
-    def _paint_activation_quantizer_node(self, nncf_graph: NNCFGraph,
-                                         quantizer_id: NonWeightQuantizerId,
-                                         quantizer_info: NonWeightQuantizerInfo,
-                                         bitwidth_color_map: Dict[int, str],
-                                         groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers):
+                nx_node_to_draw_upon["label"] += "_G" + group_id_str
+            nx_node_to_draw_upon["color"] = bitwidth_color_map[bitwidths]
+            nx_node_to_draw_upon["style"] = "filled"
+
+    def _paint_activation_quantizer_node(
+        self,
+        nncf_graph: NNCFGraph,
+        quantizer_id: NonWeightQuantizerId,
+        quantizer_info: NonWeightQuantizerInfo,
+        bitwidth_color_map: Dict[int, str],
+        groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
+    ):
         # pylint:disable=too-many-branches
         affected_insertion_points_list = quantizer_info.affected_insertions
 
         for target_point in affected_insertion_points_list:
             nncf_node_name = target_point.target_node_name
             nncf_node = nncf_graph.get_node_by_name(nncf_node_name)
             node_id = nncf_node.node_id
@@ -143,33 +150,32 @@
                 target_node = None
                 for prev_node in previous_nodes:
                     prev_edge = nncf_graph.get_nx_edge(prev_node, nncf_node)
                     if prev_edge[NNCFGraph.INPUT_PORT_ID_EDGE_ATTR] == input_port_id:
                         target_node = prev_node
                         break
 
-                assert target_node is not None, "Could not find a pre-hook quantizer node for a specific " \
-                                                "input port!"
+                assert target_node is not None, "Could not find a pre-hook quantizer node for a specific input port!"
                 target_nncf_node_id = target_node.node_id
                 target_nncf_node_key = nncf_graph.get_node_key_by_id(target_nncf_node_id)
 
             activation_fq_node = self._nx_graph.nodes[target_nncf_node_key]
             bitwidth = quantizer_info.quantizer_module_ref.num_bits
-            activation_fq_node['color'] = bitwidth_color_map[bitwidth]
-            activation_fq_node['style'] = 'filled'
+            activation_fq_node["color"] = bitwidth_color_map[bitwidth]
+            activation_fq_node["style"] = "filled"
             node_id = activation_fq_node[NNCFGraph.ID_NODE_ATTR]
 
-            activation_fq_node['label'] = 'AFQ_[{}]_#{}'.format(
-                quantizer_info.quantizer_module_ref.get_quantizer_config(),
-                str(node_id))
+            activation_fq_node["label"] = "AFQ_[{}]_#{}".format(
+                quantizer_info.quantizer_module_ref.get_quantizer_config(), str(node_id)
+            )
             grouped_mode = bool(groups_of_adjacent_quantizers)
             if grouped_mode:
-                group_id_str = 'UNDEFINED'
+                group_id_str = "UNDEFINED"
                 group_id = groups_of_adjacent_quantizers.get_group_id_for_quantizer(quantizer_id)
                 if node_id is None:
-                    nncf_logger.debug(f'No group for activation quantizer: {target_nncf_node_key}')
+                    nncf_logger.debug(f"No group for activation quantizer: {target_nncf_node_key}")
                 else:
                     group_id_str = str(group_id)
-                activation_fq_node['label'] += "_G" + group_id_str
+                activation_fq_node["label"] += "_G" + group_id_str
 
     def get(self) -> nx.DiGraph:
         return self._nx_graph
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/compression_ratio.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/compression_ratio.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,38 +1,40 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Dict
 
 from nncf.common.graph import NNCFNodeName
 from nncf.common.quantization.quantizer_setup import QuantizationPointId
 from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
 
 
 class CompressionRatioCalculator:
     """
     Calculates compression ratio - ratio between bits complexity of fully INT8 model and mixed-precision lower-bit one.
     Bit complexity of the model is a sum of bit complexities for each quantized layer, which are a multiplication of
     FLOPS for the layer by number of bits for its quantization. The compression ratio can be used for estimation of
     performance boost for quantized model.
     """
+
     DEFAULT_NUMBER_OF_BITS = 8
 
-    def __init__(self, flops_per_weighted_module_node: Dict[NNCFNodeName, int],
-                 quantizer_setup: SingleConfigQuantizerSetup,
-                 weight_qp_id_per_activation_qp_id: Dict[QuantizationPointId, QuantizationPointId]):
+    def __init__(
+        self,
+        flops_per_weighted_module_node: Dict[NNCFNodeName, int],
+        quantizer_setup: SingleConfigQuantizerSetup,
+        weight_qp_id_per_activation_qp_id: Dict[QuantizationPointId, QuantizationPointId],
+    ):
         self._weight_qp_id_per_activation_qp_id = weight_qp_id_per_activation_qp_id
         self._flops_per_weight_qp_id = {}  # type: Dict[QuantizationPointId, float]
         for qp_id, qp in quantizer_setup.quantization_points.items():
             if qp.is_weight_quantization_point():
                 target_node_name = qp.insertion_point.target_node_name
                 self._flops_per_weight_qp_id[qp_id] = flops_per_weighted_module_node[target_node_name]
         self.maximum_bits_complexity = sum(self._flops_per_weight_qp_id.values()) * self.DEFAULT_NUMBER_OF_BITS
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/hawq_debug.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/hawq_debug.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,36 +17,40 @@
 
 import torch
 from torch import Tensor
 
 from nncf.common.logging import nncf_logger
 from nncf.common.utils.decorators import skip_if_dependency_unavailable
 from nncf.common.utils.dot_file_rw import write_dot_graph
-from nncf.torch.nncf_network import ExtraCompressionModuleType, NNCFNetwork
+from nncf.torch.nncf_network import ExtraCompressionModuleType
+from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.quantization.adjust_padding import add_adjust_padding_nodes
 from nncf.torch.quantization.layers import QUANTIZATION_MODULES
 from nncf.torch.quantization.precision_init.adjacent_quantizers import GroupsOfAdjacentQuantizers
 from nncf.torch.quantization.precision_init.perturbations import PerturbationObserver
 from nncf.torch.quantization.precision_init.perturbations import Perturbations
 from nncf.torch.quantization.precision_init.traces_order import TracesPerLayer
 from nncf.torch.utils import get_all_modules_by_type
 
 
 class HAWQDebugger:
-    def __init__(self,
-                 weight_qconfig_sequences_in_trace_order: List['QConfigSequenceForHAWQToEvaluate'],
-                 perturbations: Perturbations,
-                 weight_observers_for_each_covering_configuration: List[List[PerturbationObserver]],
-                 traces_per_layer: TracesPerLayer,
-                 bitwidths: List[int]):
+    def __init__(
+        self,
+        weight_qconfig_sequences_in_trace_order: List["QConfigSequenceForHAWQToEvaluate"],
+        perturbations: Perturbations,
+        weight_observers_for_each_covering_configuration: List[List[PerturbationObserver]],
+        traces_per_layer: TracesPerLayer,
+        bitwidths: List[int],
+    ):
         self._weight_qconfig_sequences_in_trace_order = weight_qconfig_sequences_in_trace_order
         self._num_weights = len(traces_per_layer.traces_order)
         self._perturbations = perturbations
 
-        from nncf.common.utils.debug import DEBUG_LOG_DIR #pylint: disable=cyclic-import
+        from nncf.common.utils.debug import DEBUG_LOG_DIR  # pylint: disable=cyclic-import
+
         self._dump_dir = Path(DEBUG_LOG_DIR) / Path("hawq_dumps")
         self._dump_dir.mkdir(parents=True, exist_ok=True)
 
         self._traces_order = traces_per_layer.traces_order
         self._traces_per_layer = traces_per_layer.get_all()
 
         num_of_weights = []
@@ -54,146 +58,165 @@
         for i in range(self._num_weights):
             trace_index = self._traces_order.get_execution_index_by_traces_index(i)
             num_of_weights.append(weight_observers_for_each_covering_configuration[0][trace_index].get_numels())
             norm_of_weights.append(weight_observers_for_each_covering_configuration[0][trace_index].get_input_norm())
         self._num_weights_per_layer = torch.Tensor(num_of_weights)
         self._norm_weights_per_layer = torch.Tensor(norm_of_weights)
 
-        bits_in_megabyte = 2 ** 23
+        bits_in_megabyte = 2**23
         self._model_sizes = []
         for qconfig_sequence in self._weight_qconfig_sequences_in_trace_order:
-            size = torch.sum(torch.Tensor([qconfig.num_bits for qconfig in qconfig_sequence]) *
-                             self._num_weights_per_layer).item() / bits_in_megabyte
+            size = (
+                torch.sum(
+                    torch.Tensor([qconfig.num_bits for qconfig in qconfig_sequence]) * self._num_weights_per_layer
+                ).item()
+                / bits_in_megabyte
+            )
             self._model_sizes.append(size)
         self._bitwidths = bitwidths
 
     @staticmethod
     def get_all_quantizers_per_full_scope(model):
         all_quantizations = OrderedDict()
         for class_type in QUANTIZATION_MODULES.registry_dict.values():
             quantization_type = class_type.__name__
             all_quantizations.update(
                 get_all_modules_by_type(
-                    model.get_compression_modules_by_type(ExtraCompressionModuleType.EXTERNAL_QUANTIZER),
-                    quantization_type))
-            all_quantizations.update(get_all_modules_by_type(model.get_nncf_wrapped_model(), quantization_type))
+                    model.nncf.get_compression_modules_by_type(ExtraCompressionModuleType.EXTERNAL_QUANTIZER),
+                    quantization_type,
+                )
+            )
+            all_quantizations.update(get_all_modules_by_type(model, quantization_type))
         all_quantizations = OrderedDict(sorted(all_quantizations.items(), key=lambda x: str(x[0])))
         return all_quantizations
 
-    @skip_if_dependency_unavailable(dependencies=['matplotlib.pyplot'])
+    @skip_if_dependency_unavailable(dependencies=["matplotlib.pyplot"])
     def dump_avg_traces(self):
         import matplotlib.pyplot as plt
-        dump_file = os.path.join(self._dump_dir, 'avg_traces_per_layer')
+
+        dump_file = os.path.join(self._dump_dir, "avg_traces_per_layer")
         torch.save(self._traces_per_layer, dump_file)
         fig = plt.figure()
-        fig.suptitle('Average Hessian Trace')
+        fig.suptitle("Average Hessian Trace")
         ax = fig.add_subplot(2, 1, 1)
-        ax.set_yscale('log')
-        ax.set_xlabel('weight quantizers')
-        ax.set_ylabel('average hessian trace')
+        ax.set_yscale("log")
+        ax.set_xlabel("weight quantizers")
+        ax.set_ylabel("average hessian trace")
         ax.plot(self._traces_per_layer.cpu().numpy())
         plt.savefig(dump_file)
 
-    @skip_if_dependency_unavailable(dependencies=['matplotlib.pyplot'])
+    @skip_if_dependency_unavailable(dependencies=["matplotlib.pyplot"])
     def dump_metric_MB(self, metric_per_qconfig_sequence: List[Tensor]):
         import matplotlib.pyplot as plt
+
         list_to_plot = [cm.item() for cm in metric_per_qconfig_sequence]
         fig = plt.figure()
-        fig.suptitle('Pareto Frontier')
+        fig.suptitle("Pareto Frontier")
         ax = fig.add_subplot(2, 1, 1)
-        ax.set_yscale('log')
-        ax.set_xlabel('Model Size (MB)')
-        ax.set_ylabel('Metric value (total perturbation)')
-        ax.scatter(self._model_sizes, list_to_plot, s=20, facecolors='none', edgecolors='r')
+        ax.set_yscale("log")
+        ax.set_xlabel("Model Size (MB)")
+        ax.set_ylabel("Metric value (total perturbation)")
+        ax.scatter(self._model_sizes, list_to_plot, s=20, facecolors="none", edgecolors="r")
         cm = torch.Tensor(metric_per_qconfig_sequence)
         cm_m = cm.median().item()
         qconfig_index = metric_per_qconfig_sequence.index(cm_m)
         ms_m = self._model_sizes[qconfig_index]
-        ax.scatter(ms_m, cm_m, s=30, facecolors='none', edgecolors='b', label='median from all metrics')
+        ax.scatter(ms_m, cm_m, s=30, facecolors="none", edgecolors="b", label="median from all metrics")
         ax.legend()
-        plt.savefig(os.path.join(self._dump_dir, 'Pareto_Frontier'))
+        plt.savefig(os.path.join(self._dump_dir, "Pareto_Frontier"))
         nncf_logger.debug(
-            f'Distribution of HAWQ metrics: '
-            f'min_value={cm.min().item():.3f}, '
-            f'max_value={cm.max().item():.3f}, '
-            f'median_value={cm_m:.3f}, '
-            f'median_index={qconfig_index}, '
-            f'total_number={len(metric_per_qconfig_sequence)}')
-
-    @skip_if_dependency_unavailable(dependencies=['matplotlib.pyplot'])
-    def dump_metric_flops(self, metric_per_qconfig_sequence: List[Tensor], flops_per_config: List[float],
-                          choosen_qconfig_index: int):
+            f"Distribution of HAWQ metrics: "
+            f"min_value={cm.min().item():.3f}, "
+            f"max_value={cm.max().item():.3f}, "
+            f"median_value={cm_m:.3f}, "
+            f"median_index={qconfig_index}, "
+            f"total_number={len(metric_per_qconfig_sequence)}"
+        )
+
+    @skip_if_dependency_unavailable(dependencies=["matplotlib.pyplot"])
+    def dump_metric_flops(
+        self, metric_per_qconfig_sequence: List[Tensor], flops_per_config: List[float], choosen_qconfig_index: int
+    ):
         import matplotlib.pyplot as plt
+
         list_to_plot = [cm.item() for cm in metric_per_qconfig_sequence]
         fig = plt.figure()
-        fig.suptitle('Pareto Frontier')
+        fig.suptitle("Pareto Frontier")
         ax = fig.add_subplot(1, 1, 1)
-        ax.set_xlabel('Compression ratio: total INT8 Bits Complexity / total MIXED INT Bits Complexity')
-        ax.set_ylabel('Metric value (total perturbation)')
+        ax.set_xlabel("Compression ratio: total INT8 Bits Complexity / total MIXED INT Bits Complexity")
+        ax.set_ylabel("Metric value (total perturbation)")
         ax.scatter(flops_per_config, list_to_plot, s=10, alpha=0.3)  # s=20, facecolors='none', edgecolors='r')
         flops_per_config = [torch.Tensor([v]) for v in flops_per_config]
         cm = torch.Tensor(flops_per_config)
         cm_m = cm.median().item()
         configuration_index = flops_per_config.index(cm_m)
         ms_m = metric_per_qconfig_sequence[configuration_index].item()
-        ax.scatter(cm_m, ms_m, s=30, facecolors='none', edgecolors='b', label='median from all metrics')
+        ax.scatter(cm_m, ms_m, s=30, facecolors="none", edgecolors="b", label="median from all metrics")
         cm_c = metric_per_qconfig_sequence[choosen_qconfig_index].item()
         fpc_c = flops_per_config[choosen_qconfig_index].item()
-        ax.scatter(fpc_c, cm_c, s=30, facecolors='none', edgecolors='r', label='chosen config')
+        ax.scatter(fpc_c, cm_c, s=30, facecolors="none", edgecolors="r", label="chosen config")
 
         ax.legend()
-        plt.savefig(os.path.join(self._dump_dir, 'Pareto_Frontier_compress_ratio'))
+        plt.savefig(os.path.join(self._dump_dir, "Pareto_Frontier_compress_ratio"))
 
-    @skip_if_dependency_unavailable(dependencies=['matplotlib.pyplot'])
+    @skip_if_dependency_unavailable(dependencies=["matplotlib.pyplot"])
     def dump_density_of_quantization_noise(self):
         noise_per_config = []  # type: List[Tensor]
         for qconfig_sequence in self._weight_qconfig_sequences_in_trace_order:
             qnoise = 0
             for i in range(self._num_weights):
                 execution_index = self._traces_order.get_execution_index_by_traces_index(i)
                 qnoise += self._perturbations.get(layer_id=execution_index, qconfig=qconfig_sequence[i])
             noise_per_config.append(qnoise)
 
         list_to_plot = [cm.item() for cm in noise_per_config]
         import matplotlib.pyplot as plt
+
         fig = plt.figure()
-        fig.suptitle('Density of quantization noise')
+        fig.suptitle("Density of quantization noise")
         ax = fig.add_subplot(2, 1, 1)
-        ax.set_yscale('log')
-        ax.set_xlabel('Blocks')
-        ax.set_ylabel('Noise value')
+        ax.set_yscale("log")
+        ax.set_xlabel("Blocks")
+        ax.set_ylabel("Noise value")
         ax.scatter(self._model_sizes, list_to_plot, s=20, alpha=0.3)
         ax.legend()
-        plt.savefig(os.path.join(self._dump_dir, 'Density_of_quantization_noise'))
+        plt.savefig(os.path.join(self._dump_dir, "Density_of_quantization_noise"))
 
-    @skip_if_dependency_unavailable(dependencies=['matplotlib.pyplot'])
+    @skip_if_dependency_unavailable(dependencies=["matplotlib.pyplot"])
     def dump_perturbations_ratio(self):
         import matplotlib.pyplot as plt
+
         fig = plt.figure()
-        fig.suptitle('Quantization noise vs Average Trace')
+        fig.suptitle("Quantization noise vs Average Trace")
         ax = fig.add_subplot(2, 1, 1)
-        ax.set_xlabel('Blocks')
-        ax.set_yscale('log')
+        ax.set_xlabel("Blocks")
+        ax.set_yscale("log")
         perturbations_per_layer_id = list(self._perturbations.get_all().values())
         perturb = []
         max_bitwidths = []
         for perturbations_for_all_observed_qconfig_sequence_in_current_layer in perturbations_per_layer_id:
             qconfig_sequence = perturbations_for_all_observed_qconfig_sequence_in_current_layer.keys()
             max_bitwidth_qconfig = max(qconfig_sequence, key=lambda x: x.num_bits)
             perturb.append(perturbations_for_all_observed_qconfig_sequence_in_current_layer[max_bitwidth_qconfig])
             max_bitwidths.append(max_bitwidth_qconfig.num_bits)
         ax.plot(
             [p / m / n for p, m, n in zip(perturb, self._num_weights_per_layer, self._norm_weights_per_layer)],
-            label='normalized n-bit noise')
-        ax.plot(perturb, label='n-bit noise')
-        ax.plot(max_bitwidths, label='n')
-        ax.plot(self._traces_per_layer.cpu().numpy(), label='trace')
-        ax.plot([n * p for n, p in zip(self._traces_per_layer, perturb)], label='trace * noise')
+            label="normalized n-bit noise",
+        )
+        ax.plot(perturb, label="n-bit noise")
+        ax.plot(max_bitwidths, label="n")
+        ax.plot(self._traces_per_layer.cpu().numpy(), label="trace")
+        ax.plot([n * p for n, p in zip(self._traces_per_layer, perturb)], label="trace * noise")
         ax.legend()
-        plt.savefig(os.path.join(self._dump_dir, 'Quantization_noise_vs_Average_Trace'))
+        plt.savefig(os.path.join(self._dump_dir, "Quantization_noise_vs_Average_Trace"))
+
+    def dump_bitwidth_graph(
+        self,
+        algo_ctrl: "QuantizationController",
+        model: NNCFNetwork,
+        groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
+    ):
+        from nncf.torch.quantization.precision_init.bitwidth_graph import BitwidthGraph  # pylint: disable=cyclic-import
 
-    def dump_bitwidth_graph(self, algo_ctrl: 'QuantizationController', model: NNCFNetwork,
-                            groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers):
-        from nncf.torch.quantization.precision_init.bitwidth_graph import BitwidthGraph #pylint: disable=cyclic-import
         bw_graph = BitwidthGraph(algo_ctrl, model, groups_of_adjacent_quantizers).get()
         nx_graph = add_adjust_padding_nodes(bw_graph, model)
-        write_dot_graph(nx_graph, self._dump_dir / Path('bitwidth_graph.dot'))
+        write_dot_graph(nx_graph, self._dump_dir / Path("bitwidth_graph.dot"))
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/hawq_init.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/hawq_init.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,109 +1,110 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import itertools
 import json
+from bisect import bisect_left
 from collections import OrderedDict
+from copy import deepcopy
 from enum import Enum
+from operator import itemgetter
 from pathlib import Path
 from typing import Any, Callable, Dict, List, NamedTuple, Set, Tuple
 
 import torch
-from bisect import bisect_left
-from copy import deepcopy
-from operator import itemgetter
 from torch import Tensor
 from torch import nn
 from torch.nn.modules.loss import _Loss
 
 from nncf.common.graph import NNCFNodeName
-from nncf.common.quantization.structs import QuantizerConfig
 from nncf.common.logging import nncf_logger
-from nncf.common.utils.os import safe_open
+from nncf.common.quantization.quantizer_setup import QuantizationPointId
+from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
+from nncf.common.quantization.structs import QuantizerConfig
+from nncf.common.quantization.structs import QuantizerId
+from nncf.common.quantization.structs import WeightQuantizerId
 from nncf.common.utils.debug import is_debug
-from nncf.config.schemata.defaults import HAWQ_DUMP_INIT_PRECISION_DATA
-from nncf.config.schemata.defaults import PRECISION_INIT_BITWIDTHS
+from nncf.common.utils.os import safe_open
 from nncf.config.schemata.defaults import HAWQ_COMPRESSION_RATIO
+from nncf.config.schemata.defaults import HAWQ_DUMP_INIT_PRECISION_DATA
 from nncf.config.schemata.defaults import HAWQ_ITER_NUMBER
 from nncf.config.schemata.defaults import HAWQ_NUM_DATA_POINTS
 from nncf.config.schemata.defaults import HAWQ_TOLERANCE
+from nncf.config.schemata.defaults import PRECISION_INIT_BITWIDTHS
 from nncf.torch.quantization.hessian_trace import HessianTraceEstimator
 from nncf.torch.quantization.layers import QuantizersSwitcher
 from nncf.torch.quantization.precision_constraints import HardwareQuantizationConstraints
 from nncf.torch.quantization.precision_init.adjacent_quantizers import GroupsOfAdjacentQuantizers
-from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitParams
 from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitializer
+from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitParams
 from nncf.torch.quantization.precision_init.compression_ratio import CompressionRatioCalculator
 from nncf.torch.quantization.precision_init.hawq_debug import HAWQDebugger
 from nncf.torch.quantization.precision_init.perturbations import PerturbationObserver
 from nncf.torch.quantization.precision_init.perturbations import Perturbations
 from nncf.torch.quantization.precision_init.traces_order import TracesOrder
 from nncf.torch.quantization.precision_init.traces_order import TracesPerLayer
-from nncf.common.quantization.structs import QuantizerId
-from nncf.common.quantization.structs import WeightQuantizerId
-from nncf.common.quantization.quantizer_setup import QuantizationPointId
-from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
 from nncf.torch.quantization.structs import WeightQuantizerInfo
 from nncf.torch.structures import QuantizationPrecisionInitArgs
 from nncf.torch.utils import get_model_device
 
 
 class BitwidthAssignmentMode(Enum):
-    STRICT = 'strict'
-    LIBERAL = 'liberal'
+    STRICT = "strict"
+    LIBERAL = "liberal"
 
 
 class HAWQPrecisionInitParams(BasePrecisionInitParams):
-    def __init__(self,
-                 user_init_args: QuantizationPrecisionInitArgs,
-                 bitwidths: List[int] = None,
-                 bitwidth_per_scope: List[List] = None,
-                 traces_per_layer_path: str = None,
-                 num_data_points: int = None,
-                 iter_number: int = None,
-                 tolerance: float = None,
-                 compression_ratio: float = None,
-                 dump_hawq_data: bool = None,
-                 bitwidth_assignment_mode: BitwidthAssignmentMode = None):
+    def __init__(
+        self,
+        user_init_args: QuantizationPrecisionInitArgs,
+        bitwidths: List[int] = None,
+        bitwidth_per_scope: List[List] = None,
+        traces_per_layer_path: str = None,
+        num_data_points: int = None,
+        iter_number: int = None,
+        tolerance: float = None,
+        compression_ratio: float = None,
+        dump_hawq_data: bool = None,
+        bitwidth_assignment_mode: BitwidthAssignmentMode = None,
+    ):
         super().__init__(user_init_args)
         self.bitwidths = bitwidths
         self.bitwidth_per_scope = bitwidth_per_scope
         self.traces_per_layer_path = traces_per_layer_path
         self.num_data_points = num_data_points
         self.iter_number = iter_number
         self.tolerance = tolerance
         self.compression_ratio = compression_ratio
         self.dump_hawq_data = dump_hawq_data
         self.bitwidth_assignment_mode = bitwidth_assignment_mode
 
     @classmethod
-    def from_config(cls, hawq_init_config_dict: Dict,
-                    user_init_args: QuantizationPrecisionInitArgs) -> 'HAWQPrecisionInitParams':
+    def from_config(
+        cls, hawq_init_config_dict: Dict, user_init_args: QuantizationPrecisionInitArgs
+    ) -> "HAWQPrecisionInitParams":
         return cls(
             user_init_args=user_init_args,
-            bitwidths=hawq_init_config_dict.get('bits', PRECISION_INIT_BITWIDTHS),
-            traces_per_layer_path=hawq_init_config_dict.get('traces_per_layer_path', None),
-            num_data_points=hawq_init_config_dict.get('num_data_points', HAWQ_NUM_DATA_POINTS),
-            iter_number=hawq_init_config_dict.get('iter_number', HAWQ_ITER_NUMBER),
-            tolerance=hawq_init_config_dict.get('tolerance', HAWQ_TOLERANCE),
-            compression_ratio=hawq_init_config_dict.get('compression_ratio', HAWQ_COMPRESSION_RATIO),
-            dump_hawq_data=hawq_init_config_dict.get('dump_init_precision_data', HAWQ_DUMP_INIT_PRECISION_DATA),
+            bitwidths=hawq_init_config_dict.get("bits", PRECISION_INIT_BITWIDTHS),
+            traces_per_layer_path=hawq_init_config_dict.get("traces_per_layer_path", None),
+            num_data_points=hawq_init_config_dict.get("num_data_points", HAWQ_NUM_DATA_POINTS),
+            iter_number=hawq_init_config_dict.get("iter_number", HAWQ_ITER_NUMBER),
+            tolerance=hawq_init_config_dict.get("tolerance", HAWQ_TOLERANCE),
+            compression_ratio=hawq_init_config_dict.get("compression_ratio", HAWQ_COMPRESSION_RATIO),
+            dump_hawq_data=hawq_init_config_dict.get("dump_init_precision_data", HAWQ_DUMP_INIT_PRECISION_DATA),
             bitwidth_assignment_mode=BitwidthAssignmentMode(
-                hawq_init_config_dict.get('bitwidth_assignment_mode', BitwidthAssignmentMode.LIBERAL.value)
-            )
+                hawq_init_config_dict.get("bitwidth_assignment_mode", BitwidthAssignmentMode.LIBERAL.value)
+            ),
         )
 
 
 QConfigSequenceForHAWQToEvaluate = List[QuantizerConfig]
 CoveringQConfigSequenceForQuantNoiseCalculation = List[QuantizerConfig]
 
 
@@ -123,29 +124,31 @@
         m = len(bitwidths)
         L = seq_len
         for j in range(1, m + 1):
             for combo_bitwidths in itertools.combinations(bitwidths, j):
                 for combo_partitions in itertools.combinations(list(range(1, L)), j - 1):
                     bit_config = []
                     prev_p = 0
-                    for (p, b) in zip(combo_partitions + (L,), combo_bitwidths):
+                    for p, b in zip(combo_partitions + (L,), combo_bitwidths):
                         bit_config += [b] * (p - prev_p)
                         prev_p = p
                     sequences.append(bit_config)
         return sequences
 
     @staticmethod
-    def _select_first_closest_bitwidth_qconfig(qconf_list: List[QuantizerConfig],
-                                               target_bitwidth: int) -> QuantizerConfig:
+    def _select_first_closest_bitwidth_qconfig(
+        qconf_list: List[QuantizerConfig], target_bitwidth: int
+    ) -> QuantizerConfig:
         bw_diffs = [abs(qc.num_bits - target_bitwidth) for qc in qconf_list]
         _, min_idx = min((val, idx) for (idx, val) in enumerate(bw_diffs))
         return qconf_list[min_idx]
 
-    def _deduplicate(self, qconf_sequences_to_search: List[QConfigSequenceForHAWQToEvaluate]) -> \
-            List[QConfigSequenceForHAWQToEvaluate]:
+    def _deduplicate(
+        self, qconf_sequences_to_search: List[QConfigSequenceForHAWQToEvaluate]
+    ) -> List[QConfigSequenceForHAWQToEvaluate]:
         tupled_sequence = [tuple(seq) for seq in qconf_sequences_to_search]
         odict = OrderedDict.fromkeys(tupled_sequence)
         deduped_tupled_sequence = list(odict.keys())
         return [list(tup) for tup in deduped_tupled_sequence]
 
     @staticmethod
     def _generate_covering_qconfig_sequences(observed_qconfs: List[Dict[QuantizerConfig, QuantizerConfig]]):
@@ -168,178 +171,208 @@
                 if i < len(qconfs_for_trace_idx):
                     covering_conf.append(qconfs_for_trace_idx[i])
                 else:
                     covering_conf.append(qconfs_for_trace_idx[-1])
             covering_qconfig_sequences.append(covering_conf)
         return covering_qconfig_sequences
 
-    def get_qconfig_sequences_constrained_by_trace_order(self,
-                                                         possible_qconfigs_sequence_in_trace_order: List[
-                                                             List[QuantizerConfig]],
-                                                         indices_for_bitwidth_adjustment_only: Set[int]) -> \
-            Tuple[List[QConfigSequenceForHAWQToEvaluate], List[CoveringQConfigSequenceForQuantNoiseCalculation]]:
+    def get_qconfig_sequences_constrained_by_trace_order(
+        self,
+        possible_qconfigs_sequence_in_trace_order: List[List[QuantizerConfig]],
+        indices_for_bitwidth_adjustment_only: Set[int],
+    ) -> Tuple[List[QConfigSequenceForHAWQToEvaluate], List[CoveringQConfigSequenceForQuantNoiseCalculation]]:
         """
         The 'constraint' is so that the each qconfig sequence should have non-decreasing bitwidths. It
         might be impossible to apply this constraint for a given qconfig space (consider [[2], [6, 8], [4]]).
         In such a case, for trace order index positions where it was impossible to select a bitwidth so that the entire
         sequence is non-decreasing, the bitwidth closest to this target will be chosen instead.
         """
         if len(possible_qconfigs_sequence_in_trace_order) != len(self._traces_order):
             raise ValueError("The size of the qconfig space and the traces do not match!")
         retval = []  # type: List[QConfigSequenceForHAWQToEvaluate]
-        observed_qconfs_in_retval = [OrderedDict()
-                                     for _ in range(len(self._traces_order))]
+        observed_qconfs_in_retval = [OrderedDict() for _ in range(len(self._traces_order))]
         for bitwidth_sequence in self._bitwidth_sequences:
             current_qconfig_sequence_in_trace_order = []  # type: QConfigSequenceForHAWQToEvaluate
             for trace_idx, bitwidth in enumerate(bitwidth_sequence):
-
                 if trace_idx in indices_for_bitwidth_adjustment_only:
                     bitwidth_adjusted_default_qconfig = deepcopy(
-                        possible_qconfigs_sequence_in_trace_order[trace_idx][0])
+                        possible_qconfigs_sequence_in_trace_order[trace_idx][0]
+                    )
                     bitwidth_adjusted_default_qconfig.num_bits = bitwidth
                     qconfig = bitwidth_adjusted_default_qconfig
                 else:
                     # TODO: do a selection based on strategy ("exhaustive" = add all available configurations,
                     # "preset" = do a selection based on a certain preset, "first" = select first match (as below),
                     # "custom" = use a custom selection function to be passed as arg to the HAWQ initializer
                     # OR: do non-bitwidth disambiguation higher up the stack, make sure that the qconfig
                     # space at this spot only has 1 qconfig option for each bitwidth.
                     possible_qconfigs_for_current_trace_idx = possible_qconfigs_sequence_in_trace_order[trace_idx]
                     first_closest_qconfig = self._select_first_closest_bitwidth_qconfig(
-                        possible_qconfigs_for_current_trace_idx, bitwidth)
+                        possible_qconfigs_for_current_trace_idx, bitwidth
+                    )
                     qconfig = deepcopy(first_closest_qconfig)
 
                 current_qconfig_sequence_in_trace_order.append(qconfig)
                 observed_qconfs_in_retval[trace_idx][qconfig] = qconfig
             retval.append(current_qconfig_sequence_in_trace_order)
         return self._deduplicate(retval), self._generate_covering_qconfig_sequences(observed_qconfs_in_retval)
 
 
 class HAWQPrecisionInitializer(BasePrecisionInitializer):
-    def __init__(self, algo: 'ExperimentalQuantizationController',
-                 params: HAWQPrecisionInitParams,
-                 hw_precision_constraints: HardwareQuantizationConstraints):
+    def __init__(
+        self,
+        algo: "ExperimentalQuantizationController",
+        params: HAWQPrecisionInitParams,
+        hw_precision_constraints: HardwareQuantizationConstraints,
+    ):
         self._groups_of_adjacent_quantizers = algo.groups_of_adjacent_quantizers
         self._bitwidth_assignment_mode = params.bitwidth_assignment_mode
         if self._bitwidth_assignment_mode == BitwidthAssignmentMode.STRICT:
             hw_precision_constraints = self._merge_constraints_for_adjacent_quantizers(
-                self._groups_of_adjacent_quantizers,
-                hw_precision_constraints)
+                self._groups_of_adjacent_quantizers, hw_precision_constraints
+            )
         super().__init__(algo, params, hw_precision_constraints)
         init_args = params.user_init_args
         self._criterion_fn = init_args.criterion_fn
         self._criterion = init_args.criterion
         self._data_loader = init_args.data_loader
         self._traces_per_layer_path = params.traces_per_layer_path
         self._num_data_points = params.num_data_points
         self._iter_number = params.iter_number
         self._tolerance = params.tolerance
         self._compression_ratio = params.compression_ratio
-        self._bitwidths = self._hw_precision_constraints.get_all_unique_bitwidths() \
-            if self._hw_precision_constraints else params.bitwidths
+        self._bitwidths = (
+            self._hw_precision_constraints.get_all_unique_bitwidths()
+            if self._hw_precision_constraints
+            else params.bitwidths
+        )
         self._init_device = init_args.device
         if self._init_device is None:
             self._init_device = get_model_device(self._model)
         current_quantizer_setup = self._algo.get_quantizer_setup_for_current_state()
-        flops_per_module = self._model.get_flops_per_module()
+        flops_per_module = self._model.nncf.get_flops_per_module()
         self._compression_ratio_calculator = CompressionRatioCalculator(
-            flops_per_module, current_quantizer_setup,
-            self._groups_of_adjacent_quantizers.weight_qp_id_per_activation_qp_id)
+            flops_per_module,
+            current_quantizer_setup,
+            self._groups_of_adjacent_quantizers.weight_qp_id_per_activation_qp_id,
+        )
         self._dump_hawq_data = params.dump_hawq_data
         self._original_qp_id_vs_quantizer_module_id_dict = deepcopy(algo.setup_to_module_id_translation_dict)
 
     def apply_init(self) -> SingleConfigQuantizerSetup:
         if not self._weight_quantizations_by_execution_order:
             return self._algo.get_quantizer_setup_for_current_state()
 
         original_device = get_model_device(self._model)
         self._model.to(self._init_device)
 
         traces_per_layer = self._calc_traces(self._criterion_fn, self._criterion, self._iter_number, self._tolerance)
         if not traces_per_layer:
-            raise RuntimeError('Failed to calculate hessian traces!')
+            raise RuntimeError("Failed to calculate hessian traces!")
 
         traces_order = traces_per_layer.traces_order
-        weight_qconfig_sequences_in_trace_order, covering_qconfig_sequences = \
-            self.get_qconfig_sequences_constrained_by_traces_order(traces_order)
+        (
+            weight_qconfig_sequences_in_trace_order,
+            covering_qconfig_sequences,
+        ) = self.get_qconfig_sequences_constrained_by_traces_order(traces_order)
 
         weight_quantizer_ids_in_execution_order = list(self._weight_quantizations_by_execution_order.keys())
 
         if not weight_qconfig_sequences_in_trace_order:
-            nncf_logger.error('All bitwidths configurations are incompatible with HW Config!')
+            nncf_logger.error("All bitwidths configurations are incompatible with HW Config!")
             return None
 
-        weight_qconfig_sequences_in_trace_order = \
-            self._filter_qconfig_sequences_by_excessive_bitwidth(weight_qconfig_sequences_in_trace_order)
+        weight_qconfig_sequences_in_trace_order = self._filter_qconfig_sequences_by_excessive_bitwidth(
+            weight_qconfig_sequences_in_trace_order
+        )
 
         if self._bitwidth_assignment_mode == BitwidthAssignmentMode.STRICT:
-            weight_qconfig_sequences_in_trace_order = \
-                self._filter_qconfig_sequences_by_grouped_weight_quantizers(weight_qconfig_sequences_in_trace_order,
-                                                                            weight_quantizer_ids_in_execution_order,
-                                                                            self._groups_of_adjacent_quantizers,
-                                                                            traces_order)
+            weight_qconfig_sequences_in_trace_order = self._filter_qconfig_sequences_by_grouped_weight_quantizers(
+                weight_qconfig_sequences_in_trace_order,
+                weight_quantizer_ids_in_execution_order,
+                self._groups_of_adjacent_quantizers,
+                traces_order,
+            )
         if not weight_qconfig_sequences_in_trace_order:
-            nncf_logger.error('No bitwidths configurations are left after removing inconsistent groups of '
-                              'weight quantizers with adjacent activation quantizers!')
+            nncf_logger.error(
+                "No bitwidths configurations are left after removing inconsistent groups of "
+                "weight quantizers with adjacent activation quantizers!"
+            )
             return self._algo.get_quantizer_setup_for_current_state()
 
         compression_ratio_per_qconfig = self.get_compression_ratio_per_qconfig_sequence(
-            weight_qconfig_sequences_in_trace_order,
-            traces_order)
+            weight_qconfig_sequences_in_trace_order, traces_order
+        )
         min_ratio = min(compression_ratio_per_qconfig)
         max_ratio = max(compression_ratio_per_qconfig)
         if not min_ratio <= self._compression_ratio <= max_ratio:
-            raise AttributeError('Invalid compression ratio={}. Should be within range [{:.3f}, {:.3f}]'.format(
-                self._compression_ratio, min_ratio, max_ratio))
+            raise AttributeError(
+                "Invalid compression ratio={}. Should be within range [{:.3f}, {:.3f}]".format(
+                    self._compression_ratio, min_ratio, max_ratio
+                )
+            )
 
         perturbations, weight_observers = self.calc_quantization_noise(covering_qconfig_sequences, traces_order)
 
         metric_per_qconfig_sequence = self.calc_hawq_metric_per_qconfig_sequence(
-            weight_qconfig_sequences_in_trace_order, perturbations,
-            traces_per_layer, self._init_device)
+            weight_qconfig_sequences_in_trace_order, perturbations, traces_per_layer, self._init_device
+        )
 
         qconfig_sequence_index = self.choose_qconfig_sequence(
-            metric_per_qconfig_sequence, compression_ratio_per_qconfig, self._compression_ratio)
+            metric_per_qconfig_sequence, compression_ratio_per_qconfig, self._compression_ratio
+        )
         chosen_qconfig_sequence_in_traces_order = weight_qconfig_sequences_in_trace_order[qconfig_sequence_index]
         chosen_qconfig_sequence_in_execution_order = traces_order.get_execution_order_configs(
-            chosen_qconfig_sequence_in_traces_order)
+            chosen_qconfig_sequence_in_traces_order
+        )
         bitwidth_sequence = [qconfig.num_bits for qconfig in chosen_qconfig_sequence_in_execution_order]
         nncf_logger.info(
-            f'Chosen HAWQ bitwidth sequence with ratio={compression_ratio_per_qconfig[qconfig_sequence_index]:.2f}, '
-            f'bitwidth per weightable layer={bitwidth_sequence}')
-        nncf_logger.debug(f'Order of the weightable layers in the HAWQ bitwidth sequence '
-                          f'(in descending order of average Hessian traces) = {traces_order}')
+            f"Chosen HAWQ bitwidth sequence with ratio={compression_ratio_per_qconfig[qconfig_sequence_index]:.2f}, "
+            f"bitwidth per weightable layer={bitwidth_sequence}"
+        )
+        nncf_logger.debug(
+            f"Order of the weightable layers in the HAWQ bitwidth sequence "
+            f"(in descending order of average Hessian traces) = {traces_order}"
+        )
 
-        final_quantizer_setup = self.get_quantizer_setup_for_qconfig_sequence(chosen_qconfig_sequence_in_traces_order,
-                                                                              traces_order)
+        final_quantizer_setup = self.get_quantizer_setup_for_qconfig_sequence(
+            chosen_qconfig_sequence_in_traces_order, traces_order
+        )
         if is_debug() or self._dump_hawq_data:
-            hawq_debugger = HAWQDebugger(weight_qconfig_sequences_in_trace_order,
-                                         perturbations,
-                                         weight_observers, traces_per_layer, self._bitwidths)
+            hawq_debugger = HAWQDebugger(
+                weight_qconfig_sequences_in_trace_order,
+                perturbations,
+                weight_observers,
+                traces_per_layer,
+                self._bitwidths,
+            )
             hawq_debugger.dump_metric_MB(metric_per_qconfig_sequence)
             hawq_debugger.dump_metric_flops(
-                metric_per_qconfig_sequence, compression_ratio_per_qconfig, qconfig_sequence_index)
+                metric_per_qconfig_sequence, compression_ratio_per_qconfig, qconfig_sequence_index
+            )
             hawq_debugger.dump_avg_traces()
             hawq_debugger.dump_density_of_quantization_noise()
             hawq_debugger.dump_perturbations_ratio()
             new_ctrl, new_model = self._algo.apply_new_quantizer_setup(final_quantizer_setup)
             groups_of_adjacent_quantizers = new_ctrl.groups_of_adjacent_quantizers
             hawq_debugger.dump_bitwidth_graph(new_ctrl, new_model, groups_of_adjacent_quantizers)
         bitwidth_per_scope = self.get_bitwidth_per_scope(final_quantizer_setup)
-        from nncf.common.utils.debug import DEBUG_LOG_DIR #pylint: disable=cyclic-import
+        from nncf.common.utils.debug import DEBUG_LOG_DIR  # pylint: disable=cyclic-import
+
         Path(DEBUG_LOG_DIR).mkdir(parents=True, exist_ok=True)
-        with safe_open(Path(DEBUG_LOG_DIR) / 'bitwidth_per_scope.json', "w") as outfile:
-            json.dump({'bitwidth_per_scope': bitwidth_per_scope}, outfile, indent=4, sort_keys=False)
+        with safe_open(Path(DEBUG_LOG_DIR) / "bitwidth_per_scope.json", "w") as outfile:
+            json.dump({"bitwidth_per_scope": bitwidth_per_scope}, outfile, indent=4, sort_keys=False)
         self._model.to(original_device)
         return final_quantizer_setup
 
     @staticmethod
-    def _merge_constraints_for_adjacent_quantizers(groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
-                                                   hw_precision_constraints: HardwareQuantizationConstraints) -> \
-            HardwareQuantizationConstraints:
+    def _merge_constraints_for_adjacent_quantizers(
+        groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
+        hw_precision_constraints: HardwareQuantizationConstraints,
+    ) -> HardwareQuantizationConstraints:
         if not hw_precision_constraints:
             return None
         retval = deepcopy(hw_precision_constraints)
         for group in groups_of_adjacent_quantizers:
             all_bitwidths_sets = []
             quantizer_ids = []
             all_quantizers = group.weight_quantizers + group.activation_quantizers
@@ -347,46 +380,47 @@
                 bitwidths_vs_qconfig_sequence = retval.get_bitwidth_vs_qconfigs_dict(quantizer_id)
                 bitwidths = set(bitwidths_vs_qconfig_sequence.keys())
                 all_bitwidths_sets.append(bitwidths)
                 quantizer_ids.append(quantizer_id)
             minimal_set_bitwidths = set.intersection(*all_bitwidths_sets)
             if not minimal_set_bitwidths:
                 raise RuntimeError(
-                    'No bitwidths configurations are left after removing inconsistent groups of weight quantizers'
-                    ' with adjacent activation quantizers!')
+                    "No bitwidths configurations are left after removing inconsistent groups of weight quantizers"
+                    " with adjacent activation quantizers!"
+                )
             for quantizer_id in quantizer_ids:
                 qconfig_sequence = retval.get(quantizer_id)
                 filtered_qconfig_sequence = []
                 for qconf in qconfig_sequence:
                     if qconf.num_bits in minimal_set_bitwidths:
                         filtered_qconfig_sequence.append(qconf)
                 retval.replace(quantizer_id, filtered_qconfig_sequence)
         return retval
 
-    def get_compression_ratio_per_qconfig_sequence(self,
-                                                   qconfig_sequences_in_trace_order: List[
-                                                       QConfigSequenceForHAWQToEvaluate],
-                                                   traces_order: TracesOrder) -> List[float]:
+    def get_compression_ratio_per_qconfig_sequence(
+        self, qconfig_sequences_in_trace_order: List[QConfigSequenceForHAWQToEvaluate], traces_order: TracesOrder
+    ) -> List[float]:
         compression_ratio_per_qconfig = []
         for qconfig_sequence in qconfig_sequences_in_trace_order:
             quantizer_setup = self.get_quantizer_setup_for_qconfig_sequence(qconfig_sequence, traces_order)
             compression_ratio = self._compression_ratio_calculator.run_for_quantizer_setup(quantizer_setup)
             compression_ratio_per_qconfig.append(compression_ratio)
         return compression_ratio_per_qconfig
 
     class ParamsToRestore(NamedTuple):
         originally_disabled_gradients: List[str]
         skipped_gradients_to_enable: List[Tuple[nn.Module, str]]
 
     @staticmethod
     def disable_all_gradients_except_weights_of_quantized_modules(
-            quantizers_switcher: QuantizersSwitcher,
-            weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
-            model: nn.Module,
-            skipped_quantized_weight_node_names: List[NNCFNodeName] = None) -> ParamsToRestore:
+        quantizers_switcher: QuantizersSwitcher,
+        weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
+        model: nn.Module,
+        skipped_quantized_weight_node_names: List[NNCFNodeName] = None,
+    ) -> ParamsToRestore:
         """
         Disables gradients of all parameters, except for layers that have quantizers for weights, which wasn't skipped
         because of single precision constraints.
         :param quantizers_switcher: object that is responsible for enabling and disabling quantizers
         :param weight_quantizers: modules with quantized weights per scope
         :param model: model to access all parameters
         :param skipped_quantized_weight_node_names: list of weighted nodes that have a single precision
@@ -426,53 +460,65 @@
             else:
                 param.requires_grad = False
 
         # enable gradients of quantized modules that were disabled
         for wq_id in weight_quantizers.values():
             quantized_module = wq_id.quantized_module
             for param_name, param in quantized_module.named_parameters():
-                if (quantized_module, param_name) in gradients_to_enable and not 'bias' in param_name:
+                if (quantized_module, param_name) in gradients_to_enable and not "bias" in param_name:
                     param.requires_grad = True
         return HAWQPrecisionInitializer.ParamsToRestore(originally_disabled_gradients, skipped_gradients_to_enable)
 
-    def _calc_traces(self, criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor], criterion: _Loss,
-                     iter_number: int, tolerance: float) -> TracesPerLayer:
+    def _calc_traces(
+        self,
+        criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor],
+        criterion: _Loss,
+        iter_number: int,
+        tolerance: float,
+    ) -> TracesPerLayer:
         if self._traces_per_layer_path:
             return TracesPerLayer(torch.load(self._traces_per_layer_path).to(self._init_device))
 
         quantizers_switcher = QuantizersSwitcher(list(self._all_quantizers_per_scope.values()))
         params_to_restore = self.disable_all_gradients_except_weights_of_quantized_modules(
             quantizers_switcher,
             self._algo.weight_quantizers,
             self._model,
-            self._quantizers_handler.get_skipped_quantized_weight_node_names())
+            self._quantizers_handler.get_skipped_quantized_weight_node_names(),
+        )
 
-        trace_estimator = HessianTraceEstimator(self._model, criterion_fn, criterion, self._init_device,
-                                                self._data_loader, self._num_data_points)
+        trace_estimator = HessianTraceEstimator(
+            self._model, criterion_fn, criterion, self._init_device, self._data_loader, self._num_data_points
+        )
         try:
             avg_traces = trace_estimator.get_average_traces(max_iter=iter_number, tolerance=tolerance)
         except RuntimeError as error:
             if "cuda out of memory" in error.args[0].lower():
-                raise RuntimeError('Failed to estimate average Hessian traces within precision initialization. Specify '
-                                   'a smaller batch size via --batch-size-init option in the NNCF samples or register '
-                                   'a data loader with a smaller batch size. Refer to '
-                                   '`NNCFConfig.register_extra_structs` and the `QuantizationPrecisionInitArgs`'
-                                   ' class') from error
+                raise RuntimeError(
+                    "Failed to estimate average Hessian traces within precision initialization. Specify "
+                    "a smaller batch size via --batch-size-init option in the NNCF samples or register "
+                    "a data loader with a smaller batch size. Refer to "
+                    "`NNCFConfig.register_extra_structs` and the `QuantizationPrecisionInitArgs`"
+                    " class"
+                ) from error
             raise error
 
-        self.restore_disabled_gradients(quantizers_switcher, self._model, self._algo.weight_quantizers,
-                                        params_to_restore)
+        self.restore_disabled_gradients(
+            quantizers_switcher, self._model, self._algo.weight_quantizers, params_to_restore
+        )
 
         return TracesPerLayer(avg_traces)
 
     @staticmethod
-    def restore_disabled_gradients(quantizers_switcher: QuantizersSwitcher,
-                                   model: nn.Module,
-                                   weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
-                                   params_to_restore: ParamsToRestore):
+    def restore_disabled_gradients(
+        quantizers_switcher: QuantizersSwitcher,
+        model: nn.Module,
+        weight_quantizers: Dict[WeightQuantizerId, WeightQuantizerInfo],
+        params_to_restore: ParamsToRestore,
+    ):
         """
         Restore requires_grad property of all parameters back, except for ones that were originally disabled
         :param quantizers_switcher: object that is responsible for enabling and disabling quantizers
         :param model: model to access all parameters
         :param weight_quantizers: modules with quantized weights per scope
         :param params_to_restore: storage names of the parameters that should restore reguires_grad property
         """
@@ -483,16 +529,17 @@
                     param.requires_grad = True
 
         for param_name, param in model.named_parameters():
             if param_name not in params_to_restore.originally_disabled_gradients:
                 param.requires_grad = True
         quantizers_switcher.enable_quantizers()
 
-    def get_qconfig_sequences_constrained_by_traces_order(self, traces_order: TracesOrder) -> \
-            Tuple[List[QConfigSequenceForHAWQToEvaluate], List[CoveringQConfigSequenceForQuantNoiseCalculation]]:
+    def get_qconfig_sequences_constrained_by_traces_order(
+        self, traces_order: TracesOrder
+    ) -> Tuple[List[QConfigSequenceForHAWQToEvaluate], List[CoveringQConfigSequenceForQuantNoiseCalculation]]:
         possible_qconfigs_sequence_in_trace_order = []  # type: List[List[QuantizerConfig]]
         trace_order_indices_of_defaulted_qconfig_sequence = set()  # type: Set[int]
         quantizer_ids_in_exec_order = list(self._weight_quantizations_by_execution_order.keys())
         assert len(quantizer_ids_in_exec_order) == len(traces_order)
         for trace_idx in range(len(traces_order)):
             exec_idx = traces_order.get_execution_index_by_traces_index(trace_idx)
             qid = quantizer_ids_in_exec_order[exec_idx]
@@ -504,116 +551,125 @@
                 possible_qconfigs_sequence_in_trace_order.append(qconfig_constraints)
             else:
                 possible_qconfigs_sequence_in_trace_order.append([default_qconfig])
                 trace_order_indices_of_defaulted_qconfig_sequence.add(trace_idx)
 
         matcher = TraceOrderBitwidthMatcher(self._bitwidths, traces_order)
         return matcher.get_qconfig_sequences_constrained_by_trace_order(
-            possible_qconfigs_sequence_in_trace_order, trace_order_indices_of_defaulted_qconfig_sequence)
+            possible_qconfigs_sequence_in_trace_order, trace_order_indices_of_defaulted_qconfig_sequence
+        )
 
     def _get_weight_qp_ids_in_trace_order(self, traces_order: TracesOrder) -> List[Set[QuantizationPointId]]:
         quant_module_ids = list(self._weight_quantizations_by_execution_order.keys())
         qp_ids_in_trace_order = []
         for trace_idx in range(len(traces_order)):
             exec_idx = traces_order.get_execution_index_by_traces_index(trace_idx)
             quant_module_id = quant_module_ids[exec_idx]
             qp_ids_in_trace_order.append(self._algo.module_id_to_qp_id_translation_dict[quant_module_id])
         return qp_ids_in_trace_order
 
     @staticmethod
-    def _apply_qconfig_sequence_to_quantizer_setup(qconfig_sequence: CoveringQConfigSequenceForQuantNoiseCalculation,
-                                                   qp_ids_in_trace_order: List[Set[QuantizationPointId]],
-                                                   quantizer_setup: SingleConfigQuantizerSetup) -> \
-            SingleConfigQuantizerSetup:
+    def _apply_qconfig_sequence_to_quantizer_setup(
+        qconfig_sequence: CoveringQConfigSequenceForQuantNoiseCalculation,
+        qp_ids_in_trace_order: List[Set[QuantizationPointId]],
+        quantizer_setup: SingleConfigQuantizerSetup,
+    ) -> SingleConfigQuantizerSetup:
         retval = deepcopy(quantizer_setup)
         assert len(qconfig_sequence) == len(qp_ids_in_trace_order)
         for trace_idx, qp_id_set in enumerate(qp_ids_in_trace_order):
             for qp_id in qp_id_set:
                 retval.quantization_points[qp_id].qconfig = deepcopy(qconfig_sequence[trace_idx])
         return retval
 
-    def calc_quantization_noise(self, qconfig_sequences_to_run: List[CoveringQConfigSequenceForQuantNoiseCalculation],
-                                traces_order: TracesOrder) -> Tuple[Perturbations, List[List[PerturbationObserver]]]:
+    def calc_quantization_noise(
+        self, qconfig_sequences_to_run: List[CoveringQConfigSequenceForQuantNoiseCalculation], traces_order: TracesOrder
+    ) -> Tuple[Perturbations, List[List[PerturbationObserver]]]:
         perturbations = Perturbations()
         qp_ids_in_trace_order = self._get_weight_qp_ids_in_trace_order(traces_order)
         ctrl = self._algo
         observers_for_all_qconfig_sequences = []  # type: List[List[PerturbationObserver]]
         for qconfig_sequence in qconfig_sequences_to_run:
             quantizer_setup_to_run = self._apply_qconfig_sequence_to_quantizer_setup(
-                qconfig_sequence,
-                qp_ids_in_trace_order,
-                ctrl.get_quantizer_setup_for_current_state())
+                qconfig_sequence, qp_ids_in_trace_order, ctrl.get_quantizer_setup_for_current_state()
+            )
             ctrl, model = ctrl.apply_new_quantizer_setup(
-                quantizer_setup_to_run)  # type: Tuple[ExperimentalQuantizationController, NNCFNetwork]
+                quantizer_setup_to_run
+            )  # type: Tuple[ExperimentalQuantizationController, NNCFNetwork]
 
             hook_handles = []
             observers = []
             for qp_id_set in qp_ids_in_trace_order:
                 for qp_id in qp_id_set:
                     wq_id = ctrl.setup_to_module_id_translation_dict[qp_id]
                     wq_module = ctrl.weight_quantizers[wq_id].quantizer_module_ref
                     observer = PerturbationObserver(self._init_device)
                     hook_handles.append(wq_module.register_forward_hook(observer.calc_perturbation))
                     observers.append(observer)
 
-            model.do_dummy_forward(force_eval=True)
+            model.nncf.do_dummy_forward(force_eval=True)
 
             for i, observer in enumerate(observers):
-                perturbations.add(layer_id=traces_order.get_execution_index_by_traces_index(i),
-                                  qconfig=qconfig_sequence[i],
-                                  perturbation=observer.get_observation().to(self._init_device))
+                perturbations.add(
+                    layer_id=traces_order.get_execution_index_by_traces_index(i),
+                    qconfig=qconfig_sequence[i],
+                    perturbation=observer.get_observation().to(self._init_device),
+                )
 
             for handle in hook_handles:
                 handle.remove()
             observers_for_all_qconfig_sequences.append(observers)
 
         return perturbations, observers_for_all_qconfig_sequences
 
     @staticmethod
-    def calc_hawq_metric_per_qconfig_sequence(qconfig_sequences_in_trace_order: List[QConfigSequenceForHAWQToEvaluate],
-                                              perturbations: Perturbations,
-                                              traces_per_layer: TracesPerLayer, device) -> List[Tensor]:
+    def calc_hawq_metric_per_qconfig_sequence(
+        qconfig_sequences_in_trace_order: List[QConfigSequenceForHAWQToEvaluate],
+        perturbations: Perturbations,
+        traces_per_layer: TracesPerLayer,
+        device,
+    ) -> List[Tensor]:
         metric_per_qconfig_sequence = []
         for qconfig_sequence_in_trace_order in qconfig_sequences_in_trace_order:
             hawq_metric = torch.Tensor([0]).to(device)
             for trace_index, qconfig in enumerate(qconfig_sequence_in_trace_order):
                 execution_index = traces_per_layer.traces_order.get_execution_index_by_traces_index(trace_index)
                 hawq_metric += traces_per_layer.get_by_trace_index(trace_index) * perturbations.get(
-                    layer_id=execution_index, qconfig=qconfig)
+                    layer_id=execution_index, qconfig=qconfig
+                )
             metric_per_qconfig_sequence.append(hawq_metric)
         return metric_per_qconfig_sequence
 
     @staticmethod
-    def choose_qconfig_sequence(metric_per_qconfig_sequences: List[Tensor],
-                                compression_ratio_per_qconfig: List[float],
-                                compression_ratio) -> int:
+    def choose_qconfig_sequence(
+        metric_per_qconfig_sequences: List[Tensor], compression_ratio_per_qconfig: List[float], compression_ratio
+    ) -> int:
         num_qconfig_sequences = len(metric_per_qconfig_sequences)
 
         sorted_compression_ratio_per_qconfig = sorted(compression_ratio_per_qconfig)
-        indexes_of_sorted_compression_ratio = [x[0] for x in
-                                               sorted(enumerate(compression_ratio_per_qconfig), reverse=False,
-                                                      key=lambda x: x[1])]
+        indexes_of_sorted_compression_ratio = [
+            x[0] for x in sorted(enumerate(compression_ratio_per_qconfig), reverse=False, key=lambda x: x[1])
+        ]
 
         boundary_index = bisect_left(sorted_compression_ratio_per_qconfig, compression_ratio)
-        indexes_to_check = [indexes_of_sorted_compression_ratio[i] for i in
-                            range(boundary_index, num_qconfig_sequences)]
+        indexes_to_check = [
+            indexes_of_sorted_compression_ratio[i] for i in range(boundary_index, num_qconfig_sequences)
+        ]
         best_metric = min(list(itemgetter(*indexes_to_check)(metric_per_qconfig_sequences)))
         best_qconfig_sequence_index = metric_per_qconfig_sequences.index(best_metric)
         return best_qconfig_sequence_index
 
-    def get_quantizer_setup_for_qconfig_sequence(self,
-                                                 qconfig_sequence_in_traces_order: QConfigSequenceForHAWQToEvaluate,
-                                                 traces_order: TracesOrder) -> SingleConfigQuantizerSetup:
+    def get_quantizer_setup_for_qconfig_sequence(
+        self, qconfig_sequence_in_traces_order: QConfigSequenceForHAWQToEvaluate, traces_order: TracesOrder
+    ) -> SingleConfigQuantizerSetup:
         wqp_ids_in_trace_order = self._get_weight_qp_ids_in_trace_order(traces_order)
         ctrl = self._algo
 
         quantizer_setup_to_set = self._apply_qconfig_sequence_to_quantizer_setup(
-            qconfig_sequence_in_traces_order,
-            wqp_ids_in_trace_order,
-            ctrl.get_quantizer_setup_for_current_state())
+            qconfig_sequence_in_traces_order, wqp_ids_in_trace_order, ctrl.get_quantizer_setup_for_current_state()
+        )
 
         assert quantizer_setup_to_set.shared_input_operation_set_groups
         for group in quantizer_setup_to_set.shared_input_operation_set_groups.values():
             weight_qp_ids = []
             act_qp_ids = []
             for qp_id in group:
                 qp = quantizer_setup_to_set.quantization_points[qp_id]
@@ -621,31 +677,35 @@
                     weight_qp_ids.append(qp_id)
                 elif qp.is_activation_quantization_point():
                     act_qp_ids.append(qp_id)
             weight_qps = [quantizer_setup_to_set.quantization_points[qp_id] for qp_id in weight_qp_ids]
             weight_bitwidth_set = {weight_qp.qconfig.num_bits for weight_qp in weight_qps}
 
             if self._bitwidth_assignment_mode == BitwidthAssignmentMode.STRICT:
-                quantizer_setup_to_set = self._set_activations_bitwidth_strictly(quantizer_setup_to_set,
-                                                                                 act_qp_ids,
-                                                                                 weight_bitwidth_set)
+                quantizer_setup_to_set = self._set_activations_bitwidth_strictly(
+                    quantizer_setup_to_set, act_qp_ids, weight_bitwidth_set
+                )
             else:
-                quantizer_setup_to_set = self._set_activation_bitwidth_liberally(quantizer_setup_to_set,
-                                                                                 act_qp_ids,
-                                                                                 weight_bitwidth_set)
+                quantizer_setup_to_set = self._set_activation_bitwidth_liberally(
+                    quantizer_setup_to_set, act_qp_ids, weight_bitwidth_set
+                )
 
         return quantizer_setup_to_set
 
-    def _set_activation_bitwidth_liberally(self, quantizer_setup_to_set: SingleConfigQuantizerSetup,
-                                           act_qp_ids: List[QuantizationPointId],
-                                           weight_bitwidth_set: Set[int]) -> SingleConfigQuantizerSetup:
+    def _set_activation_bitwidth_liberally(
+        self,
+        quantizer_setup_to_set: SingleConfigQuantizerSetup,
+        act_qp_ids: List[QuantizationPointId],
+        weight_bitwidth_set: Set[int],
+    ) -> SingleConfigQuantizerSetup:
         for act_qp_id in act_qp_ids:
             original_quant_module_id = self._original_qp_id_vs_quantizer_module_id_dict[act_qp_id]
             activation_bitwidths_vs_qconfig_sequence = self._hw_precision_constraints.get_bitwidth_vs_qconfigs_dict(
-                original_quant_module_id)
+                original_quant_module_id
+            )
             activation_bitwidth_set = set(activation_bitwidths_vs_qconfig_sequence.keys())
             intersection = activation_bitwidth_set.intersection(weight_bitwidth_set)
             target_qp = quantizer_setup_to_set.quantization_points[act_qp_id]
             if activation_bitwidth_set.__len__() == 1:
                 target_bitwidth = activation_bitwidth_set.pop()
             elif intersection:
                 target_bitwidth = min(intersection)
@@ -661,48 +721,52 @@
             else:
                 # The activation has no constraints, so the config in the setup was defaulted
                 # and we can simply adjust the bitwidth
                 target_qp.qconfig.num_bits = target_bitwidth
 
         return quantizer_setup_to_set
 
-    def _set_activations_bitwidth_strictly(self, quantizer_setup_to_set: SingleConfigQuantizerSetup,
-                                           act_qp_ids: List[QuantizationPointId],
-                                           weight_bitwidth_set: Set[int]) -> SingleConfigQuantizerSetup:
+    def _set_activations_bitwidth_strictly(
+        self,
+        quantizer_setup_to_set: SingleConfigQuantizerSetup,
+        act_qp_ids: List[QuantizationPointId],
+        weight_bitwidth_set: Set[int],
+    ) -> SingleConfigQuantizerSetup:
         if len(weight_bitwidth_set) > 1:
-            raise RuntimeError('Invalid grouping of weight quantizers')
+            raise RuntimeError("Invalid grouping of weight quantizers")
         all_constraints = set()
-        original_quant_module_ids = [self._original_qp_id_vs_quantizer_module_id_dict[act_qp_id]
-                                     for act_qp_id in act_qp_ids]
+        original_quant_module_ids = [
+            self._original_qp_id_vs_quantizer_module_id_dict[act_qp_id] for act_qp_id in act_qp_ids
+        ]
         for act_quant_module_id in original_quant_module_ids:
             all_constraints.update(self._hw_precision_constraints.get_all_unique_bitwidths(act_quant_module_id))
         common_constraints = set(all_constraints)
         for act_quant_module_id in original_quant_module_ids:
             constraint = self._hw_precision_constraints.get_all_unique_bitwidths(act_quant_module_id)
             common_constraints = common_constraints.intersection(constraint)
         if weight_bitwidth_set:
             common_constraints = common_constraints.intersection(weight_bitwidth_set)
         if not common_constraints:
-            raise RuntimeError('No hardware compatible bitwidth for activation quantizers')
+            raise RuntimeError("No hardware compatible bitwidth for activation quantizers")
         for act_qp_id in act_qp_ids:
             quant_id = self._original_qp_id_vs_quantizer_module_id_dict[act_qp_id]
             target_bitwidth = sorted(list(common_constraints))[0]
-            bitwidths_vs_qconfig_sequence = self._hw_precision_constraints.get_bitwidth_vs_qconfigs_dict(
-                quant_id)
+            bitwidths_vs_qconfig_sequence = self._hw_precision_constraints.get_bitwidth_vs_qconfigs_dict(quant_id)
             qconfig_to_select = bitwidths_vs_qconfig_sequence[target_bitwidth][0]
             quantizer_setup_to_set.quantization_points[act_qp_id].qconfig = qconfig_to_select
 
         return quantizer_setup_to_set
 
     @staticmethod
     def _filter_qconfig_sequences_by_grouped_weight_quantizers(
-            trace_ordered_qconfig_sequences: List[QConfigSequenceForHAWQToEvaluate],
-            weight_quantization_ids_by_execution_order: List[QuantizerId],
-            groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
-            traces_order: TracesOrder) -> List[QConfigSequenceForHAWQToEvaluate]:
+        trace_ordered_qconfig_sequences: List[QConfigSequenceForHAWQToEvaluate],
+        weight_quantization_ids_by_execution_order: List[QuantizerId],
+        groups_of_adjacent_quantizers: GroupsOfAdjacentQuantizers,
+        traces_order: TracesOrder,
+    ) -> List[QConfigSequenceForHAWQToEvaluate]:
         """
         Removes configs where adjacent weight quantizers have different bitwidth. Adjacency is defined by common
         activation quantizers
         """
         filtered_qconfig_sequences = []
         all_grouped_indexes = []
         for group_of_adjacent_quantizers in groups_of_adjacent_quantizers:
@@ -728,18 +792,17 @@
                     keep_config = False
                     break
             if keep_config:
                 filtered_qconfig_sequences.append(qconfig_sequence)
 
         return filtered_qconfig_sequences
 
-    def _filter_qconfig_sequences_by_excessive_bitwidth(self,
-                                                        weight_qconfig_sequences_in_trace_order: List[
-                                                            QConfigSequenceForHAWQToEvaluate]) \
-            -> List[QConfigSequenceForHAWQToEvaluate]:
+    def _filter_qconfig_sequences_by_excessive_bitwidth(
+        self, weight_qconfig_sequences_in_trace_order: List[QConfigSequenceForHAWQToEvaluate]
+    ) -> List[QConfigSequenceForHAWQToEvaluate]:
         result = weight_qconfig_sequences_in_trace_order
         if self._hw_precision_constraints:
             all_weight_bitwidths = set()
             for wq_id in self._algo.weight_quantizers:
                 all_weight_bitwidths.update(self._hw_precision_constraints.get_all_unique_bitwidths(wq_id))
 
             all_activation_bitwidths = set()
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/manual_init.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/manual_init.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,59 +1,56 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
-from typing import Dict
-from typing import List
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import Dict, List
 
+from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
 from nncf.torch.quantization.precision_constraints import HardwareQuantizationConstraints
-from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitParams
 from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitializer
-from nncf.common.quantization.quantizer_setup import SingleConfigQuantizerSetup
+from nncf.torch.quantization.precision_init.base_init import BasePrecisionInitParams
 from nncf.torch.structures import QuantizationPrecisionInitArgs
 
 
 class ManualPrecisionInitParams(BasePrecisionInitParams):
-    def __init__(self,
-                 user_init_args: QuantizationPrecisionInitArgs = None,
-                 bitwidth_per_scope: List[List] = None):
+    def __init__(self, user_init_args: QuantizationPrecisionInitArgs = None, bitwidth_per_scope: List[List] = None):
         super().__init__(user_init_args)
         self.bitwidth_per_scope = bitwidth_per_scope
 
     @classmethod
-    def from_config(cls,
-                    manual_init_params_dict: Dict):
-        return cls(user_init_args=None,
-                   bitwidth_per_scope=manual_init_params_dict.get("bitwidth_per_scope", []))
+    def from_config(cls, manual_init_params_dict: Dict):
+        return cls(user_init_args=None, bitwidth_per_scope=manual_init_params_dict.get("bitwidth_per_scope", []))
 
 
 class ManualPrecisionInitializer(BasePrecisionInitializer):
-    def __init__(self,
-                 algo: 'ExperimentalQuantizationController',
-                 params: ManualPrecisionInitParams,
-                 hw_precision_constraints: HardwareQuantizationConstraints = None):
+    def __init__(
+        self,
+        algo: "ExperimentalQuantizationController",
+        params: ManualPrecisionInitParams,
+        hw_precision_constraints: HardwareQuantizationConstraints = None,
+    ):
         super().__init__(algo, params, hw_precision_constraints)
         self._bitwidth_per_scope = params.bitwidth_per_scope
 
     def apply_init(self) -> SingleConfigQuantizerSetup:
         quantizer_setup = self._algo.get_quantizer_setup_for_current_state()
         for pair in self._bitwidth_per_scope:
             bitwidth, scope_name = pair
             is_matched = False
-            msg = 'Failed to assign bitwidth={} to `{}`,\n' \
-                  'because it is incompatible for the specified target hardware\n' \
-                  'Supported quantization configs: {}'
+            msg = (
+                "Failed to assign bitwidth={} to `{}`,\n"
+                "because it is incompatible for the specified target hardware\n"
+                "Supported quantization configs: {}"
+            )
             for qp_id, qp in quantizer_setup.quantization_points.items():
                 if scope_name in str(qp.insertion_point):
                     if self._hw_precision_constraints:
                         q_id = self._algo.setup_to_module_id_translation_dict[qp_id]
                         q_configs = self._hw_precision_constraints.get(q_id)
                         matched_q_configs = list(filter(lambda x: x.num_bits == bitwidth, q_configs))
                         if not matched_q_configs:
@@ -61,10 +58,11 @@
                         qp.qconfig = matched_q_configs[0]
                     else:
                         qp.qconfig.num_bits = bitwidth
                     is_matched = True
                     break
             if not is_matched:
                 raise ValueError(
-                    'Could not find a quantization point at scope name `{}`, failed to assign bitwidth {} '
-                    'to it'.format(scope_name, bitwidth))
+                    "Could not find a quantization point at scope name `{}`, failed to assign bitwidth {} "
+                    "to it".format(scope_name, bitwidth)
+                )
         return quantizer_setup
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/perturbations.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/perturbations.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Dict
 
 import torch
 from torch import Tensor
 
 from nncf.common.quantization.structs import QuantizerConfig
 from nncf.torch.dynamic_graph.context import no_nncf_trace
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/precision_init/traces_order.py` & `nncf-2.5.0/nncf/torch/quantization/precision_init/traces_order.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,30 +1,29 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import List
 
 from torch import Tensor
 
 
 class TracesOrder:
     def __init__(self, execution_indexes_of_weights_ordered_by_traces: List[int]):
         self._index_by_traces_to_execution_index = execution_indexes_of_weights_ordered_by_traces
         self._num_weights = len(execution_indexes_of_weights_ordered_by_traces)
-        self._index_by_execution_to_index_by_traces = \
-            [execution_indexes_of_weights_ordered_by_traces.index(i) for i in range(self._num_weights)]
+        self._index_by_execution_to_index_by_traces = [
+            execution_indexes_of_weights_ordered_by_traces.index(i) for i in range(self._num_weights)
+        ]
 
     def get_execution_order_configs(self, trace_ordered_configuration: List) -> List:
         if len(trace_ordered_configuration) != self._num_weights:
             raise ValueError("Incompatible configuration size!")
         execution_order_config = [None] * self._num_weights
         for i, config in enumerate(trace_ordered_configuration):
             execution_order_config[self._index_by_traces_to_execution_index[i]] = config
@@ -47,16 +46,17 @@
     def __len__(self):
         return len(self._index_by_execution_to_index_by_traces)
 
 
 class TracesPerLayer:
     def __init__(self, traces_per_layer_by_execution: Tensor):
         self._traces_per_layer_by_execution = traces_per_layer_by_execution
-        execution_indexes_of_weights_in_descending_order_of_traces = \
-            [i[0] for i in sorted(enumerate(traces_per_layer_by_execution), reverse=False, key=lambda x: x[1])]
+        execution_indexes_of_weights_in_descending_order_of_traces = [
+            i[0] for i in sorted(enumerate(traces_per_layer_by_execution), reverse=False, key=lambda x: x[1])
+        ]
         self.traces_order = TracesOrder(execution_indexes_of_weights_in_descending_order_of_traces)
 
     def get_by_execution_index(self, execution_index: int) -> Tensor:
         return self._traces_per_layer_by_execution[execution_index]
 
     def get_by_trace_index(self, trace_index: int) -> Tensor:
         execution_index = self.traces_order.get_execution_index_by_traces_index(trace_index)
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/quantize.py` & `nncf-2.5.0/nncf/torch/quantization/quantize_model.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,43 +1,46 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from copy import deepcopy
 from typing import Any, Dict, Optional, Tuple
 
 import torch
+
 from nncf.common.quantization.structs import QuantizationPreset
 from nncf.config import NNCFConfig
 from nncf.config.structures import BNAdaptationInitArgs
 from nncf.config.structures import QuantizationRangeInitArgs
 from nncf.data import Dataset
-from nncf.parameters import convert_ignored_scope_to_list
-from nncf.parameters import IgnoredScope
 from nncf.parameters import ModelType
 from nncf.parameters import TargetDevice
+from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters
+from nncf.quantization.advanced_parameters import convert_advanced_parameters_to_dict
+from nncf.scopes import IgnoredScope
+from nncf.scopes import convert_ignored_scope_to_list
 from nncf.torch.dynamic_graph.context import no_nncf_trace
 from nncf.torch.dynamic_graph.io_handling import replicate_same_tensors
 from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_inputs_with_objwalk
 from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_outputs_with_objwalk
 from nncf.torch.initialization import PTInitializingDataLoader
 from nncf.torch.model_creation import create_compressed_model
 from nncf.torch.nested_objects_traversal import objwalk
 from nncf.torch.utils import get_model_device
 from nncf.torch.utils import is_tensor
 
+DEFAULT_RANGE_TYPE = "mean_min_max"
+
 
 # TODO(alexsu52): It is a workaround and should be removed.
 class CalibrarionDataLoader(PTInitializingDataLoader):
     """
     This class wraps the nncf.Dataset.
 
     This is required for proper initialization of certain compression algorithms.
@@ -45,135 +48,181 @@
 
     def __init__(self, data_loader: Dataset):
         super().__init__(data_loader)
         self._length = None
 
     @property
     def batch_size(self):
-        data_source = getattr(self._data_loader, '_data_source')
-        return getattr(data_source, 'batch_size', 1)
+        data_source = getattr(self._data_loader, "_data_source")
+        return getattr(data_source, "batch_size", 1)
 
     def __iter__(self):
         return iter(self._data_loader.get_inference_data())
 
     def __len__(self):
         if self._length is None:
             data = self._data_loader.get_inference_data()
             self._length = CalibrarionDataLoader._get_length(data)
         return self._length
 
     def get_inputs(self, dataloader_output: Any) -> Tuple[Tuple, Dict]:
         if not isinstance(dataloader_output, tuple):
-            dataloader_output = (dataloader_output, )
+            dataloader_output = (dataloader_output,)
         return dataloader_output, {}
 
     @staticmethod
     def _get_length(iterable) -> int:
         length = 0
         for _ in iterable:
             length = length + 1
 
         return length
 
 
-def _get_transformer_quantization_config(subset_size: int) -> Dict:
+def _get_transformer_quantization_config(subset_size: int) -> Dict[str, Any]:
     """
+    Returns the quantization config for transformer-based models.
+
+    :param subset_size: Size of a subset to calculate activations
+        statistics used for quantization.
     :return: The quantization config for transformer-based models.
     """
     return {
-        'algorithm': 'quantization',
-        'preset': 'mixed',
-        'initializer': {
-            'range': {'num_init_samples': subset_size, 'type': 'mean_min_max'},
-            'batchnorm_adaptation': {'num_bn_adaptation_samples': 0},
+        "algorithm": "quantization",
+        "preset": "mixed",
+        "initializer": {
+            "range": {"num_init_samples": subset_size, "type": DEFAULT_RANGE_TYPE},
+            "batchnorm_adaptation": {"num_bn_adaptation_samples": 0},
         },
-        'scope_overrides': {
-            'activations': {'{re}.*matmul_0': {'mode': 'symmetric'}}},
-        'ignored_scopes': [
-            '{re}.*Embeddings.*',
-            '{re}.*__add___[0-1]',
-            '{re}.*layer_norm_0',
-            '{re}.*matmul_1',
-            '{re}.*__truediv__*',
+        "scope_overrides": {"activations": {"{re}.*matmul_0": {"mode": "symmetric"}}},
+        "ignored_scopes": [
+            "{re}.*Embeddings.*",
+            "{re}.*__add___[0-1]",
+            "{re}.*layer_norm_0",
+            "{re}.*matmul_1",
+            "{re}.*__truediv__*",
         ],
-        'overflow_fix': 'disable',
+        "overflow_fix": "first_layer_only",
     }
 
 
-def _get_default_quantization_config(preset: QuantizationPreset,
-                                     subset_size: int) -> Dict:
+def _get_default_quantization_config(preset: QuantizationPreset, subset_size: int) -> Dict[str, Any]:
     """
+    Returns the default quantization config
+
+    :param preset: A preset that controls the quantization mode
+        (symmetric and asymmetric). It can take the following values:
+        - `performance`: Symmetric quantization of weights and activations.
+        - `mixed`: Symmetric quantization of weights and asymmetric
+          quantization of activations.
+    :param subset_size: Size of a subset to calculate activations
+        statistics used for quantization.
     :return: The default quantization config.
     """
     return {
-        'algorithm': 'quantization',
-        'preset': preset.value,
-        'initializer': {
-            'range': {'num_init_samples': subset_size},
-            'batchnorm_adaptation': {'num_bn_adaptation_samples': 0}
+        "algorithm": "quantization",
+        "preset": preset.value,
+        "initializer": {
+            "range": {"num_init_samples": subset_size, "type": DEFAULT_RANGE_TYPE},
+            "batchnorm_adaptation": {"num_bn_adaptation_samples": subset_size},
         },
-        'overflow_fix': 'first_layer_only'
+        "overflow_fix": "first_layer_only",
     }
 
 
-def _create_nncf_config(preset: QuantizationPreset,
-                        target_device: TargetDevice,
-                        subset_size: int,
-                        model_type: Optional[ModelType],
-                        ignored_scope: Optional[IgnoredScope]) -> NNCFConfig:
+def _create_nncf_config(
+    preset: QuantizationPreset,
+    target_device: TargetDevice,
+    subset_size: int,
+    model_type: Optional[ModelType],
+    ignored_scope: Optional[IgnoredScope],
+    advanced_parameters: Optional[AdvancedQuantizationParameters],
+) -> NNCFConfig:
     """
-    :return: The NNCFConfig for quantization method.
+    Creates the NNCFConfig for the quantization algorithm.
+
+    :param preset: A preset that controls the quantization mode
+        (symmetric and asymmetric). It can take the following values:
+        - `performance`: Symmetric quantization of weights and activations.
+        - `mixed`: Symmetric quantization of weights and asymmetric
+          quantization of activations.
+    :param target_device: A target device the specificity of which will be taken
+        into account while compressing in order to obtain the best performance
+        for this type of device.
+    :param subset_size: Size of a subset to calculate activations
+        statistics used for quantization.
+    :param model_type: Model type is needed to specify additional patterns
+        in the model.
+    :param ignored_scope: An ignored scope that defined the list of model control
+        flow graph nodes to be ignored during quantization.
+    :param advanced_parameters: Advanced quantization parameters for
+        fine-tuning the quantization algorithm.
+    :return: NNCFConfig for the quantization algorithm.
     """
     if model_type is None:
         compression_config = _get_default_quantization_config(preset, subset_size)
     elif model_type == ModelType.TRANSFORMER:
         compression_config = _get_transformer_quantization_config(subset_size)
 
     if ignored_scope is not None:
         _ignored_scope = convert_ignored_scope_to_list(ignored_scope)
-        if 'ignored_scopes' in compression_config:
-            compression_config['ignored_scopes'].extend(_ignored_scope)
+        if "ignored_scopes" in compression_config:
+            compression_config["ignored_scopes"].extend(_ignored_scope)
         else:
-            compression_config['ignored_scopes'] = _ignored_scope
+            compression_config["ignored_scopes"] = _ignored_scope
+
+    if advanced_parameters is not None:
+        advanced_config = convert_advanced_parameters_to_dict(advanced_parameters)
 
-    return NNCFConfig({
-        'target_device': target_device.value,
-        'compression': compression_config
-    })
-
-
-def quantize_impl(model: torch.nn.Module,
-                  calibration_dataset: Dataset,
-                  preset: QuantizationPreset,
-                  target_device: TargetDevice,
-                  subset_size: int,
-                  fast_bias_correction: bool,
-                  model_type: Optional[ModelType] = None,
-                  ignored_scope: Optional[IgnoredScope] = None) -> torch.nn.Module:
+        ranges = advanced_config.get("initializer", {}).get("range")
+        if ranges is not None:
+            for rconfig in ranges:
+                rconfig["num_init_samples"] = subset_size
+                if "type" not in rconfig:
+                    rconfig["type"] = DEFAULT_RANGE_TYPE
+
+        compression_config.update(advanced_config)
+
+    return NNCFConfig({"target_device": target_device.value, "compression": compression_config})
+
+
+def quantize_impl(
+    model: torch.nn.Module,
+    calibration_dataset: Dataset,
+    preset: QuantizationPreset,
+    target_device: TargetDevice,
+    subset_size: int,
+    fast_bias_correction: bool,
+    model_type: Optional[ModelType] = None,
+    ignored_scope: Optional[IgnoredScope] = None,
+    advanced_parameters: Optional[AdvancedQuantizationParameters] = None,
+) -> torch.nn.Module:
     """
     Implementation of the `quantize()` method for the PyTorch backend.
     """
     if fast_bias_correction is False:
-        raise ValueError(f'fast_bias_correction={fast_bias_correction} is not '
-                          'supported')
-    if ignored_scope is not None and ignored_scope.types is not None:
-        raise RuntimeError('Quantization algorithm from the PyTorch backend '
-                            'does not support operation types in the ignored '
-                            'scopes yet')
+        raise ValueError(f"fast_bias_correction={fast_bias_correction} is not " "supported")
+    if ignored_scope is not None and ignored_scope.types:
+        raise RuntimeError(
+            "Quantization algorithm from the PyTorch backend "
+            "does not support operation types in the ignored "
+            "scopes yet"
+        )
     if target_device == TargetDevice.CPU_SPR:
-        raise RuntimeError('target_device == CPU_SPR is not supported')
+        raise RuntimeError("target_device == CPU_SPR is not supported")
 
-    nncf_config = _create_nncf_config(preset, target_device, subset_size,
-                                      model_type, ignored_scope)
+    nncf_config = _create_nncf_config(
+        preset, target_device, subset_size, model_type, ignored_scope, advanced_parameters
+    )
 
     calibration_data_loader = CalibrarionDataLoader(calibration_dataset)
     nncf_config.register_extra_structs(
         [
             QuantizationRangeInitArgs(data_loader=calibration_data_loader),
-            BNAdaptationInitArgs(data_loader=calibration_data_loader)
+            BNAdaptationInitArgs(data_loader=calibration_data_loader),
         ]
     )
 
     def wrap_inputs(args, kwargs):
         return wrap_nncf_model_inputs_with_objwalk(args, kwargs)
 
     def wrap_outputs(retval):
@@ -188,28 +237,27 @@
                 def send_to_device(tensor):
                     return tensor.to(device)
 
                 args = objwalk(args, is_tensor, send_to_device)
                 kwargs = objwalk(kwargs, is_tensor, send_to_device)
 
             args, kwargs = wrap_inputs(args, kwargs)
-            retval =  model(*args, **kwargs)
+            retval = model(*args, **kwargs)
             retval = replicate_same_tensors(retval)
             return wrap_outputs(retval)
 
         return dummy_forward
 
-    dummy_forward_fn = create_dummy_forward_fn(calibration_data_loader,
-                                               get_model_device(model))
+    dummy_forward_fn = create_dummy_forward_fn(calibration_data_loader, get_model_device(model))
 
     clone_model = deepcopy(model)
     compression_ctrl, compressed_model = create_compressed_model(
         model=clone_model,
         config=nncf_config,
         dummy_forward_fn=dummy_forward_fn,
         wrap_inputs_fn=wrap_inputs,
-        wrap_outputs_fn=wrap_outputs
+        wrap_outputs_fn=wrap_outputs,
     )
     compression_ctrl.prepare_for_export()
-    compressed_model.disable_dynamic_graph_building()
+    compressed_model.nncf.disable_dynamic_graph_building()
 
     return compressed_model
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/quantize_functions.py` & `nncf-2.5.0/nncf/torch/quantization/quantize_functions.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,29 +1,28 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Any
 
 import torch
 
-from nncf.torch.utils import add_domain
 from nncf.common.logging import nncf_logger
-
-from nncf.torch.quantization.extensions import QuantizedFunctionsCPU, QuantizedFunctionsCUDA
 from nncf.torch.dynamic_graph.patch_pytorch import register_operator
-from nncf.torch.functions import STRound, clamp
+from nncf.torch.functions import STRound
+from nncf.torch.functions import clamp
+from nncf.torch.quantization.extensions import QuantizedFunctionsCPU
+from nncf.torch.quantization.extensions import QuantizedFunctionsCUDA
+from nncf.torch.utils import add_domain
 
 
 # pylint:disable=abstract-method
 class QuantizeSymmetric(torch.autograd.Function):
     @staticmethod
     def forward(ctx, input_, scale, level_low, level_high, levels):
         input_low = scale * (level_low / level_high)
@@ -121,15 +120,15 @@
             )
 
         return grad_input, grad_input_low, grad_input_range, None, None, None
 
 
 def _quantize_autograd_to_range(input_, input_low, input_high, levels):
     input_ = input_ - input_low
-    input_range = (input_high - input_low)
+    input_range = input_high - input_low
     scale = (levels - 1) / input_range
     output = clamp(input_, low=torch.zeros_like(input_), high=input_range)
     output = output * scale
     output = STRound.apply(output)
     output = output * input_range / (levels - 1) + input_low
     return output
 
@@ -170,26 +169,23 @@
     @staticmethod
     def backward(ctx: Any, *grad_outputs: Any) -> Any:
         # backward is not used during export
         return grad_outputs[0]
 
 
 def get_scale_zp_from_input_low_input_high(level_low, level_high, input_low, input_high):
-    levels = level_high - level_low + 1
-    assert levels in [255, 256], "Can only export to INT8 256-level ONNX Quantize/Dequantize pairs"
-
     y_scale = (input_high - input_low) / (level_high - level_low)
     y_zero_point = (level_low * input_high - level_high * input_low) / (input_high - input_low)
 
     type_ = torch.int8 if level_low < 0 else torch.uint8
     level_low *= torch.ones_like(y_zero_point).to(type_)
     level_high *= torch.ones_like(y_zero_point).to(type_)
     level_low = level_low.to(y_zero_point.device)
     level_high = level_high.to(y_zero_point.device)
-    y_zero_point = torch.min(torch.max(level_low, y_zero_point.to(type_)), level_high)
+    y_zero_point = torch.min(torch.max(level_low, torch.round(y_zero_point).to(type_)), level_high)
 
     y_scale = torch.squeeze(y_scale)
     y_zero_point = torch.squeeze(y_zero_point)
     return y_scale, y_zero_point
 
 
 @register_operator()
@@ -208,27 +204,34 @@
     input_range_safe = abs(input_range) + eps
     input_low_tuned, input_range_tuned = TuneRange.apply(input_low, input_range_safe, levels)
     return QuantizeAsymmetric.apply(input_, input_low_tuned, input_range_tuned, level_low, level_high, levels)
 
 
 # pylint:disable=abstract-method
 class TuneRange(torch.autograd.Function):
+    """
+    Makes sure that the zero-point quantum in the quantized domain points exactly to floating point zero,
+    e.g. that the input floating point zeroes to the fake quantization operation are translated to output
+    floating point zeroes even if we don't use rounding.
+    See [docs](../../../docs/compression_algorithms/Quantization.md#asymmetric-quantization) for details.
+    """
+
     @staticmethod
     def forward(ctx, input_low, input_range, levels):
         input_high = input_range + input_low
         input_low_copy = input_low.clone()
         input_low_copy[input_low_copy > 0] = 0
         input_high[input_high < 0] = 0
         n = levels - 1
         # Need a cast here because fp16 division yileds fp32 results sometimes
         scale = (levels / (input_high - input_low_copy)).to(dtype=input_high.dtype)
         zp = torch.round(-input_low_copy * scale)
 
         new_input_low = torch.where(zp < n, zp / (zp - n) * input_high, input_low_copy)
-        new_input_high = torch.where(zp > 0., (zp - n) / zp * input_low_copy, input_high)
+        new_input_high = torch.where(zp > 0.0, (zp - n) / zp * input_low_copy, input_high)
 
         range_1 = input_high - new_input_low
         range_2 = new_input_high - input_low_copy
 
         mask = (range_1 > range_2).to(input_high.dtype)
         inv_mask = (1 - mask).abs()
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/reference.py` & `nncf-2.5.0/nncf/torch/quantization/reference.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,84 +1,92 @@
 from enum import Enum
-from typing import List
-from typing import Tuple
-from typing import TypeVar
+from typing import List, Tuple, TypeVar
 
 import numpy as np
 import torch
 
 from nncf.torch.utils import sum_like
 
-GeneralizedTensor = TypeVar('GeneralizedTensor', torch.Tensor, np.ndarray)
+GeneralizedTensor = TypeVar("GeneralizedTensor", torch.Tensor, np.ndarray)
 
 
 class ReferenceBackendType(Enum):
-    NUMPY = 'numpy'
-    TORCH = 'torch'
+    NUMPY = "numpy"
+    TORCH = "torch"
 
 
 class ReferenceQuantize:
     def __init__(self, backend_type: ReferenceBackendType):
         if backend_type is ReferenceBackendType.NUMPY:
             self.backend = np
         elif backend_type is ReferenceBackendType.TORCH:
             self.backend = torch
         else:
             raise RuntimeError("Unknown backend for ReferenceQuantize")
 
-    def forward(self,
-                input_: GeneralizedTensor,
-                input_low: GeneralizedTensor,
-                input_range: GeneralizedTensor,
-                levels: int) -> GeneralizedTensor:
+    def _astype(self, tensor: GeneralizedTensor, dtype) -> GeneralizedTensor:
+        if self.backend is np:
+            return tensor.astype(dtype)
+        return tensor.type(dtype)
+
+    def forward(
+        self, input_: GeneralizedTensor, input_low: GeneralizedTensor, input_range: GeneralizedTensor, levels: int
+    ) -> GeneralizedTensor:
         scale = (levels - 1) / input_range
         output = input_.clip(min=input_low, max=input_low + input_range)
-        zero_point = (- input_low * scale).round()
+        zero_point = (-input_low * scale).round()
         output -= input_low
         output *= scale
         output -= zero_point
         output = output.round()
         output = output / scale
         return output
 
-    def backward(self,
-                 grad_output: GeneralizedTensor,
-                 input_: GeneralizedTensor,
-                 input_low: GeneralizedTensor,
-                 input_range: GeneralizedTensor,
-                 output: GeneralizedTensor,
-                 level_low: int,
-                 level_high: int,
-                 range_sign: int) -> List[GeneralizedTensor]:
-        mask_hi = (input_ > (input_low + input_range)).astype(input_.dtype)
-        mask_lo = (input_ < input_low).astype(input_.dtype)
+    def backward(
+        self,
+        grad_output: GeneralizedTensor,
+        input_: GeneralizedTensor,
+        input_low: GeneralizedTensor,
+        input_range: GeneralizedTensor,
+        output: GeneralizedTensor,
+        level_low: int,
+        level_high: int,
+        is_asymmetric: bool = False,
+    ) -> List[GeneralizedTensor]:
+        # is_asymmetric is unused, present only to correspond to the CPU signature of calling "backward"
+        mask_hi = input_ > (input_low + input_range)
+        mask_hi = self._astype(mask_hi, input_.dtype)
+        mask_lo = input_ < input_low
+        mask_lo = self._astype(mask_lo, input_.dtype)
 
         mask_in = 1 - mask_hi - mask_lo
+        range_sign = np.sign(input_range)
         err = (output - input_) * np.reciprocal(input_range * range_sign)
         grad_range = grad_output * (err * mask_in + range_sign * (level_low / level_high) * mask_lo + mask_hi)
         grad_range = sum_like(grad_range, input_range)
 
         grad_input = grad_output * mask_in
 
         grad_low = grad_output * (mask_hi + mask_lo)
         grad_low = sum_like(grad_low, input_low)
         return [grad_input, grad_low, grad_range]
 
-    def tune_range(self, input_low: GeneralizedTensor, input_range: GeneralizedTensor, levels: int) \
-            -> Tuple[GeneralizedTensor, GeneralizedTensor]:
+    def tune_range(
+        self, input_low: GeneralizedTensor, input_range: GeneralizedTensor, levels: int
+    ) -> Tuple[GeneralizedTensor, GeneralizedTensor]:
         input_high = input_range + input_low
         input_low[input_low > 0] = 0
         input_high[input_high < 0] = 0
         n = levels - 1
         scale = levels / (input_high - input_low)
         scale = scale.astype(dtype=input_high.dtype)
         zp = self.backend.round(-input_low * scale)
 
         new_input_low = self.backend.where(zp < n, zp / (zp - n) * input_high, input_low)
-        new_input_high = self.backend.where(zp > 0., (zp - n) / zp * input_low, input_high)
+        new_input_high = self.backend.where(zp > 0.0, (zp - n) / zp * input_low, input_high)
 
         range_1 = input_high - new_input_low
         range_2 = new_input_high - input_low
 
         mask = (range_1 > range_2).astype(input_high.dtype)
         inv_mask = abs(1 - mask)
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/schedulers.py` & `nncf-2.5.0/nncf/torch/quantization/schedulers.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,53 +1,51 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import logging
 
-from nncf.common.utils.registry import Registry
-from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.api.compression import CompressionStage
+from nncf.common.schedulers import BaseCompressionScheduler
+from nncf.common.utils.registry import Registry
 from nncf.config.schemata.defaults import ACTIVATIONS_QUANT_START_EPOCH
 from nncf.config.schemata.defaults import WEIGHTS_QUANT_START_EPOCH
 
 logger = logging.getLogger(__name__)
 
 QUANTIZATION_SCHEDULERS = Registry("quantization_schedulers")
 
 
 @QUANTIZATION_SCHEDULERS.register("staged")
 class StagedQuantizationScheduler(BaseCompressionScheduler):
-    def __init__(self, quantization_ctrl: 'QuantizationController', params=None):
+    def __init__(self, quantization_ctrl: "QuantizationController", params=None):
         super().__init__()
         if params is None:
             params = {}
         self.algo = quantization_ctrl
-        self.activations_quant_start_epoch = params.get('activations_quant_start_epoch', ACTIVATIONS_QUANT_START_EPOCH)
-        self.weights_quant_start_epoch = params.get('weights_quant_start_epoch', WEIGHTS_QUANT_START_EPOCH)
+        self.activations_quant_start_epoch = params.get("activations_quant_start_epoch", ACTIVATIONS_QUANT_START_EPOCH)
+        self.weights_quant_start_epoch = params.get("weights_quant_start_epoch", WEIGHTS_QUANT_START_EPOCH)
         self._set_quantization_status()
 
     def epoch_step(self, next_epoch=None):
         super().epoch_step(next_epoch)
         should_call_init = False
         if self.current_epoch == self.activations_quant_start_epoch:
-            logger.info('Enabled quantization of activations')
+            logger.info("Enabled quantization of activations")
             self.algo.enable_activation_quantization()
             should_call_init = True
 
         if self.current_epoch == self.weights_quant_start_epoch:
-            logger.info('Enabled quantization of weights')
+            logger.info("Enabled quantization of weights")
             self.algo.enable_weight_quantization()
             should_call_init = True
 
         if should_call_init:
             self.algo.init_range()
 
     def load_state(self, state):
@@ -55,24 +53,24 @@
         # Just enables/disables quantizers without calling initialization of ranges, because it's called on epoch_step
         # in the end of previous epoch before saving the scheduler's state dict.
         self._set_quantization_status()
 
     def _set_quantization_status(self):
         if max(self.current_epoch, 0) >= self.activations_quant_start_epoch:
             self.algo.enable_activation_quantization()
-            logger.info('Enabled quantization of activations')
+            logger.info("Enabled quantization of activations")
         else:
             self.algo.disable_activation_quantization()
-            logger.info('Disabled quantization of activations')
+            logger.info("Disabled quantization of activations")
         if max(self.current_epoch, 0) >= self.weights_quant_start_epoch:
             self.algo.enable_weight_quantization()
-            logger.info('Enabled quantization of weights')
+            logger.info("Enabled quantization of weights")
         else:
             self.algo.disable_weight_quantization()
-            logger.info('Disabled quantization of weights')
+            logger.info("Disabled quantization of weights")
 
     def _calc_density_level(self):
         raise NotImplementedError
 
     def compression_stage(self):
         is_activations_enabled = self.current_epoch >= self.activations_quant_start_epoch
         is_weights_enabled = self.current_epoch >= self.weights_quant_start_epoch
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/statistics.py` & `nncf-2.5.0/nncf/torch/quantization/statistics.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.api.statistics import Statistics
 from nncf.common.utils.helpers import create_table
 
 
 def _proportion_str(num: int, total_count: int):
     percentage = 100 * (num / max(total_count, 1))
-    return f'{percentage:.2f} % ({num} / {total_count})'
+    return f"{percentage:.2f} % ({num} / {total_count})"
 
 
 class MemoryConsumptionStatistics(Statistics):
     """
     Contains statistics of the memory consumption.
     """
 
-    def __init__(self,
-                 fp32_weight_size: int = 0,
-                 quantized_weight_size: int = 0,
-                 max_fp32_activation_size: int = 0,
-                 max_compressed_activation_size: int = 0,
-                 weight_memory_consumption_decrease: float = 0.0):
+    def __init__(
+        self,
+        fp32_weight_size: int = 0,
+        quantized_weight_size: int = 0,
+        max_fp32_activation_size: int = 0,
+        max_compressed_activation_size: int = 0,
+        weight_memory_consumption_decrease: float = 0.0,
+    ):
         """
         Initializes statistics of the memory consumption.
 
         :param fp32_weight_size: Memory consumption for full-precision weights (Mbyte).
         :param quantized_weight_size: Memory consumption for quantized weights (Mbyte).
         :param max_fp32_activation_size: Max memory consumption for an activation
             tensor in FP32 model (Mbyte).
@@ -46,31 +46,31 @@
         self.quantized_weight_size = quantized_weight_size
         self.max_fp32_activation_size = max_fp32_activation_size
         self.max_compressed_activation_size = max_compressed_activation_size
         self.weight_memory_consumption_decrease = weight_memory_consumption_decrease
 
     def to_str(self) -> str:
         memory_consumption_string = create_table(
-            header=['Statistic\'s name', 'Value'],
+            header=["Statistic's name", "Value"],
             rows=[
-                ['Memory consumption for full-precision weights (Mbyte)', self.fp32_weight_size],
-                ['Memory consumption for quantized weights (Mbyte)', self.quantized_weight_size],
+                ["Memory consumption for full-precision weights (Mbyte)", self.fp32_weight_size],
+                ["Memory consumption for quantized weights (Mbyte)", self.quantized_weight_size],
                 [
-                    'Max memory consumption for an activation tensor in FP32 model (Mbyte)',
-                    self.max_fp32_activation_size
+                    "Max memory consumption for an activation tensor in FP32 model (Mbyte)",
+                    self.max_fp32_activation_size,
                 ],
                 [
-                    'Max memory consumption for an activation tensor in compressed model (Mbyte)',
-                    self.max_compressed_activation_size
+                    "Max memory consumption for an activation tensor in compressed model (Mbyte)",
+                    self.max_compressed_activation_size,
                 ],
-                ['Memory consumption decrease for weights', self.weight_memory_consumption_decrease],
-            ]
+                ["Memory consumption decrease for weights", self.weight_memory_consumption_decrease],
+            ],
         )
 
-        pretty_string = f'Statistics of the memory consumption:\n{memory_consumption_string}'
+        pretty_string = f"Statistics of the memory consumption:\n{memory_consumption_string}"
         return pretty_string
 
 
 class QuantizationConfigurationStatistics(Statistics):
     """
     Contains statistics of the quantization configuration.
     """
@@ -82,17 +82,17 @@
         :param quantized_edges_in_cfg: Number of quantized edges in quantization configuration.
         :param total_edges_in_cfg: Total number of edges in quantization configuration.
         """
         self.quantized_edges_in_cfg = quantized_edges_in_cfg
         self.total_edges_in_cfg = total_edges_in_cfg
 
     def to_str(self) -> str:
-        header = ['Statistic\'s name', 'Value']
+        header = ["Statistic's name", "Value"]
         rows = [
             [
-                'Share edges of the quantized data path',
-                _proportion_str(self.quantized_edges_in_cfg, self.total_edges_in_cfg)
+                "Share edges of the quantized data path",
+                _proportion_str(self.quantized_edges_in_cfg, self.total_edges_in_cfg),
             ]
         ]
         qc_string = create_table(header, rows)
-        pretty_string = f'Statistics of the quantization configuration:\n{qc_string}'
+        pretty_string = f"Statistics of the quantization configuration:\n{qc_string}"
         return pretty_string
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/structs.py` & `nncf-2.5.0/nncf/torch/quantization/structs.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,38 +1,37 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import List
 
 import torch
 
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.quantization.layers import BaseQuantizer
 
 
 class QuantizerInfo:
-    def __init__(self, quantizer_module_ref: BaseQuantizer,
-                 affected_insertions: List[PTTargetPoint]):
+    def __init__(self, quantizer_module_ref: BaseQuantizer, affected_insertions: List[PTTargetPoint]):
         self.quantizer_module_ref = quantizer_module_ref
         self.affected_insertions = affected_insertions
 
 
 class NonWeightQuantizerInfo(QuantizerInfo):
     pass
 
 
 class WeightQuantizerInfo(QuantizerInfo):
-    def __init__(self,
-                 quantizer_module_ref: BaseQuantizer,
-                 quantized_module: torch.nn.Module,
-                 affected_insertions: List[PTTargetPoint]):
+    def __init__(
+        self,
+        quantizer_module_ref: BaseQuantizer,
+        quantized_module: torch.nn.Module,
+        affected_insertions: List[PTTargetPoint],
+    ):
         super().__init__(quantizer_module_ref, affected_insertions)
         self.quantized_module = quantized_module
```

### Comparing `nncf-2.4.0/nncf/torch/quantization/translator.py` & `nncf-2.5.0/nncf/torch/quantization/translator.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,34 +1,32 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from nncf.common.graph.transformations.commands import TargetType
 from nncf.common.quantization.quantizer_setup import ActivationQuantizationInsertionPoint
 from nncf.common.quantization.quantizer_setup import QuantizationInsertionPointBase
 from nncf.common.quantization.quantizer_setup import WeightQuantizationInsertionPoint
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 
 
 class PTTargetPointTranslator:
     @staticmethod
     def translate(qip: QuantizationInsertionPointBase) -> PTTargetPoint:
         if isinstance(qip, WeightQuantizationInsertionPoint):
-            return PTTargetPoint(target_type=TargetType.OPERATION_WITH_WEIGHTS,
-                                 target_node_name=qip.target_node_name)
+            return PTTargetPoint(target_type=TargetType.OPERATION_WITH_WEIGHTS, target_node_name=qip.target_node_name)
         assert isinstance(qip, ActivationQuantizationInsertionPoint)
         input_port_id = qip.input_port_id
         if input_port_id is not None:
-            return PTTargetPoint(target_type=TargetType.OPERATOR_PRE_HOOK,
-                                 target_node_name=qip.target_node_name,
-                                 input_port_id=input_port_id)
-        return PTTargetPoint(target_type=TargetType.OPERATOR_POST_HOOK,
-                             target_node_name=qip.target_node_name)
+            return PTTargetPoint(
+                target_type=TargetType.OPERATOR_PRE_HOOK,
+                target_node_name=qip.target_node_name,
+                input_port_id=input_port_id,
+            )
+        return PTTargetPoint(target_type=TargetType.OPERATOR_POST_HOOK, target_node_name=qip.target_node_name)
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/base_algo.py` & `nncf-2.5.0/nncf/torch/sparsity/base_algo.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,48 +1,50 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+Base classes for NNCF PyTorch sparsity algorithm builder and controller objects.
 """
-
 from typing import List
 
 import torch
 
 from nncf.api.compression import CompressionLoss
 from nncf.api.compression import CompressionScheduler
 from nncf.api.compression import CompressionStage
 from nncf.common.graph import NNCFNode
 from nncf.common.graph import NNCFNodeName
 from nncf.common.graph.transformations.commands import TargetType
+from nncf.common.logging import nncf_logger
 from nncf.common.schedulers import BaseCompressionScheduler
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.sparsity.controller import SparsityController
-from nncf.common.logging import nncf_logger
+from nncf.common.utils.api_marker import api
+from nncf.common.utils.backend import copy_model
 from nncf.torch.algo_selector import ZeroCompressionLoss
 from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
 from nncf.torch.graph.transformations.commands import TransformationPriority
 from nncf.torch.graph.transformations.layout import PTTransformationLayout
 from nncf.torch.nncf_network import NNCFNetwork
+from nncf.torch.sparsity.layers import BinaryMask
 from nncf.torch.utils import get_model_device
 
 
 class SparseModuleInfo:
-    def __init__(self, module_node_name: NNCFNodeName, module: torch.nn.Module,
-                 operand):
+    def __init__(self, module_node_name: NNCFNodeName, module: torch.nn.Module, operand):
         self.module_node_name = module_node_name
         self.module = module
         self.operand = operand
 
 
 class BaseSparsityAlgoBuilder(PTCompressionAlgorithmBuilder):
     def __init__(self, config, should_init: bool = True):
@@ -54,48 +56,56 @@
         commands = self._sparsify_weights(target_model)
         for command in commands:
             layout.register(command)
         return layout
 
     def _sparsify_weights(self, target_model: NNCFNetwork) -> List[PTInsertionCommand]:
         device = get_model_device(target_model)
-        sparsified_module_nodes = target_model.get_weighted_original_graph_nodes(
-            nncf_module_names=self.compressed_nncf_module_names)
+        sparsified_module_nodes = target_model.nncf.get_weighted_original_graph_nodes(
+            nncf_module_names=self.compressed_nncf_module_names
+        )
         insertion_commands = []
         for module_node in sparsified_module_nodes:
             node_name = module_node.node_name
 
             if not self._should_consider_scope(node_name):
                 nncf_logger.info(f"Ignored adding weight sparsifier for operation: {node_name}")
                 continue
 
-            compression_lr_multiplier = \
-                self.config.get_redefinable_global_param_value_for_algo('compression_lr_multiplier',
-                                                                        self.name)
+            compression_lr_multiplier = self.config.get_redefinable_global_param_value_for_algo(
+                "compression_lr_multiplier", self.name
+            )
             operation = self.create_weight_sparsifying_operation(module_node, compression_lr_multiplier)
             hook = operation.to(device)
-            insertion_commands.append(PTInsertionCommand(PTTargetPoint(TargetType.OPERATION_WITH_WEIGHTS,
-                                                                       target_node_name=node_name),
-                                                         hook, TransformationPriority.SPARSIFICATION_PRIORITY))
-            sparsified_module = target_model.get_containing_module(node_name)
-            self._sparsified_module_info.append(
-                SparseModuleInfo(node_name, sparsified_module, hook))
+            insertion_commands.append(
+                PTInsertionCommand(
+                    PTTargetPoint(TargetType.OPERATION_WITH_WEIGHTS, target_node_name=node_name),
+                    hook,
+                    TransformationPriority.SPARSIFICATION_PRIORITY,
+                )
+            )
+            sparsified_module = target_model.nncf.get_containing_module(node_name)
+            self._sparsified_module_info.append(SparseModuleInfo(node_name, sparsified_module, hook))
 
         return insertion_commands
 
     def create_weight_sparsifying_operation(self, target_module_node: NNCFNode, compression_lr_multiplier: float):
         raise NotImplementedError
 
     def initialize(self, model: NNCFNetwork) -> None:
         pass
 
 
+@api()
 class BaseSparsityAlgoController(PTCompressionAlgorithmController, SparsityController):
-    def __init__(self, target_model: NNCFNetwork,
-                 sparsified_module_info: List[SparseModuleInfo]):
+    """
+    Base class for sparsity algorithm controllers in PT.
+    """
+
+    def __init__(self, target_model: NNCFNetwork, sparsified_module_info: List[SparseModuleInfo]):
         super().__init__(target_model)
         self._loss = ZeroCompressionLoss(get_model_device(target_model))
         self._scheduler = BaseCompressionScheduler()
         self.sparsified_module_info = sparsified_module_info
 
     @property
     def loss(self) -> CompressionLoss:
@@ -108,7 +118,24 @@
     def disable_scheduler(self):
         self._scheduler = StubCompressionScheduler()
         self._scheduler.target_level = 0.0
         self._scheduler.current_sparsity_level = 0.0
 
     def compression_stage(self) -> CompressionStage:
         return CompressionStage.FULLY_COMPRESSED
+
+    def strip_model(self, model: NNCFNetwork, do_copy: bool = False) -> NNCFNetwork:
+        if do_copy:
+            model = copy_model(model)
+
+        for node in model.nncf.get_original_graph().get_all_nodes():
+            if node.node_type in ["nncf_model_input", "nncf_model_output"]:
+                continue
+            nncf_module = model.nncf.get_containing_module(node.node_name)
+            if hasattr(nncf_module, "pre_ops"):
+                for key in list(nncf_module.pre_ops.keys()):
+                    op = nncf_module.get_pre_op(key)
+                    if isinstance(op.operand, BinaryMask):
+                        nncf_module.weight.data = op.operand.apply_binary_mask(nncf_module.weight.data)
+                        nncf_module.remove_pre_forward_operation(key)
+
+        return model
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/collector.py` & `nncf-2.5.0/nncf/torch/sparsity/collector.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,36 +1,35 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List
 
 from nncf.common.sparsity.collector import BaseSparseModelStatisticsCollector
 from nncf.common.sparsity.collector import WeightDescription
 from nncf.torch.layer_utils import COMPRESSION_MODULES
-from nncf.torch.sparsity.base_algo import SparseModuleInfo
 from nncf.torch.nncf_network import NNCFNetwork
+from nncf.torch.sparsity.base_algo import SparseModuleInfo
 
 
 class PTSparseModelStatisticsCollector(BaseSparseModelStatisticsCollector):
     """
     Collects statistics for the sparse NNCFNetwork.
     """
 
-    def __init__(self, model: NNCFNetwork, sparse_modules_info: List[SparseModuleInfo],
-                 supports_sparse_bias: bool = False):
+    def __init__(
+        self, model: NNCFNetwork, sparse_modules_info: List[SparseModuleInfo], supports_sparse_bias: bool = False
+    ):
         """
         Initializes statistics collector of the sparse tf.keras.Model.
 
         :param model: Sparse model.
         :param sparse_modules_info: List of `SparseModuleInfo`.
         """
         self._model = model
@@ -45,40 +44,41 @@
             sparse_weight = minfo.operand.apply_binary_mask(minfo.module.weight)
 
             weights_descriptions.append(
                 WeightDescription(
                     minfo.module_node_name,
                     list(sparse_weight.shape),
                     sparse_weight.count_nonzero().item(),
-                    is_sparse=True
+                    is_sparse=True,
                 )
             )
 
-            if hasattr(minfo.module, 'bias') and minfo.module.bias is not None:
+            if hasattr(minfo.module, "bias") and minfo.module.bias is not None:
                 bias = minfo.module.bias
-                name = f'{minfo.module_node_name}/bias'
+                name = f"{minfo.module_node_name}/bias"
                 if self._supports_sparse_bias:
                     sparse_bias = minfo.operand.apply_binary_mask(bias, is_bias=True)  # TODO(yujie): breaking changes
                     weights_descriptions.append(
-                        WeightDescription(name, list(sparse_bias.shape),
-                                          sparse_bias.count_nonzero().item(), is_sparse=True)
+                        WeightDescription(
+                            name, list(sparse_bias.shape), sparse_bias.count_nonzero().item(), is_sparse=True
+                        )
                     )
                 else:
                     weights_descriptions.append(
-                        WeightDescription(name, list(bias.shape),
-                                          bias.count_nonzero().item(), is_sparse=False)
+                        WeightDescription(name, list(bias.shape), bias.count_nonzero().item(), is_sparse=False)
                     )
 
             processed_modules.append(minfo.module)
 
         compression_types = tuple(COMPRESSION_MODULES.registry_dict.values())
-        for module_name, module in self._model.get_nncf_wrapped_model().named_modules():
+        modules_to_process = {k: v for k, v in self._model.named_modules() if not k.startswith("_nncf")}
+        for module_name, module in modules_to_process.items():
             if isinstance(module, compression_types) or module in processed_modules:
                 continue
 
             for param_name, param in module.named_parameters(recurse=False):
-                name = f'{module_name}/{param_name}'
+                name = f"{module_name}/{param_name}"
                 weights_descriptions.append(
                     WeightDescription(name, list(param.shape), param.count_nonzero().item(), is_sparse=False)
                 )
 
         return weights_descriptions
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/const/algo.py` & `nncf-2.5.0/nncf/torch/sparsity/const/algo.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,52 +1,57 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Tuple
 
 from nncf.common.graph import NNCFNode
 from nncf.common.sparsity.statistics import ConstSparsityStatistics
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
+from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.nncf_network import NNCFNetwork
-from nncf.torch.sparsity.layers import BinaryMask
-from nncf.torch.sparsity.base_algo import BaseSparsityAlgoBuilder, BaseSparsityAlgoController
+from nncf.torch.sparsity.base_algo import BaseSparsityAlgoBuilder
+from nncf.torch.sparsity.base_algo import BaseSparsityAlgoController
 from nncf.torch.sparsity.collector import PTSparseModelStatisticsCollector
-from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
+from nncf.torch.sparsity.layers import BinaryMask
 
 
-@PT_COMPRESSION_ALGORITHMS.register('const_sparsity')
+@PT_COMPRESSION_ALGORITHMS.register("const_sparsity")
 class ConstSparsityBuilder(BaseSparsityAlgoBuilder):
     def create_weight_sparsifying_operation(self, target_module_node: NNCFNode, compression_lr_multiplier: float):
         return BinaryMask(target_module_node.layer_attributes.get_weight_shape())
 
     def _build_controller(self, model: NNCFNetwork) -> PTCompressionAlgorithmController:
         return ConstSparsityController(model, self._sparsified_module_info)
 
     def _are_frozen_layers_allowed(self) -> Tuple[bool, str]:
-        return True, 'Frozen layers are allowed for const sparsity'
+        return True, "Frozen layers are allowed for const sparsity"
 
 
+@api()
 class ConstSparsityController(BaseSparsityAlgoController):
+    """
+    Controller for the auxiliary constant sparsity algorithm in PT.
+    """
+
     def freeze(self):
         pass
 
     def set_sparsity_level(self, sparsity_level: float):
         pass
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         collector = PTSparseModelStatisticsCollector(self.model, self.sparsified_module_info)
         model_statistics = collector.collect()
         stats = ConstSparsityStatistics(model_statistics)
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('const_sparsity', stats)
+        nncf_stats.register("const_sparsity", stats)
         return nncf_stats
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/functions.py` & `nncf-2.5.0/nncf/torch/sparsity/functions.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,16 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from nncf.torch.dynamic_graph.patch_pytorch import register_operator
 
 
 @register_operator()
 def apply_binary_mask(mask, weight):
     return mask * weight
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/layers.py` & `nncf-2.5.0/nncf/torch/sparsity/layers.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import List
 
 import torch
 from torch import nn
 
 from nncf.torch.layer_utils import COMPRESSION_MODULES
 from nncf.torch.sparsity.functions import apply_binary_mask as apply_binary_mask_impl
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/magnitude/algo.py` & `nncf-2.5.0/nncf/torch/sparsity/magnitude/algo.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from copy import deepcopy
 from typing import List
 
 import torch
 
 from nncf import NNCFConfig
 from nncf.api.compression import CompressionStage
@@ -21,14 +19,15 @@
 from nncf.common.graph import NNCFNode
 from nncf.common.initialization.batchnorm_adaptation import BatchnormAdaptationAlgorithm
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.sparsity.schedulers import SPARSITY_SCHEDULERS
 from nncf.common.sparsity.statistics import LayerThreshold
 from nncf.common.sparsity.statistics import MagnitudeSparsityStatistics
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.extractors import extract_bn_adaptation_init_params
 from nncf.config.schemata.defaults import MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE
 from nncf.config.schemata.defaults import SPARSITY_INIT
 from nncf.config.schemata.defaults import SPARSITY_LEVEL_SETTING_MODE
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
@@ -38,77 +37,83 @@
 from nncf.torch.sparsity.base_algo import SparseModuleInfo
 from nncf.torch.sparsity.collector import PTSparseModelStatisticsCollector
 from nncf.torch.sparsity.layers import BinaryMask
 from nncf.torch.sparsity.magnitude.functions import WEIGHT_IMPORTANCE_FUNCTIONS
 from nncf.torch.sparsity.magnitude.functions import calc_magnitude_binary_mask
 
 
-@PT_COMPRESSION_ALGORITHMS.register('magnitude_sparsity')
+@PT_COMPRESSION_ALGORITHMS.register("magnitude_sparsity")
 class MagnitudeSparsityBuilder(BaseSparsityAlgoBuilder):
     def create_weight_sparsifying_operation(self, target_module_node: NNCFNode, compression_lr_multiplier: float):
         return BinaryMask(target_module_node.layer_attributes.get_weight_shape())
 
     def _build_controller(self, model: NNCFNetwork) -> PTCompressionAlgorithmController:
         return MagnitudeSparsityController(model, self._sparsified_module_info, self.config)
 
 
-@ADAPTIVE_COMPRESSION_CONTROLLERS.register('pt_magnitude_sparsity')
+@api()
+@ADAPTIVE_COMPRESSION_CONTROLLERS.register("pt_magnitude_sparsity")
 class MagnitudeSparsityController(BaseSparsityAlgoController):
-    def __init__(self, target_model: NNCFNetwork, sparsified_module_info: List[SparseModuleInfo],
-                 config: NNCFConfig):
+    """
+    Controller for the magnitude sparsity algorithm in PT.
+    """
+
+    def __init__(self, target_model: NNCFNetwork, sparsified_module_info: List[SparseModuleInfo], config: NNCFConfig):
         super().__init__(target_model, sparsified_module_info)
         self._config = config
-        self._algo_config = extract_algo_specific_config(self._config, 'magnitude_sparsity')
-        params = self._algo_config.get('params', {})
+        self._algo_config = extract_algo_specific_config(self._config, "magnitude_sparsity")
+        params = self._algo_config.get("params", {})
 
-        self._weight_importance_fn = WEIGHT_IMPORTANCE_FUNCTIONS[params.get('weight_importance',
-                                                                            MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE)]
-        self._mode = params.get('sparsity_level_setting_mode', SPARSITY_LEVEL_SETTING_MODE)
+        self._weight_importance_fn = WEIGHT_IMPORTANCE_FUNCTIONS[
+            params.get("weight_importance", MAGNITUDE_SPARSITY_WEIGHT_IMPORTANCE)
+        ]
+        self._mode = params.get("sparsity_level_setting_mode", SPARSITY_LEVEL_SETTING_MODE)
         self._scheduler = None
-        sparsity_init = self._algo_config.get('sparsity_init', SPARSITY_INIT)
+        sparsity_init = self._algo_config.get("sparsity_init", SPARSITY_INIT)
 
-        if self._mode == 'global':
+        if self._mode == "global":
             scheduler_params = deepcopy(params)
-            scheduler_params['sparsity_init'] = sparsity_init
-            scheduler_cls = SPARSITY_SCHEDULERS.get(params.get('schedule', 'polynomial'))
+            scheduler_params["sparsity_init"] = sparsity_init
+            scheduler_cls = SPARSITY_SCHEDULERS.get(params.get("schedule", "polynomial"))
             self._scheduler = scheduler_cls(self, scheduler_params)
         else:
             self._scheduler = StubCompressionScheduler()
 
         self._bn_adaptation = None
 
         self.set_sparsity_level(sparsity_init)
 
     def statistics(self, quickly_collected_only: bool = False) -> NNCFStatistics:
         collector = PTSparseModelStatisticsCollector(self.model, self.sparsified_module_info)
         model_statistics = collector.collect()
 
         threshold_statistics = []
-        if self._mode == 'global':
-            global_threshold = self._select_threshold(model_statistics.sparsity_level_for_layers,
-                                                      self.sparsified_module_info)
+        if self._mode == "global":
+            global_threshold = self._select_threshold(
+                model_statistics.sparsity_level_for_layers, self.sparsified_module_info
+            )
 
         module_name_to_sparsity_level_map = {
             s.name: s.sparsity_level for s in model_statistics.sparsified_layers_summary
         }
         for minfo in self.sparsified_module_info:
-            if self._mode == 'global':
+            if self._mode == "global":
                 threshold = global_threshold
             else:
                 sparsity_level_for_sparse_module = module_name_to_sparsity_level_map[minfo.module_node_name]
                 threshold = self._select_threshold(sparsity_level_for_sparse_module, [minfo])
 
             threshold_statistics.append(LayerThreshold(minfo.module_node_name, threshold))
 
-        target_sparsity_level = self.scheduler.current_sparsity_level if self._mode == 'global' else None
+        target_sparsity_level = self.scheduler.current_sparsity_level if self._mode == "global" else None
 
         stats = MagnitudeSparsityStatistics(model_statistics, threshold_statistics, target_sparsity_level)
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('magnitude_sparsity', stats)
+        nncf_stats.register("magnitude_sparsity", stats)
         return nncf_stats
 
     def freeze(self, freeze: bool = True):
         for layer in self.sparsified_module_info:
             layer.operand.frozen = freeze
 
     @property
@@ -117,22 +122,26 @@
 
     @compression_rate.setter
     def compression_rate(self, sparsity_level: float):
         self.freeze(False)
         self.set_sparsity_level(sparsity_level)
         self.freeze(True)
 
-    def set_sparsity_level(self, sparsity_level,
-                           target_sparsified_module_info: SparseModuleInfo = None,
-                           run_batchnorm_adaptation: bool = False):
+    def set_sparsity_level(
+        self,
+        sparsity_level,
+        target_sparsified_module_info: SparseModuleInfo = None,
+        run_batchnorm_adaptation: bool = False,
+    ):
         if sparsity_level >= 1 or sparsity_level < 0:
             raise AttributeError(
-                'Sparsity level should be within interval [0,1), actual value to set is: {}'.format(sparsity_level))
+                "Sparsity level should be within interval [0,1), actual value to set is: {}".format(sparsity_level)
+            )
         if target_sparsified_module_info is None:
-            target_sparsified_module_info_list = self.sparsified_module_info # List[SparseModuleInfo]
+            target_sparsified_module_info_list = self.sparsified_module_info  # List[SparseModuleInfo]
         else:
             target_sparsified_module_info_list = [target_sparsified_module_info]
         threshold = self._select_threshold(sparsity_level, target_sparsified_module_info_list)
         self._set_masks_for_threshold(threshold, target_sparsified_module_info_list)
 
         if run_batchnorm_adaptation:
             self._run_batchnorm_adaptation()
@@ -144,33 +153,33 @@
         all_weights_tensor, _ = torch.cat(all_weights).sort()
         threshold = all_weights_tensor[int((all_weights_tensor.size(0) - 1) * sparsity_level)].item()
         return threshold
 
     def _set_masks_for_threshold(self, threshold_val, target_sparsified_module_info_list):
         for layer in target_sparsified_module_info_list:
             if not layer.operand.frozen:
-                layer.operand.binary_mask = calc_magnitude_binary_mask(layer.module.weight,
-                                                                       self._weight_importance_fn,
-                                                                       threshold_val)
+                layer.operand.binary_mask = calc_magnitude_binary_mask(
+                    layer.module.weight, self._weight_importance_fn, threshold_val
+                )
 
     def _collect_all_weights(self, target_sparsified_module_info_list: List[SparseModuleInfo]):
         all_weights = []
         for minfo in target_sparsified_module_info_list:
             all_weights.append(self._weight_importance_fn(minfo.module.weight).view(-1))
         return all_weights
 
     def compression_stage(self) -> CompressionStage:
-        if self._mode == 'local':
+        if self._mode == "local":
             return CompressionStage.FULLY_COMPRESSED
 
         if self.scheduler.current_sparsity_level >= self.scheduler.target_level:
             return CompressionStage.FULLY_COMPRESSED
         if self.scheduler.current_sparsity_level == 0:
             return CompressionStage.UNCOMPRESSED
         return CompressionStage.PARTIALLY_COMPRESSED
 
     def _run_batchnorm_adaptation(self):
         if self._bn_adaptation is None:
             self._bn_adaptation = BatchnormAdaptationAlgorithm(
-                **extract_bn_adaptation_init_params(self._config,
-                                                    'magnitude_sparsity'))
+                **extract_bn_adaptation_init_params(self._config, "magnitude_sparsity")
+            )
         self._bn_adaptation.run(self.model)
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/magnitude/functions.py` & `nncf-2.5.0/nncf/torch/sparsity/magnitude/functions.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,32 +1,27 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import torch
 
 
 def abs_magnitude(weight):
     return torch.abs(weight)
 
 
 def normed_magnitude(weight):
     return torch.abs(weight) / weight.norm(2)
 
 
-WEIGHT_IMPORTANCE_FUNCTIONS = {
-    'abs': abs_magnitude,
-    'normed_abs': normed_magnitude
-}
+WEIGHT_IMPORTANCE_FUNCTIONS = {"abs": abs_magnitude, "normed_abs": normed_magnitude}
 
 
 def calc_magnitude_binary_mask(weight, weight_importance, threshold):
     return (weight_importance(weight) > threshold).float()
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/rb/algo.py` & `nncf-2.5.0/nncf/torch/sparsity/rb/algo.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,109 +1,120 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from copy import deepcopy
 from typing import List
 
 import torch
 import torch.distributed as dist
 
 from nncf import NNCFConfig
+from nncf.api.compression import CompressionStage
+from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
+from nncf.common.graph import NNCFNode
+from nncf.common.schedulers import StubCompressionScheduler
+from nncf.common.sparsity.schedulers import SPARSITY_SCHEDULERS
+from nncf.common.sparsity.statistics import RBSparsityStatistics
+from nncf.common.statistics import NNCFStatistics
+from nncf.common.utils.api_marker import api
 from nncf.config.extractors import extract_algo_specific_config
 from nncf.config.schemata.defaults import SPARSITY_INIT
 from nncf.config.schemata.defaults import SPARSITY_LEVEL_SETTING_MODE
 from nncf.torch.algo_selector import PT_COMPRESSION_ALGORITHMS
-from nncf.api.compression import CompressionStage
-from nncf.common.graph import NNCFNode
 from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.nncf_network import NNCFNetwork
-from nncf.torch.sparsity.base_algo import BaseSparsityAlgoBuilder, BaseSparsityAlgoController, SparseModuleInfo
+from nncf.torch.sparsity.base_algo import BaseSparsityAlgoBuilder
+from nncf.torch.sparsity.base_algo import BaseSparsityAlgoController
+from nncf.torch.sparsity.base_algo import SparseModuleInfo
+from nncf.torch.sparsity.collector import PTSparseModelStatisticsCollector
 from nncf.torch.sparsity.rb.layers import RBSparsifyingWeight
-from nncf.torch.sparsity.rb.loss import SparseLoss, SparseLossForPerLayerSparsity
+from nncf.torch.sparsity.rb.loss import SparseLoss
+from nncf.torch.sparsity.rb.loss import SparseLossForPerLayerSparsity
 from nncf.torch.utils import get_model_device
 from nncf.torch.utils import get_world_size
-from nncf.common.accuracy_aware_training.training_loop import ADAPTIVE_COMPRESSION_CONTROLLERS
-from nncf.torch.sparsity.collector import PTSparseModelStatisticsCollector
-from nncf.common.sparsity.schedulers import SPARSITY_SCHEDULERS
-from nncf.common.schedulers import StubCompressionScheduler
-from nncf.common.sparsity.statistics import RBSparsityStatistics
-from nncf.common.statistics import NNCFStatistics
 
 
-@PT_COMPRESSION_ALGORITHMS.register('rb_sparsity')
+@PT_COMPRESSION_ALGORITHMS.register("rb_sparsity")
 class RBSparsityBuilder(BaseSparsityAlgoBuilder):
     def create_weight_sparsifying_operation(self, target_module_node: NNCFNode, compression_lr_multiplier: float):
-        return RBSparsifyingWeight(target_module_node.layer_attributes.get_weight_shape(), frozen=False,
-                                   compression_lr_multiplier=compression_lr_multiplier)
+        return RBSparsifyingWeight(
+            target_module_node.layer_attributes.get_weight_shape(),
+            frozen=False,
+            compression_lr_multiplier=compression_lr_multiplier,
+        )
 
     def _build_controller(self, model: NNCFNetwork) -> PTCompressionAlgorithmController:
         return RBSparsityController(model, self._sparsified_module_info, self.config)
 
 
-@ADAPTIVE_COMPRESSION_CONTROLLERS.register('pt_rb_sparsity')
+@api()
+@ADAPTIVE_COMPRESSION_CONTROLLERS.register("pt_rb_sparsity")
 class RBSparsityController(BaseSparsityAlgoController):
-    def __init__(self, target_model: NNCFNetwork, sparsified_module_info: List[SparseModuleInfo],
-                 config: NNCFConfig):
+    """
+    Controller for the regularization-based (RB) sparsity algorithm in PT.
+    """
+
+    def __init__(self, target_model: NNCFNetwork, sparsified_module_info: List[SparseModuleInfo], config: NNCFConfig):
         super().__init__(target_model, sparsified_module_info)
-        algo_config = extract_algo_specific_config(config, 'rb_sparsity')
-        params = deepcopy(algo_config.get('params', {}))
+        algo_config = extract_algo_specific_config(config, "rb_sparsity")
+        params = deepcopy(algo_config.get("params", {}))
 
         self._distributed = False
-        self._mode = params.get('sparsity_level_setting_mode', SPARSITY_LEVEL_SETTING_MODE)
-        self._check_sparsity_masks = params.get('check_sparsity_masks', False)
+        self._mode = params.get("sparsity_level_setting_mode", SPARSITY_LEVEL_SETTING_MODE)
+        self._check_sparsity_masks = params.get("check_sparsity_masks", False)
 
         sparsify_operations = [m.operand for m in self.sparsified_module_info]
-        if self._mode == 'local':
+        if self._mode == "local":
             self._loss = SparseLossForPerLayerSparsity(sparsify_operations)
             self._scheduler = StubCompressionScheduler()
         else:
             self._loss = SparseLoss(sparsify_operations)
 
-            sparsity_init = algo_config.get('sparsity_init', SPARSITY_INIT)
-            params['sparsity_init'] = sparsity_init
-            scheduler_cls = SPARSITY_SCHEDULERS.get(params.get('schedule', 'exponential'))
+            sparsity_init = algo_config.get("sparsity_init", SPARSITY_INIT)
+            params["sparsity_init"] = sparsity_init
+            scheduler_cls = SPARSITY_SCHEDULERS.get(params.get("schedule", "exponential"))
             self._scheduler = scheduler_cls(self, params)
             self.set_sparsity_level(sparsity_init)
 
     def set_sparsity_level(self, sparsity_level, target_sparsified_module_info: SparseModuleInfo = None):
         if target_sparsified_module_info is None:
-            #pylint:disable=no-value-for-parameter
+            # pylint:disable=no-value-for-parameter
             self._loss.set_target_sparsity_loss(sparsity_level)
         else:
             sparse_op = target_sparsified_module_info.operand
             self._loss.set_target_sparsity_loss(sparsity_level, sparse_op)
 
     def compression_stage(self) -> CompressionStage:
-        if self._mode == 'local':
+        if self._mode == "local":
             return CompressionStage.FULLY_COMPRESSED
 
         if self.scheduler.current_sparsity_level == 0:
             return CompressionStage.UNCOMPRESSED
         if self.scheduler.current_sparsity_level >= self.scheduler.target_level:
             return CompressionStage.FULLY_COMPRESSED
         return CompressionStage.PARTIALLY_COMPRESSED
 
     def freeze(self):
         self._loss.disable()
 
     def distributed(self):
         if not dist.is_initialized():
-            raise KeyError('Could not set distributed mode for the compression algorithm '
-                           'because the default process group has not been initialized.')
+            raise KeyError(
+                "Could not set distributed mode for the compression algorithm "
+                "because the default process group has not been initialized."
+            )
 
-        if 'cuda' in get_model_device(self._model).type:
+        if "cuda" in get_model_device(self._model).type:
             state = torch.cuda.get_rng_state()
             if dist.get_backend() == dist.Backend.NCCL:
                 state = state.cuda()
             torch.distributed.broadcast(state, src=0)
             torch.cuda.set_rng_state(state.cpu())
         else:
             state = torch.get_rng_state()
@@ -133,22 +144,22 @@
 
         return ncor_values / nvalues
 
     def statistics(self, quickly_collected_only=False) -> NNCFStatistics:
         collector = PTSparseModelStatisticsCollector(self.model, self.sparsified_module_info)
         model_statistics = collector.collect()
 
-        target_sparsity_level = self.scheduler.current_sparsity_level if self._mode == 'global' else None
+        target_sparsity_level = self.scheduler.current_sparsity_level if self._mode == "global" else None
 
         mean_sparse_prob = 1.0 - self.loss.mean_sparse_prob
 
         stats = RBSparsityStatistics(model_statistics, target_sparsity_level, mean_sparse_prob)
 
         nncf_stats = NNCFStatistics()
-        nncf_stats.register('rb_sparsity', stats)
+        nncf_stats.register("rb_sparsity", stats)
         return nncf_stats
 
     @property
     def compression_rate(self):
         return self._loss.target_sparsity_rate
 
     @compression_rate.setter
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/rb/functions.py` & `nncf-2.5.0/nncf/torch/sparsity/rb/functions.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,24 +1,23 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import torch
 
 from nncf.torch.dynamic_graph.patch_pytorch import register_operator
-from nncf.torch.functions import STThreshold, logit
+from nncf.torch.functions import STThreshold
+from nncf.torch.functions import logit
 
 
 def binary_mask(mask):
     return STThreshold.apply(torch.sigmoid(mask))
 
 
 @register_operator()
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/rb/layers.py` & `nncf-2.5.0/nncf/torch/sparsity/rb/layers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,38 +1,40 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import List
 
 import torch
 
-from nncf.torch.sparsity.layers import BinaryMask
-from nncf.torch.sparsity.rb.functions import calc_rb_binary_mask, binary_mask
 from nncf.torch.functions import logit
-from nncf.torch.layer_utils import COMPRESSION_MODULES, CompressionParameter
-
+from nncf.torch.layer_utils import COMPRESSION_MODULES
+from nncf.torch.layer_utils import CompressionParameter
+from nncf.torch.sparsity.layers import BinaryMask
+from nncf.torch.sparsity.rb.functions import binary_mask
+from nncf.torch.sparsity.rb.functions import calc_rb_binary_mask
 
 
 @COMPRESSION_MODULES.register()
 class RBSparsifyingWeight(BinaryMask):
     def __init__(self, weight_shape: List[int], frozen=True, compression_lr_multiplier=None, eps=1e-6):
         super().__init__(weight_shape)
         self.frozen = frozen
         self.eps = eps
-        self._mask = CompressionParameter(logit(torch.ones(weight_shape) * 0.99), requires_grad=not self.frozen,
-                                          compression_lr_multiplier=compression_lr_multiplier)
+        self._mask = CompressionParameter(
+            logit(torch.ones(weight_shape) * 0.99),
+            requires_grad=not self.frozen,
+            compression_lr_multiplier=compression_lr_multiplier,
+        )
         self.binary_mask = binary_mask(self._mask)
         self.register_buffer("uniform", torch.zeros(weight_shape))
         self.mask_calculation_hook = MaskCalculationHook(self)
 
     @property
     def mask(self):
         return self._mask
@@ -46,19 +48,19 @@
         u = self.uniform if self.training and not self.frozen else None
         return calc_rb_binary_mask(self._mask, u, self.eps)
 
     def loss(self):
         return binary_mask(self._mask)
 
 
-class MaskCalculationHook():
+class MaskCalculationHook:
     def __init__(self, module):
         # pylint: disable=protected-access
         self.hook = module._register_state_dict_hook(self.hook_fn)
 
     def hook_fn(self, module, destination, prefix, local_metadata):
         module.binary_mask = binary_mask(module.mask)
-        destination[prefix + '_binary_mask'] = module.binary_mask
+        destination[prefix + "_binary_mask"] = module.binary_mask
         return destination
 
     def close(self):
         self.hook.remove()
```

### Comparing `nncf-2.4.0/nncf/torch/sparsity/rb/loss.py` & `nncf-2.5.0/nncf/torch/sparsity/rb/loss.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,23 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import torch
 
 from nncf.torch.compression_method_api import PTCompressionLoss
 
+
 # Actually in responsible to lean density to target value
 class SparseLoss(PTCompressionLoss):
     def __init__(self, sparse_layers=None, target=1.0, p=0.05):
         super().__init__()
         self._sparse_layers = sparse_layers
         self.target = target
         self.p = p
@@ -42,15 +41,16 @@
 
         params = 0
         loss = 0
         sparse_prob_sum = 0
         for sparse_layer in self._sparse_layers:
             if not self.disabled and sparse_layer.frozen:
                 raise AssertionError(
-                    "Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss")
+                    "Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss"
+                )
             if not sparse_layer.frozen:
                 sw_loss = sparse_layer.loss()
                 params = params + sw_loss.view(-1).size(0)
                 loss = loss + sw_loss.sum()
                 sparse_prob_sum += torch.sigmoid(sparse_layer.mask).sum()
 
         self.mean_sparse_prob = (sparse_prob_sum / params).item()
@@ -81,15 +81,16 @@
 
         params = 0
         sparse_prob_sum = 0
         sparse_layers_loss = 0
         for sparse_layer in self._sparse_layers:
             if not self.disabled and not sparse_layer.sparsify:
                 raise AssertionError(
-                    "Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss")
+                    "Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss"
+                )
             if sparse_layer.sparsify:
                 sw_loss = sparse_layer.loss()
                 params_layer = sw_loss.view(-1).size(0)
                 params += params_layer
                 sparse_layers_loss -= torch.abs(sw_loss.sum() / params_layer - self.per_layer_target[sparse_layer])
                 sparse_prob_sum += torch.sigmoid(sparse_layer.mask).sum()
```

### Comparing `nncf-2.4.0/nncf/torch/structures.py` & `nncf-2.5.0/nncf/torch/structures.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,143 +1,174 @@
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 """
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
+PyTorch-specific structure definitions for passing arguments into certain NNCF calls.
 """
-from typing import Callable, Any, Optional, Tuple
+from typing import Any, Callable, Optional, Tuple
 
 import torch
 from torch import nn
 from torch.nn.modules.loss import _Loss
 from torch.utils.data import DataLoader
 
+from nncf.common.utils.api_marker import api
 from nncf.config.structures import NNCFExtraConfigStruct
 
 
+@api()
 class QuantizationPrecisionInitArgs(NNCFExtraConfigStruct):
     """
     Stores arguments for initialization of quantization's bitwidth.
     Initialization is based on calculating a measure reflecting layers' sensitivity to perturbations. The measure is
     calculated by estimation of average trace of Hessian for modules using the Hutchinson algorithm.
+
     :param criterion_fn: callable object, that implements calculation of loss by given outputs of the model, targets,
-    and loss function. It's not needed when the calculation of loss is just a direct call of the criterion with 2
-    arguments: outputs of model and targets. For all other specific cases, the callable object should be provided.
-    E.g. for inception-v3, the losses for two outputs of the model are combined with different weight.
+      and loss function. It's not needed when the calculation of loss is just a direct call of the criterion with 2
+      arguments: outputs of model and targets. For all other specific cases, the callable object should be provided.
+      E.g. for inception-v3, the losses for two outputs of the model are combined with different weight.
     :param criterion: loss function, instance of descendant of `torch.nn.modules.loss._Loss`,
     :param data_loader: 'data_loader' - provides an iterable over the given dataset. Instance of
-                nncf.initialization.PTInitializingDataLoader; a regular 'torch.utils.data.DataLoader' may
-                also be passed, but only in the simple case when it returns a tuple of (input, target) tensors.
-                *WARNING*: The final quantizer setup of the created compressed model is dependent on the data
-                provided by the data_loader. When using PyTorch's DistributedDataParallel with precision
-                initialization, make sure that each process in the distributed group receives the same data
-                from the data_loader as the other processes, otherwise the create_compressed_model call may
-                create different compressed model objects for each distributed process and the distributed training
-                will fail.
+      nncf.initialization.PTInitializingDataLoader; a regular 'torch.utils.data.DataLoader' may
+      also be passed, but only in the simple case when it returns a tuple of (input, target) tensors.
+    .. WARNING:: The final quantizer setup of the created compressed model is dependent on the data
+      provided by the data_loader. When using PyTorch's DistributedDataParallel with precision
+      initialization, make sure that each process in the distributed group receives the same data
+      from the data_loader as the other processes, otherwise the create_compressed_model call may
+      create different compressed model objects for each distributed process and the distributed training
+      will fail.
     :param device: Device to perform initialization at. Either 'cpu', 'cuda', or None (default); if None, will
-                   use the device of the model's parameters.
+      use the device of the model's parameters.
     """
 
-    def __init__(self, criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor], criterion: _Loss,
-                 data_loader: DataLoader, device: str = None):
+    def __init__(
+        self,
+        criterion_fn: Callable[[Any, Any, _Loss], torch.Tensor],
+        criterion: _Loss,
+        data_loader: DataLoader,
+        device: str = None,
+    ):
         self.criterion_fn = criterion_fn
         self.criterion = criterion
         self.data_loader = data_loader
         self.device = device
 
     @classmethod
     def get_id(cls) -> str:
         return "quantization_precision_init_args"
 
 
+@api()
 class AutoQPrecisionInitArgs(NNCFExtraConfigStruct):
     """
     :param data_loader: 'data_loader' - provides an iterable over the given dataset. Instance of
-                nncf.initialization.PTInitializingDataLoader; a regular 'torch.utils.data.DataLoader' may
-                also be passed, but only in the simple case when it returns a tuple of (input, target) tensors.
-                *WARNING*: The final quantizer setup of the created compressed model is dependent on the data
-                provided by the data_loader. When using PyTorch's DistributedDataParallel with precision
-                initialization, make sure that each process in the distributed group receives the same data
-                from the data_loader as the other processes, otherwise the create_compressed_model call may
-                create different compressed model objects for each distributed process and the distributed training
-                will fail.
-    """
-    def __init__(self, data_loader: DataLoader,
-                 eval_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader], float],
-                 nncf_config: 'NNCFConfig'):
+      nncf.initialization.PTInitializingDataLoader; a regular 'torch.utils.data.DataLoader' may
+      also be passed, but only in the simple case when it returns a tuple of (input, target) tensors.
+     .. WARNING:: The final quantizer setup of the created compressed model is dependent on the data
+      provided by the data_loader. When using PyTorch's DistributedDataParallel with precision
+      initialization, make sure that each process in the distributed group receives the same data
+      from the data_loader as the other processes, otherwise the create_compressed_model call may
+      create different compressed model objects for each distributed process and the distributed training
+      will fail.
+    """
+
+    def __init__(
+        self,
+        data_loader: DataLoader,
+        eval_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader], float],
+        nncf_config: "NNCFConfig",
+    ):
         self.data_loader = data_loader
         self.eval_fn = eval_fn
         self.config = nncf_config
 
     @classmethod
     def get_id(cls) -> str:
         return "autoq_precision_init_args"
 
 
+@api()
 class LeGRInitArgs(NNCFExtraConfigStruct):
     """
     Stores arguments for learning global ranking in pruning algorithm.
+
     :param train_loader: provides an iterable over the given training (or initialising) dataset.
     :param train_fn: callable for training compressed model. Train model for one epoch or train_steps (if specified) by
-    given args: [dataloader, model, optimizer, compression algorithm controller, train_steps number].
+      given args: [dataloader, model, optimizer, compression algorithm controller, train_steps number].
     :param val_loader: provides an iterable over the given validation dataset.
     :param val_fn: callable to validate model, calculates pair of validation [acc, loss] by given model and dataloader.
     :param train_optimizer: optional, optimizer for model training.
     :param nncf_config: NNCF config for compression.
     """
-    def __init__(self,
-                 train_loader: torch.utils.data.DataLoader,
-                 train_fn: Callable[[torch.utils.data.DataLoader, torch.nn.Module,
-                                     torch.optim.Optimizer, 'CompressionAlgorithmController',
-                                     Optional[int]], type(None)],
-                 val_loader: torch.utils.data.DataLoader,
-                 val_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader],
-                                  Tuple[float, float]],
-                 train_optimizer: Optional[torch.optim.Optimizer],
-                 nncf_config: 'NNCFConfig'):
+
+    def __init__(
+        self,
+        train_loader: torch.utils.data.DataLoader,
+        train_fn: Callable[
+            [
+                torch.utils.data.DataLoader,
+                torch.nn.Module,
+                torch.optim.Optimizer,
+                "CompressionAlgorithmController",
+                Optional[int],
+            ],
+            type(None),
+        ],
+        val_loader: torch.utils.data.DataLoader,
+        val_fn: Callable[[torch.nn.Module, torch.utils.data.DataLoader], Tuple[float, float]],
+        train_optimizer: Optional[torch.optim.Optimizer],
+        nncf_config: "NNCFConfig",
+    ):
         self.train_loader = train_loader
         self.train_steps_fn = train_fn
         self.val_loader = val_loader
         self.val_fn = val_fn
         self.train_optimizer = train_optimizer
         self.config = nncf_config
 
     @classmethod
     def get_id(cls) -> str:
         return "legr_init_args"
 
 
+@api()
 class DistributedCallbacksArgs(NNCFExtraConfigStruct):
     """
     A pair of callbacks that is needed for distributed training of the model: wrapping model with wrapping_callback for
     distributed training, and after all training steps unwrapping model to the initial not-distributed state with
     unwrapping_callback.
+
     :param wrapping_callback: Callback that wraps the model for distributed training with any necessary structure (for
-    example, torch.nn.DataParallel or any custom class), returns wrapped model ready for distributed training
+      example, torch.nn.DataParallel or any custom class), returns wrapped model ready for distributed training
     :param unwrapping_callback: Callback for unwrapping the model wrapped with wrapping_callback, returns original model
     """
-    def __init__(self,
-                 wrapping_callback: Callable[[nn.Module], nn.Module],
-                 unwrapping_callback: Callable[[nn.Module], nn.Module]):
+
+    def __init__(
+        self, wrapping_callback: Callable[[nn.Module], nn.Module], unwrapping_callback: Callable[[nn.Module], nn.Module]
+    ):
         self.wrap_model = wrapping_callback
         self.unwrap_model = unwrapping_callback
 
     @classmethod
     def get_id(cls) -> str:
         return "distributed_callbacks_args"
 
 
+@api()
 class ExecutionParameters:
     """
     Parameters that are necessary for distributed training of the model.
+
     :param cpu_only: whether cpu-only mode is using for training
     :param current_gpu: id of GPU that should be used for training (if only one of all is used)
     """
+
     def __init__(self, cpu_only: bool, current_gpu: Optional[int]):
         self.cpu_only = cpu_only
         self.current_gpu = current_gpu
```

### Comparing `nncf-2.4.0/nncf/torch/tensor.py` & `nncf-2.5.0/nncf/torch/dynamic_graph/structs.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,34 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import torch
 
-from nncf.common.tensor import NNCFTensor
+from enum import Enum
 
 
-class PTNNCFTensor(NNCFTensor):
+class NamespaceTarget(Enum):
     """
-    A realisation of torch tensors wrapper for common NNCF algorithms.
+    NamespaceTarget stores modules from which patched operators were obtained.
     """
 
-    def __init__(self, tensor: torch.tensor):
-        # In case somebody attempts to wrap
-        # tensor twice
-        if isinstance(tensor, self.__class__):
-            tensor = tensor.tensor
-
-        super().__init__(tensor)
-
-    @property
-    def device(self) -> torch.device:
-        return self._tensor.device
+    TORCH_NN_FUNCTIONAL = "torch.nn.functional"
+    TORCH_TENSOR = "torch.tensor"
+    TORCH = "torch"
+    EXTERNAL = "external_function"
```

### Comparing `nncf-2.4.0/nncf/torch/tensor_statistics/algo.py` & `nncf-2.5.0/nncf/torch/tensor_statistics/algo.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,92 +1,93 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 from typing import Dict, Set
 
-from nncf.torch.algo_selector import ZeroCompressionLoss
 from nncf.api.compression import CompressionStage
-from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
-from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.common.schedulers import StubCompressionScheduler
 from nncf.common.statistics import NNCFStatistics
+from nncf.common.tensor_statistics.collectors import ReductionShape
+from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
 from nncf.config import NNCFConfig
-from nncf.torch.graph.transformations.layout import PTTransformationLayout
+from nncf.torch.algo_selector import ZeroCompressionLoss
+from nncf.torch.compression_method_api import PTCompressionAlgorithmBuilder
+from nncf.torch.compression_method_api import PTCompressionAlgorithmController
 from nncf.torch.graph.transformations.commands import PTInsertionCommand
 from nncf.torch.graph.transformations.commands import PTTargetPoint
-from nncf.torch.nncf_network import NNCFNetwork
 from nncf.torch.graph.transformations.commands import TransformationPriority
-from nncf.common.tensor_statistics.collectors import ReductionShape
-from nncf.common.tensor_statistics.collectors import TensorStatisticCollectorBase
+from nncf.torch.graph.transformations.layout import PTTransformationLayout
+from nncf.torch.nncf_network import NNCFNetwork
 
 
 class TensorStatisticObservationPoint:
-    def __init__(self, insertion_point: PTTargetPoint,
-                 reduction_shapes: Set[ReductionShape] = None):
-        self.insertion_point = insertion_point
+    def __init__(self, target_point: PTTargetPoint, reduction_shapes: Set[ReductionShape] = None):
+        self.target_point = target_point
         self.reduction_shapes = reduction_shapes
 
     def __hash__(self):
-        return hash(self.insertion_point)
+        return hash(self.target_point)
 
-    def __eq__(self, other: 'TensorStatisticObservationPoint'):
-        return self.insertion_point == other.insertion_point
+    def __eq__(self, other: "TensorStatisticObservationPoint"):
+        return self.target_point == other.target_point
 
 
 class TensorStatisticsCollectionBuilder(PTCompressionAlgorithmBuilder):
-    def __init__(self, config: NNCFConfig,
-                 observation_points_vs_collectors: Dict[TensorStatisticObservationPoint,
-                                                        TensorStatisticCollectorBase]):
+    def __init__(
+        self,
+        config: NNCFConfig,
+        observation_points_vs_collectors: Dict[TensorStatisticObservationPoint, TensorStatisticCollectorBase],
+    ):
         super().__init__(config)
         self._observation_points_vs_collectors = observation_points_vs_collectors
 
     def _get_transformation_layout(self, target_model: NNCFNetwork) -> PTTransformationLayout:
         # Will it really suffice to use a single collector for all threads? After all, each of the threads
         # receives its own data, and should we use a thread-local collector, there would have to be a
         # separate thread reduction step involved. Still, is there a better option here than to rely on GIL?
         layout = PTTransformationLayout()
         for op, rs_vs_collector in self._observation_points_vs_collectors.items():
             for collector in rs_vs_collector.values():
                 hook_obj = collector.register_input
-                command = PTInsertionCommand(op.insertion_point, hook_obj,
-                                             TransformationPriority.FP32_TENSOR_STATISTICS_OBSERVATION)
+                command = PTInsertionCommand(
+                    op.target_point, hook_obj, TransformationPriority.FP32_TENSOR_STATISTICS_OBSERVATION
+                )
                 layout.register(command)
         return layout
 
-    def _build_controller(self, model: NNCFNetwork) -> 'TensorStatisticsCollectionController':
-        return TensorStatisticsCollectionController(model,
-                                                    {k.insertion_point: v
-                                                     for k, v in self._observation_points_vs_collectors.items()})
+    def _build_controller(self, model: NNCFNetwork) -> "TensorStatisticsCollectionController":
+        return TensorStatisticsCollectionController(
+            model, {k.target_point: v for k, v in self._observation_points_vs_collectors.items()}
+        )
 
     def _handle_frozen_layers(self, target_model: NNCFNetwork):
         pass
 
     def initialize(self, model: NNCFNetwork) -> None:
         pass
 
     def _get_algo_specific_config_section(self) -> Dict:
         return {}
 
 
 class TensorStatisticsCollectionController(PTCompressionAlgorithmController):
-    def __init__(self, target_model: NNCFNetwork,
-                 ip_vs_collector_dict: Dict[PTTargetPoint, TensorStatisticCollectorBase]):
+    def __init__(
+        self, target_model: NNCFNetwork, ip_vs_collector_dict: Dict[PTTargetPoint, TensorStatisticCollectorBase]
+    ):
         super().__init__(target_model)
         self.ip_vs_collector_dict = ip_vs_collector_dict
         self._scheduler = StubCompressionScheduler()
-        self._loss = ZeroCompressionLoss('cpu')
+        self._loss = ZeroCompressionLoss("cpu")
 
     @property
     def loss(self) -> ZeroCompressionLoss:
         return self._loss
 
     @property
     def scheduler(self) -> StubCompressionScheduler:
```

### Comparing `nncf-2.4.0/nncf/torch/tensor_statistics/collectors.py` & `nncf-2.5.0/nncf/torch/tensor_statistics/collectors.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,93 +1,120 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from typing import Union, List, Deque
+from typing import Any, Callable, Deque, List, Optional, Union
 
 import torch
 
 from nncf.common.tensor import NNCFTensor
 from nncf.common.tensor import TensorElementsType
+from nncf.common.tensor_statistics.collectors import MeanMinMaxStatisticCollector
+from nncf.common.tensor_statistics.collectors import MeanPercentileStatisticCollector
+from nncf.common.tensor_statistics.collectors import MedianMADStatisticCollector
 from nncf.common.tensor_statistics.collectors import MinMaxStatisticCollector
+from nncf.common.tensor_statistics.collectors import MixedMinMaxStatisticCollector
 from nncf.common.tensor_statistics.collectors import NNCFCollectorTensorProcessor
-from nncf.common.tensor_statistics.collectors import MedianMADStatisticCollector
 from nncf.common.tensor_statistics.collectors import PercentileStatisticCollector
-from nncf.common.tensor_statistics.collectors import MeanPercentileStatisticCollector
-from nncf.common.tensor_statistics.collectors import MixedMinMaxStatisticCollector
-from nncf.common.tensor_statistics.collectors import MeanMinMaxStatisticCollector
 from nncf.common.tensor_statistics.collectors import ReductionShape
 from nncf.common.tensor_statistics.reduction import np_percentile_reduce_like
-from nncf.torch.tensor_statistics.reduction import  expand_like
-from nncf.torch.tensor_statistics.statistics import PTMinMaxTensorStatistic
-from nncf.torch.tensor_statistics.statistics import PTMedianMADTensorStatistic
-from nncf.torch.tensor_statistics.statistics import PTPercentileTensorStatistic
 from nncf.torch.dynamic_graph.context import no_nncf_trace
 from nncf.torch.tensor import PTNNCFTensor
+from nncf.torch.tensor_statistics.reduction import expand_like
+from nncf.torch.tensor_statistics.statistics import PTMedianMADTensorStatistic
+from nncf.torch.tensor_statistics.statistics import PTMinMaxTensorStatistic
+from nncf.torch.tensor_statistics.statistics import PTPercentileTensorStatistic
 
 
 class PTNNCFCollectorTensorProcessor(NNCFCollectorTensorProcessor):
     """
     A realization of the processing methods for PTNNCFTensors.
     """
 
     @staticmethod
-    def reduce_min(x: NNCFTensor, axis: Union[int, tuple]) -> NNCFTensor:
-        return PTNNCFTensor(torch.amin(x.tensor, dim=axis))
+    def reduce_min(x: NNCFTensor, axis: Union[int, tuple, list], keepdims: bool = False) -> NNCFTensor:
+        return PTNNCFTensor(torch.amin(x.tensor, dim=axis, keepdim=keepdims))
 
     @staticmethod
-    def reduce_max(x: NNCFTensor, axis: Union[int, tuple]) -> NNCFTensor:
-        return PTNNCFTensor(torch.amax(x.tensor, dim=axis))
+    def reduce_max(x: NNCFTensor, axis: Union[int, tuple, list], keepdims: bool = False) -> NNCFTensor:
+        return PTNNCFTensor(torch.amax(x.tensor, dim=axis, keepdim=keepdims))
 
     @staticmethod
     def abs(x: NNCFTensor) -> NNCFTensor:
         return PTNNCFTensor(torch.abs(x.tensor))
 
     @staticmethod
     def min(x1: NNCFTensor, x2: NNCFTensor) -> NNCFTensor:
         return PTNNCFTensor(torch.min(x1.tensor, x2.tensor))
 
     @staticmethod
     def max(x1: NNCFTensor, x2: NNCFTensor) -> NNCFTensor:
         return PTNNCFTensor(torch.max(x1.tensor, x2.tensor))
 
     @staticmethod
-    def mean(x: NNCFTensor, axis: Union[int, tuple]) -> NNCFTensor:
-        return PTNNCFTensor(x.tensor.mean(dim=axis))
+    def mean(x: NNCFTensor, axis: Union[int, tuple, list], keepdims=False) -> NNCFTensor:
+        return PTNNCFTensor(x.tensor.mean(dim=axis, keepdim=keepdims))
+
+    @staticmethod
+    def median(x: NNCFTensor, axis: Union[int, tuple, list], keepdims=False) -> NNCFTensor:
+        return PTNNCFTensor(x.tensor.median(dim=axis, keepdim=keepdims))
+
+    @staticmethod
+    def masked_mean(x: NNCFTensor, axis: Union[int, tuple, list], mask: NNCFTensor, keepdims=False) -> NNCFTensor:
+        raise NotImplementedError()
+
+    @staticmethod
+    def masked_median(x: NNCFTensor, axis: Union[int, tuple, list], mask: NNCFTensor, keepdims=False) -> NNCFTensor:
+        raise NotImplementedError()
 
     @staticmethod
     def stack(x: Union[List[NNCFTensor], Deque[NNCFTensor]], axis: int = 0) -> NNCFTensor:
         x = [t.tensor for t in x]
         return PTNNCFTensor(torch.stack(x, dim=axis))
 
     @staticmethod
     def unstack(x: NNCFTensor, axis: int = 0) -> List[NNCFTensor]:
         tensor = x.tensor
-        if list(tensor.shape) == []: #pylint: disable=C1803
+        if list(tensor.shape) == []:  # pylint: disable=C1803
             tensor = tensor.unsqueeze(0)
         tensor_list = torch.unbind(tensor, dim=axis)
         return [PTNNCFTensor(t) for t in tensor_list]
 
     @staticmethod
     def sum(tensor: NNCFTensor) -> TensorElementsType:
         return torch.sum(tensor.tensor).item()
 
+    @staticmethod
+    def quantile(
+        tensor: NNCFTensor, quantile: Union[float, List[float]], axis: Union[int, tuple, list], keepdims: bool = False
+    ) -> List[NNCFTensor]:
+        raise NotImplementedError()
+
+    @staticmethod
+    def mean_per_channel(x: NNCFTensor, axis: int) -> NNCFTensor:
+        raise NotImplementedError()
+
+    @classmethod
+    def no_outliers_map(
+        cls, x: NNCFTensor, fn: Callable[[NNCFTensor, Optional[int]], Any], axis: int = 0, alpha: float = 0.01
+    ):
+        raise NotImplementedError()
+
 
 class PTMinMaxStatisticCollector(MinMaxStatisticCollector):
-    def __init__(self, use_abs_max: bool, reduction_shape: ReductionShape, output_shape: ReductionShape,
-                 num_samples: int = None):
+    def __init__(
+        self, use_abs_max: bool, reduction_shape: ReductionShape, output_shape: ReductionShape, num_samples: int = None
+    ):
         super().__init__(use_abs_max, reduction_shape, num_samples)
         self._output_shape = output_shape
 
     @staticmethod
     def _get_processor() -> NNCFCollectorTensorProcessor:
         return PTNNCFCollectorTensorProcessor()
 
@@ -98,25 +125,34 @@
     def _get_statistics(self) -> PTMinMaxTensorStatistic:
         min_values = self._min_values.tensor.view(self._output_shape)
         max_values = self._max_values.tensor.view(self._output_shape)
         return PTMinMaxTensorStatistic(min_values, max_values)
 
 
 class PTMixedMinMaxStatisticCollector(MixedMinMaxStatisticCollector):
-    def __init__(self,
-                 use_per_sample_stats: bool,
-                 use_abs_max: bool,
-                 use_means_of_mins: bool,
-                 use_means_of_maxs: bool,
-                 reduction_shape: ReductionShape,
-                 output_shape: ReductionShape,
-                 num_samples: int = None,
-                 window_size: int = None):
-        super().__init__(use_per_sample_stats, use_abs_max, use_means_of_mins,
-                         use_means_of_maxs, reduction_shape, num_samples, window_size)
+    def __init__(
+        self,
+        use_per_sample_stats: bool,
+        use_abs_max: bool,
+        use_means_of_mins: bool,
+        use_means_of_maxs: bool,
+        reduction_shape: ReductionShape,
+        output_shape: ReductionShape,
+        num_samples: int = None,
+        window_size: int = None,
+    ):
+        super().__init__(
+            use_per_sample_stats,
+            use_abs_max,
+            use_means_of_mins,
+            use_means_of_maxs,
+            reduction_shape,
+            num_samples,
+            window_size,
+        )
         self._output_shape = output_shape
 
     @staticmethod
     def _get_processor() -> NNCFCollectorTensorProcessor:
         return PTNNCFCollectorTensorProcessor()
 
     def _register_input(self, x: torch.Tensor):
@@ -126,23 +162,24 @@
     def _get_statistics(self) -> PTMinMaxTensorStatistic:
         min_values = self._min_aggregate().tensor.view(self._output_shape)
         max_values = self._max_aggregate().tensor.view(self._output_shape)
         return PTMinMaxTensorStatistic(min_values, max_values)
 
 
 class PTMeanMinMaxStatisticCollector(MeanMinMaxStatisticCollector):
-    def __init__(self,
-                 use_per_sample_stats: bool,
-                 use_abs_max: bool,
-                 reduction_shape: ReductionShape,
-                 output_shape: ReductionShape,
-                 num_samples: int = None,
-                 window_size: int = None):
-        super().__init__(use_per_sample_stats, use_abs_max, reduction_shape,
-                                                             num_samples, window_size)
+    def __init__(
+        self,
+        use_per_sample_stats: bool,
+        use_abs_max: bool,
+        reduction_shape: ReductionShape,
+        output_shape: ReductionShape,
+        num_samples: int = None,
+        window_size: int = None,
+    ):
+        super().__init__(use_per_sample_stats, use_abs_max, reduction_shape, num_samples, window_size)
         self._output_shape = output_shape
 
     @staticmethod
     def _get_processor() -> NNCFCollectorTensorProcessor:
         return PTNNCFCollectorTensorProcessor()
 
     def _register_input(self, x: torch.Tensor):
```

### Comparing `nncf-2.4.0/nncf/torch/tensor_statistics/reduction.py` & `nncf-2.5.0/nncf/torch/tensor_statistics/reduction.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,24 +1,22 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 from typing import List, Tuple
 
-import torch
 import numpy as np
+import torch
 
 
 def max_reduce_like(input_: torch.Tensor, ref_tensor_shape: List[int]) -> torch.Tensor:
     numel = np.prod(ref_tensor_shape)
     if numel == 1:
         retval = input_.max()
         for _ in ref_tensor_shape:
```

### Comparing `nncf-2.4.0/nncf/torch/tensor_statistics/statistics.py` & `nncf-2.5.0/nncf/torch/tensor_statistics/statistics.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import torch
 
-from nncf.common.tensor_statistics.statistics import TensorStatistic
-from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
 from nncf.common.tensor_statistics.statistics import MedianMADTensorStatistic
+from nncf.common.tensor_statistics.statistics import MinMaxTensorStatistic
 from nncf.common.tensor_statistics.statistics import PercentileTensorStatistic
+from nncf.common.tensor_statistics.statistics import TensorStatistic
 
 
 class PTMinMaxTensorStatistic(MinMaxTensorStatistic):
     @staticmethod
     def tensor_eq(tensor1: torch.Tensor, tensor2: torch.Tensor, rtol=1e-6) -> bool:
         return bool(torch.allclose(tensor1, tensor2, rtol=rtol))
 
@@ -39,17 +37,20 @@
 
 def pt_convert_stat_to_min_max_tensor_stat(statistic: TensorStatistic) -> PTMinMaxTensorStatistic:
     if isinstance(statistic, PTMinMaxTensorStatistic):
         return statistic
     if isinstance(statistic, PTMedianMADTensorStatistic):
         # Using three-sigma approach to estimate min and max
         # Constant factor depends on the distribution form - assuming normal and the factor is 1.4826
-        return PTMinMaxTensorStatistic(statistic.median_values - 3 * 1.4826230 * statistic.mad_values,
-                                     statistic.median_values + 3 * 1.4826230 * statistic.mad_values)
+        return PTMinMaxTensorStatistic(
+            statistic.median_values - 3 * 1.4826230 * statistic.mad_values,
+            statistic.median_values + 3 * 1.4826230 * statistic.mad_values,
+        )
     if isinstance(statistic, PTPercentileTensorStatistic):
         if len(statistic.percentile_vs_values_dict.keys()) < 2:
             raise ValueError("Cannot create a min-max statistic for less than 2 percentile values")
         min_pct = min(statistic.percentile_vs_values_dict.keys())
         max_pct = max(statistic.percentile_vs_values_dict.keys())
-        return PTMinMaxTensorStatistic(statistic.percentile_vs_values_dict[min_pct],
-                                     statistic.percentile_vs_values_dict[max_pct])
+        return PTMinMaxTensorStatistic(
+            statistic.percentile_vs_values_dict[min_pct], statistic.percentile_vs_values_dict[max_pct]
+        )
     raise ValueError("Unknown TensorStatistic to generate min-max stat from!")
```

### Comparing `nncf-2.4.0/nncf/torch/utils.py` & `nncf-2.5.0/nncf/torch/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,36 +1,32 @@
-"""
- Copyright (c) 2019-2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 import random
 from collections import OrderedDict
 from contextlib import contextmanager
-from typing import Any
-from typing import Dict
-from typing import List
+from typing import Any, Dict, List
 
 import numpy as np
 import torch
 from torch import distributed as dist
 from torch import nn
 from torch.nn import Module
 
 from nncf.common.compression import BaseCompressionAlgorithmController as BaseController
+from nncf.common.deprecation import warning_deprecated
 from nncf.common.graph import NNCFNodeName
 from nncf.common.logging import nncf_logger
-from nncf.common.logging.logger import warning_deprecated
 from nncf.common.scopes import matches_any
 from nncf.torch.dynamic_graph.trace_tensor import TracedTensor
 from nncf.torch.layer_utils import _NNCFModuleMixin
 
 
 def get_node_name(module, module_name, prefix):
     return "{prefix}/{cls}[{name}]".format(prefix=prefix, cls=module.__class__.__name__, name=module_name)
@@ -45,52 +41,64 @@
         found[full_node_name] = module
         sub_found = get_all_modules(module, prefix=full_node_name)
         if sub_found:
             found.update(sub_found)
     return found
 
 
-def get_all_modules_by_type(model, module_types=None, current_scope=None,
-                            ignored_scopes=None, target_scopes=None) -> Dict['Scope', Module]:
+def get_all_modules_by_type(
+    model, module_types=None, current_scope=None, ignored_scopes=None, target_scopes=None, memo=None
+) -> Dict["Scope", Module]:
+    if memo is None:
+        memo = set()
     if isinstance(module_types, str):
         module_types = [module_types]
     found = OrderedDict()
     from nncf.torch.dynamic_graph.scope import Scope  # pylint: disable=cyclic-import
     from nncf.torch.dynamic_graph.scope import ScopeElement  # pylint: disable=cyclic-import
+
     if current_scope is None:
         current_scope = Scope()
         current_scope.push(ScopeElement(model.__class__.__name__))
     for name, module in model.named_children():
+        if id(module) in memo:
+            continue
+        memo.add(id(module))
         child_scope_element = ScopeElement(module.__class__.__name__, name)
         child_scope = current_scope.copy()
         child_scope.push(child_scope_element)
 
         if matches_any(str(child_scope), ignored_scopes):
             continue
 
         if target_scopes is None or matches_any(str(child_scope), target_scopes):
             if module_types is None or module_types.count(str(type(module).__name__)) != 0:
                 found[child_scope] = module
-            sub_found = get_all_modules_by_type(module, module_types,
-                                                current_scope=child_scope,
-                                                ignored_scopes=ignored_scopes,
-                                                target_scopes=target_scopes)
+            sub_found = get_all_modules_by_type(
+                module,
+                module_types,
+                current_scope=child_scope,
+                ignored_scopes=ignored_scopes,
+                target_scopes=target_scopes,
+                memo=memo,
+            )
             if sub_found:
                 found.update(sub_found)
     return found
 
 
-def get_state_dict_names_with_modules(model: 'NNCFNetwork',
-                                      str_types: List[str] = None, prefix='') -> Dict[str, torch.nn.Module]:
+def get_state_dict_names_with_modules(
+    model: "NNCFNetwork", str_types: List[str] = None, prefix=""
+) -> Dict[str, torch.nn.Module]:
     found = OrderedDict()
     for name, module in model.named_children():
         full_node_name = "{}{}".format(prefix, name)
         if str_types is not None and type(module).__name__ in str_types:
             found[full_node_name] = module
-        sub_found = get_state_dict_names_with_modules(module, str_types, prefix=full_node_name + '.')
+        sub_found = get_state_dict_names_with_modules(module, str_types, prefix=full_node_name + ".")
         if sub_found:
             found.update(sub_found)
     return found
 
 
 def get_filters_num(module):
     if isinstance(module, _NNCFModuleMixin):
@@ -115,23 +123,25 @@
         self.state = torch._C._get_tracing_state()
         torch._C._set_tracing_state(None)
 
     def __exit__(self, *args):
         torch._C._set_tracing_state(self.state)
         self.state = None
 
+
 def fp32_accum_wrapper(func):
     def wrapper(tensor_to_sum, ret_tensor):
         half = tensor_to_sum.dtype == np.float16
         if half:
             tensor_to_sum = tensor_to_sum.astype(np.float)
         retval = func(tensor_to_sum, ret_tensor)
         if half:
             retval = retval.astype(np.float16)
         return retval
+
     return wrapper
 
 
 @fp32_accum_wrapper
 def sum_like(tensor_to_sum, ref_tensor):
     """Warning: may modify tensor_to_sum"""
     if ref_tensor.size == 1:
@@ -193,14 +203,15 @@
         result = main_call_fn()
     return result
 
 
 def is_tensor(obj):
     return isinstance(obj, torch.Tensor)
 
+
 def is_traced_tensor(obj):
     return isinstance(obj, TracedTensor)
 
 
 class _ModuleState:
     def __init__(self, base_module: Module = None):
         self._training_state = {}  # type: Dict[str, bool]
@@ -248,35 +259,38 @@
     try:
         yield
     finally:
         load_module_state(model, saved_state)
 
 
 def compute_FLOPs_hook(module, input_, output, dict_to_save, module_node_name: NNCFNodeName):
-    if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d, nn.Conv2d, nn.ConvTranspose2d, nn.Conv3d,
-                           nn.ConvTranspose3d)):
+    # WARNING: numpy should be explicitly given np.int64 as dtype, since default integer type on Win is np.int32
+    if isinstance(
+        module, (nn.Conv1d, nn.ConvTranspose1d, nn.Conv2d, nn.ConvTranspose2d, nn.Conv3d, nn.ConvTranspose3d)
+    ):
         ks = module.weight.data.shape
-        mac_count = np.prod(ks) * np.prod(output.shape[2:])
+        mac_count = np.prod(ks, dtype=np.int64) * np.prod(output.shape[2:], dtype=np.int64)
     elif isinstance(module, nn.Linear):
         if len(input_[0].shape) == 1:
             # In some test cases input tensor could have dimension [N]
             mac_count = input_[0].shape[0] * output.shape[-1]
         else:
-            mac_count = np.prod(input_[0].shape[1:]) * output.shape[-1]
+            mac_count = np.prod(input_[0].shape[1:], dtype=np.int64) * output.shape[-1]
     else:
         return
     dict_to_save[module_node_name] = 2 * mac_count
 
 
 def add_domain(name_operator: str) -> str:
     from nncf.torch.compression_method_api import DOMAIN_CUSTOM_OPS_NAME  # pylint: disable=cyclic-import
+
     return DOMAIN_CUSTOM_OPS_NAME + "::" + name_operator
 
 
-def default_distributed_wrapper(model: nn.Module, execution_parameters: 'ExecutionParameters'):
+def default_distributed_wrapper(model: nn.Module, execution_parameters: "ExecutionParameters"):
     """
     Wrapping model for distributed training with DataParallel or DistributedDataParallel depending on execution mode
     chosen by user.
     :param execution_parameters: structure with necessary execution parameters
     :param model: model to wrap  in accordance with execution mode chosen by user
     :return: wrapped model
     """
@@ -310,34 +324,36 @@
     :param model: model to unwrap.
     :return: model without parallelization
     """
     if isinstance(model, (torch.nn.parallel.DataParallel, torch.nn.parallel.DistributedDataParallel)):
         return model.module
     return model
 
-def rename_legacy_names_in_state_dict(state_dict_to_load: Dict[str, Any],
-                                      legacy_names: List[str],
-                                      legacy_name: str,
-                                      new_name: str):
 
+def rename_legacy_names_in_state_dict(
+    state_dict_to_load: Dict[str, Any], legacy_names: List[str], legacy_name: str, new_name: str
+):
     for name in legacy_names:
         tensor = state_dict_to_load.pop(name)
         new_key = name.replace(legacy_name, new_name) if not new_name in name else name
         state_dict_to_load[new_key] = tensor
 
     if legacy_names:
-        warning_deprecated('Legacy Batch Norm layer names was detected in checkpoint model state dict.'
-                           ' All occurrences of `{}` in nodes names was replaced by `{}`'.format(legacy_name, new_name))
+        warning_deprecated(
+            "Legacy Batch Norm layer names was detected in checkpoint model state dict."
+            " All occurrences of `{}` in nodes names was replaced by `{}`".format(legacy_name, new_name)
+        )
+
 
 LEGACY_VS_NEW_BN_MAP = {
-    'BatchNorm1d': 'NNCFBatchNorm1d',
-    'BatchNorm2d': 'NNCFBatchNorm2d',
-    'BatchNorm3d': 'NNCFBatchNorm3d',
-    'NNCFBatchNorm': 'NNCFBatchNorm2d',
-    'ConvBNActivation': 'Conv2dNormActivation',
+    "BatchNorm1d": "NNCFBatchNorm1d",
+    "BatchNorm2d": "NNCFBatchNorm2d",
+    "BatchNorm3d": "NNCFBatchNorm3d",
+    "NNCFBatchNorm": "NNCFBatchNorm2d",
+    "ConvBNActivation": "Conv2dNormActivation",
 }
 
 
 def maybe_convert_legacy_names_in_model_state(state_dict_to_load: Dict[str, Any]) -> None:
     """
     Convert legacy layer names in compressed model state dict in case such names exist.
 
@@ -359,50 +375,53 @@
 
     :param compression_state: Compression state to convert.
     """
     if not compression_state or BaseController.BUILDER_STATE not in compression_state:
         return
 
     controller_state = compression_state[BaseController.BUILDER_STATE]
-    if not controller_state or 'quantization' not in controller_state:
+    if not controller_state or "quantization" not in controller_state:
         return
 
     from nncf.torch.quantization.algo import QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME  # pylint: disable=cyclic-import
-    if not controller_state['quantization'].get(QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME):
-        qips = controller_state['quantization']['quantizer_setup']['quantization_points']
+
+    if not controller_state["quantization"].get(QUANTIZER_BUILDER_STATE_VERSION_SAVE_NAME):
+        qips = controller_state["quantization"]["quantizer_setup"]["quantization_points"]
 
         detected_legacy_names = {
-            'BatchNorm1d': False,
-            'BatchNorm2d': False,
-            'BatchNorm3d': False,
-            'NNCFBatchNorm': False,
+            "BatchNorm1d": False,
+            "BatchNorm2d": False,
+            "BatchNorm3d": False,
+            "NNCFBatchNorm": False,
         }
 
         for point in qips.values():
-            name = point['qip']['target_node_name']
+            name = point["qip"]["target_node_name"]
             for old_name, new_name in LEGACY_VS_NEW_BN_MAP.items():
                 if old_name in name and not new_name in name:
                     detected_legacy_names[old_name] = True
-                    point['qip']['target_node_name'] = name.replace(old_name, new_name)
+                    point["qip"]["target_node_name"] = name.replace(old_name, new_name)
                     break
 
         for old_name, was_detected in detected_legacy_names.items():
             if was_detected:
                 new_name = LEGACY_VS_NEW_BN_MAP[old_name]
-                warning_deprecated('Legacy Batch Norm layer names was detected in quantization setup target'
-                                   ' point names. All occurrences of `{}` in nodes names was replaced by'
-                                   ' `{}`'.format(old_name, new_name))
+                warning_deprecated(
+                    "Legacy Batch Norm layer names was detected in quantization setup target"
+                    " point names. All occurrences of `{}` in nodes names was replaced by"
+                    " `{}`".format(old_name, new_name)
+                )
 
 
 def get_model_device(model: torch.nn.Module) -> torch.device:
     try:
         device = next(model.parameters()).device
     except StopIteration:
         # The model had no parameters at all, doesn't matter which device to choose
-        device = torch.device('cpu')
+        device = torch.device("cpu")
     return device
 
 
 def get_model_dtype(model: torch.nn.Module) -> torch.dtype:
     try:
         dtype = next(model.parameters()).dtype
     except StopIteration:
```

### Comparing `nncf-2.4.0/nncf.egg-info/PKG-INFO` & `nncf-2.5.0/nncf.egg-info/PKG-INFO`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nncf
-Version: 2.4.0
+Version: 2.5.0
 Summary: Neural Networks Compression Framework
 Home-page: https://github.com/openvinotoolkit/nncf
 Author: Intel
 Author-email: alexander.kozlov@intel.com
 License: Apache-2.0
 Keywords: compression,quantization,sparsity,mixed-precision-training,quantization-aware-training,hawq,classification,pruning,object-detection,semantic-segmentation,nas,nlp,bert,transformers,mmdetection
 Classifier: Programming Language :: Python :: 3
@@ -24,163 +24,112 @@
 Provides-Extra: all
 License-File: LICENSE
 
 <div align="center">
 
 # Neural Network Compression Framework (NNCF)
 
+[Key Features](#key-features) 
+[Installation](#Installation-guide) 
+[Documentation](#documentation) 
+[Usage](#usage) 
+[Tutorials and Samples](#Model-compression-tutorials-and-samples) 
+[Third-party integration](#Third-party-repository-integration) 
+[Model Zoo](#NNCF-Compressed-Model-Zoo)
+ 
 [![GitHub Release](https://img.shields.io/github/v/release/openvinotoolkit/nncf?color=green)](https://github.com/openvinotoolkit/nncf/releases)
 [![Website](https://img.shields.io/website?up_color=blue&up_message=docs&url=https%3A%2F%2Fdocs.openvino.ai%2Flatest%2Fopenvino_docs_model_optimization_guide.html)](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
 [![Apache License Version 2.0](https://img.shields.io/badge/license-Apache_2.0-green.svg)](LICENSE)
 [![PyPI Downloads](https://static.pepy.tech/badge/nncf)](https://pypi.org/project/nncf/)
  
 </div>
 
-_For the installation instructions, [click here](#installation)._
-
-NNCF provides a suite of advanced algorithms for Neural Networks inference optimization in [OpenVINO&trade;](https://docs.openvino.ai/latest/home.html) with minimal accuracy drop.
+Neural Network Compression Framework (NNCF) provides a suite of post-training and training-time algorithms for neural networks inference optimization in [OpenVINO&trade;](https://docs.openvino.ai) with minimal accuracy drop.
 
 NNCF is designed to work with models from [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), [ONNX](https://onnx.ai/) and [OpenVINO&trade;](https://docs.openvino.ai/latest/home.html).
 
-NNCF provides samples that demonstrate the usage of compression algorithms for three different use cases on public PyTorch and 
-TensorFlow models and datasets: Image Classification, Object Detection and Semantic Segmentation. 
+NNCF provides [samples](#Model-Compression-Samples) that demonstrate the usage of compression algorithms for different use cases and models. 
 [Compression results](#nncf-compressed-model-zoo) achievable with the NNCF-powered samples can be found in a table at 
 the end of this document.
 
 The framework is organized as a Python\* package that can be built and used in a standalone mode. The framework 
 architecture is unified to make it easy to add different compression algorithms for both PyTorch and TensorFlow deep 
 learning frameworks.
 
 ## Key Features
 ### Post-Training Compression Algorithms
 
-| Compression algorithm                                                       |PyTorch|TensorFlow|   ONNX   |       OpenVINO     |
+| Compression algorithm                                                       |OpenVINO|PyTorch|   TensorFlow   |     ONNX       |
 |:----------------------------------------------------------------------------| :---: | :---: |:--------:|:------------------:|
-| [Quantization](./docs/compression_algorithms/post_training/Quantization.md) | Supported | Supported |Supported| Preview |
+| [Post-Training Quantization](./docs/compression_algorithms/post_training/Quantization.md) | Supported | Supported |Supported| Supported |
 
-_Preview means that this is a work in progress and NNCF does not guarantee the full functional support._
-
-### Training-time Compression Algorithms
+### Training-Time Compression Algorithms
 
 |Compression algorithm|PyTorch|TensorFlow|
 | :--- | :---: | :---: |
-|[Quantization](./docs/compression_algorithms/Quantization.md) | Supported | Supported |
-|[Mixed-Precision Quantization](./docs/compression_algorithms/Quantization.md#mixed_precision_quantization) | Supported | Not supported | Not supported |
-|[Binarization](./docs/compression_algorithms/Binarization.md) | Supported | Not supported | Not supported |
-|[Sparsity](./docs/compression_algorithms/Sparsity.md) | Supported | Supported | Not supported |
-|[Filter pruning](./docs/compression_algorithms/Pruning.md) | Supported | Supported | Not supported |
-
-
-
+|[Quantization Aware Training](./docs/compression_algorithms/Quantization.md) | Supported | Supported |
+|[Mixed-Precision Quantization](./docs/compression_algorithms/Quantization.md#mixed_precision_quantization) | Supported | Not supported |
+|[Binarization](./docs/compression_algorithms/Binarization.md) | Supported | Not supported |
+|[Sparsity](./docs/compression_algorithms/Sparsity.md) | Supported | Supported |
+|[Filter pruning](./docs/compression_algorithms/Pruning.md) | Supported | Supported |
+|[Movement pruning](./nncf/experimental/torch/sparsity/movement/MovementSparsity.md) | Experimental | Not supported |
 
 - Automatic, configurable model graph transformation to obtain the compressed model.
   > **NOTE**: Limited support for TensorFlow models. The models created using Sequential or Keras Functional API are only supported.
 - Common interface for compression methods.
 - GPU-accelerated layers for faster compressed model fine-tuning.
 - Distributed training support.
-- Configuration file examples for each supported compression algorithm.
-- Git patches for prominent third-party repositories ([huggingface-transformers](https://github.com/huggingface/transformers)) demonstrating the process of integrating NNCF into custom training pipelines
-- Exporting PyTorch compressed models to ONNX\* checkpoints and TensorFlow compressed models to SavedModel or Frozen Graph format, ready to use with [OpenVINO&trade; toolkit](https://docs.openvino.ai/latest/home.html).
+- Git patch for prominent third-party repository ([huggingface-transformers](https://github.com/huggingface/transformers)) demonstrating the process of integrating NNCF into custom training pipelines
+- Seamless combination of pruning, sparsity and quantization algorithms. Please refer to [optimum-intel](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino) for examples of 
+joint (movement) pruning, quantization and distillation (JPQD), end-to-end from NNCF optimization to compressed OpenVINO IR.
+- Exporting PyTorch compressed models to ONNX\* checkpoints and TensorFlow compressed models to SavedModel or Frozen Graph format, ready to use with [OpenVINO&trade; toolkit](https://docs.openvino.ai).
 - Support for [Accuracy-Aware model training](./docs/Usage.md#accuracy-aware-model-training) pipelines via the [Adaptive Compression Level Training](./docs/accuracy_aware_model_training/AdaptiveCompressionLevelTraining.md) and [Early Exit Training](./docs/accuracy_aware_model_training/EarlyExitTraining.md).
 
-## Usage
-The NNCF is organized as a regular Python package that can be imported in your target training pipeline script.
-The basic workflow is loading a JSON configuration script containing NNCF-specific parameters determining the compression to be applied to your model, and then passing your model along with the configuration script to the `create_compressed_model` function.
-This function returns a model with additional modifications necessary to enable algorithm-specific compression during fine-tuning and handle to the object allowing you to control the compression during the training process:
-
-### Usage example with PyTorch 
-
-```python
-import torch
-import nncf  # Important - must be imported before any other external package that depends on torch
-
-from nncf import NNCFConfig
-from nncf.torch import create_compressed_model, register_default_init_args
-
-# Instantiate your uncompressed model
-from torchvision.models.resnet import resnet50
-model = resnet50()
+## Documentation
 
-# Load a configuration file to specify compression
-nncf_config = NNCFConfig.from_json("resnet50_int8.json")
+This documentation covers detailed information about NNCF algorithms and functions needed for the contribution to NNCF.  
 
-# Provide data loaders for compression algorithm initialization, if necessary
-import torchvision.datasets as datasets
-representative_dataset = datasets.ImageFolder("/path")
-init_loader = torch.utils.data.DataLoader(representative_dataset)
-nncf_config = register_default_init_args(nncf_config, init_loader)
+The latest user documentation for NNCF is available [here](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html).
 
-# Apply the specified compression algorithms to the model
-compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+NNCF API documentation can be found [here](https://openvinotoolkit.github.io/nncf/autoapi/nncf/).
 
-# Now use compressed_model as a usual torch.nn.Module 
-# to fine-tune compression parameters along with the model weights
+## Usage
 
-# ... the rest of the usual PyTorch-powered training pipeline
+### Post-Training Quantization
 
-# Export to ONNX or .pth when done fine-tuning
-compression_ctrl.export_model("compressed_model.onnx")
-torch.save(compressed_model.state_dict(), "compressed_model.pth")
-```
+The NNCF PTQ is the simplest way to apply 8-bit quantization. To run the algorithm you only need your model and a small (~300 samples) calibration dataset.
 
-**NOTE (PyTorch)**: Due to the way NNCF works within the PyTorch backend, `import nncf` must be done before any other import of `torch` in your package _or_ in third-party packages that your code utilizes, otherwise the compression may be applied incompletely.
+[OpenVINO](https://github.com/openvinotoolkit/openvino) is the preferred backend to run PTQ with, and PyTorch, TensorFlow and ONNX are also supported.
 
+<details open><summary><b>OpenVINO</b></summary>
 
-### Usage example with TensorFlow
 ```python
-import tensorflow as tf
-
-from nncf import NNCFConfig
-from nncf.tensorflow import create_compressed_model, register_default_init_args
+import nncf
+import openvino.runtime as ov
+import torch
+from torchvision import datasets
 
 # Instantiate your uncompressed model
-from tensorflow.keras.applications import ResNet50
-model = ResNet50()
-
-# Load a configuration file to specify compression
-nncf_config = NNCFConfig.from_json("resnet50_int8.json")
-
-# Provide dataset for compression algorithm initialization
-representative_dataset = tf.data.Dataset.list_files("/path/*.jpeg")
-nncf_config = register_default_init_args(nncf_config, representative_dataset, batch_size=1)
-
-# Apply the specified compression algorithms to the model
-compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
-
-# Now use compressed_model as a usual Keras model
-# to fine-tune compression parameters along with the model weights
+model = ov.Core().read_model("/model_path")
+# Provide validation part of the dataset to collect statistics needed for the compression algorithm
+val_dataset = datasets.ImageFolder("/path")
+dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
 
-# ... the rest of the usual TensorFlow-powered training pipeline
+# Step 1: Initialize transformation function
+def transform_fn(data_item):
+    images, _ = data_item
+    return images
 
-# Export to Frozen Graph, TensorFlow SavedModel or .h5  when done fine-tuning 
-compression_ctrl.export_model("compressed_model.pb", save_format='frozen_graph')
+# Step 2: Initialize NNCF Dataset
+calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
+# Step 3: Run the quantization pipeline
+quantized_model = nncf.quantize(model, calibration_dataset)
 ```
 
-For a more detailed description of NNCF usage in your training code, see [this tutorial](docs/Usage.md). 
-For in-depth examples of NNCF integration, browse the [sample scripts](#compression-aware-training-samples) code, or the [example patches](#third-party-repository-integration) to third-party repositories.
-For FAQ, visit this [link](./docs/FAQ.md).
-
-### Usage examples of Post-Training Quantization
-
-NNCF provides [samples](#post-training-quantization-samples), which demonstrate Post-Training Quantization usage for PyTorch, TensorFlow, ONNX, OpenVINO.
-
-To start the algorithm, provide the following entities:
-* Original model.
-* Validation part of the dataset.
-* [Data transformation function](./docs/compression_algorithms/post_training/Quantization.md#data-transformation-function) transforming data items from the original dataset to the model input data. 
-
-
-The basic workflow steps:
-1) Create the [data transformation function](./docs/compression_algorithms/post_training/Quantization.md#data-transformation-function).
-2) Create an instance of `nncf.Dataset` class by passing two parameters:
-* `data_source` - Iterable python object that contains data items for model calibration.
-* `transform_fn` - Data transformation function from the Step 1.
-3) Run the quantization pipeline.
-
-Below are the usage examples for every backend.
+</details>
 
 <details><summary><b>PyTorch</b></summary>
 
 ```python
 import nncf
 import torch
 from torchvision import datasets, models
@@ -211,15 +160,15 @@
 import nncf
 import tensorflow as tf
 import tensorflow_datasets as tfds
 
 # Instantiate your uncompressed model
 model = tf.keras.applications.MobileNetV2()
 # Provide validation part of the dataset to collect statistics needed for the compression algorithm
-val_dataset = tfds.load('/path', split='validation', 
+val_dataset = tfds.load("/path", split="validation", 
                         shuffle_files=False, as_supervised=True)
 
 # Step 1: Initialize transformation function
 def transform_fn(data_item):
     images, _ = data_item
     return images
 
@@ -236,15 +185,15 @@
 ```python
 import onnx
 import nncf
 import torch
 from torchvision import datasets
 
 # Instantiate your uncompressed model
-onnx_model = onnx.load_model('/model_path')
+onnx_model = onnx.load_model("/model_path")
 # Provide validation part of the dataset to collect statistics needed for the compression algorithm
 val_dataset = datasets.ImageFolder("/path")
 dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
 
 # Step 1: Initialize transformation function
 input_name = onnx_model.graph.input[0].name
 def transform_fn(data_item):
@@ -255,168 +204,183 @@
 calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
 # Step 3: Run the quantization pipeline
 quantized_model = nncf.quantize(onnx_model, calibration_dataset)
 ```
 
 </details>
 
-<details><summary><b>OpenVINO</b></summary>
+
+[//]: # (NNCF provides full  [samples]&#40;#post-training-quantization-samples&#41;, which demonstrate Post-Training Quantization usage for PyTorch, TensorFlow, ONNX, OpenVINO.)
+
+### Training-Time Compression
+
+Below is an example of Accuracy Aware Quantization pipeline where model weights and compression parameters may be fine-tuned to achieve a higher accuracy.
+
+<details><summary><b>PyTorch</b></summary>
 
 ```python
-import nncf
-import openvino.runtime as ov
 import torch
-from torchvision import datasets
+import nncf.torch  # Important - must be imported before any other external package that depends on torch
+
+from nncf import NNCFConfig
+from nncf.torch import create_compressed_model, register_default_init_args
 
 # Instantiate your uncompressed model
-model = ov.Core().read_model('/model_path')
-# Provide validation part of the dataset to collect statistics needed for the compression algorithm
-val_dataset = datasets.ImageFolder("/path")
-dataset_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1)
+from torchvision.models.resnet import resnet50
+model = resnet50()
 
-# Step 1: Initialize transformation function
-def transform_fn(data_item):
-    images, _ = data_item
-    return images
+# Load a configuration file to specify compression
+nncf_config = NNCFConfig.from_json("resnet50_int8.json")
 
-# Step 2: Initialize NNCF Dataset
-calibration_dataset = nncf.Dataset(dataset_loader, transform_fn)
-# Step 3: Run the quantization pipeline
-quantized_model = nncf.quantize(model, calibration_dataset)
+# Provide data loaders for compression algorithm initialization, if necessary
+import torchvision.datasets as datasets
+representative_dataset = datasets.ImageFolder("/path")
+init_loader = torch.utils.data.DataLoader(representative_dataset)
+nncf_config = register_default_init_args(nncf_config, init_loader)
+
+# Apply the specified compression algorithms to the model
+compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+
+# Now use compressed_model as a usual torch.nn.Module 
+# to fine-tune compression parameters along with the model weights
+
+# ... the rest of the usual PyTorch-powered training pipeline
+
+# Export to ONNX or .pth when done fine-tuning
+compression_ctrl.export_model("compressed_model.onnx")
+torch.save(compressed_model.state_dict(), "compressed_model.pth")
 ```
 
+**NOTE (PyTorch)**: Due to the way NNCF works within the PyTorch backend, `import nncf` must be done before any other import of `torch` in your package _or_ in third-party packages that your code utilizes, otherwise the compression may be applied incompletely.
+
 </details>
 
-## Model Compression Samples
+<details><summary><b>Tensorflow</b></summary>
 
-For a quicker start with NNCF-powered compression, you can also try the sample scripts, each of which provides a basic training pipeline for classification, semantic segmentation and object detection neural network training correspondingly.
+```python
+import tensorflow as tf
 
-To run the samples please refer to the corresponding tutorials:
+from nncf import NNCFConfig
+from nncf.tensorflow import create_compressed_model, register_default_init_args
 
-### Compression-Aware Training Samples
-- PyTorch samples:
-  - [Image Classification sample](examples/torch/classification/README.md)
-  - [Object Detection sample](examples/torch/object_detection/README.md)
-  - [Semantic Segmentation sample](examples/torch/semantic_segmentation/README.md)
-- TensorFlow samples:
-    - [Image Classification sample](examples/tensorflow/classification/README.md)
-    - [Object Detection sample](examples/tensorflow/object_detection/README.md)
-    - [Instance Segmentation sample](examples/tensorflow/segmentation/README.md)
+# Instantiate your uncompressed model
+from tensorflow.keras.applications import ResNet50
+model = ResNet50()
 
-### Post-Training Quantization Samples
+# Load a configuration file to specify compression
+nncf_config = NNCFConfig.from_json("resnet50_int8.json")
+
+# Provide dataset for compression algorithm initialization
+representative_dataset = tf.data.Dataset.list_files("/path/*.jpeg")
+nncf_config = register_default_init_args(nncf_config, representative_dataset, batch_size=1)
+
+# Apply the specified compression algorithms to the model
+compression_ctrl, compressed_model = create_compressed_model(model, nncf_config)
+
+# Now use compressed_model as a usual Keras model
+# to fine-tune compression parameters along with the model weights
+
+# ... the rest of the usual TensorFlow-powered training pipeline
+
+# Export to Frozen Graph, TensorFlow SavedModel or .h5  when done fine-tuning 
+compression_ctrl.export_model("compressed_model.pb", save_format="frozen_graph")
+```
+
+</details>
 
-- [PyTorch Post-Training Quantization sample](examples/post_training_quantization/torch/mobilenet_v2/README.md)
-- [TensorFlow Post-Training Quantization sample](examples/post_training_quantization/tensorflow/mobilenet_v2/README.md)
-- [ONNX Post-Training Quantization sample](examples/post_training_quantization/onnx/mobilenet_v2/README.md)
-- [OpenVINO Post-Training Quantization sample](examples/post_training_quantization/openvino/mobilenet_v2/README.md)
+For a more detailed description of NNCF usage in your training code, see [this tutorial](docs/Usage.md).
 
-## Model Compression Notebooks 
+## Model Compression Tutorials and Samples
 
-A collection of ready-to-run Jupyter* notebooks are also available to demonstrate how to use NNCF compression algorithms
-to optimize models for inference with the OpenVINO Toolkit.
+For a quicker start with NNCF-powered compression, try sample notebooks and scripts presented below.
+
+### Model Compression Tutorials 
+
+A collection of ready-to-run Jupyter* notebooks are available to demonstrate how to use NNCF compression algorithms to optimize models for inference with the OpenVINO Toolkit:
+- [Accelerate Inference of NLP models with Post-Training Qunatization API of NNCF](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/105-language-quantize-bert)
+- [Convert and Optimize YOLOv8 with OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/230-yolov8-optimization)
+- [Convert and Optimize YOLOv7 with OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/226-yolov7-optimization)
+- [NNCF Post-Training Optimization of Segment Anything Model](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/237-segment-anything)
+- [Quantize a Segmentation Model and Show Live Inference](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize)
+- [Training to Deployment with TensorFlow and OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/301-tensorflow-training-openvino)
+- [Migrate quantization from POT API to NNCF API](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/111-yolov5-quantization-migration)
+- [Post-Training Quantization of Pytorch model with NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)
 - [Optimizing PyTorch models with NNCF of OpenVINO by 8-bit quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/302-pytorch-quantization-aware-training)
 - [Optimizing TensorFlow models with NNCF of OpenVINO by 8-bit quantization](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/305-tensorflow-quantization-aware-training)
-- [Post-Training Quantization of Pytorch model with NNCF](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/112-pytorch-post-training-quantization-nncf)
+- [Accelerate Inference of Sparse Transformer Models with OpenVINO and 4th Gen Intel Xeon Scalable Processors](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/116-sparsity-optimization)
+
+### Post-Training Quantization Samples
+Compact scripts demonstrating quantization and corresponding inference speed boost: 
+- [Post-Training Quantization of MobileNet v2 OpenVINO Model](examples/post_training_quantization/openvino/mobilenet_v2/README.md)
+- [Post-Training Quantization of YOLOv8 OpenVINO Model](examples/post_training_quantization/openvino/yolov8/README.md)
+- [Post-Training Quantization of Anomaly Classification OpenVINO model with control of accuracy metric](examples/post_training_quantization/openvino/quantize_with_accuracy_control/README.md)
+- [Post-Training Quantization of YOLOv8 OpenVINO Model with control of accuracy metric](examples/post_training_quantization/openvino/yolov8_quantize_with_accuracy_control/README.md)
+- [Post-Training Quantization of MobileNet v2 PyTorch Model](examples/post_training_quantization/torch/mobilenet_v2/README.md)
+- [Post-Training Quantization of SSD PyTorch Model](examples/post_training_quantization/torch/ssd300_vgg16/README.md)
+- [Post-Training Quantization of MobileNet v2 ONNX Model](examples/post_training_quantization/onnx/mobilenet_v2/README.md)
+- [Post-Training Quantization of MobileNet v2 TensorFlow Model](examples/post_training_quantization/tensorflow/mobilenet_v2/README.md)
+
+### Training-Time Compression Samples
+These examples provide full pipelines including compression, training and inference for classification, object detection and segmentation tasks.
+- PyTorch samples:
+  - [Image Classification sample](examples/torch/classification/README.md)
+  - [Object Detection sample](examples/torch/object_detection/README.md)
+  - [Semantic Segmentation sample](examples/torch/semantic_segmentation/README.md)
+- TensorFlow samples:
+  - [Image Classification sample](examples/tensorflow/classification/README.md)
+  - [Object Detection sample](examples/tensorflow/object_detection/README.md)
+  - [Instance Segmentation sample](examples/tensorflow/segmentation/README.md)
 
 ## Third-party repository integration
 NNCF may be straightforwardly integrated into training/evaluation pipelines of third-party repositories.
 
 ### Used by
 
 - [OpenVINO Training Extensions](https://github.com/openvinotoolkit/training_extensions)
   
   NNCF is integrated into OpenVINO Training Extensions as model optimization backend. So you can train, optimize and export new models based on the available model templates as well as run exported models with OpenVINO.
 
+- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov) 
+
+  NNCF is used as a compression backend within the renowned `transformers` repository in HuggingFace Optimum Intel.
+
 ### Git patches for third-party repository
 See [third_party_integration](./third_party_integration) for examples of code modifications (Git patches and base commit IDs are provided) that are necessary to integrate NNCF into the following repositories:
   - [huggingface-transformers](third_party_integration/huggingface_transformers/README.md)
 
-## System requirements
-- Ubuntu\* 18.04 or later (64-bit)
-- Python\* 3.7 or later
-- Supported frameworks:
-  - PyTorch\* 1.12.1
-  - TensorFlow\* >=2.4.0, <=2.8.2
-
-This repository is tested on Python* 3.8.10, PyTorch* 1.12.1 (NVidia CUDA\* Toolkit 11.6) and TensorFlow* 2.8.2 (NVidia CUDA\* Toolkit 11.2).
-
-## Installation
-We suggest to install or use the package in the [Python virtual environment](https://docs.python.org/3/tutorial/venv.html).
-
-If you want to optimize a model from PyTorch, install PyTorch by following [PyTorch installation guide](https://pytorch.org/get-started/locally/#start-locally). 
-If you want to optimize a model from TensorFlow, install TensorFlow by following [TensorFlow installation guide](https://www.tensorflow.org/install/).
-
-#### As a package built from a checked-out repository:
-
-Install the package and its dependencies by running the following in the repository root directory:
-```
-pip install .
-```
-
-Note that if you install NNCF in this manner, the backend frameworks supported by NNCF will not be explicitly installed. NNCF will try to work with whatever backend versions you have installed in your Python environment.
-
-If you want to install both NNCF and the supported PyTorch version in one line, you can do this by running:
-```
-pip install .[torch]
-```
-For installation of NNCF along with TensorFlow, run:
-```
-pip install .[tf]
-```
-For installation of NNCF for ONNX, run:
-```
-pip install .[onnx]
-```
-(Preview) For installation of NNCF for OpenVINO, run:
-```
-pip install .[openvino]
-```
-
-
-_NB_: For launching example scripts in this repository, we recommend setting the `PYTHONPATH` variable to the root of the checked-out repository once the installation is completed.
-
-#### As a PyPI package:
+## Installation Guide
+For detailed installation instructions please refer to the [Installation](./docs/Installation.md) page.
 
 NNCF can be installed as a regular PyPI package via pip:
 ```
 pip install nncf
 ```
-Use the same `pip install` syntax as above to install NNCF along with the backend package versions in one go, i.e. for NNCF with PyTorch, run:
+If you want to install both NNCF and the supported PyTorch version in one line, you can do this by simply running:
 ```
 pip install nncf[torch]
 ```
-For installation of NNCF along with TensorFlow, run:
-```
-pip install nncf[tf]
-```
-For installation of NNCF for ONNX, run:
-```
-pip install nncf[onnx]
-```
-(Preview) For installation of NNCF for OpenVINO, run:
-```
-pip install nncf[openvino]
-```
+Other viable options besides `[torch]` are `[tf]`, `[onnx]` and `[openvino]`.
 
 NNCF is also available via [conda](https://anaconda.org/conda-forge/nncf):
 ```
 conda install -c conda-forge nncf
 ```
 
-#### From a specific commit hash using pip:
-```python
-pip install git+https://github.com/openvinotoolkit/nncf@bd189e2#egg=nncf
-```
-Note that in order for this to work for pip versions >= 21.3, your Git version must be at least 2.22.
+You may also use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#model-compression-samples).
 
-#### As a Docker image
-Use one of the Dockerfiles in the [docker](./docker) directory to build an image with an environment already set up and ready for running NNCF [sample scripts](#model-compression-samples).
+### System requirements
+- Ubuntu\* 18.04 or later (64-bit)
+- Python\* 3.7 or later
+- Supported frameworks:
+  - PyTorch\* >=1.9.1, <1.14
+  - TensorFlow\* >=2.4.0, <=2.11.1
+  - ONNX\* ~=1.13.1
+  - OpenVINO\* >=2022.3.0
 
-## Contributing
-Refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines on contributions to the NNCF repository.
+This repository is tested on Python* 3.8.10, PyTorch* 1.13.1 (NVidia CUDA\* Toolkit 11.6) and TensorFlow* 2.11.1 (NVidia CUDA\* Toolkit 11.2).
 
 ## NNCF Compressed Model Zoo
 
 Results achieved using sample scripts, example patches to third-party repositories and NNCF configuration files provided 
 with this repository. See README.md files for [sample scripts](#model-compression-samples) and [example patches](#third-party-repository-integration) 
 to find instruction and links to exact configuration files and final checkpoints.
 - [PyTorch models](#pytorch-models)
@@ -430,63 +394,63 @@
   * [Instance segmentation](#tensorflow_instance_segmentation)
 
 ### PyTorch models
 
 <a name="pytorch_classification"></a>
 #### Classification
 
-|PyTorch Model|<img width="115" height="1">Compression algorithm<img width="115" height="1">|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|Accuracy (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|ResNet-50|INT8|ImageNet|76.42 (-0.26)|
-|ResNet-50|INT8 (per-tensor for weights)|ImageNet|76.37 (-0.21)|
-|ResNet-50|Mixed, 44.8% INT8 / 55.2% INT4|ImageNet|76.2 (-0.04)|
-|ResNet-50|INT8 + Sparsity 61% (RB)|ImageNet|75.43 (0.73)|
-|ResNet-50|INT8 + Sparsity 50% (RB)|ImageNet|75.55 (0.61)|
-|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|75.62 (0.54)|
-|Inception V3|INT8|ImageNet|78.25 (-0.91)|
-|Inception V3|INT8 + Sparsity 61% (RB)|ImageNet|77.58 (-0.24)|
-|MobileNet V2|INT8|ImageNet|71.35 (0.58)|
-|MobileNet V2|INT8 (per-tensor for weights)|ImageNet|71.3 (0.63)|
-|MobileNet V2|Mixed, 46.6% INT8 / 53.4% INT4|ImageNet|70.92 (1.01)|
-|MobileNet V2|INT8 + Sparsity 52% (RB)|ImageNet|71.11 (0.82)|
-|MobileNet V3 small|INT8|ImageNet|66.94 (0.73)|
-|SqueezeNet V1.1|INT8|ImageNet|58.28 (-0.04)|
-|SqueezeNet V1.1|INT8 (per-tensor for weights)|ImageNet|58.26 (-0.02)|
-|SqueezeNet V1.1|Mixed, 54.7% INT8 / 45.3% INT4|ImageNet|58.9 (-0.66)|
-|ResNet-18|XNOR (weights), scale/threshold (activations)|ImageNet|61.63 (8.17)|
-|ResNet-18|DoReFa (weights), scale/threshold (activations)|ImageNet|61.61 (8.19)|
-|ResNet-18|Filter pruning, 40%, magnitude criterion|ImageNet|69.26 (0.54)|
-|ResNet-18|Filter pruning, 40%, geometric median criterion|ImageNet|69.32 (0.48)|
+|ResNet-50|INT8|ImageNet|76.46 (-0.31)|
+|ResNet-50|INT8 (per-tensor only)|ImageNet|76.39 (-0.24)|
+|ResNet-50|Mixed, 43.12% INT8 / 56.88% INT4|ImageNet|76.05 (0.10)|
+|ResNet-50|INT8 + Sparsity 61% (RB)|ImageNet|75.42 (0.73)|
+|ResNet-50|INT8 + Sparsity 50% (RB)|ImageNet|75.50 (0.65)|
+|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|75.57 (0.58)|
+|Inception V3|INT8|ImageNet|77.45 (-0.12)|
+|Inception V3|INT8 + Sparsity 61% (RB)|ImageNet|76.36 (0.97)|
+|MobileNet V2|INT8|ImageNet|71.07 (0.80)|
+|MobileNet V2|INT8 (per-tensor only)|ImageNet|71.24 (0.63)|
+|MobileNet V2|Mixed, 58.88% INT8 / 41.12% INT4|ImageNet|70.95 (0.92)|
+|MobileNet V2|INT8 + Sparsity 52% (RB)|ImageNet|71.09 (0.78)|
+|MobileNet V3 small|INT8|ImageNet|66.98 (0.68)|
+|SqueezeNet V1.1|INT8|ImageNet|58.22 (-0.03)|
+|SqueezeNet V1.1|INT8 (per-tensor only)|ImageNet|58.11 (0.08)|
+|SqueezeNet V1.1|Mixed, 52.83% INT8 / 47.17% INT4|ImageNet|57.57 (0.62)|
+|ResNet-18|XNOR (weights), scale/threshold (activations)|ImageNet|61.67 (8.09)|
+|ResNet-18|DoReFa (weights), scale/threshold (activations)|ImageNet|61.63 (8.13)|
+|ResNet-18|Filter pruning, 40%, magnitude criterion|ImageNet|69.27 (0.49)|
+|ResNet-18|Filter pruning, 40%, geometric median criterion|ImageNet|69.31 (0.45)|
 |ResNet-34|Filter pruning, 50%, geometric median criterion + KD|ImageNet|73.11 (0.19)|
-|GoogLeNet|Filter pruning, 40%, geometric median criterion|ImageNet|68.82 (0.93)|
+|GoogLeNet|Filter pruning, 40%, geometric median criterion|ImageNet|69.47 (0.30)|
 
 <a name="pytorch_object_detection"></a>
 #### Object detection
 
-|PyTorch Model|Compression algorithm|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|SSD300-MobileNet|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|62.94 (-0.71)|
-|SSD300-VGG-BN|INT8|VOC12+07 train, VOC07 eval|77.96 (0.32)|
-|SSD300-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|77.59 (0.69)|
-|SSD300-VGG-BN|Filter pruning, 40%, geometric median criterion|VOC12+07 train, VOC07 eval|77.72 (0.56)|
-|SSD512-VGG-BN|INT8|VOC12+07 train, VOC07 eval|80.12 (0.14)|
-|SSD512-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|79.67 (0.59)|
+|SSD300-MobileNet|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|62.95 (-0.72)|
+|SSD300-VGG-BN|INT8|VOC12+07 train, VOC07 eval|77.81 (0.47)|
+|SSD300-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|77.66 (0.62)|
+|SSD300-VGG-BN|Filter pruning, 40%, geometric median criterion|VOC12+07 train, VOC07 eval|78.35 (-0.07)|
+|SSD512-VGG-BN|INT8|VOC12+07 train, VOC07 eval|80.04 (0.22)|
+|SSD512-VGG-BN|INT8 + Sparsity 70% (Magnitude)|VOC12+07 train, VOC07 eval|79.68 (0.58)|
 
 <a name="pytorch_semantic_segmentation"></a>
 #### Semantic segmentation
 
-|PyTorch Model|<img width="125" height="1">Compression algorithm<img width="125" height="1">|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|mIoU (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|UNet|INT8|CamVid|71.8 (0.15)|
-|UNet|INT8 + Sparsity 60% (Magnitude)|CamVid|72.03 (-0.08)|
-|ICNet|INT8|CamVid|67.86 (0.03)|
-|ICNet|INT8 + Sparsity 60% (Magnitude)|CamVid|67.18 (0.71)|
-|UNet|INT8|Mapillary|55.87 (0.36)|
-|UNet|INT8 + Sparsity 60% (Magnitude)|Mapillary|55.65 (0.58)|
-|UNet|Filter pruning, 25%, geometric median criterion|Mapillary|55.62 (0.61)|
+|UNet|INT8|CamVid|71.89 (0.06)|
+|UNet|INT8 + Sparsity 60% (Magnitude)|CamVid|72.46 (-0.51)|
+|ICNet|INT8|CamVid|67.89 (0.00)|
+|ICNet|INT8 + Sparsity 60% (Magnitude)|CamVid|67.16 (0.73)|
+|UNet|INT8|Mapillary|56.09 (0.15)|
+|UNet|INT8 + Sparsity 60% (Magnitude)|Mapillary|55.69 (0.55)|
+|UNet|Filter pruning, 25%, geometric median criterion|Mapillary|55.64 (0.60)|
 
 <a name="pytorch_nlp"></a>
 #### NLP (HuggingFace Transformers-powered models)
 
 |PyTorch Model|<img width="20" height="1">Compression algorithm<img width="20" height="1">|Dataset|Accuracy (Drop) %|
 | :---: | :---: | :---: | :---: |
 |BERT-base-chinese|INT8|XNLI|77.22 (0.46)|
@@ -499,52 +463,52 @@
 |GPT-2|INT8|WikiText-2 (raw)|perplexity: 20.9 (-1.17)|
 
 ### TensorFlow models
 
 <a name="tensorflow_classification"></a>
 #### Classification
 
-|Tensorflow Model|Compression algorithm|Dataset|Accuracy (Drop) %|
+|Model|Compression algorithm|Dataset|Accuracy (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|Inception V3|INT8 (per-tensor for weights)|ImageNet|78.36 (-0.44)|
-|Inception V3|Sparsity 54% (Magnitude)|ImageNet|77.87 (0.03)|
-|Inception V3|INT8 (per-tensor for weights) + Sparsity 61% (RB)|ImageNet|77.58 (0.32)|
-|MobileNet V2|INT8 (per-tensor for weights)|ImageNet|71.66 (0.19)|
-|MobileNet V2|Sparsity 50% (RB)|ImageNet|71.34 (0.51)|
-|MobileNet V2|INT8 (per-tensor for weights) + Sparsity 52% (RB)|ImageNet|71.0 (0.85)|
-|MobileNet V3 small|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) |ImageNet|67.75 (0.63)|
-|MobileNet V3 small|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) + Sparsity 42% (RB)|ImageNet|67.59 (0.79)|
-|MobileNet V3 large|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) |ImageNet|75.04 (0.77)|
-|MobileNet V3 large|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations) + Sparsity 42% (RB)|ImageNet|75.29 (0.52)|
-|ResNet50|INT8 (per-tensor for weights)|ImageNet|75.0 (0.04)|
-|ResNet50|Sparsity 80% (RB)|ImageNet|74.36 (0.68)|
-|ResNet50|INT8 (per-tensor for weightsy) + Sparsity 65% (RB)|ImageNet|74.3 (0.74)|
-|ResNet50|Filter Pruning 40%, geometric_median criterion|ImageNet|74.98 (0.06)|
-|ResNet50|Filter Pruning 40%, geometric_median criterion + INT8 (per-tensor for weights)|ImageNet|75.08 (-0.04)|
-|TensorFlow Hub MobileNet V2|Sparsity 35% (Magnitude)|ImageNet|71.90 (-0.06)|
+|Inception V3|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|78.39 (-0.48)|
+|Inception V3|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations), Sparsity 61% (RB)|ImageNet|77.52 (0.39)|
+|Inception V3|Sparsity 54% (Magnitude)|ImageNet|77.86 (0.05)|
+|MobileNet V2|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|71.63 (0.22)|
+|MobileNet V2|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations), Sparsity 52% (RB)|ImageNet|70.94 (0.91)|
+|MobileNet V2| Sparsity 50% (RB)|ImageNet|71.34 (0.51)|
+|MobileNet V2 (TensorFlow Hub MobileNet V2)|Sparsity 35% (Magnitude)|ImageNet|71.87 (-0.02)|
+|MobileNet V3 (Small)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|67.79 (0.59)|
+|MobileNet V3 (Small)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 42% (Magnitude)|ImageNet|67.44 (0.94)|
+|MobileNet V3 (Large)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|ImageNet|75.04 (0.76)|
+|MobileNet V3 (Large)|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 42% (RB)|ImageNet|75.24 (0.56)|
+|ResNet-50|INT8|ImageNet|74.99 (0.06)|
+|ResNet-50|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + Sparsity 65% (RB)|ImageNet|74.36 (0.69)|
+|ResNet-50|Sparsity 80% (RB)|ImageNet|74.38 (0.67)|
+|ResNet-50|Filter pruning, 40%, geometric median criterion|ImageNet|74.96 (0.09)|
+|ResNet-50|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + Filter pruning, 40%, geometric median criterion|ImageNet|75.09 (-0.04)|
 
 <a name="tensorflow_object_detection"></a>
 #### Object detection
 
-|TensorFlow Model|Compression algorithm|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|RetinaNet|INT8 (per-tensor for weights)|COCO2017|33.18 (0.26)|
-|RetinaNet|Sparsity 50% (Magnitude)|COCO2017|33.13 (0.31)|
-|RetinaNet|Filter Pruning 40%, geometric_median criterion|COCO2017|32.7 (0.74)|
-|RetinaNet|Filter Pruning 40%, geometric_median criterion + INT8 (per-tensor for weights)|COCO2017|32.68 (0.76)|
-|YOLOv4|INT8 (per-channel, symmetric for weights; per-tensor, asymmetric for activations)|COCO2017|46.30 (0.74)|
-|YOLOv4|Sparsity 50% (Magnitude)|COCO2017|46.54 (0.50)|
+|RetinaNet|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|33.12 (0.31)|
+|RetinaNet|Magnitude sparsity (50%)|COCO 2017|33.10 (0.33)|
+|RetinaNet|Filter pruning, 40%|COCO 2017|32.72 (0.71)|
+|RetinaNet|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations) + filter pruning 40%|COCO 2017|32.67 (0.76)|
+|YOLO v4|INT8 (per-channel symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|46.20 (0.87)|
+|YOLO v4|Magnitude sparsity, 50%|COCO 2017|46.49 (0.58)|
 
 <a name="tensorflow_instance_segmentation"></a>
 #### Instance segmentation
 
-|TensorFlow Model|<img width="110" height="1">Compression algorithm<img width="110" height="1">|Dataset|mAP (drop) %|
+|Model|Compression algorithm|Dataset|mAP (_drop_) %|
 | :---: | :---: | :---: | :---: |
-|MaskRCNN|INT8 (per-tensor for weights)|COCO2017|bbox: 37.27 (0.06)<br/>segm: 33.54 (0.02)|
-|MaskRCNN|Sparsity 50% (Magnitude)|COCO2017|bbox: 36.93 (0.40)<br/>segm: 33.23 (0.33)|
+|Mask-R-CNN|INT8 (per-tensor symmetric for weights, per-tensor asymmetric half-range for activations)|COCO 2017|37.19 (0.14)|
+|Mask-R-CNN|Magnitude sparsity, 50%|COCO 2017|36.94 (0.39)|
 
 ### ONNX models
 
 <a name="onnx_classification"></a>
 #### Classification
 
 |   ONNX Model    | Compression algorithm |Dataset|Accuracy (Drop) %|
@@ -572,19 +536,19 @@
     title =   {Neural network compression framework for fast model inference},
     author =  {Kozlov, Alexander and Lazarevich, Ivan and Shamporov, Vasily and Lyalyushkin, Nikolay and Gorbachev, Yury},
     journal = {arXiv preprint arXiv:2002.08679},
     year =    {2020}
 }
 ```
 
+## Contributing Guide
+Refer to the [CONTRIBUTING.md](./CONTRIBUTING.md) file for guidelines on contributions to the NNCF repository.
+
 ## Useful links
 - [Documentation](./docs)
 - Example scripts (model objects available through links in respective README.md files):
     - [PyTorch](./examples/torch)
     - [TensorFlow](./examples/tensorflow)
 - [FAQ](./docs/FAQ.md)
 - [Notebooks](https://github.com/openvinotoolkit/openvino_notebooks#-model-training)
-- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov) utilizes NNCF as a compression backend within the renowned `transformers` repository.
-- [Model Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
-
-## Legal Information
-[*] Other names and brands may be claimed as the property of others.
+- [HuggingFace Optimum Intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
+- [OpenVINO Model Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `nncf-2.4.0/nncf.egg-info/SOURCES.txt` & `nncf-2.5.0/nncf.egg-info/SOURCES.txt`

 * *Files 7% similar despite different names*

```diff
@@ -2,27 +2,29 @@
 MANIFEST.in
 README.md
 setup.py
 licensing/third-party-programs.txt
 nncf/__init__.py
 nncf/definitions.py
 nncf/parameters.py
+nncf/scopes.py
 nncf/version.py
 nncf.egg-info/PKG-INFO
 nncf.egg-info/SOURCES.txt
 nncf.egg-info/dependency_links.txt
 nncf.egg-info/requires.txt
 nncf.egg-info/top_level.txt
 nncf/api/__init__.py
 nncf/api/compression.py
 nncf/api/statistics.py
 nncf/common/__init__.py
 nncf/common/collector.py
 nncf/common/composite_compression.py
 nncf/common/compression.py
+nncf/common/deprecation.py
 nncf/common/engine.py
 nncf/common/exporter.py
 nncf/common/factory.py
 nncf/common/insertion_point_graph.py
 nncf/common/schedulers.py
 nncf/common/scopes.py
 nncf/common/stateful_classes_registry.py
@@ -36,17 +38,20 @@
 nncf/common/graph/__init__.py
 nncf/common/graph/definitions.py
 nncf/common/graph/graph.py
 nncf/common/graph/graph_matching.py
 nncf/common/graph/layer_attributes.py
 nncf/common/graph/model_transformer.py
 nncf/common/graph/operator_metatypes.py
-nncf/common/graph/patterns.py
 nncf/common/graph/utils.py
+nncf/common/graph/patterns/__init__.py
+nncf/common/graph/patterns/manager.py
+nncf/common/graph/patterns/patterns.py
 nncf/common/graph/transformations/__init__.py
+nncf/common/graph/transformations/command_creation.py
 nncf/common/graph/transformations/commands.py
 nncf/common/graph/transformations/layout.py
 nncf/common/hardware/__init__.py
 nncf/common/hardware/config.py
 nncf/common/hardware/opset.py
 nncf/common/hardware/configs/cpu.json
 nncf/common/hardware/configs/gpu.json
@@ -71,14 +76,15 @@
 nncf/common/pruning/symbolic_mask.py
 nncf/common/pruning/tensor_processor.py
 nncf/common/pruning/utils.py
 nncf/common/pruning/weights_flops_calculator.py
 nncf/common/quantization/__init__.py
 nncf/common/quantization/collectors.py
 nncf/common/quantization/config_assignment.py
+nncf/common/quantization/quantizer_removal.py
 nncf/common/quantization/quantizer_setup.py
 nncf/common/quantization/quantizers.py
 nncf/common/quantization/statistics.py
 nncf/common/quantization/structs.py
 nncf/common/quantization/initialization/__init__.py
 nncf/common/quantization/initialization/range.py
 nncf/common/quantization/quantizer_propagation/__init__.py
@@ -95,22 +101,25 @@
 nncf/common/tensor_statistics/__init__.py
 nncf/common/tensor_statistics/aggregator.py
 nncf/common/tensor_statistics/collectors.py
 nncf/common/tensor_statistics/reduction.py
 nncf/common/tensor_statistics/statistic_point.py
 nncf/common/tensor_statistics/statistics.py
 nncf/common/utils/__init__.py
+nncf/common/utils/api_marker.py
 nncf/common/utils/backend.py
 nncf/common/utils/debug.py
 nncf/common/utils/decorators.py
 nncf/common/utils/dot_file_rw.py
 nncf/common/utils/helpers.py
 nncf/common/utils/os.py
+nncf/common/utils/patcher.py
 nncf/common/utils/registry.py
 nncf/common/utils/tensorboard.py
+nncf/common/utils/timer.py
 nncf/common/utils/logger/__init__.py
 nncf/config/__init__.py
 nncf/config/config.py
 nncf/config/definitions.py
 nncf/config/extractors.py
 nncf/config/schema.py
 nncf/config/structures.py
@@ -133,24 +142,27 @@
 nncf/config/schemata/common/compression.py
 nncf/config/schemata/common/initialization.py
 nncf/config/schemata/common/sparsity.py
 nncf/config/schemata/common/targeting.py
 nncf/data/__init__.py
 nncf/data/dataset.py
 nncf/experimental/__init__.py
-nncf/experimental/openvino_native/__init__.py
-nncf/experimental/openvino_native/engine.py
-nncf/experimental/openvino_native/graph/__init__.py
-nncf/experimental/openvino_native/graph/model_transformer.py
-nncf/experimental/openvino_native/graph/nncf_graph_builder.py
-nncf/experimental/openvino_native/graph/metatypes/__init__.py
-nncf/experimental/openvino_native/graph/metatypes/openvino_metatypes.py
-nncf/experimental/openvino_native/quantization/__init__.py
-nncf/experimental/openvino_native/quantization/algorithms/__init__.py
-nncf/experimental/openvino_native/quantization/algorithms/min_max/__init__.py
+nncf/experimental/common/__init__.py
+nncf/experimental/common/graph/__init__.py
+nncf/experimental/common/graph/netron.py
+nncf/experimental/common/pruning/__init__.py
+nncf/experimental/common/pruning/block_hierarchy.py
+nncf/experimental/common/pruning/nodes_grouping.py
+nncf/experimental/common/pruning/operations.py
+nncf/experimental/common/pruning/propagation_data.py
+nncf/experimental/common/tensor_statistics/__init__.py
+nncf/experimental/common/tensor_statistics/collectors.py
+nncf/experimental/openvino/__init__.py
+nncf/experimental/openvino/quantization/__init__.py
+nncf/experimental/openvino/quantization/quantize_model.py
 nncf/experimental/tensorflow/__init__.py
 nncf/experimental/tensorflow/context.py
 nncf/experimental/tensorflow/nncf_network.py
 nncf/experimental/tensorflow/patch_tf.py
 nncf/experimental/tensorflow/scope.py
 nncf/experimental/tensorflow/graph/__init__.py
 nncf/experimental/tensorflow/graph/argprovider.py
@@ -187,73 +199,118 @@
 nncf/experimental/torch/nas/bootstrapNAS/training/lr_scheduler.py
 nncf/experimental/torch/nas/bootstrapNAS/training/model_creator_helpers.py
 nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_builder.py
 nncf/experimental/torch/nas/bootstrapNAS/training/progressive_shrinking_controller.py
 nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py
 nncf/experimental/torch/nas/bootstrapNAS/training/stage_descriptor.py
 nncf/experimental/torch/nas/bootstrapNAS/training/training_algorithm.py
+nncf/experimental/torch/pruning/__init__.py
+nncf/experimental/torch/pruning/operations.py
+nncf/experimental/torch/quantization/__init__.py
+nncf/experimental/torch/quantization/quantize_model.py
 nncf/experimental/torch/search_building_blocks/__init__.py
 nncf/experimental/torch/search_building_blocks/search_blocks.py
 nncf/experimental/torch/search_building_blocks/search_graph.py
 nncf/experimental/torch/sparsity/__init__.py
 nncf/experimental/torch/sparsity/movement/__init__.py
 nncf/experimental/torch/sparsity/movement/algo.py
 nncf/experimental/torch/sparsity/movement/functions.py
 nncf/experimental/torch/sparsity/movement/layers.py
 nncf/experimental/torch/sparsity/movement/loss.py
 nncf/experimental/torch/sparsity/movement/scheduler.py
 nncf/experimental/torch/sparsity/movement/structured_mask_handler.py
-nncf/experimental/torch/sparsity/movement/structured_mask_strategy.py
 nncf/onnx/__init__.py
 nncf/onnx/engine.py
 nncf/onnx/tensor.py
 nncf/onnx/graph/__init__.py
 nncf/onnx/graph/model_transformer.py
 nncf/onnx/graph/nncf_graph_builder.py
+nncf/onnx/graph/node_utils.py
 nncf/onnx/graph/onnx_graph.py
 nncf/onnx/graph/metatypes/__init__.py
 nncf/onnx/graph/metatypes/onnx_metatypes.py
 nncf/onnx/graph/transformations/__init__.py
+nncf/onnx/graph/transformations/command_creation.py
 nncf/onnx/graph/transformations/commands.py
 nncf/onnx/hardware/__init__.py
 nncf/onnx/hardware/config.py
 nncf/onnx/hardware/fused_patterns.py
 nncf/onnx/hardware/pattern_operations.py
-nncf/onnx/hardware/patterns.py
 nncf/onnx/quantization/__init__.py
 nncf/onnx/quantization/default_quantization.py
-nncf/onnx/quantization/quantize.py
+nncf/onnx/quantization/ignored_patterns.py
+nncf/onnx/quantization/quantize_model.py
 nncf/onnx/quantization/quantizer_parameters.py
 nncf/onnx/statistics/__init__.py
 nncf/onnx/statistics/aggregator.py
 nncf/onnx/statistics/collectors.py
 nncf/onnx/statistics/statistics.py
 nncf/openvino/__init__.py
 nncf/openvino/engine.py
+nncf/openvino/tensor.py
+nncf/openvino/graph/__init__.py
+nncf/openvino/graph/model_transformer.py
+nncf/openvino/graph/nncf_graph_builder.py
+nncf/openvino/graph/node_utils.py
+nncf/openvino/graph/metatypes/__init__.py
+nncf/openvino/graph/metatypes/common.py
+nncf/openvino/graph/metatypes/openvino_metatypes.py
+nncf/openvino/graph/transformations/__init__.py
+nncf/openvino/graph/transformations/command_creation.py
+nncf/openvino/graph/transformations/commands.py
+nncf/openvino/hardware/__init__.py
+nncf/openvino/hardware/config.py
+nncf/openvino/hardware/fused_patterns.py
+nncf/openvino/hardware/pattern_operations.py
+nncf/openvino/pot/__init__.py
+nncf/openvino/pot/engine.py
+nncf/openvino/pot/telemetry_extractors.py
+nncf/openvino/pot/quantization/__init__.py
+nncf/openvino/pot/quantization/accuracy_aware.py
+nncf/openvino/pot/quantization/quantize_model.py
 nncf/openvino/quantization/__init__.py
-nncf/openvino/quantization/accuracy_aware.py
-nncf/openvino/quantization/quantize.py
+nncf/openvino/quantization/backend_parameters.py
+nncf/openvino/quantization/default_quantization.py
+nncf/openvino/quantization/ignored_patterns.py
+nncf/openvino/quantization/quantize_model.py
+nncf/openvino/statistics/__init__.py
+nncf/openvino/statistics/aggregator.py
+nncf/openvino/statistics/collectors.py
+nncf/openvino/statistics/statistics.py
 nncf/quantization/__init__.py
-nncf/quantization/quantize.py
+nncf/quantization/advanced_parameters.py
+nncf/quantization/fake_quantize.py
+nncf/quantization/passes.py
+nncf/quantization/quantize_model.py
+nncf/quantization/range_estimator.py
 nncf/quantization/telemetry_extractors.py
 nncf/quantization/algorithms/__init__.py
 nncf/quantization/algorithms/algorithm.py
-nncf/quantization/algorithms/definitions.py
+nncf/quantization/algorithms/accuracy_control/__init__.py
+nncf/quantization/algorithms/accuracy_control/algorithm.py
+nncf/quantization/algorithms/accuracy_control/backend.py
+nncf/quantization/algorithms/accuracy_control/openvino_backend.py
+nncf/quantization/algorithms/accuracy_control/rank_functions.py
+nncf/quantization/algorithms/accuracy_control/ranker.py
 nncf/quantization/algorithms/bias_correction/__init__.py
 nncf/quantization/algorithms/bias_correction/algorithm.py
 nncf/quantization/algorithms/bias_correction/backend.py
 nncf/quantization/algorithms/bias_correction/onnx_backend.py
+nncf/quantization/algorithms/bias_correction/openvino_backend.py
 nncf/quantization/algorithms/fast_bias_correction/__init__.py
 nncf/quantization/algorithms/fast_bias_correction/algorithm.py
 nncf/quantization/algorithms/fast_bias_correction/backend.py
 nncf/quantization/algorithms/fast_bias_correction/onnx_backend.py
+nncf/quantization/algorithms/fast_bias_correction/openvino_backend.py
 nncf/quantization/algorithms/min_max/__init__.py
 nncf/quantization/algorithms/min_max/algorithm.py
 nncf/quantization/algorithms/min_max/backend.py
 nncf/quantization/algorithms/min_max/onnx_backend.py
+nncf/quantization/algorithms/min_max/openvino_backend.py
+nncf/quantization/algorithms/min_max/torch_backend.py
 nncf/quantization/algorithms/post_training/__init__.py
 nncf/quantization/algorithms/post_training/algorithm.py
 nncf/telemetry/__init__.py
 nncf/telemetry/decorator.py
 nncf/telemetry/events.py
 nncf/telemetry/extractors.py
 nncf/telemetry/wrapper.py
@@ -314,15 +371,15 @@
 nncf/tensorflow/quantization/__init__.py
 nncf/tensorflow/quantization/algorithm.py
 nncf/tensorflow/quantization/collectors.py
 nncf/tensorflow/quantization/default_quantization.py
 nncf/tensorflow/quantization/functions.py
 nncf/tensorflow/quantization/init_range.py
 nncf/tensorflow/quantization/layers.py
-nncf/tensorflow/quantization/quantize.py
+nncf/tensorflow/quantization/quantize_model.py
 nncf/tensorflow/quantization/quantizers.py
 nncf/tensorflow/quantization/utils.py
 nncf/tensorflow/sparsity/__init__.py
 nncf/tensorflow/sparsity/base_algorithm.py
 nncf/tensorflow/sparsity/callbacks.py
 nncf/tensorflow/sparsity/collector.py
 nncf/tensorflow/sparsity/utils.py
@@ -347,47 +404,58 @@
 nncf/torch/__init__.py
 nncf/torch/algo_selector.py
 nncf/torch/batchnorm_adaptation.py
 nncf/torch/checkpoint_loading.py
 nncf/torch/composite_compression.py
 nncf/torch/compression_method_api.py
 nncf/torch/debug.py
+nncf/torch/engine.py
 nncf/torch/exporter.py
 nncf/torch/functions.py
 nncf/torch/initialization.py
 nncf/torch/layer_utils.py
 nncf/torch/layers.py
 nncf/torch/model_creation.py
 nncf/torch/module_operations.py
 nncf/torch/nested_objects_traversal.py
+nncf/torch/nncf_module_replacement.py
 nncf/torch/nncf_network.py
 nncf/torch/structures.py
 nncf/torch/tensor.py
 nncf/torch/utils.py
 nncf/torch/accuracy_aware_training/__init__.py
 nncf/torch/accuracy_aware_training/runner.py
 nncf/torch/accuracy_aware_training/utils.py
+nncf/torch/automl/__init__.py
+nncf/torch/automl/agent/__init__.py
+nncf/torch/automl/agent/ddpg/__init__.py
+nncf/torch/automl/agent/ddpg/ddpg.py
+nncf/torch/automl/agent/ddpg/memory.py
+nncf/torch/automl/environment/__init__.py
+nncf/torch/automl/environment/quantization_env.py
 nncf/torch/binarization/__init__.py
 nncf/torch/binarization/algo.py
 nncf/torch/binarization/binarize_functions.py
 nncf/torch/binarization/extensions.py
 nncf/torch/binarization/layers.py
 nncf/torch/binarization/reference.py
 nncf/torch/dynamic_graph/__init__.py
 nncf/torch/dynamic_graph/context.py
 nncf/torch/dynamic_graph/graph.py
 nncf/torch/dynamic_graph/graph_tracer.py
 nncf/torch/dynamic_graph/io_handling.py
+nncf/torch/dynamic_graph/layer_attributes_handlers.py
 nncf/torch/dynamic_graph/op_input_processing.py
 nncf/torch/dynamic_graph/operation_address.py
 nncf/torch/dynamic_graph/patch_pytorch.py
 nncf/torch/dynamic_graph/scope.py
+nncf/torch/dynamic_graph/scope_access.py
+nncf/torch/dynamic_graph/structs.py
 nncf/torch/dynamic_graph/trace_functions.py
 nncf/torch/dynamic_graph/trace_tensor.py
-nncf/torch/dynamic_graph/transform_graph.py
 nncf/torch/dynamic_graph/wrappers.py
 nncf/torch/extensions/__init__.py
 nncf/torch/extensions/include/common_cpu_funcs.h
 nncf/torch/extensions/include/common_cuda_defs.cuh
 nncf/torch/extensions/include/common_cuda_funcs.cuh
 nncf/torch/extensions/include/common_defs.h
 nncf/torch/extensions/include/binarization/functions_cuda_impl.h
@@ -400,15 +468,14 @@
 nncf/torch/extensions/src/quantization/cuda/functions_cuda.cpp
 nncf/torch/extensions/src/quantization/cuda/functions_cuda_impl.cu
 nncf/torch/graph/__init__.py
 nncf/torch/graph/graph.py
 nncf/torch/graph/graph_builder.py
 nncf/torch/graph/operator_metatypes.py
 nncf/torch/graph/pattern_operations.py
-nncf/torch/graph/patterns.py
 nncf/torch/graph/transformations/__init__.py
 nncf/torch/graph/transformations/commands.py
 nncf/torch/graph/transformations/layout.py
 nncf/torch/hardware/__init__.py
 nncf/torch/hardware/config.py
 nncf/torch/hardware/fused_patterns.py
 nncf/torch/knowledge_distillation/__init__.py
@@ -431,24 +498,26 @@
 nncf/torch/pruning/filter_pruning/global_ranking/legr.py
 nncf/torch/quantization/__init__.py
 nncf/torch/quantization/adjust_padding.py
 nncf/torch/quantization/algo.py
 nncf/torch/quantization/default_quantization.py
 nncf/torch/quantization/extensions.py
 nncf/torch/quantization/hessian_trace.py
+nncf/torch/quantization/ignored_patterns.py
 nncf/torch/quantization/init_precision.py
 nncf/torch/quantization/init_range.py
 nncf/torch/quantization/layers.py
 nncf/torch/quantization/metrics.py
 nncf/torch/quantization/precision_constraints.py
-nncf/torch/quantization/quantize.py
 nncf/torch/quantization/quantize_functions.py
+nncf/torch/quantization/quantize_model.py
 nncf/torch/quantization/reference.py
 nncf/torch/quantization/schedulers.py
 nncf/torch/quantization/statistics.py
+nncf/torch/quantization/strip.py
 nncf/torch/quantization/structs.py
 nncf/torch/quantization/translator.py
 nncf/torch/quantization/precision_init/__init__.py
 nncf/torch/quantization/precision_init/adjacent_quantizers.py
 nncf/torch/quantization/precision_init/autoq_init.py
 nncf/torch/quantization/precision_init/base_init.py
 nncf/torch/quantization/precision_init/bitwidth_graph.py
@@ -469,12 +538,14 @@
 nncf/torch/sparsity/magnitude/algo.py
 nncf/torch/sparsity/magnitude/functions.py
 nncf/torch/sparsity/rb/__init__.py
 nncf/torch/sparsity/rb/algo.py
 nncf/torch/sparsity/rb/functions.py
 nncf/torch/sparsity/rb/layers.py
 nncf/torch/sparsity/rb/loss.py
+nncf/torch/statistics/__init__.py
+nncf/torch/statistics/aggregator.py
 nncf/torch/tensor_statistics/__init__.py
 nncf/torch/tensor_statistics/algo.py
 nncf/torch/tensor_statistics/collectors.py
 nncf/torch/tensor_statistics/reduction.py
 nncf/torch/tensor_statistics/statistics.py
```

### Comparing `nncf-2.4.0/setup.py` & `nncf-2.5.0/setup.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,17 @@
-"""
- Copyright (c) 2023 Intel Corporation
- Licensed under the Apache License, Version 2.0 (the "License");
- you may not use this file except in compliance with the License.
- You may obtain a copy of the License at
-      http://www.apache.org/licenses/LICENSE-2.0
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
-"""
+# Copyright (c) 2023 Intel Corporation
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#      http://www.apache.org/licenses/LICENSE-2.0
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 # *WARNING*: Do not run this file directly by `python setup.py install`
 # or with any other parameter - this is an outdated and error-prone way
 # to install Python packages and causes particular problems with namespace
 # packages such as `protobuf`.
 # Refer to the table below for well-known and supported alternatives with
 # the same behaviour:
@@ -30,31 +28,32 @@
 #
 # PyPA in general recommends to move away from setup.py and use pyproject.toml
 # instead. This doesn't fit us as we currently want to do custom stuff during
 # installation such as setting version based on the commit SHA for repo-based
 # installs.
 
 
+import codecs
 import glob
+import os
+import re
 import stat
 import sys
 import sysconfig
 
-import codecs
-import os
-import re
 import setuptools
-from setuptools import setup, find_packages
 from pkg_resources import parse_version
+from setuptools import find_packages
+from setuptools import setup
 
 here = os.path.abspath(os.path.dirname(__file__))
-BKC_SETUPTOOLS_VERSION = '59.5.0'
+BKC_SETUPTOOLS_VERSION = "59.5.0"
 
 setuptools_version = parse_version(setuptools.__version__).base_version
-if setuptools_version < '43.0.0':
+if setuptools_version < "43.0.0":
     raise RuntimeError(
         "To properly install NNCF, please install setuptools>=43.0.0, "
         f"while current setuptools version is {setuptools.__version__}. "
         f"Recommended version is {BKC_SETUPTOOLS_VERSION}."
     )
 
 python_version = sys.version_info
@@ -67,105 +66,105 @@
 is_installing_editable = "develop" in sys.argv
 is_building_release = not is_installing_editable and "--release" in sys.argv
 if "--release" in sys.argv:
     sys.argv.remove("--release")
 
 
 def read(*parts):
-    with codecs.open(os.path.join(here, *parts), 'r') as fp:
+    with codecs.open(os.path.join(here, *parts), "r") as fp:
         return fp.read()
 
 
 def find_version(*file_paths):
     version_file = read(*file_paths)
-    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]",
-                              version_file, re.M)
+    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]", version_file, re.M)
     if not version_match:
         raise RuntimeError("Unable to find version string.")
     version_value = version_match.group(1)
     if not is_building_release:
         if is_installing_editable:
             return version_value + ".dev0+editable"
         import subprocess  # nosec
+
         dev_version_id = "unknown_version"
         try:
             repo_root = os.path.dirname(os.path.realpath(__file__))
-            dev_version_id = subprocess.check_output(["git", "rev-parse", "--short", "HEAD"],   # nosec
-                                                     cwd=repo_root).strip().decode()
+            dev_version_id = (
+                subprocess.check_output(["git", "rev-parse", "--short", "HEAD"], cwd=repo_root)  # nosec
+                .strip()
+                .decode()
+            )
         except subprocess.CalledProcessError:
             pass
         return version_value + f".dev0+{dev_version_id}"
 
     return version_value
 
 
-INSTALL_REQUIRES = ["ninja>=1.10.0.post2, <1.11",
-                    "texttable>=1.6.3",
-                    "scipy>=1.3.2, <=1.10.0",
-                    "networkx>=2.6, <=2.8.2",  # see ticket 94048 or https://github.com/networkx/networkx/issues/5962
-                    "numpy>=1.19.1, <1.24",
-
-                    # The recent pyparsing major version update seems to break
-                    # integration with networkx - the graphs parsed from current .dot
-                    # reference files no longer match against the graphs produced in tests.
-                    # Using 2.x versions of pyparsing seems to fix the issue.
-                    # Ticket: 69520
-                    "pyparsing<3.0",
-                    "pymoo==0.5.0",
-                    "jsonschema>=3.2.0",
-                    "pydot>=1.4.1",
-                    "jstyleson>=0.0.2",
-                    "tqdm>=4.54.1",
-                    "natsort>=7.1.0",
-                    "pandas>=1.1.5,<=1.5.2",
-                    "scikit-learn>=0.24.0",
-                    "openvino-telemetry"]
+INSTALL_REQUIRES = [
+    "ninja>=1.10.0.post2, <1.11",
+    "texttable>=1.6.3",
+    "scipy>=1.3.2, <1.11",
+    "networkx>=2.6, <=2.8.2",  # see ticket 94048 or https://github.com/networkx/networkx/issues/5962
+    "numpy>=1.19.1, <1.24",
+    # The recent pyparsing major version update seems to break
+    # integration with networkx - the graphs parsed from current .dot
+    # reference files no longer match against the graphs produced in tests.
+    # Using 2.x versions of pyparsing seems to fix the issue.
+    # Ticket: 69520
+    "pyparsing<3.0",
+    "pymoo==0.5.0",
+    "jsonschema>=3.2.0",
+    "pydot>=1.4.1",
+    "jstyleson>=0.0.2",
+    "tqdm>=4.54.1",
+    "natsort>=7.1.0",
+    "pandas>=1.1.5,<2.1",
+    "scikit-learn>=0.24.0",
+    "openvino-telemetry",
+]
 
 
 TF_EXTRAS = [
-        "tensorflow~=2.8.4",
-    ]
+    "tensorflow~=2.11.1",
+    # The workaround of the protobuf issue and should be fixed with migration on TF 2.12
+    "tensorflow-metadata<=1.13.0",
+]
 
 TORCH_EXTRAS = [
-        "torch>=1.8.2,<1.14",
-    ]
+    "torch>=1.9.1,<1.14;python_version < '3.11'",
+]
+
+ONNX_EXTRAS = ["onnx~=1.13.1", "onnxruntime~=1.14.1;python_version < '3.11'"]
 
-ONNX_EXTRAS = [
-        "onnx==1.12.0",
-        "onnxruntime==1.13.1"
-    ]
-
-OPENVINO_EXTRAS = [
-        "openvino-dev"
-    ]
+OPENVINO_EXTRAS = ["openvino==2023.0.0"]
 
 
 EXTRAS_REQUIRE = {
-    "dev": ["matplotlib>=3.3.4, <3.6",
-            "pillow>=9.0.0"],
+    "dev": [
+        "kaleido>=0.2.1",
+        "matplotlib>=3.3.4, <3.6",
+        "pillow>=9.0.0",
+        "plotly-express>=0.4.1",
+    ],
     "tests": ["pytest"],
     "docs": [],
-
     "tf": TF_EXTRAS,
     "tensorflow": TF_EXTRAS,
     "tensorflow2": TF_EXTRAS,
-
     "torch": TORCH_EXTRAS,
     "pytorch": TORCH_EXTRAS,
-
     "onnx": ONNX_EXTRAS,
-
     "openvino": OPENVINO_EXTRAS,
-
     "all": [
         TF_EXTRAS,
         TORCH_EXTRAS,
         ONNX_EXTRAS,
         OPENVINO_EXTRAS,
-    ]
+    ],
 }
 
 with open("{}/README.md".format(here), "r", encoding="utf8") as fh:
     long_description = fh.read()
 
 setup(
     name="nncf",
@@ -173,30 +172,41 @@
     author="Intel",
     author_email="alexander.kozlov@intel.com",
     description="Neural Networks Compression Framework",
     long_description=long_description,
     long_description_content_type="text/markdown",
     url="https://github.com/openvinotoolkit/nncf",
     license="Apache-2.0",
-    packages=find_packages(exclude=["tests", "tests.*",
-                                    "examples", "examples.*",
-                                    "tools", "tools.*"]),
+    packages=find_packages(exclude=["tests", "tests.*", "examples", "examples.*", "tools", "tools.*"]),
     classifiers=[
         "Programming Language :: Python :: 3",
         "License :: OSI Approved :: Apache Software License",
         "Operating System :: OS Independent",
     ],
     install_requires=INSTALL_REQUIRES,
     extras_require=EXTRAS_REQUIRE,
-    keywords=["compression", "quantization", "sparsity", "mixed-precision-training",
-              "quantization-aware-training", "hawq", "classification",
-              "pruning", "object-detection", "semantic-segmentation", "nas", "nlp",
-              "bert", "transformers", "mmdetection"],
-    include_package_data=True
+    keywords=[
+        "compression",
+        "quantization",
+        "sparsity",
+        "mixed-precision-training",
+        "quantization-aware-training",
+        "hawq",
+        "classification",
+        "pruning",
+        "object-detection",
+        "semantic-segmentation",
+        "nas",
+        "nlp",
+        "bert",
+        "transformers",
+        "mmdetection",
+    ],
+    include_package_data=True,
 )
 
-path_to_ninja = glob.glob(str(sysconfig.get_paths()["purelib"]+"/ninja*/ninja/data/bin/"))
+path_to_ninja = glob.glob(str(sysconfig.get_paths()["purelib"] + "/ninja*/ninja/data/bin/"))
 if path_to_ninja:
-    path_to_ninja = str(path_to_ninja[0]+"ninja")
+    path_to_ninja = str(path_to_ninja[0] + "ninja")
     if not os.access(path_to_ninja, os.X_OK):
         st = os.stat(path_to_ninja)
         os.chmod(path_to_ninja, st.st_mode | stat.S_IEXEC)
```

