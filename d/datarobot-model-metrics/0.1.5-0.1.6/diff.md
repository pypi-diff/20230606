# Comparing `tmp/datarobot_model_metrics-0.1.5-py2.py3-none-any.whl.zip` & `tmp/datarobot_model_metrics-0.1.6-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,24 +1,24 @@
-Zip file size: 28095 bytes, number of entries: 22
--rw-r--r--  2.0 unx      385 b- defN 23-May-22 09:29 dmm/__init__.py
--rw-r--r--  2.0 unx     8663 b- defN 23-May-22 09:29 dmm/batch_metric_evaluator.py
--rw-r--r--  2.0 unx      687 b- defN 23-May-22 09:29 dmm/constants.py
--rw-r--r--  2.0 unx     6656 b- defN 23-May-22 09:29 dmm/example_data_helper.py
--rw-r--r--  2.0 unx    12824 b- defN 23-May-22 09:29 dmm/metric_evaluator.py
--rw-r--r--  2.0 unx     3164 b- defN 23-May-22 09:29 dmm/time_bucket.py
--rw-r--r--  2.0 unx     1690 b- defN 23-May-22 09:29 dmm/utils.py
--rw-r--r--  2.0 unx      525 b- defN 23-May-22 09:29 dmm/data_source/__init__.py
--rw-r--r--  2.0 unx     2096 b- defN 23-May-22 09:29 dmm/data_source/data_source_base.py
--rw-r--r--  2.0 unx     2698 b- defN 23-May-22 09:29 dmm/data_source/dataframe_source.py
--rw-r--r--  2.0 unx    55894 b- defN 23-May-22 09:29 dmm/data_source/datarobot_source.py
--rw-r--r--  2.0 unx     3003 b- defN 23-May-22 09:29 dmm/data_source/generator_source.py
--rw-r--r--  2.0 unx      396 b- defN 23-May-22 09:29 dmm/metric/__init__.py
--rw-r--r--  2.0 unx     1620 b- defN 23-May-22 09:29 dmm/metric/asymmetric_error.py
--rw-r--r--  2.0 unx      320 b- defN 23-May-22 09:29 dmm/metric/median_absolute_error.py
--rw-r--r--  2.0 unx     3569 b- defN 23-May-22 09:29 dmm/metric/metric_base.py
--rw-r--r--  2.0 unx      957 b- defN 23-May-22 09:29 dmm/metric/missing_values.py
--rw-r--r--  2.0 unx     2014 b- defN 23-May-22 09:29 dmm/metric/sklearn_metric.py
--rw-r--r--  2.0 unx      886 b- defN 23-May-22 09:30 datarobot_model_metrics-0.1.5.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-May-22 09:30 datarobot_model_metrics-0.1.5.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 23-May-22 09:30 datarobot_model_metrics-0.1.5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1857 b- defN 23-May-22 09:30 datarobot_model_metrics-0.1.5.dist-info/RECORD
-22 files, 110018 bytes uncompressed, 25069 bytes compressed:  77.2%
+Zip file size: 28951 bytes, number of entries: 22
+-rw-r--r--  2.0 unx      385 b- defN 23-Jun-06 10:49 dmm/__init__.py
+-rw-r--r--  2.0 unx     9663 b- defN 23-Jun-06 10:49 dmm/batch_metric_evaluator.py
+-rw-r--r--  2.0 unx      747 b- defN 23-Jun-06 10:49 dmm/constants.py
+-rw-r--r--  2.0 unx     6955 b- defN 23-Jun-06 10:49 dmm/example_data_helper.py
+-rw-r--r--  2.0 unx    13942 b- defN 23-Jun-06 10:49 dmm/metric_evaluator.py
+-rw-r--r--  2.0 unx     3164 b- defN 23-Jun-06 10:49 dmm/time_bucket.py
+-rw-r--r--  2.0 unx     1690 b- defN 23-Jun-06 10:49 dmm/utils.py
+-rw-r--r--  2.0 unx      525 b- defN 23-Jun-06 10:49 dmm/data_source/__init__.py
+-rw-r--r--  2.0 unx     2096 b- defN 23-Jun-06 10:49 dmm/data_source/data_source_base.py
+-rw-r--r--  2.0 unx     2698 b- defN 23-Jun-06 10:49 dmm/data_source/dataframe_source.py
+-rw-r--r--  2.0 unx    58576 b- defN 23-Jun-06 10:49 dmm/data_source/datarobot_source.py
+-rw-r--r--  2.0 unx     3003 b- defN 23-Jun-06 10:49 dmm/data_source/generator_source.py
+-rw-r--r--  2.0 unx      396 b- defN 23-Jun-06 10:49 dmm/metric/__init__.py
+-rw-r--r--  2.0 unx     1620 b- defN 23-Jun-06 10:49 dmm/metric/asymmetric_error.py
+-rw-r--r--  2.0 unx      320 b- defN 23-Jun-06 10:49 dmm/metric/median_absolute_error.py
+-rw-r--r--  2.0 unx     3569 b- defN 23-Jun-06 10:49 dmm/metric/metric_base.py
+-rw-r--r--  2.0 unx      957 b- defN 23-Jun-06 10:49 dmm/metric/missing_values.py
+-rw-r--r--  2.0 unx     2014 b- defN 23-Jun-06 10:49 dmm/metric/sklearn_metric.py
+-rw-r--r--  2.0 unx      886 b- defN 23-Jun-06 10:49 datarobot_model_metrics-0.1.6.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-Jun-06 10:49 datarobot_model_metrics-0.1.6.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 23-Jun-06 10:49 datarobot_model_metrics-0.1.6.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1857 b- defN 23-Jun-06 10:49 datarobot_model_metrics-0.1.6.dist-info/RECORD
+22 files, 115177 bytes uncompressed, 25925 bytes compressed:  77.5%
```

## zipnote {}

```diff
@@ -48,20 +48,20 @@
 
 Filename: dmm/metric/missing_values.py
 Comment: 
 
 Filename: dmm/metric/sklearn_metric.py
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.5.dist-info/METADATA
+Filename: datarobot_model_metrics-0.1.6.dist-info/METADATA
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.5.dist-info/WHEEL
+Filename: datarobot_model_metrics-0.1.6.dist-info/WHEEL
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.5.dist-info/top_level.txt
+Filename: datarobot_model_metrics-0.1.6.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.5.dist-info/RECORD
+Filename: datarobot_model_metrics-0.1.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dmm/batch_metric_evaluator.py

```diff
@@ -23,36 +23,46 @@
         metric: Union[str, MetricBase],
         source: BatchDataRobotSource,
         prediction_col: str = ColumnName.PREDICTIONS,
         batch_id_col: str = ColumnName.BATCH_ID_COLUMN,
         timestamp_col: str = ColumnName.TIMESTAMP,
         filter_predictions: bool = False,
         filter_scoring_data: bool = False,
+        segment_attribute: str = None,
+        segment_value: str = None,
     ):
         """
         Initialize the MetricEvaluator framework.
         :param metric: A single metric or a list of metrics. Metrics need to be based on ModelMetric class or a string
         representing a name of an SKLearn metric.
         :param source: A data source object that will be usd to get the data in chunks
         :param prediction_col: The name of the prediction column
         :param batch_id_col: The name of the batch ID column
         :param filter_predictions: whether the metric evaluator removes missing predictions values before scoring.
         :param filter_scoring_data: whether the metric evaluator removes missing scoring values before scoring.
+        :param segment_attribute The name of the column with segment values
+        :param segment_value The value of the segment attribute to segment on
         """
         super().__init__(metric=metric)
         self._data_source = source
         if self._data_source is None:
             raise Exception("Can not evaluate a metric without a data source")
 
         self._prediction_col = prediction_col
         self._batch_id_col = batch_id_col
         self._timestamp_col = timestamp_col
         self._groups_to_filter = self._get_groups_to_filter(
             filter_predictions, filter_scoring_data
         )
+        self._segment_attribute = segment_attribute
+        self._segment_value = segment_value
+        if self._segment_value and not self._segment_attribute:
+            raise Exception(
+                "Segment attribute must be specified when segment value is specified"
+            )
 
     def _run_score(self, chunk_of_data: pd.DataFrame) -> Dict[str, float]:
         scored_values = {}
         predictions = (
             chunk_of_data[self._prediction_col].to_numpy()
             if self._prediction_col in chunk_of_data
             else None
@@ -65,25 +75,37 @@
                 scoring_data=self._get_scoring_data(chunk_of_data)
                 if metric.need_scoring_data()
                 else None,
             )
             self._stats.nr_calls_to_score += 1
         return scored_values
 
-    def _get_data_chunk(self) -> Tuple[pd.DataFrame, int, bool]:
+    def _get_data_chunk(
+        self,
+    ) -> Tuple[Union[pd.DataFrame, None], int, bool]:
         done = False
         chunk_of_data, bucket_id = self._data_source.get_data()
 
         if chunk_of_data is None:
             done = True
-        else:
-            if len(chunk_of_data) > self._data_source.max_rows:
-                raise Exception("Chunk of data has too many rows")
-            self._stats.total_rows += len(chunk_of_data)
+            return chunk_of_data, bucket_id, done
+
+        if len(chunk_of_data) > self._data_source.max_rows:
+            raise Exception("Chunk of data has too many rows")
+
+        if self._segment_attribute:
+            if self._segment_attribute not in chunk_of_data:
+                raise Exception(
+                    f"Segment attribute: {self._segment_attribute} not found in exported data"
+                )
+            chunk_of_data = chunk_of_data.loc[
+                chunk_of_data[self._segment_attribute] == self._segment_value
+            ]
 
+        self._stats.total_rows += len(chunk_of_data)
         return chunk_of_data, bucket_id, done
 
     def _validate_data_chunk(self, data_chunk: pd.DataFrame) -> pd.DataFrame:
         for metric_name, metric in self._metrics.items():
             if metric.need_predictions() and self._prediction_col not in data_chunk:
                 raise Exception(
                     f"Metric {metric_name} requires predictions, but data chunk is missing column "
@@ -176,16 +198,17 @@
 
             # Moved to a new time bucket so calling reduce on the previous values
             if bucket_id != prev_bucket_id:
                 # We don't reduce in the first time we get data (as prev is None and id is not None)
                 if prev_bucket_id is not None:
                     # Moved to a new time bucket (or done).. need to reduce
                     reduced_values = self._reduce_bucket_metric(bucket_scores)
-                    output_nr_samples_list.append(bucket_metric_nr_samples)
-                    output_bucket_batch_list.append(bucket_batch_id)
+                    if reduced_values:
+                        output_nr_samples_list.append(bucket_metric_nr_samples)
+                        output_bucket_batch_list.append(bucket_batch_id)
                     for metric, value in reduced_values.items():
                         final_scores[metric].append(value)
                         bucket_scores[metric].clear()
                 bucket_metric_nr_samples = 0
                 prev_bucket_id = bucket_id
 
             # Running the score and keeping the value until it is time to reduce
```

## dmm/constants.py

```diff
@@ -6,14 +6,16 @@
     NR_SAMPLES = "samples"
     METRIC_VALUE = "value"
     DR_TIMESTAMP_COLUMN = "DR_RESERVED_PREDICTION_TIMESTAMP"
     DR_PREDICTION_COLUMN = "DR_RESERVED_PREDICTION_VALUE"
     DR_BATCH_ID_COLUMN = "DR_RESERVED_BATCH_ID"
     ASSOCIATION_ID_COLUMN = "association_id"
     BATCH_ID_COLUMN = "batch_id"
+    LABEL = "label"
+    PREDICTED_CLASS = "predicted_class"
 
 
 class TimeBucket:
     SECOND = "second"
     MINUTE = "minute"
     HOUR = "hour"
     DAY = "day"
```

## dmm/example_data_helper.py

```diff
@@ -49,14 +49,16 @@
     prediction_col: ColumnName = ColumnName.PREDICTIONS,
     with_actuals: bool = True,
     actuals_col: ColumnName = ColumnName.ACTUALS,
     with_dr_timestamp_column: bool = False,
     dr_timestamp_column: ColumnName = ColumnName.DR_TIMESTAMP_COLUMN,
     with_association_id: bool = False,
     association_id_col: ColumnName = ColumnName.ASSOCIATION_ID_COLUMN,
+    positive_label: str = None,
+    label_column: ColumnName = ColumnName.LABEL,
     prediction_value: int = None,
     random_predictions: bool = False,
     time_bucket: TimeBucket = TimeBucket.MINUTE,
     rows_per_time_bucket: int = 100,
     prediction_actual_diff: float = 0.001,
 ) -> pd.DataFrame:
     """
@@ -66,14 +68,16 @@
     :param prediction_col: Name of prediction column
     :param prediction_value: A fixed value to use for predictions
     :param random_predictions: If True generate random predictions instead of a fixed value
     :param with_actuals: Add actuals column to the data
     :param actuals_col: Name of actuals column
     :param with_association_id: Add association id column to the data
     :param association_id_col: Name of association id column
+    :param positive_label: If specified, will add a column with a positive label
+    :param label_column: Name of label column
     :param with_dr_timestamp_column: Add predictions timestamp column to the data
     :param dr_timestamp_column: Name of predictions timestamp column
     :param time_bucket: Time bucket to generate predictions for
     :param rows_per_time_bucket: How many rows per time bucket to generate
     :param prediction_actual_diff: diff between predictions and actuals
     :param timestamp_col: Name of timestamp column
     :return: Dataframe with the generated data
@@ -93,14 +97,16 @@
         df[prediction_col] = np.array(predictions)
     if with_actuals:
         df[actuals_col] = [x - prediction_actual_diff for x in predictions]
     if with_association_id:
         df[association_id_col] = [x for x in range(nr_rows)]
     if with_dr_timestamp_column:
         df[dr_timestamp_column] = df[timestamp_col]
+    if positive_label:
+        df[label_column] = [positive_label for _ in range(nr_rows)]
 
     return df
 
 
 def gen_dataframe_for_batch_tests(
     nr_rows: int = 1000,
     batch_id_col: ColumnName = ColumnName.DR_BATCH_ID_COLUMN,
```

## dmm/metric_evaluator.py

```diff
@@ -79,14 +79,18 @@
     ) -> Dict[str, float]:
         """
         :param bucket_metric_parts: Bucket to list of values mapping to be reduced.
         :return: Dict of metric name: reduced value
         """
         reduced_values = {}
         for metric_name, metric in self._metrics.items():
+            # skip reduce if no values to reduce
+            if not bucket_metric_parts[metric_name]:
+                continue
+
             reduced_values[metric_name] = metric.reduce_func()(
                 bucket_metric_parts[metric_name]
             )
             self._stats.nr_calls_to_reduce += 1
         return reduced_values
 
 
@@ -103,27 +107,31 @@
         time_bucket: TimeBucket,
         prediction_col: str = ColumnName.PREDICTIONS,
         actuals_col: str = ColumnName.ACTUALS,
         timestamp_col: str = ColumnName.TIMESTAMP,
         filter_actuals: bool = False,
         filter_predictions: bool = False,
         filter_scoring_data: bool = False,
+        segment_attribute: str = None,
+        segment_value: str = None,
     ):
         """
         Initialize the MetricEvaluator framework.
         :param metric: A single metric or a list of metrics. Metrics need to be based on ModelMetric class or a string
         representing a name of an SKLearn metric.
         :param source: A data source object that will be usd to get the data in chunks
         :param time_bucket: The time bucket size to use for evaluating metrics
         :param prediction_col: The name of the prediction column
         :param actuals_col: The name of the actuals column
         :param timestamp_col: The name of the timestamp column
         :param filter_actuals: whether the metric evaluator removes missing actuals values before scoring.
         :param filter_predictions: whether the metric evaluator removes missing predictions values before scoring.
         :param filter_scoring_data: whether the metric evaluator removes missing scoring values before scoring.
+        :param segment_attribute The name of the column with segment values
+        :param segment_value The value of the segment attribute to segment on
         """
         super().__init__(metric)
         self._metrics = {
             metric.name: metric for metric in self._validate_and_convert_metrics(metric)
         }
         self._data_source = source
         if self._data_source is None:
@@ -132,14 +140,20 @@
         self._time_bucket = time_bucket
         self._prediction_col = prediction_col
         self._actuals_col = actuals_col
         self._timestamp_col = timestamp_col
         self._groups_to_filter = self._get_groups_to_filter(
             filter_actuals, filter_predictions, filter_scoring_data
         )
+        self._segment_attribute = segment_attribute
+        self._segment_value = segment_value
+        if self._segment_value and not self._segment_attribute:
+            raise Exception(
+                "Segment attribute must be specified when segment value is specified"
+            )
 
     def _run_score(self, chunk_of_data: pd.DataFrame) -> Dict[str, float]:
         scored_values = {}
         predictions = (
             chunk_of_data[self._prediction_col].to_numpy()
             if self._prediction_col in chunk_of_data
             else None
@@ -157,25 +171,35 @@
                 scoring_data=self._get_scoring_data(chunk_of_data)
                 if metric.need_scoring_data()
                 else None,
             )
             self._stats.nr_calls_to_score += 1
         return scored_values
 
-    def _get_data_chunk(self) -> Tuple[pd.DataFrame, int, bool]:
+    def _get_data_chunk(self) -> Tuple[Union[pd.DataFrame, None], int, bool]:
         done = False
         chunk_of_data, time_bucket_id = self._data_source.get_data()
 
         if chunk_of_data is None:
             done = True
-        else:
-            if len(chunk_of_data) > self._data_source.max_rows:
-                raise Exception("Chunk of data has too many rows")
-            self._stats.total_rows += len(chunk_of_data)
+            return chunk_of_data, time_bucket_id, done
+
+        if len(chunk_of_data) > self._data_source.max_rows:
+            raise Exception("Chunk of data has too many rows")
+
+        if self._segment_attribute:
+            if self._segment_attribute not in chunk_of_data:
+                raise Exception(
+                    f"Segment attribute: {self._segment_attribute} not found in exported data"
+                )
+            chunk_of_data = chunk_of_data.loc[
+                chunk_of_data[self._segment_attribute] == self._segment_value
+            ]
 
+        self._stats.total_rows += len(chunk_of_data)
         return chunk_of_data, time_bucket_id, done
 
     def _validate_data_chunk(self, data_chunk: pd.DataFrame) -> pd.DataFrame:
         for metric_name, metric in self._metrics.items():
             if metric.need_predictions() and self._prediction_col not in data_chunk:
                 raise Exception(
                     f"Metric {metric_name} requires predictions, but data chunk is missing column "
@@ -284,16 +308,17 @@
 
             # Moved to a new time bucket so calling reduce on the previous values
             if time_bucket_id != prev_time_bucket_id:
                 # We don't reduce in the first time we get data (as prev is None and id is not None)
                 if prev_time_bucket_id is not None:
                     # Moved to a new time bucket (or done).. need to reduce
                     reduced_values = self._reduce_bucket_metric(time_bucket_scores)
-                    output_nr_samples_list.append(time_bucket_metric_nr_samples)
-                    output_bucket_timestamp_list.append(time_bucket_timestamp)
+                    if reduced_values:
+                        output_nr_samples_list.append(time_bucket_metric_nr_samples)
+                        output_bucket_timestamp_list.append(time_bucket_timestamp)
                     for metric, value in reduced_values.items():
                         final_scores[metric].append(value)
                         time_bucket_scores[metric].clear()
                 time_bucket_metric_nr_samples = 0
                 prev_time_bucket_id = time_bucket_id
 
             # Running the score and keeping the value until it is time to reduce
```

## dmm/data_source/datarobot_source.py

```diff
@@ -295,18 +295,34 @@
         association_id = self._api.get_association_id(self._deployment_id)
         if association_id:
             rename_columns.update({association_id: ColumnName.ASSOCIATION_ID_COLUMN})
 
         if self._deployment.type == DeploymentType.REGRESSION:
             prediction_col = ColumnName.DR_PREDICTION_COLUMN
         elif self._deployment.type == DeploymentType.BINARY_CLASSIFICATION:
-            prediction_col = f"{ColumnName.DR_PREDICTION_COLUMN}_{self._deployment.positive_class_label}"
+            positive_class_column = f"{ColumnName.DR_PREDICTION_COLUMN}_{self._deployment.positive_class_label}"
+            negative_class_column = f"{ColumnName.DR_PREDICTION_COLUMN}_{self._deployment.negative_class_label}"
+
+            # drop predictions from negative class
+            pred_df = pred_df.drop(columns=[negative_class_column], axis=1)
+
+            # get predictions from positive class
+            prediction_col = positive_class_column
+
             # apply prediction threshold
-            pred_df[prediction_col] = (
-                pred_df[prediction_col] >= self._deployment.prediction_threshold
+            pred_df[ColumnName.PREDICTED_CLASS] = (
+                pred_df[positive_class_column] >= self._deployment.prediction_threshold
+            )
+            pred_df[ColumnName.PREDICTED_CLASS] = pred_df[
+                ColumnName.PREDICTED_CLASS
+            ].map(
+                {
+                    True: self._deployment.positive_class_label,
+                    False: self._deployment.negative_class_label,
+                }
             )
         else:
             raise Exception(f"Unsupported deployment type {self._deployment.type}")
 
         rename_columns.update({prediction_col: ColumnName.PREDICTIONS})
         pred_df = pred_df.rename(rename_columns, axis=1)
         return pred_df
@@ -316,22 +332,19 @@
         Formats data from actuals data export, before merging with prediction data.
         Drops columns to avoid duplication
 
         Parameters
         ----------
         actuals_df: pd.DataFrame
         """
+        columns_to_drop = [ColumnName.PREDICTIONS, ColumnName.TIMESTAMP]
+        # for binary classification, drop the predicted class to avoid columns with the same content
         if self._deployment.type == DeploymentType.BINARY_CLASSIFICATION:
-            actuals_df = actuals_df[
-                actuals_df.label.astype("str") == self._deployment.positive_class_label
-            ]
-
-        actuals_df = actuals_df.drop(
-            [ColumnName.PREDICTIONS, ColumnName.TIMESTAMP], axis=1
-        )
+            columns_to_drop.append(ColumnName.PREDICTED_CLASS)
+        actuals_df = actuals_df.drop(columns_to_drop, axis=1)
         return actuals_df
 
     def _remove_unwanted_actuals_caches(self, start_dt: datetime.datetime) -> None:
         """
         Removes actuals from caches, that are no longer needed.
 
         Parameters
@@ -493,18 +506,34 @@
             ColumnName.DR_TIMESTAMP_COLUMN: ColumnName.TIMESTAMP,
             ColumnName.DR_BATCH_ID_COLUMN: ColumnName.BATCH_ID_COLUMN,
         }
 
         if self._deployment.type == DeploymentType.REGRESSION:
             prediction_col = ColumnName.DR_PREDICTION_COLUMN
         elif self._deployment.type == DeploymentType.BINARY_CLASSIFICATION:
-            prediction_col = f"{ColumnName.DR_PREDICTION_COLUMN}_{self._deployment.positive_class_label}"
+            positive_class_column = f"{ColumnName.DR_PREDICTION_COLUMN}_{self._deployment.positive_class_label}"
+            negative_class_column = f"{ColumnName.DR_PREDICTION_COLUMN}_{self._deployment.negative_class_label}"
+
+            # drop predictions from negative class
+            pred_df = pred_df.drop(columns=[negative_class_column], axis=1)
+
+            # get predictions from positive class
+            prediction_col = positive_class_column
+
             # apply prediction threshold
-            pred_df[prediction_col] = (
-                pred_df[prediction_col] >= self._deployment.prediction_threshold
+            pred_df[ColumnName.PREDICTED_CLASS] = (
+                pred_df[positive_class_column] >= self._deployment.prediction_threshold
+            )
+            pred_df[ColumnName.PREDICTED_CLASS] = pred_df[
+                ColumnName.PREDICTED_CLASS
+            ].map(
+                {
+                    True: self._deployment.positive_class_label,
+                    False: self._deployment.negative_class_label,
+                }
             )
         else:
             raise Exception(f"Unsupported deployment type {self._deployment.type}")
 
         rename_columns.update({prediction_col: ColumnName.PREDICTIONS})
         pred_df = pred_df.rename(rename_columns, axis=1)
         return pred_df
@@ -846,28 +875,49 @@
 
         if self._time_bucket_chunks.finished():
             return None, -1
 
         chunk_df, chunk_ts = self._time_bucket_chunks.load_time_bucket_chunk(
             time_bucket=self._time_bucket, max_rows=self._max_rows
         )
+        if self._deployment.type == DeploymentType.BINARY_CLASSIFICATION:
+            chunk_df = self._drop_negative_class(chunk_df)
+            self._add_column_with_predicted_class(chunk_df)
 
         self._update_chunk_info(chunk_ts)
         return chunk_df, self._current_chunk_id
 
     @staticmethod
     def get_supported_time_buckets():
         return [
             TimeBucket.HOUR,
             TimeBucket.DAY,
             TimeBucket.WEEK,
             TimeBucket.MONTH,
             TimeBucket.ALL,
         ]
 
+    def _drop_negative_class(self, actuals_df: pd.DataFrame) -> pd.DataFrame:
+        return actuals_df[
+            actuals_df.label.astype("str") == self._deployment.positive_class_label
+        ]
+
+    def _add_column_with_predicted_class(self, actuals_df: pd.DataFrame) -> None:
+        actuals_df[ColumnName.PREDICTED_CLASS] = (
+            actuals_df[ColumnName.PREDICTIONS] >= self._deployment.prediction_threshold
+        )
+        actuals_df[ColumnName.PREDICTED_CLASS] = actuals_df[
+            ColumnName.PREDICTED_CLASS
+        ].map(
+            {
+                True: self._deployment.positive_class_label,
+                False: self._deployment.negative_class_label,
+            }
+        )
+
 
 class TrainingDataExportProvider:
     def __init__(
         self,
         api: DataRobotApiClient,
         deployment_id: str,
         model_id: str = None,
@@ -967,14 +1017,23 @@
             raise Exception(
                 "Positive class label can only be retrieved for binary classification deployments"
             )
         # according to API spec - positive class comes first
         return self._model_package["target"]["classNames"][0]
 
     @property
+    def negative_class_label(self) -> str:
+        if self.type != DeploymentType.BINARY_CLASSIFICATION:
+            raise Exception(
+                "Negative class label can only be retrieved for binary classification deployments"
+            )
+        # according to API spec - negative class is second
+        return self._model_package["target"]["classNames"][1]
+
+    @property
     def prediction_threshold(self) -> float:
         if self.type != DeploymentType.BINARY_CLASSIFICATION:
             raise Exception(
                 "Prediction threshold can only be retrieved for binary classification deployments"
             )
         return self._model_package["target"]["predictionThreshold"]
 
@@ -1491,15 +1550,15 @@
         """
         Returns a subframe with records that belong to the bucket defined by `chunk_ts` + `time_bucket` arguments.
         It respects `max_rows` constraint - thus some records from that bucket might end up in `others` frame.
         The remaining records are present in the latter data frame.
         Assumes that the rows in `df` are sorted by timestamp. `same_bucket` frame is always a prefix of `df`.
         """
         filter_condition = check_if_in_same_time_bucket_vectorized(
-            pd.to_datetime(df[self._timestamp_column]), chunk_ts, time_bucket
+            df[self._timestamp_column].apply(lambda x: parse(x)), chunk_ts, time_bucket
         )
         same_bucket = df[filter_condition].iloc[:max_rows]
         others = df.iloc[len(same_bucket) :]
         return same_bucket, others
 
     def _validate_row(self, row: pd.Series) -> None:
         if self._timestamp_column not in row:
```

## Comparing `datarobot_model_metrics-0.1.5.dist-info/METADATA` & `datarobot_model_metrics-0.1.6.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobot-model-metrics
-Version: 0.1.5
+Version: 0.1.6
 Summary: datarobot-model-metrics is a framework to compute model ML metrics
 Home-page: https://github.com/datarobot/datarobot-model-metrics
 Author: DataRobot
 Author-email: info@datarobot.com
 License: DataRobot
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

## Comparing `datarobot_model_metrics-0.1.5.dist-info/RECORD` & `datarobot_model_metrics-0.1.6.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 dmm/__init__.py,sha256=w3zRSdtt2_upBZCJNj93DqZeXKPyfMKqo3Xo1yOHLeY,385
-dmm/batch_metric_evaluator.py,sha256=84khhvGDGyDYi_U2YhqB_vv97b3VZ9M_nb7imM_lc_Q,8663
-dmm/constants.py,sha256=u8Vcm7TEthcgDzvGjf-s9KagpvQXgjMiasxkNHl67qc,687
-dmm/example_data_helper.py,sha256=FOQidtHYsDj6zgDLBMhSDtpgC1869r17MRlsTv_v6dE,6656
-dmm/metric_evaluator.py,sha256=L-0j6JQrDmMWWvSb1MeAvzq_yR-LPBK7jl1rxALo-HA,12824
+dmm/batch_metric_evaluator.py,sha256=LmZkvxYeF8Dk98x0PWhbyWCGSqAO4cBeWAMzhyWvo50,9663
+dmm/constants.py,sha256=sNtJziqxxioScZUK_YlF08__C3zjxk16YYAOhRjAd6Y,747
+dmm/example_data_helper.py,sha256=oLcOTo-dNV0Pyapq9G6e7DZoVjfIZMWj7QrkBx61Mj4,6955
+dmm/metric_evaluator.py,sha256=3HK5k8ynCLpF5UsqD6AaFjiksbkSfWQFvHw2ei52-7E,13942
 dmm/time_bucket.py,sha256=cg42R1U_o7is6nZf5qUuvdc-2HBH9UKuACKjKe3dLto,3164
 dmm/utils.py,sha256=kaZ7erHZofSwqZL1ddd8GXK3wrZfvYZ7I0ziz2CgbL0,1690
 dmm/data_source/__init__.py,sha256=rsK8s1quedmNYchnUodsc4gUXAENM-N_rVVohhX9HMA,525
 dmm/data_source/data_source_base.py,sha256=5zJ9SZYs1ufbku11GYa4PQRY1lwOOsXbx8kDkiG1VMk,2096
 dmm/data_source/dataframe_source.py,sha256=mKEGXyNFVAsqiocAVKQlhbGlUlwjx77sl5KAlXT6nx4,2698
-dmm/data_source/datarobot_source.py,sha256=lXojCRMRTI7Irz2o51YtfET6OpWSUIO3BT_xrTubqTg,55894
+dmm/data_source/datarobot_source.py,sha256=YgOtXH5uL0i_9APSDzUBJd_7HP5mQJ4WA9uDHosRsq4,58576
 dmm/data_source/generator_source.py,sha256=O-GsreX8mQognj3u6LQxHkZ3yWj3sOgx_dojvObVdlI,3003
 dmm/metric/__init__.py,sha256=nIX41kZJKfQztTw93CXVLRbgTt5W1qhdK1uakWxL9CE,396
 dmm/metric/asymmetric_error.py,sha256=h4J7nzXw9BurtGrkoe4oe7QBI2l69XeB9CiGfpcKUxc,1620
 dmm/metric/median_absolute_error.py,sha256=qV0NpJdF6daTzJOaGpd8KAPG3gZWPS43FZxOb-rR0kE,320
 dmm/metric/metric_base.py,sha256=Rzsju5eZhoyafxo7ErFmO76sDcsvkb3SyODDwcNudy4,3569
 dmm/metric/missing_values.py,sha256=i9ujXCuOWEPrUteFXTCDGX6SM8RVd7cQoI6byQPga4E,957
 dmm/metric/sklearn_metric.py,sha256=Bv4ukOSZyOKjXK-_4b7KKmlkOVrYnwSX5GST_Hf-qpc,2014
-datarobot_model_metrics-0.1.5.dist-info/METADATA,sha256=9tahHkZW9QGhR3SFMQx94N8ba-HeHnKs4mb1IqIPq7U,886
-datarobot_model_metrics-0.1.5.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-datarobot_model_metrics-0.1.5.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
-datarobot_model_metrics-0.1.5.dist-info/RECORD,,
+datarobot_model_metrics-0.1.6.dist-info/METADATA,sha256=9fHrKIBsrvlVuxDRF8jsz-CEyVURUj8lnAd2KEgjRW0,886
+datarobot_model_metrics-0.1.6.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+datarobot_model_metrics-0.1.6.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
+datarobot_model_metrics-0.1.6.dist-info/RECORD,,
```

